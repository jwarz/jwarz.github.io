---
title: "Modeling with Data in the Tidyverse"
author: "Joschka Schwarz"
---

```{r}
#| include: false
source(here::here("R/setup-ggplot2-tie.R"))
options(dplyr.summarise.inform = FALSE)
```

In this course, you will learn to model with data. Models attempt to capture the relationship between an outcome variable of interest and a series of explanatory/predictor variables. Such models can be used for both explanatory purposes, e.g. "Does knowing professors' ages help explain their teaching evaluation scores?", and predictive purposes, e.g., "How well can we predict a house's price based on its size and condition?" You will leverage your tidyverse skills to construct and interpret such models. This course centers around the use of linear regression, one of the most commonly-used and easy to understand approaches to modeling. Such modeling and thinking is used in a wide variety of fields, including statistics,  causal inference, machine learning, and artificial intelligence.

# 1. Introduction to Modeling

This chapter will introduce you to some background theory and terminology for modeling, in particular, the general modeling framework, the difference between modeling for explanation and modeling for prediction, and the modeling problem. Furthermore, you'll start performing your first exploratory data analysis, a crucial first step before any formal modeling.

## Background on modeling for explanation

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Background on modeling for explanation**

Hello, welcome to the next course in DataCamp's "Learn the tidyverse" track: "Modeling with data in the tidyverse". In this course, you'll leverage the data wrangling and visualization toolbox you developed in previous courses to learn about modeling. The ideas behind modeling are crucial to many fields, including statistics, causal inference, machine learning, and artificial intelligence.

**2. Course overview**

You'll start by equipping yourself with some theory and terminology related to modeling.In Chapters 2+3, you'll learn one of the most widely used techniques for modeling: linear regression.You'll end by assessing the quality of models. For example, how well does a model fit given data? OR how good are a model's predictions?

**3. General modeling framework formula**

Let's start with the general modeling framework as expressed by following formula where you have: y, an outcome variable, the phenomenon you wish to model. x, a set of explanatory or predictor variables used to inform your model. The arrow on the x indicates that x can be a vector, in other words a series of values. f, a mathematical function making explicit the relationship between y and x. f(x) is also called the "signal". And finally epsilon, an unsystematic error component. epsilon is also called the noise.Let's first focus only on y and x, and revisit f and epsilon later.

**4. Two modeling scenarios**

Previously I called x both explanatory and predictor variables. Which term you use when roughly depends on which modeling scenario you're addressing:-If you want to explain what factors are associated with or cause the outcome variable, you are "modeling for explanation" and thus x are "explanatory" variables.-If you want to make predictions of the outcome variable, you are "modeling for prediction" and thus x are "predictor" variables.Let's start with an example of modeling for explanation.

**5. Modeling for explanation example**

At the end of academic terms at many universities and colleges, instructors are given teaching evaluation scores by students. A study conducted at the University of Texas Austin investigated whether differences in scores could be explained by differences in instructor attributes.The outcome variable is average teaching score for different courses. Explanatory variables include:-rank-gender, which at the time of this study was recorded as a binary variable: male or female-age-And even the instructor's "beauty score" bty_avg, we'll talk more about that later.

**6. Modeling for explanation example**

The evals dataframe included in the moderndive package contains this data. The moderndive package is used in ModernDive.com, an open-source written and published electronic textbook on statistical and data sciences that Chester Ismay of DataCamp and I have co-authored. This package includes other data and functions you'll be using in this course.Let's preview the data using the glimpse function from the dplyr package. Observe that there are 463 instructors and 13 variables in this data frame.

**7. Exploratory data analysis**

A crucial first step is an exploratory data analysis, or EDA. EDA gives you a sense of your data and it can help inform model construction. -There are three basic steps to an EDA:-Most fundamentally, looking at the data, via a spreadsheet viewer or using glimpse as I did earlier.-Creating visualizations.-Computing summary statistics.Let's do this for the outcome variable score.

**8. Exploratory data analysis**

Since score is numerical, let's construct a histogram to visualize its distribution by using a geom-histogram from the ggplot2 package, where the x-aesthetic is mapped to score. Let's also set a binwidth of 0.25.

**9. Exploratory data analysis**

Observe... the largest score is 5 and most scores are between about 3-5. But what's the average? Let's perform the third step in our EDA, computing summary statistics.

**10. Exploratory data analysis**

Summary statistics summarize many values with a single value called a statistic. Let's compute three such summary statistics using the summarize() function.The mean, or average, score is 4.17, whereas the median of 4.3 indicates about half the instructors had scores below 4.3 and about half above. The standard deviation, a measure of spread and variation, is 0.544.

**11. Let's practice!**

In our first exercise, you'll be performing an EDA on a different numerical variable, this time instructor age.

## Exploratory visualization of age

Let's perform an exploratory data analysis (EDA) of the numerical explanatory variable `age`. You should always perform an exploratory analysis of your variables before any formal modeling. This will give you a sense of your variable's distributions, any outliers, and any patterns that might be useful when constructing your eventual model.

**Steps**

1. Using the `ggplot2` package, create a histogram of `age` with bins in 5 year increments.
2. Label the `x` axis with `"age"` and the `y` axis with `"count"`.

```{r}
# Load packages
library(moderndive)
library(ggplot2)

# Plot the histogram
ggplot(evals, aes(x = age)) +
  geom_histogram(binwidth = 5) +
  labs(x = "age", y = "count")
```

Nice! The 463 instructors' ages are centered roughly at age 50. You'll now compute notions of center numerically!

## Numerical summaries of age

Let's continue our exploratory data analysis of the numerical explanatory variable `age` by computing **summary statistics**. Summary statistics take many values and summarize them with a single value. Let's compute three such values using `dplyr` data wrangling: mean (AKA the average), the median (the middle value), and the standard deviation (a measure of spread/variation).

**Steps**

1. Calculate the mean, median, and standard deviation of `age`.


```{r}
# Load packages
library(moderndive)
library(dplyr)

# Compute summary stats
evals %>%
  summarize(mean_age = mean(age),
            median_age = median(age),
            sd_age = sd(age))
```

Great! As suggested in the previous histogram for age, the center of the distribution as quantified by the mean and median is around 48 years old!

## Background on modeling for prediction

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Background on modeling for prediction**

Previously, you were introduced to an example of modeling for explanation: understanding what factors might explain teacher evaluation scores as given by students at the University of Texas Austin. Let's now consider an example of modeling for prediction.

**2. Modeling for prediction example**

The dataset I'll use is the "House Sales in King County USA" available at Kaggle.com. It consists of homes sold near Seattle Washington in 2014 and 2015.I'll predict the sale price of houses based on their features such as: size as measured by sqfeet of living space, where 1 sqft is approximately 1/10th of a sqmeter, condition, number of bedrooms, year built, and whether it had a view of the waterfront.

**3. Modeling for prediction example**

Just as with the evals dataset, I've included this data in the moderndive package, which I preview using the glimpse() function. Observe. There are 21k rows representing houses and 21 variables…As before, let's perform an exploratory data analysis, or EDA, of the outcome variable, price. Recall the three approaches to EDA: looking at the data, visualizations, and summary statistics. Since I've just done the first approach, let's now visualize our data.

**4. Exploratory data analysis**

Just as with the outcome variable score from our explanatory modeling example, let's get a sense of the distribution of our new numerical outcome variable, price, using a histogram.

**5. Histogram of outcome variable**

First, let's look at the x-axis tick marks. Since e+06 means 10^6, or one million, we see that a majority of houses are less than 2 million dollars.But why does the x-axis stretch so far to the right? Its because there are a very small number of houses with price closer to 8 million. You say that the variable price is "right skewed" as exhibited by the long right tail. This skew makes it difficult to compare prices of the less expensive houses.Recall that you saw something similar in the intro to the tidyverse course when visualizing the variable country population from the gapminder dataset.

**6. Gapminder data**

You visualized the relationship between countries' life expectancies and populations using a scatterplot similar to this one. Because the populations of the two green dots corresponding to India and China were so large, it was hard to study the relationship for less populated countries. To remedy this, you re-scaled the x-axis to be on a log10-scale.

**7. Log10 rescaling of x-axis**

Now you can better distinguish points for countries with smaller populations. Furthermore, horizontal intervals on the x-axis now correspond to multiplicative differences instead of additive ones. For example, distances between successive vertical white gridlines correspond to multiplicative increases by a factor of 10.

**8. Log10 transformation**

Now I'll do something similar, where I log10-transform price using the mutate function to create a new variable log10-price.Let's view the effects of this transformation on these two variables.Observe in particular the house in the 6th row with price 1.225 million. Since 10^6 is one million, its log10_price is 6.09. Contrast this with all other houses with log10-price less than 6.I'll treat log10-price as our new outcome variable. I can do this since log-transformations are monotonic, meaning they preserve orderings, so if house A's price is lower than house B's, then house A's log10-price will also be lower than house B's log10-price.

**9. Histogram of new outcome variable**

Let's take the earlier code to plot our histogram of the ORIGINAL outcome variable, copy, paste, and tweak the code to plot a histogram of the NEW log10-transformed outcome variable.

**10. Comparing before and after log10-transformation**

Observe that after the transformation, the distribution is much less skewed, and in this case, more symmetric and bell-shaped, although this isn't always necessarily the case. You can now better discriminate between houses at the lower end of the price scale.

**11. Let's practice!**

Now that you've seen that a log10-transformation was warranted for the outcome variable price, let's see if the predictor variable size as measured by the square feet of living space warrants a similar transformation.

## Exploratory visualization of house size

Let's create an exploratory visualization of the predictor variable reflecting the size of houses: `sqft_living` the square footage of living space where 1 sq.foot ≈ 0.1 sq.meter. 

After plotting the histogram, what can you say about the distribution of the variable `sqft_living`?

**Steps**

1. Create a histogram of `sqft_living`.
2. Label the `x` axis with `"Size (sq.feet)"` and the `y` axis with `"count"`.

```{r}
# Load packages
library(moderndive)
library(ggplot2)

# Plot the histogram
ggplot(house_prices, aes(x = sqft_living)) +
  geom_histogram() +
  labs(x = "Size (sq.feet)", y = "count")
```

> *Question*
> ---
> <br>
> <br>
> ⬜ There is no skew.<br>
> ✅ `sqft_living` is right-skewed.<br>
> ⬜ `sqft_living` is left-skewed.<br>

Correct! A variable is right-skewed if it has a long tail to the right.

## Log10 transformation of house size

You just saw that the predictor variable `sqft_living` is *right*-skewed and hence a log base 10 transformation is warranted to unskew it. Just as we transformed the outcome variable `price` to create `log10_price` in the video, let's do the same for `sqft_living`.

**Steps**

1. Using the `mutate()` function from `dplyr`, create a new column `log10_size` and assign it to `house_prices_2` by applying a `log10()` transformation to `sqft_living`.

```{r}
# Load packages
library(moderndive)
library(dplyr)
library(ggplot2)

# Add log10_size
house_prices_2 <- house_prices %>%
  mutate(log10_size = log10(sqft_living))

```

2. Visualize the effect of the `log10()` transformation by creating a histogram of the new variable `log10_size`.


```{r}
# Load packages
library(moderndive)
library(dplyr)
library(ggplot2)

# Add log10_size
house_prices_2 <- house_prices %>%
  mutate(log10_size = log10(sqft_living))

# Plot the histogram  
ggplot(house_prices_2, aes(x = log10_size)) +
  geom_histogram() +
  labs(x = "log10 size", y = "count")
```

Huzzah! Notice how the distribution is much less skewed. Going forward, you'll use this new transformed variable to represent the size of houses.

## The modeling problem for explanation

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. The modeling problem for explanation**

Now that you have some background on modeling, let's introduce the goal of modeling for explanation and use the evals teaching score data as an example.

**2. Recall: General modeling framework formula**

Recall that of the elements of the general modeling framework, we previously covered the the outcome variable y and explanatory/predictor variables x.Now let's first study f, which defines an explicit relationship between y and x, and then the error component epsilon.

**3. The modeling problem**

Let's address some points about the modeling problem.-Usually you won't know the true form of f nor the mechanism that generates the errors epsilon.-However you will know the observations y and x, as they are given in our data-Using y and x, the goal is to construct or "fit" a model f-hat that approximates the true f, but not epsilon. -In other words, you want to separate the signal from the noise.-With the fitted model f-hat, you can apply it to x to obtain fitted or predicted values of y called y-hat. In this course, you'll keep things simple and only fit models that are linear. But first, let's now perform an EDA of the relationship between the variables in our modeling for explanation example.

**4. Modeling for explanation example**

Earlier you performed a univariate EDA on the outcome variable score and the explanatory variable age. By univariate we mean they only considered one variable at a time. The goal of modeling, however, is exploring relationships between variables. So how can you visually explore such relationships? Using a scatterplot!

**5. EDA of relationship**

You use a geom_point() to create a scatterplot with x mapped to age and y mapped to score. This will mark each instructor's age and score with a point.

**6. EDA of relationship**

Let's ask ourselves, is the relationship positive, meaning as professors age do they also get higher scores? Or is it negative? It’s hard to say, as the pattern isn’t super clear.Before you attempt to answer this, let's first address an issue known as overplotting. For example, focus on the point at age = 70 with the highest score of 4.6. Although not immediately apparent, there are actually not one but two perfectly superimposed points. How can you visually bring this fact to light?By adding a little random jitter to each point, meaning nudge each point just enough so you can distinguish them, but not so much that the plot is overly altered.

**7. Jittered scatterplot**

This is done by taking the same code that generated the scatterplotand replacing the geom_point() with a geom_jitter().

**8. Jittered scatterplot**

Observe there are indeed two values at age 70 and score 4.6. Other overplotted points similarly get broken up. Note that the jittering is strictly a visualization tool; it does not alter the original values in the dataset. So back to our earlier question: is the relationship positive or negative? You can answer this using the correlation coefficient.

**9. Correlation coefficient**

A correlation coefficient is a summary statistic between -1/1 measuring the strength of linear association of two numerical variables, or the degree to which points fall on a line.In the top left plot where the correlation is -1, the points fall perfectly on a negatively sloped line. So as values of x increase, values of y decrease in lock-step.In the bottom right plot where the correlation is +1, the relationship is perfectly positive.In the middle where the correlation is 0, there is no relationship; x and y behave independently.The remaining plots illustrate other in-between values.Let's compute the correlation coefficient for age and score!

**10. Computing the correlation coefficient**

The cor() function takes two numerical variables and returns the correlation, which you embed in the summarize() function.-0.107 indicates a negative relationship, meaning as professors age, they also tend to get lower scores. However, this relationship is only weakly negative.

**11. Let's practice!**

In the next exercise, you'll perform an EDA of the relationship between teaching score and beauty score.

## EDA of relationship of teaching & "beauty" scores

The researchers in the UT Austin created a "beauty score" by asking a panel of 6 students to rate the "beauty" of all 463 instructors. They were interested in studying any possible impact of "beauty" of teaching evaluation scores. Let's do an EDA of this variable and its relationship with teaching `score`.

From now on, assume that `ggplot2`, `dplyr`, and `moderndive` are all available in your workspace unless you're told otherwise.

**Steps**

1. Create a histogram of `bty_avg` "beauty scores" with bins of size 0.5.


```{r}
# Plot the histogram
ggplot(evals, aes(x = bty_avg)) +
  geom_histogram(binwidth = 0.5) +
  labs(x = "Beauty score", y = "count")
```

2. Create a scatterplot with the outcome variable `score` on the y-axis and the explanatory variable `bty_avg` on the x-axis.


```{r}
# Scatterplot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "teaching score")
```

3. Let's now investigate if this plot suffers from *overplotting*, whereby points are stacked perfectly on top of each other, obscuring the number of points involved. You can do this by `jitter`ing the points. Update the code accordingly!


```{r}
# Jitter plot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "beauty score", y = "teaching score")
```

\\"Beauty\\"-ful! It seems the original scatterplot did suffer from overplotting since the jittered scatterplot reveals many originally hidden points. Most `bty_avg` scores range from 2-8, with 5 being about the center.

## Correlation between teaching and "beauty" scores

Let's numerically summarize the relationship between teaching `score` and beauty score `bty_avg` using the correlation coefficient. Based on this, what can you say about the relationship between these two variables?

**Steps**

1. Compute the correlation coefficient of `score` and `bty_avg`.


```{r}
# Compute correlation
evals %>%
  summarize(correlation = cor(score, bty_avg))
```

> *Question*
> ---
> <br>
> <br>
> ⬜ `score` and `bty_avg` are strongly negatively associated.<br>
> ⬜ `score` and `bty_avg` are weakly negatively associated.<br>
> ✅ `score` and `bty_avg` are weakly positively associated.<br>
> ⬜ `score` and `bty_avg` are strongly positively associated.<br>

Correct! While there seems to be a positive relationship, +0.187 is still a long ways from +1, so the correlation is only weakly positive.

## The modeling problem for prediction

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. The modeling problem for prediction**

You’ll finish this first chapter by studying the modeling problem for prediction. While the mechanics for predictive modeling might be similar to those for explanatory modeling, you'll see there are some subtle differences in the goals.

**2. Modeling problem**

Recall the modeling for explanation problem: that the f and epsilon components are unknown and that you only observe the y and x components. Based on y and x, you fit a model f-hat that hopefully closely approximates the true f while ignoring the error epsilon. In other words you want the fitted model to separate the signal from the noise. You then use this fitted model to obtain fitted/predicted values of y called y-hat.

**3. Difference between explanation and prediction**

In explanatory modeling, the form of f-hat matters greatly, in particular any values that quantify the relationship between y and x. For example, for every increase in 1 in instructor age, what is the typical associated change in teaching score?However, in predictive modeling, you don't care so much about the form of f-hat, but more that it yields good predictions. So, if I give inputs x to f-hat, can I get a prediction y-hat that is close to the true value of y?Let's build our intuition about predictive modeling through a further EDA of house prices. However, instead of using a numerical explanatory variable, let's use a categorical predictor variable, house condition.

**4. Condition of house**

Let's glimpse() just the variables price and condition. Condition is a categorical variable with 5 levels, where 1 indicates poor and 5 indicates excellent. Note that while condition is a number between 1 and 5, observe they are represented in R as fct, or factors, so they are treated as categorical.

**5. Exploratory data visualization: boxplot**

Since the original price variable was right-skewed, recall you applied a log10-transformation to unskew them.Now, how can you visualize the relationship between the numerical outcome variable log10-price and the categorical variable condition?Using a boxplot! You use geom_boxplot(), where x maps to condition and y maps to log10-price.

**6. Exploratory data visualization: boxplot**

Observe.For each condition level, the 25th/75th percentiles are marked by ends of the boxes, while the medians are marked with solid horizontal lines. You also observe outliers.As the house condition goes up, as expected, there is a corresponding increasing trend in the median log10-price. Furthermore for each condition, there is variation in log10-price as evidenced by the lengths of the boxplots. Let's now also summarize each group by computing their means. While both MEDIAN and mean are measures of center, means are at the heart of the modeling techniques we'll cover in the next chapter.

**7. Exploratory data summaries**

Recall from earlier courses that to obtain summaries of log10-price split by condition, you first group_by() condition, and then summarize(). You summarize() using the mean of the log10-price, the standard deviation, and also the sample size using the n() function, which simply counts the number of rows in each condition level.Observe. The group-level means exhibit a similar increasing pattern as the medians in the boxplot from before. There is also variation within each condition level as quantified by the standard deviation. Lastly most houses are either condition 3, 4, or 5. Let's start predicting. Say a house is put on the Seattle market and you only know that its condition is 4. A reasonable prediction of its log10-price is the group mean for the condition 4 houses of 5.65.

**8. Exploratory data summaries**

To obtain this prediction in dollars however, you undo the log10-transform by raising 10 to the power 5.65 to get a predicted sale price of $446k.But given the variation in prices within the condition 4 group, not all houses of condition 4 are exactly $446k. In other words, this prediction is bound to have some error.Using our earlier terminology, the value 5.65 can be thought of as the "signal" and any difference between this prediction and the actual log10-price can be thought of as the "noise".

**9. Let's practice!**

Let's close out this introductory chapter with further exercises on exploratory data analysis for modeling for prediction.

## EDA of relationship of house price and waterfront

Let's now perform an exploratory data analysis of the relationship between `log10_price`, the log base 10 house price, and the binary variable `waterfront`. Let's look at the raw values of `waterfront` and then visualize their relationship.

The column `log10_price` has been added for you in the `house_prices` dataset.

**Steps**

1. Use `glimpse()` to view the structure of only two columns: `log10_price` and `waterfront`.

```{r}
# Data
house_prices <- house_prices |> 
                  mutate(log10_price = log10(price))

# View the structure of log10_price and waterfront
house_prices %>%
  select(log10_price, waterfront) %>%
  glimpse()

```

2. Visualize the relationship between `waterfront` and `log10_price` using an appropriate `geom_*` function. Remember that `waterfront` is categorical.


```{r}
# View the structure of log10_price and waterfront
house_prices %>%
  select(log10_price, waterfront) %>%
  glimpse()

# Plot
ggplot(house_prices, aes(x = waterfront, y = log10_price)) +
  geom_boxplot() +
  labs(x = "waterfront", y = "log10 price")
```

A+! Look at that boxplot! Houses that have a view of the waterfront tend to be MUCH more expensive as evidenced by the much higher log10 prices!

## Predicting house price with waterfront

You just saw that houses with a view of the `waterfront` tend to be much more expensive. But by how much? Let's compute group means of `log10_price`, convert them back to dollar units, and compare!

The variable `log10_price` has already been added to `house_prices` for you.

**Steps**

1. Return both the mean of `log10_price` and the count of houses in each level of `waterfront`.


```{r}
# Calculate stats
house_prices %>%
  group_by(waterfront) %>%
  summarize(mean_log10_price = mean(log10_price), n = n())
  
```

2. Using these group means for `log10_price`, return "good" predicted house prices in the original units of US dollars.


```{r}
# Calculate stats
house_prices %>%
  group_by(waterfront) %>%
  summarize(mean_log10_price = mean(log10_price), n = n())
  
# Prediction of price for houses without view of waterfront
10^(5.66)

# Prediction of price for houses with view of waterfront
10^(6.12)
```

100%! Most houses don't have a view of the `waterfront` (n = 21,450), but those that do (n = 163) have a MUCH higher predicted price. Look at that difference! $457,088 versus $1,318,257! In the upcoming Chapter 2 on basic regression, we'll build on such intuition and construct our first formal explanatory and predictive models using basic regression!

# 2. Modeling with Basic Regression

Equipped with your understanding of the general modeling framework, in this chapter, we'll cover basic linear regression where you'll keep things simple and model the outcome variable y as a function of a single explanatory/ predictor variable x. We'll use both numerical and categorical x variables. The outcome variable of interest in this chapter will be teaching evaluation scores of instructors at the University of Texas, Austin.

## Explaining teaching score with age

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Explaining teaching score with age**

Equipped with the background and terminology from Chapter 1, you can begin formal modeling using "basic" linear regression, where you model an outcome variable y using a single explanatory/predictor variable x.

**2. Refresher: Exploratory data visualization**

Earlier, you explored the relationship between teaching score and age via a scatterplot and the correlation coefficient of -0.107, indicating a weakly negative relationship. You also saw that this scatterplot suffers from overplotting. Let's keep this overplotting in mind as we move forward.Now, can you visually summarize the above relationship with a "best fitting" line? A line that cuts through the cloud of points, separating the signal from the noise? Yes! Using a regression line!

**3. Regression line**

Here is the ggplot2 code that produced the previous scatterplot.You can add a "best fitting" line by adding a geom_smooth() layer, with method equal lm for linear model, and SE equal false to omit standard error bars, SE bars are a concept for a more advanced course.

**4. Regression line**

Observe. The overall relationship is negative: as ages increase, scores decrease. This is consistent with our computed correlation coefficient of -0.107. Now, does that mean aging directly causes decreases in score? &lt;pause&gt; Not necessarily, as there may be other factors I'm not accounting for above. After all, correlation isn't necessarily causation. This "best-fitting" line is the “linear regression line”, and is a "fitted linear model" f-hat. Let's draw connections with our earlier modeling theory.

**5. Refresher: Modeling in general**

Recall the general modeling framework. Using only y and x, you fit a model f-hat that hopefully closely approximates the true unknown f while ignoring the error. F-hat yields fitted/predicted values y-hat which hopefully closely match the observed y's.-I'll later define what "closely match" means.

**6. Modeling with basic linear regression**

In linear regression, you assume f is a linear function i.e. a line, necessitating an intercept beta-0 and a slope for x beta-1.The observed value y thus has the following form.The fitted model f-hat is also assumed linear, but with fitted, or estimated, intercept beta-0-hat and slope for x beta-1-hat. These values are computed using our observed data.Plugging x into f-hat yields fitted/predicted values y-hat. Note that there is no epsilon term here, as our fitted model f-hat should only capture signal and not noise.

**7. Back to regression line**

The "best-fitting line" is thus beta-0-hat + beta-1-hat times x. But what are the numerical values of the fitted intercept and slope? I'll let R compute these for us.

**8. Computing slope and intercept of regression line**

You first fit an lm() linear model, using as arguments the data and a model formula of form y tilde x, where y is the outcome and x is the explanatory variable. I'll save this in model_score_1 and display its contents. While the intercept of 4.461 has a mathematical interpretation; the value of y when x equals 0, here it doesn't have a practical interpretation, the teaching score when age is 0.The slope of -0.0059 quantifies the relationship between score and age. Its interpretation is rise-over-run: for every increase of one in age, there's an associated decrease of on average 0.0059 units in score. The negative slope emphasizes the negative relationship.However, the latter output is a bit sparse and not in dataframe format. Let's improve this.

**9. Computing slope and intercept of regression line**

Let’s apply the get_regression_table() function from the moderndive package to model_score_1. This produces what's known as a regression table. This function is an example of a wrapper function: it takes other existing functions and hides its internal workings, so that all you need to worry about are the input and output format.The fitted intercept/slope are now in the second column estimate. The additional columns like standard-error and p-value, all speak to the statistical significance of our results. However, I'll leave these concepts for a more advanced course on statistical inference.

**10. Let's practice!**

Your turn! Instead of linearly modeling score as a function of age, you're going to linearly model teaching score as a function of beauty score!

## Plotting a "best-fitting" regression line

Previously you visualized the relationship of teaching score and "beauty score" via a scatterplot. Now let's add the "best-fitting" regression line to provide a sense of any overall trends. Even though you know this plot suffers from overplotting, you'll stick to the non-`jitter` version.

**Steps**

1. Add a regression line without the error bars to the scatterplot.


```{r}
# Load packages
library(ggplot2)
library(dplyr)
library(moderndive)

# Plot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "score") +
  geom_smooth(method = "lm", se = FALSE)
```

Fantastic! The overall trend seems to be positive! As instructors have higher \\"beauty\\" scores, so also do they tend to have higher teaching scores.

## Fitting a regression with a numerical x

Let's now explicity quantify the linear relationship between `score` and `bty_avg` using linear regression. You will do this by first "fitting" the model. Then you will get the *regression table*, a standard output in many statistical software packages. Finally, based on the output of `get_regression_table()`, which interpretation of the slope coefficient is correct?

**Steps**

1. Fit a linear regression model between score and average beauty using the `lm()` function and save this model to `model_score_2`.


```{r}
# Load package
library(moderndive)

# Fit model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Output content
model_score_2
```

2. Given the sparsity of the output, let's get the *regression table* using the `get_regression_table()` wrapper function.


```{r}
# Load package
library(moderndive)

# Fit model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Output regression table
get_regression_table(model_score_2)
```

> *Question*
> ---
> <br>
> <br>
> ⬜ For every person who has a beauty score of one, their teaching score will be 0.0670.<br>
> ✅ For every increase of one in beauty score, you should observe an associated increase of on average 0.0670 units in teaching score.<br>
> ⬜ Less "beautiful" instructors tend to get higher teaching evaluation scores.<br>

Correct! As suggested by your exploratory visualization, there is a positive relationship between 'beauty' and teaching score.

## Predicting teaching score using age

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Predicting teaching score using age**

Let's take our basic linear regression model of score as a function of age and now use it for predictive ends. For example, say we have demographic information about a professor at the UT Austin, can you make a good guess of their score? Or more generally, based on a single predictor variable x, can you make good predictions y-hat?

**2. Refresher: Regression line**

Recall our "best-fitting" regression line from the last video. Now say all you know about an instructor is that they are aged 40. What is a good guess of their score? Can you use the above visualization?

**3. New instructor prediction**

A good guess is the fitted value on the regression line for age = 40, marked by the square. It seems that this is roughly 4.25. To compute this precisely however, you need use the fitted intercept and fitted slope for age from the regression table.

**4. Refresher: Regression table**

Previously, you learned how to fit a linear regression model with one numerical explanatory variable and applied the get_regression_table() function from the moderndive package to obtain the fitted intercept and fitted slopes values.Recall that these values are in the estimate column and 4.46 and -0.006 respectively

**5. Predicted value**

Generally, you can use a fitted regression model f-hat for predictive as well as explanatory purposes.Specific to our model, I've compute the predicted score via the equation 4.46 minus 0.006 times age. So our new instructor, using this model, is predicted to get a score of 4.22, very close to my earlier visual prediction of 4.25.

**6. Prediction error**

Now say we find out the instructor got a score of 3.5, marked with a circle. Our prediction of 4.25 over-predicted! Let's mark the magnitude of the error with an arrow.

**7. Prediction error**

The length of this arrow is about 0.75 units. While the direction of the arrow was somewhat arbitrarily chosen, I set it to point downwards, indicating a negative error. What I've just illustrated is the modeling concept of a residual.

**8. Residuals as model errors**

A residual is the observed value y minus the fitted/predicted value y-hat. This discrepancy between the two corresponds to the epsilon from the general modeling framework.Here the negative residual of -0.72 corresponds to our over-prediction. With linear regression, sometimes you'll obtain positive residuals and other times negative. In linear regression, these residuals average out to zero. Now say you want predicted y-hats and residuals for ALL 463 instructors. You could repeat the procedure we just followed 463 times, but this would be tedious, so let's automate this procedure using another wrapper function from the moderndive package.

**9. Computing all predicted values**

Recall our earlier fitted linear model saved in model_score_1.Instead of using get_regression_table(), let's now use the get_regression_points() function to get information on all 463 points in our dataset.The first column ID identifies the rows.The 2nd and third columns are the original outcome and explanatory variables score and age. The fourth column score-hat is the predicted y-hat, as computed using the equation of the regression line.The fifth column is the residual: score minus score-hat.Let's go back to our original scatterplot and illustrate a few more residuals.

**10. "Best fitting" regression line**

Here, I'll plot 6 arbitrarily chosen residuals.  Our earlier statement that the regression line is "best-fitting" means that of all possible lines, the blue regression line "minimizes" the residuals. What do I mean by "minimize"? Imagine you drew all 463 residuals, squared their lengths so that positive and negative residuals were treated equally,then summed them.The regression line is the line that minimizes this quantity.You'll see later that this quantity is called the sum-of-squared-residuals and it measures the "lack-of-fit" of a model to a set of points.The blue regression line is "best" in that it minimizes this lack-of-fit.But first, time for some exercises!

**11. Let's practice!**

You'll be making predictions of teaching score of your own, however this time not using age as a predictor, but rather beauty score.

## Making predictions using "beauty score"

Say there is an instructor at UT Austin and you know nothing about them except that their beauty score is 5. What is your prediction \\(\\hat{y}\\) 
of their teaching score \\(y\\)? 

```{r}
#| eval: false
get_regression_table(model_score_2)
  term      estimate std_error statistic p_value lower_ci upper_ci
  <chr>        <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>
1 intercept    3.88      0.076     51.0        0    3.73     4.03 
2 bty_avg      0.067     0.016      4.09       0    0.035    0.099
```

**Steps**

1. Using the values of the intercept and slope from above, predict this instructor's score.

```{r}
# Use fitted intercept and slope to get a prediction
y_hat <- 3.88 + 5 * 0.0670
y_hat

```

2. Say it's revealed that the instructor's score is 4.7. Compute the residual for this prediction, i.e., the *residual* \\(y - \\hat{y}\\).


```{r}
# Use fitted intercept and slope to get a prediction
y_hat <- 3.88 + 5 * 0.0670
y_hat

# Compute residual y - y_hat
4.7 - 4.215
```

Awesome! Was your visual guess close to the predicted teaching score of 4.215? Also, note that this prediction is off by about 0.485 units in teaching score.

## Computing fitted/predicted values & residuals

Now say you want to repeat this for all 463 instructors in `evals`. Doing this manually as you just did would be long and tedious, so as seen in the video, let's automate this using the `get_regression_points()` function. 

Furthemore, let's unpack its output.

**Steps**

1. Let's once again get the regression table for `model_score_2`.  
2. Apply `get_regression_points()` from the `moderndive` package to automate making predictions and computing residuals.

```{r}
# Fit regression model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Get regression table
get_regression_table(model_score_2)

# Get all fitted/predicted values and residuals
get_regression_points(model_score_2)
```

3. Let's unpack the contents of the `score_hat` column. First, run the code that fits the model and outputs the regression table.
4. Add a new column `score_hat_2` which *replicates* how `score_hat` is computed using the table's values.

```{r}
# Fit regression model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Get regression table
get_regression_table(model_score_2)

# Get all fitted/predicted values and residuals
get_regression_points(model_score_2) %>% 
  mutate(score_hat_2 = 3.88 + 0.067 * bty_avg)
```

5. Now let's unpack the contents of the `residual` column. First, run the code that fits the model and outputs the regression table.
6. Add a new column `residual_2` which *replicates* how `residual` is computed using the table's values.

```{r}
# Fit regression model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Get regression table
get_regression_table(model_score_2)

# Get all fitted/predicted values and residuals
get_regression_points(model_score_2) %>% 
  mutate(residual_2 = score - score_hat)
```

Bingo! You'll see later that the residuals can provide useful information about the quality of your regression models. Stay tuned!

## Explaining teaching score with gender

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Explaining teaching score with gender**

Let's now expand your modeling toolbox with basic regression models where the explanatory/predictor variable is not numerical, but rather categorical. Much of the world's data is categorical in nature and its important to be equipped to handle it. You'll continue constructing explanatory and predictive models of teaching score, now using the variable gender, which at the time of this study was recorded as a binary categorical variable.

**2. Exploratory data visualization**

As we similarly did in Chapter 1 for house price and house condition, let's construct an exploratory boxplot of the relationship between score and gender.

**3. Boxplot of score over gender**

You can easily compare the distribution of scores for men and women using single horizontal lines. For example, it seems male instructors tended to get higher scores as evidenced by the higher median. Remember, the solid line in boxplots are the median, not the mean. So before I perform any formal regression modeling, I expect men will tend to be rated higher than women by students. Let's make a mental note: treating the median for women of about 4.1 as a "baseline for comparison", I observe a difference of about +0.2 for men. These aren't exact values, just a rough eyeballing.

**4. Facetted histogram**

An alternative exploratory visualization is a faceted histogram. You use geom_histogram(), where x maps to the numerical variable score, and where we now have facets split by gender.

**5. Facetted histogram**

Unlike the boxplots, you now get a sense for the shape of the distributions. They both exhibit a slight left-skew. Nothing drastic like the right-skew of Seattle house prices, but still a slight skew nonetheless.However, it's now harder to say which distribution is centered at a higher point. This is because the median isn't clearly marked like in boxplots. Furthermore, comparisons between groups can't be made using single lines. So which plot is better? Boxplots or facetted histograms? There is no universal right answer; it all depends on what you are trying to emphasize to the consumers of these visualizations.

**6. Fitting a regression model**

You fit the regression as before where the model formula y tilde x in this case has x set to gender. Using get_regression_table(), you see again the regression table yields an fitted intercept and a fitted slope, but what do these mean when the explanatory variable is categorical?The intercept 4.09 is the average score for the women, the "baseline for comparison" group. Why in this case is the baseline group female and not male? For no other reason than "female" is alphabetically ahead of "male".The slope of 0.142 is the difference in average score for men relative to women. This is known as a "dummy" or "indicator variable".  Its not that men had an average score of 0.142, rather they differed on average from the women by +0.142, so their average score is the sum of 4.09 + .142 = 4.23.

**7. Fitting a regression model**

Let's convince ourselves of this by computing group means using the group_by() and summarize() verbs.So, the latter table shows the means for men and women separately, whereas the regression table shows the average teaching score for the baseline group of women, and the relative difference to this baseline for the men.

**8. A different categorical explanatory variable: rank**

Let's now consider a different categorical variable: rank. Let's group the evals data by rank and obtain counts using the n() function in the summarize call.Observe three levels: First teaching instructors responsibilities' lie primarily only in teaching courses. Second, tenure track faculty also have research expectations and, generally speaking, are on the path to getting promoted to the third rank and highest, tenured.

**9. Let's practice!**

Your turn! Instead of modeling teaching score as a function of the categorical variable gender which has two levels, let's use the categorical variable rank which has 3 levels.

## EDA of relationship of score and rank

Let's perform an EDA of the relationship between an instructor's score and their rank in the `evals` dataset. You'll both visualize this relationship and compute summary statistics for each level of `rank`: `teaching`, `tenure track`, and `tenured`.

**Steps**

1. Write the code to create a boxplot of the relationship between teaching score and rank.


```{r}
ggplot(evals, aes(x = rank, y = score)) +
  geom_boxplot() +
  labs(x = "rank", y = "score")
```

2. For each unique value in `rank`: \nCount the number of observations in each group\nFind the mean and standard deviation of `score`
3. Count the number of observations in each group
4. Find the mean and standard deviation of `score`

```{r}
evals %>%
  group_by(rank) %>%
  summarize(n = n(), mean_score = mean(score), sd_score = sd(score))
```

Cool! The boxplot and summary statistics suggest that teaching get the highest scores while tenured professors get the lowest. However, there is clearly variation around the respective means.

## Fitting a regression with a categorical x

You'll now fit a regression model with the categorical variable `rank` as the explanatory variable and interpret the values in the resulting regression table. Note here the rank "teaching" is treated as the *baseline for comparison group* for the "tenure track" and "tenured" groups.

**Steps**

1. Fit a linear regression model between score and rank, and then apply the wrapper function to `model_score_4` that returns the regression table.


```{r}
# Fit regression model
model_score_4 <- lm(score ~ rank, data = evals)

# Get regression table
get_regression_table(model_score_4)

```

2. Based on the regression table, compute the 3 possible fitted values \\(\\hat{y}\\), which are the group means. Since "teaching" is the *baseline for comparison* group, the `intercept` is the mean score for the "teaching" group and `ranktenure track`/`ranktenured` are relative offsets to this baseline for the "tenure track"/"tenured" groups.


```{r}
# Fit regression model
model_score_4 <- lm(score ~ rank, data = evals)

# Get regression table
get_regression_table(model_score_4)

# teaching mean
teaching_mean <- 4.28

# tenure track mean
tenure_track_mean <- 4.28 - 0.130

# tenured mean
tenured_mean <- 4.28 - 0.145
```

Kudos! Remember that regressions with a categorical variable return group means expressed relative to a baseline for comparison!

## Predicting teaching score using gender

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Predicting teaching score using gender**

You'll finish your exploration of basic regression, with modeling for prediction using one categorical predictor variable. The idea remains the same as predicting with one numerical variable: based on information contained in the predictor variables, in this case gender, can you accurately guess teaching scores?

**2. Group means as predictions**

You previously computed gender-specific means using group_by() and summarize(). This time, however, let's also compute the standard deviation, which is a measure of the variation/spread. On average, the male instructors got a score 4.23 and the female got a score of 4.09. Furthermore, there is variation around the mean scores as evidenced by the standard deviations. The women had slightly more variation with a standard deviation of .564.So say you had an instructor at the UT Austin and you knew nothing about them other than that they were male. A reasonable prediction of their teaching score would be the group mean for the men of 4.23. However, surely there are more factors associated with teaching scores than just gender. How good can this prediction be? There must be a fair amount of error involved.What was our method for quantifying error? It was the residuals! Let's get the predicted values and residuals for all 463 instructors.

**3. Computing all predicted values and residuals**

We use the get_regression_points() function I introduced earlier. Recall that whereas get_regression_table() returns the regression table, get_regression_points() returns information on each of the 463 rows in the evals dataframe, each representing one instructor. In the 2nd column are the observed y outcome variable: score.In the fourth column score_hat, observe that there are only two possible predicted values: either 4.09 or 4.23, corresponding to the group means for the women and men respectively. The final column are the residuals, which are the observed values y minus the predicted values y-hat, or here, score minus score-hat. In the first row, since the prediction 4.09 was lower than the observed value of +4.7, there's a positive residual, whereas in the third row, since the prediction of 4.09 was greater than the observed 3.9, there's a negative residual. Say you observed a predicted value that was exactly equal to the observed value, then in this case the residual would be 0.

**4. Histogram of residuals**

This time, let's take the output of the get_regression_points() function and save it in a new dataframe called model_score_3_points. This way you can use this dataframe in a ggplot() function call to plot a histogram of the residuals.

**5. Histogram of residuals**

This histogram appears to be roughly centered at 0, indicating on average the residuals, or errors, are 0. Sometimes I make errors as large as +1 or -1 out of a 5-unit scale! Those are fairly large! Also note that I tend to make larger negative errors than positive errors.But these shortcomings aren't necessarily bad. Remember, this is a very simplistic model with only one predictor: gender. The analysis of the residuals above is suggesting to us that we probably need more predictors than just gender to make good predictions of teaching scores. Wouldn't it be great if you could fit regression models where you could use more than one explanatory or predictor variable? You'll see in the upcoming Chapter 3 on multiple regression, that you can! But first, let's do some exercises!

**6. Let's practice!**

Let’s make predictions using the categorical predictor variable rank and study the resulting residuals!

## Making predictions using rank

Run `get_regression_table(model_score_4)` in the console to regenerate the regression table where you modeled `score` as a function of `rank`. Now say using this table you want to predict the teaching `score` of an instructor about whom you know nothing except that they are a `tenured` professor. Which of these statements is true?

> *Question*
> ---
> ???<br>
> <br>
> ✅ A good prediction of their `score` would be `4.28 - 0.145 = 4.135`.<br>
> ⬜ A good prediction of their `score` would be `-0.145`.<br>
> ⬜ There is no information in the table that can aid your prediction.<br>

Yay! Regression tables for categorical explanatory variables show differences in means relative to a baseline.

## Visualizing the distribution of residuals

Let's now compute both the predicted score \\(\\hat{y}\\) and the residual \\(y - \\hat{y}\\) for all \\(n = 463\\) instructors in the `evals` dataset. Furthermore, you'll plot a histogram of the residuals and see if there are any patterns to the residuals, i.e. your predictive errors. 

`model_score_4` from the previous exercise is available in your workspace.

**Steps**

1. Apply the function that automates making predictions and computing residuals, and save these values to the dataframe `model_score_4_points`.

```{r}
# Calculate predictions and residuals
model_score_4_points <- get_regression_points(model_score_4)
model_score_4_points

```

2. Now take the `model_score_4_points` dataframe to plot a histogram of the `residual` column so you can see the distribution of the residuals, i.e., the prediction errors.


```{r}
# Calculate predictions and residuals
model_score_4_points <- get_regression_points(model_score_4)
model_score_4_points

# Plot residuals
ggplot(model_score_4_points, aes(x = residual)) +
  geom_histogram() +
  labs(x = "residuals", title = "Residuals from score ~ rank model")
```

Congrats! Look at the distribution of the residuals. While it seems there are fewer negative residuals corresponding to overpredictions of score, the magnitude of the error seems to be larger (ranging all the way to -2).

# 3. Modeling with Multiple Regression

In the previous chapter, you learned about basic regression using either a single numerical or a categorical predictor. But why limit ourselves to using only one variable to inform your explanations/predictions? You will now extend basic regression to multiple regression, which allows for incorporation of more than one explanatory or one predictor variable in your models. You'll be modeling house prices using a dataset of houses in the Seattle, WA metropolitan area. 

## Explaining house price with year & size

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Explaining house price with year &amp; size**

In Chapter 2, you created your first basic regression models that incorporated one explanatory/predictor variable x at a time. In this chapter on *multiple* regression, we now consider regression models that incorporate *more* than one x variable.You'll use the Seattle house-prices dataset we introduced in Chapter 1.

**2. Refresher: Seattle house prices**

Recall you performed EDA's of the relationship of price with variables like:sqft_living, a measure of a house's sizecondition of the house, a categorical variable with 5 levels, andwhether the house had a view of the waterfront.

**3. Refresher: Price and size variables**

You also saw that the outcome variable price was right-skewed, as evidenced by the long right tail in the left histogram. This skew was caused by a small number of very expensive houses, resulting in a plot where its difficult to compare prices of the less expensive houses. This was also the case for the explanatory variable sqft_living on the right.

**4. Refresher: log10 transformation**

You unskewed both variables using a log10-transformation, rendering them in this case both more symmetric and bell-shaped.Also, recall that a log10-price of 6 corresponds to a price of 10^6 = 1 million.

**5. Refresher: Data transformation**

The dplyr mutate() code that created the log10-transformed variables is shown here. For the rest of this course, you'll assume that that this code was run, hence both the variables log10-price and log10-size exist in house_prices.

**6. Model for house price**

Let's explore the relationship between price and two explanatory variables: house size and year built. As always before any modeling, let's perform an EDA. While a scatterplot displays the relationship between two numerical variables, here you have three numerical variables. How can you visualize their joint relationship? By using a 3D scatterplot!

**7. Exploratory visualizing of house price, size &amp; year**

Here's a display of the 3D scatterplot for 500 randomly chosen houses. The outcome variable log10-price is on the vertical axis, while the two explanatory variables are on the bottom grid. Now, how can you visually summarize the relationship between these points? In Chapter 2 when we had a 2D scatterplot, we used a regression line. The generalization of regression line in a 3D scatterplot is a regression plane!

**8. Regression plane**

Here's a snapshot of the corresponding regression plane that cuts through the cloud of points and "best fits" them. Unfortunately, this snapshot is non-interactive. For an interactive version, click on the above link.These 3D visualizations were created with the plotly package, which is a topic that would take too long to cover in this course, so our exercises won't involve creating such plots.How can you quantify the relationships between these variables? By obtaining the values of the fitted regression plane!

**9. Regression table**

Similarly as in Chapter 2, you fit the model using the lm() function, but now with a model formula of form: y tilde x1 PLUS x2, where the plus indicates you are using more than one explanatory variable.You get the regression table as before.The intercept here has no practical meaning, in particular since there are no houses in Seattle built in the year 0!The first slope coefficient suggests that "taking into account all other variables", increases of 1 in log10_size are associated with increases of on average 0.913 in log10-price. In other words, taking into account the age of the home, larger homes tend to cost more. Note, you preface your statement with "taking into account all other variables" since you are now jointly interpreting the associated effect of *multiple* explanatory variables in the same model.Similarly, taking into account, or "controlling for", log10_size, for every additional year in recency of construction, there is an associated decrease of on average -0.00138 in log10-price, suggesting that, taking into account house size, newer houses tend to cost less.

**10. Let's practice!**

Your turn! You'll create your first multiple regression model for price, using year built and the number of bedrooms as explanatory variables.

## EDA of relationship

Unfortunately, making 3D scatterplots to perform an EDA is beyond the scope of this course. So instead let's focus on making standard 2D scatterplots of the relationship between price and the number of bedrooms, keeping an eye out for outliers.

The log10 transformations have been made for you and are saved in `house_prices`.

**Steps**

1. Complete the `ggplot()` code to create a scatterplot of `log10_price` over `bedrooms` along with the best-fitting regression line.


```{r}
# Create scatterplot with regression line
ggplot(house_prices, aes(x = bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)
```

2. There is one house that has 33 bedrooms. While this could truly be the case, given the number of bedrooms in the other houses, this is probably an outlier.
3. Remove this outlier using `filter()` to recreate the plot.

```{r}
# Remove outlier
house_prices_transform <- house_prices %>% 
  filter(bedrooms < 33)

# Create scatterplot with regression line
ggplot(house_prices_transform, aes(x = bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)
```

Excellent! Another important reason to perform EDA is to discard any potential outliers that are likely data entry errors. In our case, after removing an outlier, you can see a clear positive relationship between the number of bedrooms and price, as one would expect.

## Fitting a regression

`house_prices`, which is available in your environment, has the log base 10 transformed variables included and the outlier house with 33 bedrooms removed. Let's fit a multiple regression model of price as a function of size and the number of bedrooms and generate the regression table. In this exercise, you will first fit the model, and based on the regression table, in the second part, you will answer the following question:

Which of these interpretations of the slope coefficent for bedrooms is correct?

**Steps**

1. Fit a linear model `lm` with `log10_price` as a function of `log10_size` and `bedrooms`.
2. Print the regression table.

```{r}
# Data
house_prices <- house_prices |> 
                  mutate(log10_size = log10(sqft_living))

# Fit model
model_price_2 <- lm(log10_price ~ log10_size + bedrooms, 
                    data = house_prices)

# Get regression table
get_regression_table(model_price_2)
```

> *Question*
> ---
> <br>
> <br>
> ⬜ Every extra bedroom is associated with a decrease of on average 0.033 in `log10_price`.<br>
> ✅ Accounting for `log10_size`, every extra bedroom is associated with a decrease of on average 0.033 in `log10_price`.<br>

Splendid! In this multiple regression setting, the associated effect of any variable must be viewed in light of the other variables in the model. In our case, accounting for the size of the house reverses the relationship of the number of bedrooms and price from positive to negative!

## Predicting house price using year & size

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Predicting house price using year &amp; size**

Let's now use your first multiple regression model to make predictions! Just as you did for basic regression, let's first visually illustrate what it means to make predictions and then make explicit numerical ones.

**2. Refresher: regression plane**

Let's consider only the regression plane from the last video to make predictions visually. Say a house is for sale in Seattle and all you know is that its log10-size size is 3.07, and that it was built in 1980. What is a good prediction of its log10-price?

**3. Regression plane for prediction**

This combination of year = 1980 and log10-size = 3.07, marked with the intersecting dashed lines, corresponds to a fitted value y-hat of 5.45 on the regression plane. This value is marked with a red dot.Let's now do the same, but numerically.

**4. Predicted value**

Let's regenerate the corresponding regression table by first fitting the model and then using the get_regression_table() function. Recall that in multiple regression, all fitted slope coefficients are interpreted "taking into account all other variables". So for example, taking into account the size of the house, every additional year in recency of the house's construction, there is an associated decrease of on average -0.00138 in log10-price, suggesting a negative relationship.

**5. Predicted value**

Let's now use these values to numerically make the prediction you visually made earlier. You plug in log10-size = 3.07 and year = 1980 into the fitted equation.This yields a fitted value for log10-price of 5.45.You undo the log10-transformation by raising 10 to the power of this value.Yielding a predicted price for this house of about $282K.

**6. Computing all predicted values and residuals**

Just as you did for your predictive modeling examples from the last chapter, let's automate what I just did for all 21k houses using the get_regression_points() function from the moderndive package.You previously saw that this function returns information on each point involved in a regression model. In particular -The 2nd-to-last column log10_price_hat are the fitted/predicted values, as I manually computed earlier for our example house. -The last column consists of the residuals, i.e., the observed log10-price minus the predicted log10-price. Using the residuals, let's compute a measure of the model’s fit, or more precisely speaking, lack thereof. Let's take the 3D scatterplot and regression plane from before and mark a selection of residuals.

**7. Best fit and residuals**

We plot an arbitrarily chosen set of residuals with red vertical lines. Remember, residuals are the discrepancies between-the observed values marked by the blue dots and-the fitted/predicted values, marked by the corresponding point on the regression plane.These correspond to the epsilon error term in the general modeling framework we saw in Chap 1.Say you compute the residual for all 21k points, square the residuals, and sum them. You saw earlier that this quantity is called the "sum of squared residuals". It is a numerical summary of the "lack-of-fit" of a model to a set of points, in this case the regression plane. Hence, larger values of the sum of squared residuals indicate poorer fit, and smaller values indicate better fit.Just as with "best-fitting" regression lines, of all possible planes the regression plane minimizes the sum of squared residuals. This is what is meant by "best fitting" plane.Let's now compute the sum of squared residuals.

**8. Sum of squared residuals**

You start with all 21k residuals as shown above.-You then square them using mutate() ...-and then summarize() the squared residuals with their sum ...The resulting value of 585 is hard to interpret in absolute terms. However, in relative terms it can be used to compare fits of different models that use different explanatory variables, and hence allow us to identify which models fit "best". This is a theme we'll revist in Chapter 4 on model assessment and selection.

**9. Let's practice!**

Your turn. Let's use size and number of bedrooms as predictor variables instead, and start predicting house prices!

## Making predictions using size and bedrooms

Say you want to predict the price of a house using this model and you know it has: 


* 1000 square feet of living space, and 
* 3 bedrooms 
What is your prediction both in log10 dollars and then dollars?

The regression model from the previous exercise is available in your workspace as `model_price_2`. 

```{r}
#| eval: false
get_regression_table(model_price_2)
# A tibble: 3 x 7
  term       estimate std_error statistic p_value lower_ci upper_ci
  <chr>         <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>
1 intercept     2.69      0.023     116.        0    2.65     2.74 
2 log10_size    0.941     0.008     118.        0    0.925    0.957
3 bedrooms     -0.033     0.002     -20.5       0   -0.036   -0.03 
```

**Steps**

1. Using the fitted values of the intercept and slopes from the regression table on the left to predict this house's price in log10 dollars.

```{r}
# Make prediction in log10 dollars
2.69 + 0.941 * log10(1000) - 0.033 * 3

```

2. Now predict this house's price in dollars.


```{r}
# Make prediction in log10 dollars
2.69 + 0.941 * log10(1000) - 0.033 * 3

# Make prediction dollars
10^(2.69 + 0.941 * log10(1000) - 0.033 * 3)
```

Spot on! Using the values in the regression table you can make predictions of house prices! In this case, your prediction is about $260,000. Let's now apply this procedure to all 21k houses!

## Interpreting residuals

Let's automate this process for all 21K rows in `house_prices` to obtain residuals, which you'll use to compute the *sum of squared residuals*: a measure of the lack of fit of a model. After computing the sum of squared residuals, you will answer the following question:

Which of these statements about residuals is *incorrect*?

**Steps**

1. Apply the relevant wrapper function to automate computation of fitted/predicted values and hence also residuals for all 21K houses using `model_price_2`.


```{r}
# Automate prediction and residual computation
get_regression_points(model_price_2)
```

2. Compute the sum of squared residuals using `dplyr` commands.


```{r}
# Automate prediction and residual computation
get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(sum_sq_residuals = sum(sq_residuals))
```

> *Question*
> ---
> <br>
> <br>
> ⬜ The residual is the observed outcome variable minus the predicted variable.<br>
> ✅ Residuals are leftover points not accounted for in the our regression model.<br>
> ⬜ They can be thought of as prediction errors.<br>
> ⬜ They can be thought of as the lack-of-fit of the predictions to truth.<br>

Good job! Residual does suggest 'leftovers', but not in the sense that they are leftover points.

## Explaining house price with size & condition

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Explaining house price with size &amp; condition**

Previously, you created a multiple regression model for house price using two numerical explanatory or predictor variables. However, multiple regression is not just limited to combinations of numerical variables; you can also use categorical variables!In this video, you'll once again model log10-price as a function of the log10_size, but now consider the house's condition, a categorical variable with 5 levels as seen in Chapter 1.

**2. Refresher: Exploratory data analysis**

After creating the transformed log10_price and log10_size variables, let's review the EDA we performed in Chapter 1 of the relationship between log10_price and condition.

**3. Refresher: Exploratory data analysis**

You previously saw: a roughly increasing trend in the mean as condition goes from 1 to 5, variation within each of the five levels of condition, and the fact that most houses are of conditions 3, 4, or 5. Let's continue this EDA with some more exploratory visualizations.

**4. House price, size, and condition**

Here you plot all 21k points in a colored scatterplot, where-x maps to the log10_size-y maps to log10_price and-the colors of the points map to the condition.You also plot the overall regression line in black, in other words, the regression line for all houses irrespective of condition. However, wouldn't it be nice if you had separate regression lines for each color in other words each condition level? This would allow us to consider the relationship between size and price separately for each condition. Let's do this!

**5. Parallel slopes model**

Here's the same colored scatterplot, but now with five separate regression lines. Note, we’re keeping things simple by having all 5 lines have the same slope, but allowing for different intercepts. Observe houses of condition 5 have the highest regression line, followed by 4, 3, 1, and then 2. This is known as the "parallel slopes" model.

**6. Parallel slopes model**

An alternative visualization is one split by facets. This plot really brings to light that there are very few houses of condition 1 or 2. However, comparing the 5 regression lines here is harder than before. Which plot is better? Again, there is no universal right answer, you need to make a choice depending on what you want to convey, and own it.

**7. House price, size, and condition relationship**

Let's explicitly quantify these relationships by looking at the regression table. You once again fit a regression using a formula where the plus sign separates our two explanatory variables and apply the get_regression_table() function to it.Recall the notion of a "baseline for comparison" level when using a categorical variable in a regression model. In this case, the baseline group is houses of condition = 1. Let's interpret the terms.The fitted intercept of 2.88 corresponds to the intercept of the baseline group, which are the condition 1 houses in red in the previous plot.The fitted slope 0.837 for log10-size corresponds to the associated average increase in log10-price for every increase of one in log10-size. Recall, the parallel slopes model dictates that all 5 groups have this same slope.condition2 = -0.0385 is the difference in intercept, or the offset, for condition 2 houses relative to condition 1 houses, a negative value. This is reflected in the previous plot by the fact that the yellow regression line has a lower intercept than the red line.Conditions 3, 4, and 5 are interpreted similarly. Observe that this offset is largest for condition 5 at +0.0956. This is reflected in the previous plot by the purple regression line having the highest intercept.

**8. Let's practice!**

Your turn! You'll model price using the same numerical variable, but with a different categorical variable: whether the house had a view of the waterfront.

## Parallel slopes model

Let's now fit a "parallel slopes" model with the numerical explanatory/predictor variable `log10_size` and the categorical, in this case binary, variable `waterfront`. The visualization corresponding to this model is below:

<p align="center">
  <img src="https://assets.datacamp.com/production/repositories/1575/datasets/8916d665b69b702b19e74c673dd75eb3b8ec4a99/03-03-slides-parallel_slopes_2.png" width="400">


**Steps**

1. Fit a multiple regression of `log10_price` using `log10_size` and `waterfront` as the predictors. Recall that the data frame that contains these variables is `house_prices`.


```{r}
# Fit model
model_price_4 <- lm(log10_price ~ log10_size + waterfront,
                    data = house_prices)

# Get regression table
get_regression_table(model_price_4)
```

Success! Notice how the regression table has three rows: intercept, the slope for log10_size, and an offset for houses that do have a waterfront.

## Interpreting the parallel slopes model

Let's interpret the values in the regression table for the parallel slopes model you just fit. Run `get_regression_table(model_price_4)` in the console to view the regression table again. The visualization for this model is below. Which of these interpretations is *incorrect*?

<p align="center">
  <img src="https://assets.datacamp.com/production/repositories/1575/datasets/8916d665b69b702b19e74c673dd75eb3b8ec4a99/03-03-slides-parallel_slopes_2.png" width="400">


> *Question*
> ---
> ???<br>
> <br>
> ⬜ The intercept for houses with a view of the waterfront is 3.282.<br>
> ⬜ All houses are assumed to have the same slope between `log10_price` &amp; `log10_size`.<br>
> ⬜ The intercept for houses without a view of the waterfront is 2.96.<br>
> ✅ The intercept for houses with a view of the waterfront is 0.322.<br>

Right! 0.322 is the offset in the intercept for houses with a view of the waterfront relative to those which don't.

## Predicting house price using size & condition

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Predicting house price using size &amp; condition**

Just as with multiple regression models using two numerical predictor variables, let's now predict house prices using models with one numerical and one categorical predictor variable. Previously, however you made predictions on the same data you used to fit the model to. So you had access to the true observed sale price of these houses.Now let's consider a different scenario: making predictions on "new data". This scenario could be thought of as the following: let's say the market hasn't changed much and a completely new house is put on the market. When you make the prediction, you won't know how good your prediction is since you don't know the true observed sale price yet. It's only after the house is sold that you can compare the prediction to the truth.

**2. Refresher: Parallel slopes**

Recall our plot of the parallel slopes model, where for each of the 5 levels of condition, you plot a separate regression line of log10_price over log10_size. While these 5 lines shared a common slope, they had different intercepts, with the houses of condition 5 having the highest intercept.

**3. Making a prediction**

Say two new houses enter the market. One is of condition 3 and log10_size 2.9 as marked with the green dashed line on the left. The other is of condition 4 and log10_size 3.6 as marked with the blue dashed line on the right.What are this model's predicted log10 prices? They're the points at which the blue and green dashed lines intersect their corresponding regression lines!

**4. Visualizing predictions**

For the first house marked with the green dashed line, you'd predict a log10_price of about 5.4, for a sale price of 10^5.4 or about $250K. For the other house marked with the blue dashed line, you'd predict a log10_price of just under 6, for a sale price of just under 10^6 = one million dollars.Great! Now instead of just making visual predictions, let's also make explicit numerical ones.

**5. Numerical predictions**

Recall our regression table, where the intercept corresponds to the baseline group condition 1, the common slope associated with log10_size, and the 4 offsets.The predicted log10_price for the first house is: the intercept 2.88plus the offset for condition 3 houses 0.032plus 0.837 times the house’s log10_size 2.9.The predicted sale price for the second house is similarly the intercept 2.88plus the offset for condition 4 houses 0.0440plus 0.837 times the house’s log10_size 3.6.*While doing this by hand is fine for two new houses, imagine doing this for 1000 new houses! This would take us forever! Fortunately, if the new houses' information is saved in a spreadsheet or data frame, you can automate the above procedure!

**6. Defining "new" data**

Let's represent these two new houses in a manually created data frame new_houses using the data_frame() function from the dplyr package.Observe.New houses has two rows corresponding to our two new houses.And has two variables whose names and formats match exactly what they are in the original dataframe house-prices. For example, the variable condition was saved as a categorical variable in the original data frame house-prices, so in new-houses we convert the condition values 3 &amp; 4 from numerical to categorical variables using the factor function.

**7. Making predictions using new data**

You can once again use the get_regression_points() function to automate the prediction process, but this time I'll use a new argument. We set newdata to the dataframe new_houses, indicating that I want to apply the fitted model to a new set of observations.Observe that the output contains the same predicted values log10_price_hat just like before.

**8. Making predictions using new data**

Now say you want to obtain predictions of price instead of log10_price. You use the mutate() function to raise 10 to the power the variable log10_price_hat to obtain price_hat.Our predicted house prices are about $220K and $870K respectively, as done visually earlier.

**9. Let's practice!**

For our final set of exercises for Chapter 3 on multiple regression, you'll now similarly make your own predictions on a set of "new" houses.

## Making predictions using size and waterfront

Using your model for `log10_price` as a function of `log10_size` and the binary variable `waterfront`, let's make some predictions! Say you have the two following "new" houses, what would you predict their prices to be *in dollars*?


* House A: `log10_size = 2.9` that has a view of the waterfront
* House B: `log10_size = 3.1` that does not have a view of the waterfront
We make the corresponding visual predictions below:

<p align="center">
  <img src="https://assets.datacamp.com/production/repositories/1575/datasets/23ebd6a1575db2a343a530dc419ed5e1c067685b/03-04-slides-parallel_slopes_predictions_points_2.png" width="400">


**Steps**

1. After running the code on line 2 to get the regression table based on `model_price_4`, compute the predicted prices for both houses. First you'll use an equation based on values in this regression table to get a predicted value in log10 dollars, then raise 10 to this predicted value to get a predicted value in dollars.


```{r}
# Get regression table
get_regression_table(model_price_4)

# Prediction for House A
10^(2.96 + 0.825 * 2.9 + 0.322)

# Prediction for House B
10^(2.96 + 0.825 * 3.1)
```

Yay! Your modeling toolbox is getting quite extensive! Let's now automate this!

## Automating predictions on "new" houses

Let's now repeat what you did in the last exercise, but in an automated fashion assuming the information on these "new" houses is saved in a dataframe. 

Your model for `log10_price` as a function of `log10_size` and the binary variable `waterfront` (`model_price_4`) is available in your workspace, and so is `new_houses_2`, a dataframe with data on 2 new houses.
While not so beneficial with only 2 "new" houses, this will save a lot of work if you had 2000 "new" houses.

**Steps**

1. Apply `get_regression_points()` as you would normally, but with the `newdata` argument set to our two "new" houses. This returns predicted values for just those houses.


```{r}
# View the "new" houses
new_houses_2 <- tibble(
  log10_size = c(2.9, 3.1),
  waterfront = c(T,F)
)

# Get predictions on "new" houses
get_regression_points(model_price_4, newdata = new_houses_2)
```

2. Now take these two predictions in `log10_price_hat` and return a new column, `price_hat`, consisting of fitted price in dollars.


```{r}
# View the "new" houses
new_houses_2

# Get predictions price_hat in dollars on "new" houses
get_regression_points(model_price_4, newdata = new_houses_2) %>% 
  mutate(price_hat = 10^log10_price_hat)
```

Predictions of $472,000 and $328,000! Exceptional! You're done with the multiple regression chapter, and now you're onto model assessment and selection!

# 4. Model Assessment and Selection

In the previous chapters, you fit various models to explain or predict an outcome variable of interest. However, how do we know which models to choose? Model assessment measures allow you to assess how well an explanatory model "fits" a set of data or how accurate a predictive model is. Based on these measures, you'll learn about criteria for determining which models are "best". 

## Model selection and assessment

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Model assessment and selection**

Let's recap what we've learned so far. After learning some background modeling theory and terminology in Chapter 1, in Chapter 2 you modeled basic regressions using one explanatory/predictor X variable. In Chapter 3, you extended this by using two X variables. You created many models for both teaching score and house price.However, you may be asking: how do you know which model to choose, in other words, which models are best? What do we mean by best and how does one assess this? In this final chapter, you'll answer these questions via elementary model assessment and selection. In particular, you'll assess the quality of the multiple regression models for Seattle house prices from Chapter 3. But first, a brief refresher.

**2. Refresher: Multiple regression**

In Chapter 3 you studied two different multiple regression models for the outcome variable log10_price. The first using two numerical explanatory/predictor X variables: log10_size and year. The other used one numerical and one categorical X variable: log10_size and condition.If you wanted to explain or predict house prices, and you had to choose from these two models, which one would you select? Presumably the "better" one. As suggested earlier, this necessitates an explicit criteria for “better.” Have you seen one so far? Yes, the sum of squared residuals!

**3. Refresher: Sum of squared residuals**

Recall a residual is an observed value y minus, its corresponding fitted/predicted value y-hat, in our case log10_price minus log10_price_hat. Visually, they are the vertical distances between the blue points and their corresponding fitted value on the regression plane.I've marked a small selection on the snapshot of the 3D visualization.Furthermore, you learned that of all possible planes, the regression plane minimizes the sum of squared residuals. The latter is computed by squaring all 21k residuals and summing them. You saw that this quantity can thought of as a measure of lack-of-fit, where larger values indicate a worse fit.

**4. Refresher: Sum of squared residuals**

You computed this value explicitly in a previous video for model_price_1, which uses log10_size and year as x variables.You saw that this model's sum of squared residuals was 585, a number that's a bit hard to make sense of on its own.

**5. Refresher: Sum of squared residuals**

However, let's compute the sum of squared residuals for model_price_3 as well, which uses the categorical variable condition instead of the numerical variable year.The sum of squared residuals is now 608. So it seems that model 3 using the variable condition has a bigger lack-of-fit, so is "worse" suggesting that model 1 using year is better.

**6. Let's practice!**

Your turn. Let's compute the sum of squared residuals like you did in Chapter 3, but now with an eye towards model assessment and selection.

## Refresher: sum of squared residuals

Let's remind you how to compute the sum of squared residuals. You'll do this for two models.

**Steps**

1. Use the appropriate function to get a dataframe with the residuals for `model_price_2`. 
2. Add a new column of squared residuals called `sq_residuals`. 
3. Then summarize `sq_residuals` with their sum. Call this sum `sum_sq_residuals`.

```{r}
# Model 2
model_price_2 <- lm(log10_price ~ log10_size + bedrooms, 
                    data = house_prices)

# Calculate squared residuals
get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(sum_sq_residuals = sum(sq_residuals))
```

4. Compute the sum of squared residuals for `model_price_4` which uses the categorical variable `waterfront` instead of the numerical variable `bedrooms`.


```{r}
# Model 4
model_price_4 <- lm(log10_price ~ log10_size + waterfront, 
                    data = house_prices)

# Calculate squared residuals
get_regression_points(model_price_4) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(sum_sq_residuals = sum(sq_residuals))
```

Wonderful! Let's use these two measures of model assessment to choose between these two models, or in other words, perform model selection!

## Which model to select?

Based on these two values of the sum of squared residuals, which of these two models do you think is "better", and hence which would you select?


* `model_price_2` that uses `log10_size` and `bedrooms`?
* `model_price_4` that uses `log10_size` and `waterfront`?
> *Question*
> ---
> ???<br>
> <br>
> ⬜ Since `model_price_2`'s value was 605, select this one.<br>
> ✅ Since `model_price_4`'s value was 599, select this one.<br>
> ⬜ No information about which is better is provided whatsoever.<br>

Correct! Given the choice of just these two models, the evidence suggests using size and waterfront yield a better fit, so you should choose this one!

## Assessing model fit with R-squared

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Assessing model fit with R-squared**

Now that you've reviewed the sum of squared residuals with an eye towards model assessment and selection, let's learn about another measure of a model's fit: the widely known R-squared.

**2. R-squared**

R-squared is another numerical summary of how well a model fits points. It is 1 minus the variance of the residuals over the variance of the outcome variable. If you've never heard of a variance, it’s another measure of variability/spread and its the standard deviation squared. Instead of focusing on the formula however, let's first focus on the intuition:While the sum of squared residuals is unbounded, meaning there is no theoretical upper limit to its value, R-squared is standardized to be between 0 and 1.Unlike the sum of squared residuals where smaller values indicate better fit, larger values of R-squared indicate better fit.So 1 indicates perfect fit, and 0 indicates perfect lack of fit, in other words no relationship between the outcome and explanatory/predictor variables. Let's explore these ideas visually.

**3. High R-squared value example**

Let's revisit basic regression with one numerical variable and consider a set of points with a perfectly linear relationship. In other words, the points fall perfectly on a line.

**4. High R-squared value: "Perfect" fit**

Recall residuals are the vertical distances between the observed values, here the black points, and the corresponding fitted/predicted values on the blue regression line. Here, the residuals are all invariably 0. Thus the variance, or variation, of the residuals is 0, and thus R^2 is equal to 1-0, which is 1. Let's now consider an example where the R-squared is closer to 0, indicating a poorer fit.

**5. Low R-squared value example**

Now the points don't fit tightly on a line, but rather exhibit a large amount of scatter. Let's add the best fitting regression line.

**6. Low R-squared value example**

Unlike the previous example, there are now varying residuals, thus the numerator is greater than zero, so R-squared will be smaller. Note that it is a mathematical fact that the variance of y is greater than or equal to the variance of the residuals, guaranteeing that R-squared is between 0 and 1.

**7. Numerical interpretation**

Using this fact, the numerical interpretation of R-squared is as follows: it is the proportion of the total variation in the outcome variable y that the model explains.Our models attempt to explain the variation in house prices. For example, what makes certain houses expensive and others not? The question is, how much of this variation can our models explain? If it's 100%, then our model explains everything! If its 0%, then our model has no explanatory power.

**8. Computing R-squared**

Let's compute the R-squared statistic for both models we saw in the last video. In both cases, the outcome variable y is the observed log10_price. For model 1, which used log10_size and year, the R^2 is .483 or 48.3%. So you can explain about half the total variation in house prices using Model 1.

**9. Computing R-squared**

For model 3, which used condition instead of year, the R^2 is .462 or 46.2%. Now a lower proportion of the total variation in house prices is explained by Model 3.Since R-squared values closer to 1 mean better fit, the results suggest you choose Model 1, and thus using size and year is preferred to using size and condition. This is the same conclusion reached as when you used sum of squared residuals as the assessment criteria. Note however, sometimes there are no models that yield R-squared values close to 1. Sometimes the phenomenon you are modeling is so complex, no choice of variables will capture its behavior, and thus you only get low R-squared values.

**10. Let's practice!**

Let's now compute R-squared for the models you created in the exercises for Chapter 3!

## Computing the R-squared of a model

Let's compute the \\(R^2\\) summary value for the two numerical explanatory/predictor variable model you fit in the Chapter 3, price as a function of size and the number of bedrooms. 

Recall that \\(R^2\\) can be calculated as: 

$$1 - \\frac{\\text{Var}(\\text{residuals})}{\\text{Var}(y)}$$

**Steps**

1. Compute \\(R^2\\) by summarizing the `residual` and `log10_price` columns.


```{r}
# Fit model
model_price_2 <- lm(log10_price ~ log10_size + bedrooms,
                    data = house_prices)
                    
# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model_price_2) %>%
  summarize(r_squared = 1 - var(residual) / var(log10_price))
```

Nice job! You observed an R-squared value of 0.465, which means that 46.5% of the total variability of the outcome variable log base 10 price can be explained by this model.

## Comparing the R-squared of two models

Let's now compute \\(R^2\\) for the one numerical and one categorical explanatory/predictor variable model you fit in the Chapter 3, price as a function of size and whether the house had a view of the `waterfront`, and compare its \\(R^2\\) with the one you just computed.

**Steps**

1. Compute \\(R^2\\) for `model_price_4`.


```{r}
# Fit model
model_price_4 <- lm(log10_price ~ log10_size + waterfront,
                    data = house_prices)

# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model_price_4) %>%
  summarize(r_squared = 1 - var(residual) / var(log10_price))
```

> *Question*
> ---
> <br>
> <br>
> ⬜ Since `model_price_2` had a lower \\(R^2\\) of 0.465, it "fit" the data better.<br>
> ✅ Since `model_price_4` had a higher \\(R^2\\) of 0.470, it "fit" the data better.<br>
> ⬜ \\(R^2\\) doesn't tell us anything about quality of model "fit".<br>

Correct! Since using waterfront explained a higher proportion of the total variance of the outcome variable than using the number of bedrooms, using waterfront in our model is preferred.

## Assessing predictions with RMSE

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Assessing predictions with RMSE**

You just learned about R-squared, the proportion of the total variation in house prices explained by a model. This numerical summary can be used to assess model fit, where models with R-squared values closer to 1 have better fit, and values closer to 0 have poorer fit.Let's now consider another assessment measure, but one more associated with modeling for prediction. In particular, how can you assess the quality of a model's predictions? You'll use a quantity called the root mean square error, which is a slight variation of the sum of squared residuals.

**2. Refresher: Residuals**

Once again, recall in your visualization of modeling with two numerical predictor variables, you marked a selection of residuals with red lines: the difference between the observed values and their corresponding fitted/predicted values on the regression plane. The sum of squared residuals takes all such residuals, squares them, and sums them. But what if you took the average instead of the sum? For example, a model might have a large sum of squared residuals, merely because it involves a large number of points! By using the average, we’ll correct for this and get a notion of "average prediction error”.

**3. Mean squared error**

You've seen the computation of the sum of squared residuals for Model 1 a few times now.

**4. Mean squared error**

Instead of using sum() in the summarize() call however, let's use the mean() function and assign this to mse, meaning mean squared error. This is the average squared error a predictive model makes. The closer your predictions y-hat are to the observed values y, the smaller the residuals will be, and hence the closer the MSE will be to 0. The further your predictions are, the larger the MSE will be. You observe an MSE of 0.0271, which is 585 divided by 21613, the total number of houses. Why is this called the MSE, and not the mean of squared residuals? No reason other than convention, they mean the same thing.Since the MSE involves squared errors, the units of MSE are the units of the outcome variable y squared. Let's instead obtain a measure of error whose units match the units of y.

**5. Root mean squared error**

You do this via the root mean squared error, or RMSE, which is the square-root of the MSE. Note the added mutate() line of code to compute the sqrt().This can be thought of as the "typical prediction error" our model will make and its units match the units of the outcome variable y. While the interpretation in our case of the units of log10 dollars might not be immediately apparent to everyone, you can imagine in many other cases it being very useful for these units match.

**6. RMSE of predictions on new houses**

Let's now assess the quality of the predictions of log10_price for the two new houses you saw in the previous video, whose information are saved in the data frame new-houses you created earlier.  Recall that you apply the get-regression-points function to model-price-3,

**7. RMSE of predictions on new houses**

but also with the newdata argument set to new_houses. You thus obtain predicted values log10_price_hat of 5.34 and 5.94. Now let's take this output and compute the RMSE

**8. RMSE of predictions on new houses**

by taking the residual, squaring them, taking the mean not the sum, and then square rooting.You get the following error message: it says the residual column is not found. Why is it not found? Because to compute residuals, you need both the predicted/fitted values y-hat, in this case log10-price-hat and the observed values y, in this case log10-priceBut if you don't have the latter, you can't compute the residuals, and hence you can’t compute the RMSE. This illustrates a key restriction in predictive modeling assessment: you can only assess the quality of predictions when you have access to the observed value y. You'll learn about a workaround to this issue shortly.

**9. Let's practice!**

But first some exercises: you'll compute on your own the RMSE, a measure of prediction error used for model assessment and selection.

## Computing the MSE & RMSE of a model

Just as you did earlier with \\(R^2\\), which is a measure of model fit, let's now compute the root mean square error (RMSE) of our models, which is a commonly used measure of preditive error. Let's use the model of price as a function of size and number of bedrooms.

The model is available in your workspace as `model_price_2`.

**Steps**

1. Let's start by computing the mean squared error (`mse`), which is the `mean` of the squared `residual`.


```{r}
# Get all residuals, square them, and take mean                    
get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals))
```

2. Now that you've computed the mean squared error, let's compute the root mean squared error.


```{r}
# Get all residuals, square them, take the mean and square root               
get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals)) %>%
  mutate(rmse = sqrt(mse))
```

Woo hoo! The RMSE is 0.167. You can think of this as the \\"typical\\" prediction error this model makes.

## Comparing the RMSE of two models

As you did using the sum of squared residuals and \\(R^2\\), let's once again assess and compare the quality of your two models using the root mean squared error (RMSE). Note that RMSE is more typically used in prediction settings than explanatory settings. 

`model_price_2` and `model_price_4` are available in your workspace.

**Steps**

1. Based on the code provided that computes MSE and RMSE for `model_price_2`, compute the MSE and RMSE for `model_price_4`.


```{r}
# MSE and RMSE for model_price_2
get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))

# MSE and RMSE for model_price_4
get_regression_points(model_price_4) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))
```

> *Question*
> ---
> <br>
> <br>
> ⬜ Since `model_price_2` had a higher `rmse` of 0.167, this is suggestive that this model has better preditive power.<br>
> ⬜ `rmse` doesn't tell us anything about predictive power.<br>
> ✅ Since `model_price_4` had a lower `rmse` of 0.166, this is suggestive that this model has better preditive power.<br>

Correct! RMSE can be thought of as the 'typical' error a predicive model will make.

## Validation set prediction framework

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Validation set prediction framework**

In this lesson I'll introduce the "validation set" prediction framework. This framework allows us to get a sense of how well a predictive model will perform, in our case, on new, previously unseen houses. This forms the backbone of a well-known machine learning method for model assessment called cross-validation.

**2. Validation set approach**

The underlying idea of the validation set approach is to first fit, or "train" a model on one set of data, but evaluate, or “validate" its performance on a different set of data. If you used the same data to both train your model and evaluate its performance, you could imagine your model easily being “overfit” to this data. In other words, you’d construct a model that's so overly specific to one dataset that it wouldn't generalize well to other datasets.

**3. Training/test set split**

Say your dataset has n observations. You randomly split the data into two sets: a training set in blue and a test set in orange. I'll use the blue observations to train or fit our model, then apply the model to get predictions y-hat for the orange observations.Then for these orange observations again, you'll assess these predictions y-hat by comparing them to the observed outcome variables y. By using independent training and test data as above, you can get a sense of a model's predictive performance on "new" data.

**4. Training/test set split in R**

Let's do this with some nifty dplyr functions. You first use sample_frac() with size set to 1 and replace set to FALSE to randomly sample 100% of the rows of house-prices without replacement. This has the effect of randomly shuffling the order of the rows.You then set the training data to be the 1st 10k rows of house-prices-shuffled using slice(). You similarly set the test data to be the remaining 11613 rows. Note that these two datasets have none of the original rows in common, and by randomly shuffling the rows before the slicing, we’ve effectively randomly assigned the rows to train and test. Also you're not limited to a rough 50/50 split between train and test as I just did; I only did this for simplicity.

**5. Training models on training data**

Let's fit the same regression model as earlier using log10_size and year as predictor variables, however we set the data to be train and not house-prices. Let's then output the regression table.You again obtain values for the intercept and the slopes for log10_size and year in the estimate column, but these values are slightly different than before when using all of the house-prices data, as they are based on a randomly chosen subset of points in the training data.

**6. Making predictions on test data**

Let's then apply this model to the test data to make predictions. In other words, take all 11613 houses in test and compute the predicted values log10-price-hat. Recall from earlier that you can do this quickly by using the get_regression_points() function with the newdata argument set to test. You observe a log10_price_hat column of predicted values and the corresponding residuals. Note that since you have both the predicted values y-hat, in this case log10-price-hat, AND the observed values y, in this case log10-price, you can compute the residuals in the final column.

**7. Assessing predictions with RMSE**

Let's now compute the root mean square error to assess our predictions as before: you first mutate() a new column of the squared residuals, then you summarize() these values with the square root of their mean, in this case in a single mutate step.The RMSE is 0.165. Let's now repeat this for the model that used condition instead of year and compare the RMSEs.

**8. Comparing RMSE**

You again fit the model to the training data and then use the get_regression_points() function with the newdata argument again set to test to make predictions, and compute the RMSE. This RMSE of 0.168 is larger than the previous one of 0.165, suggesting that using condition instead of year yields worse predictions.

**9. Let's practice!**

Now it's your turn. Let's use the validation set prediction framework to compare the predictive abilities of our other models for log10-price.

## Fitting model to training data

It's time to split your data into a *training* set to fit a model and a separate *test* set to evaluate the predictive power of the model. Before making this split however, we first sample 100% of the rows of `house_prices` *without* replacement and assign this to `house_prices_shuffled`. This has the effect of "shuffling" the rows, thereby ensuring that the training and test sets are *randomly* sampled.

**Steps**

1. Use `slice()` to set `train` to the first 10,000 rows of `house_prices_shuffled` and `test` to the remainder of the 21,613 rows.

```{r}
# Set random number generator seed value for reproducibility
set.seed(76)

# Randomly reorder the rows
house_prices_shuffled <- house_prices %>% 
  sample_frac(size = 1, replace = FALSE)

# Train/test split
train <- house_prices_shuffled %>%
  slice(1:10000)
test <- house_prices_shuffled %>%
  slice(10001:21613)

```

2. Now fit a linear regression to predict `log10_price` using `log10_size` and `bedrooms` using just the training data.


```{r}
# Set random number generator seed value for reproducibility
set.seed(76)

# Randomly reorder the rows
house_prices_shuffled <- house_prices %>% 
  sample_frac(size = 1, replace = FALSE)

# Train/test split
train <- house_prices_shuffled %>%
  slice(1:10000)
test <- house_prices_shuffled %>%
  slice(10001:21613)

# Fit model to training set
train_model_2 <- lm(log10_price ~ log10_size + bedrooms, data = train)
```

Fabulous! Since you've fit/trained the predictive model on the training set, let's now apply it to the test set to make predictions!

## Predicting on test data

Now that you've trained the model on the `train` set, let's apply the model to the `test` data, make predictions, and evaluate the predictions. Recall that having a separate `test` set here simulates the gathering of a "new" independent dataset to test our model's predictive performance on. 

The datasets `train` and `test`, and the trained model, `train_model_2` are available in your workspace.

**Steps**

1. Use the `get_regression_points()` function to apply `train_model_2` on your new dataset: `test`.

```{r}
# Make predictions on test set
get_regression_points(train_model_2, newdata = test)

```

2. Compute the root mean square error using this output.


```{r}
# Make predictions on test set
get_regression_points(train_model_2, newdata = test)

# Compute RMSE
get_regression_points(train_model_2, newdata = test) %>% 
  mutate(sq_residuals = residual^2) %>%
  summarize(rmse = sqrt(mean(sq_residuals)))
```

Magnificent! Your RMSE using size and condition as predictor variables is 0.167, which is higher than 0.165 when you used size and year built! It seems the latter is marginally better!

## Conclusion - Where to go from here?

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Conclusion - Where to go from here?**

Congratulations on completing Modeling with Data in the Tidyverse. In this course we leveraged the data science toolbox you developed in previous courses to perform exploratory data analysis, fit both explanatory and predictive models, and study methods for model assessment and selection.But where can we go from here?

**2. R source code for all videos**

First, a great way to learn how to code in a new language is to find code that you know works, then copy it, paste it, and tweak it to serve your goals. To help facilitate your new R modeling skills using the tidyverse, I've included all the R source code used in the videos for this course on GitHub. You can access this code using the URL above.

**3. Other Tidyverse courses**

Second, here is a link to other courses on DataCamp that use tidyverse. Being an effective data scientist requires you to develop a wide array of tools for your data science toolbox, and a lot of practice, practice, practice. These courses will help in this journey.

**4. Refresher: General modeling framework**

Perhaps the theory behind modeling interests you more. For example, recall our general modeling framework, which at its heart has a function f making explicit the relationship between y and x. We kept things simple and only studied models f where f was linear. But by no means is one restricted to such models! What do we mean?

**5. Parallel slopes model**

Recall our parallel slopes model for house price as a function of size and condition. But why restrict ourselves to parallel lines?

**6. Polynomial model**

Here we have something known as a polynomial model where we allow for curvature by incorporating log10-size squared as an explanatory/predictor variable! This gives our model more flexibility. Furthermore we're not restricted to models based on lines either! Yet another form of model are trees!

**7. Tree models**

Tree models are a form of triage. You start at the top of the tree, and based on answers to true/false questions, you progress down branches of the tree, where if the answer is true, you go left, if the answer is false, you go right. For example, say a house has log10_size = 3.2. Since 3.2 is &lt; 3.387, you first go down the left branch. Next, since 3.2 is !&lt; than 3.184, you then go down the right branch. This model's fitted/predicted value of this house's log10_price is 5.642, or about $438K. Repeating this triage for all 21k houses, there are 8875 houses that fall into this final branch.

**8. DataCamp courses using other models**

We've only scratched the surface of possible other models to consider. Here are other DataCamp courses you can take that consider more complex, but also potentially, more powerful models.

**9. Refresher: Regression table**

Finally, recall the regression table of the model of teaching score as a function of age. We only looked at values in the estimate column, like the negative slope for age of -0.006, suggesting that as professors age, they tend to have lower teaching scores. But what do the other columns tell us?They speak to the "statistical significance" of our results. For example, can we conclusively say that age and score are negatively related for ALL instructors, or was this relationship just a fluke occurrence for these 463 instructors? How would these results differ if we selected 463 different instructors? To be able to answer questions like these, we need to understand statistical inference.

**10. ModernDive: Online textbook**

If you are interested in statistical inference, we suggest you check out moderndive, an electronic textbook that Chester Ismay and I co-authored. -moderndive uses the same tidyverse tools as in this course-expands on the regression models from this course and others-uses the evals and house_prices datasets and others-all towards the goal of learning statistical inference via data science.-It's available at moderndive.com

**11. Good luck!**

I hope you had fun in this course, and continue to enjoy your data science journey! A special thanks goes to any student who's ever taken a class with me; I couldn't have created this course without you.

