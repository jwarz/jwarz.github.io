---
title: "Introduction to A/B Testing in R"
author: "Joschka Schwarz"
---

```{r}
#| include: false
source(here::here("R/setup-ggplot2-tie.R"))
options(dplyr.summarise.inform = FALSE)
```

In this course, you will learn the foundations of A/B testing, including hypothesis testing, experimental design, and confounding variables. You will also be exposed to a couple more advanced topics, sequential analysis and multivariate testing. The first dataset will be a generated example of a cat adoption website. You will investigate if changing the homepage image affects conversion rates (the percentage of people who click a specific button). For the remainder of the course you will use another generated dataset of a hypothetical data visualization website.

# 1. Chapter 1: Mini case study in A/B Testing

Short case study on building and analyzing an A/B experiment.

## Introduction

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Introduction**

Hi! My name is Page Piccinini. I'm a data scientist, and I'll be your instructor for this course on A/B Testing in R. A/B testing is a powerful way to try out a new design or program changes before making final decisions. In this course, we'll go over the fundamentals of A/B testing so you can get started on building and analyzing your own A/B experiments.

**2. Why A/B testing**

Before getting into A/B testing, let's talk about what it is and why it's useful for you.A/B testing is a framework for you to test different ideas for how to improve upon an existing design, often a website. With A/B testing you're able to take a set of new ideas,

**3. Why A/B testing**

test them with a new experiment,

**4. Why A/B testing**

statistically analyze the results to confidently say which idea is better,

**5. Why A/B testing**

update your website or app to use the winning idea,

**6. Why A/B testing**

and then continue the cycle over again. What's key to remember is A/B testing is not something you do just once. You want to be constantly updating your website or app to maximize things like conversion rates or usage time. With A/B testing you will always be making minor updates to push those numbers up.While A/B testing is often discussed in the context of websites and tech, really it can be used in any context where you have a question you want to test and then make updates accordingly. A/B testing is just experimental design. You could A/B test two different fertilizer types in your garden, or secretly test two coffee brands at work to see which people like more. The world is your A/B testing playground! In Chapter 3 I'll go over some more example uses of A/B testing.Now, let's walk through a simplified set of steps with a hypothetical experiment. In future chapters, we'll see how A/B testing can be more complicated than our hypothetical example here. We'll be covering A/B testing concepts in more depth in Chapters 3 and 4.

**7. Cat adoption site picture**

Let's say you run a cat adoption website. Right now your homepage looks like this.You want to know if a different homepage picture would result in more visitors clicking the "ADOPT TODAY!" button. This is also referred to as "conversion rate". If someone clicks you say they "converted". The conversion rate is generally the number of people who did an action (for example, clicked a button) divided by the number of people who went to the page. In our case, to test this you need two conditions: one, a control (your current photo),

**8. Cat adoption site picture**

and two, a test (a new photo). For your test photo, you decide to use this photo. Your hypothesis is that seeing a cat in a hat will make people more likely to want to adopt.

**9. Variables**

Let's go over the variables that you know:You have your question, will changing the homepage photo result in more "ADOPT TODAY" clicks, and your hypothesis, using a photo of a cat wearing a hat will result in more "ADOPT TODAY!" clicks.You also have your dependent variable, whether a person clicked the "ADOPT TODAY!" button or not, and your independent variable, the Homepage photo, either the control photo (no hat) or the test photo (hat).By building up from question to independent variable, we know exactly what we're asking, and we can already see the shape of our experiment for how to answer our question.

**10. Preliminary dataset**

Before we start building our experiment, we want to know what our conversion rates looked like before changing anything. Let's take a quick look at that dataset.Here I'm using the suite of packages called the tidyverse. It should be familiar to you from DataCamp's courses on the tidyverse.From the tidyverse we'll use the function read_csv() from the readr package to load our data, here called click_data.If we look at the first few rows we can see that we have two columns: one, visit_date, which gives the day when someone visited the website, and two, clicked_adopt_today, which is a 1 if someone clicked on the button, and a 0 if they didn't.

**11. Let's practice!**

Okay, now that we have the basic motivations of A/B testing let's practice what we've learned and take another look at our preliminary dataset.

## Goals of A/B testing

The "hypothesis" for an A/B testing experiment refers to?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ The variable you are going to measure.<br>
> ⬜ The problem you are interested in testing.<br>
> ⬜ The variable you are going to differ between conditions.<br>
> ✅ What you think will happen as a result of the experiment.<br>

The **hypothesis** is your predictions for how the results of the experiment will turn out.

## Preliminary data exploration

In the video, we read in our preliminary dataset and had a quick look at it. Let's get some practice reading in data and do a little more exploration.

**Steps**

1. Load the `tidyverse` package.
2. Read in the CSV `click_data.csv` with `read_csv()`.
3. Take a look at the data. As you can see, it has two columns: 1) `visit_date` for the date when a person visited the page, and 2) `clicked_adopt_today`, which is 1 if they clicked the button and 0 if they didn't.
4. Find oldest and most recent date in the data using `min()` and `max()` on the `visit_date` column.

```{r}
# Load tidyverse
library(tidyverse)

# Read in data
click_data <- read_csv("data/click_data.csv")
click_data

# Find oldest and most recent date
min(click_data$visit_date)
max(click_data$visit_date)
```

Well done! We now know that the first date in the dataset is January 1, 2017, and the oldest December 31, 2017.

## Baseline conversion rates

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Baseline Conversion Rates**

In the previous lesson, we learned some of the principles of A/B testing and took a look at our preliminary dataset. Let's spend some more time looking at our pre-experiment, or baseline values for an experiment.

**2. Why baseline values?**

Before starting any A/B testing experiment you'll want to know your baseline value, or the current value before any experimental changes happen. Why is this? Well, let's come back to our hypothesis.We said we expect a different photo to result in more conversions,

**3. Why baseline values?**

but what does "more" really mean in this context?Is it

**4. Why baseline values?**

compared to the conversion rate in the last year?

**5. Why baseline values?**

today?

**6. Why baseline values?**

next week?

**7. Why baseline values?**

or what about relative to when the experiment is actually run? If you're not planning to run your experiment for a couple of months there could be other factors that change your conversion rates between now and when the experiment is run.To have a clearly defined hypothesis and experiment you need to know what your baseline for comparison is, otherwise you can't really know if your experiment had an effect or not.

**8. Current conversion rate**

For our experiment, to start, we'll compute the current, pre-experiment conversion rate over all of time.As mentioned earlier, for most of our analyses we'll use a suite of packages referred to as the tidyverse. Here, these packages will help us manipulate and plot our baseline data.We'll also read in our click_data just as we did in the previous exercises.From here we can find the mean of our clicked_adopt_today column to see what percentage of the time people clicked, also known as our conversion rate. We can do that with the dplyr function summarize, using the pipe to connect our data to the function. We then use the mean() function to compute the conversion rate (averaging the 1s and 0s in the clicked_adopt_today column).If we look at the value of this new summarized column we see that it is 0-point-277, so a conversion rate of 27-point-7%. Meaning about 27 out of 100 visitors to the website clicked "ADOPT TODAY" with the current homepage picture.

**9. Current conversion rate seasonality**

We've successfully computed our pre-experiment conversion rate. However, we computed the conversion rate over the entire year. Maybe in certain months, people are more likely to adopt than in other months. We'll compute conversion rates for each month to see if there is an effect of seasonality.Instead of summarizing over the entire dataset

**10. Current conversion rate seasonality**

we'll add a group_by() from dplyr by month, so we can find the conversion rate from each month of the year. Currently, the visit_date column gives dates up to the day of the month. To round off just to the month we'll use the lubridate package, and the function month.Now instead of getting a single number, our output is a conversion rate for each month of the year, as we can see in the updated dataframe.To really understand how conversion rates change throughout the year, it's useful to plot our data. We can do that with the package ggplot2 from the tidyverse.

**11. Plotting current conversion rate seasonality**

First we'll save our summarized data in a data frame click_data_sum. Then we'll use that new data frame in a ggplot call, setting the x-axis to our month column and the y-axis to our computed conversion rate column.To display the data we'll plot the dots and connect them by a line with geom_point() and geom_line().

**12. Plotting current conversion rate seasonality**

From this plot, it's clear that conversion rates are not steady across the year. Instead, rates are much higher in the summer months and at the end of the year than during the rest of the year.

**13. Let's practice!**

In the following exercises we'll get some practice looking at baseline values.

## Current conversion rate day of week

Just as we did in the slides, let's start by computing our pre-experiment conversion rate as a baseline. Rather than computing it by month though, let's compute by day of the week. The core `tidyverse` packages and `lubridate` are pre-loaded for you.

**Steps**

1. Read in the CSV `click_data.csv` using the `read_csv()` function.
2. Use the function `wday()` from `lubridate` to compute the conversion rate by day of the week.
3. Take a look at the data to see how much conversion rates vary throughout the week.

```{r}
# Package
library(lubridate)

# Read in the data
click_data <- read_csv("data/click_data.csv")

# Calculate the mean conversion rate by day of the week
click_data %>%
  group_by(wday(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))
```

It looks like conversion rates vary a little bit from day to day, but not too much.

## Current conversion rate week

Let's try one more grouping for our data and see how much conversion rates vary by week in the calendar year.

**Steps**

1. Use the `group_by()` function to compute conversion rates by week.
2. Use the `week()` function to separate dates into weeks of the year.
3. Compute the `mean()` over the correct column in the dataframe.

```{r}
# Read in the data
click_data <- read_csv("data/click_data.csv")

# Calculate the mean conversion rate by week of the year
click_data %>%
  group_by(week(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))
```

Great! You've now looked at four different ways to compute baseline conversion rates: overall, by month, by day of the week, and by week of the year.

## Plotting conversion rate seasonality

Earlier we said that based on the summary by month, it looks like there are seasonal effects. Conversion rates are higher in the summer and in December. Let's visualize that effect, but by week of the year. We'll use `ggplot2` to build our plot. The `tidyverse` packages and `lubridate` are pre-loaded for you.

**Steps**

1. Save your data frame with conversion rates by week of the year to a data frame called `click_data_sum`.
2. Use the newly created data frame to make a plot where the x-axis is the week of the year and the y-axis is the conversion rate.
3. Use the `geom_point()` and `geom_line()` geoms to build the plot.

```{r}
# Compute conversion rate by week of the year
click_data_sum <- click_data %>%
  group_by(week(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Build plot
ggplot(click_data_sum, aes(x = `week(visit_date)`,
                           y = conversion_rate)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1),
                     labels = scales::percent)
```

Fantastic! We now have a plot to visualize our knowledge about seasonal conversion rates by week of the year.

## Experimental design, power analysis

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Designing an Experiment - Power Analysis**

Now that we have a good sense of our baseline numbers we're ready to design our experiment. Here we'll use our knowledge of seasonality along with power analysis to figure out how long we need to run our experiment.

**2. Conversion rate seasonality**

In preparing our experiment we learned about historical conversion rates. On average conversion rates are about 28%, but that can change throughout the year.What does this mean for building our experiment? Well it would be bad to run the

**3. Conversion rate seasonality**

control condition in August and the

**4. Conversion rate seasonality**

test condition September, because the control may look better simply due to seasonality, not because it's actually a better condition.This is why A/B experiments try to run both conditions

**5. Conversion rate seasonality**

simultaneously, to ensure both conditions are exposed to similar seasonal variables.We also need to consider seasonal effects for knowing how we expect our control condition to perform. If the experiment is run in

**6. Conversion rate seasonality**

January we expect the control to have a conversion rate of roughly 20%, but if it's run in

**7. Conversion rate seasonality**

August the control should be closer to 50%.With this knowledge, we use a power analysis to determine how long we should run our experiment.

**8. Power analysis**

Experiment length is one of the big questions in A/B testing. If you stop too soon you may not get enough data to see an effect. Too long and you may waste valuable resources on a failed experiment.One way to safeguard against this is with a power analysis. A power analysis will tell you how many data points (or your sample size) that you need to be sure an effect is real. Once you have your sample size, you can figure out how long you will need to run the experiment to get your number of required data points. This will depend on variables such as how many website hits you get per day. Running a power analysis is also good because it makes you think about what statistical test you want to run before starting data collection.When running a power analysis you should know:one, the planned statistical test,two, the value of the control condition,and three, the *desired* or *expected* value of the test condition.You also need to know:one, the proportion of the data from the test condition (ideally 0.5, or half), two, the significance threshold or alpha (generally 0.05),and three, the power (generally 0.8). Terms such as "alpha" and "power" should be familiar to you already from DataCamp's course on experimental design.

**9. Power analysis in R**

There are several packages you can use to run a power analysis in R. Here we'll use the `powerMediation` package.The first thing we need to decide is what statistical test we'll be running. Since the value we're collecting is binary (clicked or didn't click) we'll run a logistic regression.To run a power analysis for a logistic regression we'll use the function `SSizeLogisticBin`. We'll also save the result of our power analysis to a variable `total_sample_size`.Now we need to fill in each of the pieces of our equation to get our final sample size. We'll work backwards to figure out each of our variables.

**10. Power analysis in R**

For sample proportion (beta), alpha, and power we'll use the most common values (0.5, 0.05, and 0.8).

**11. Power analysis in R**

For our conversion rate for our control condition (p1) let's say we're going to run the experiment starting in January, so we expect roughly a 20% conversion rate. Now the last and hardest part is deciding the expected conversion rate for the test condition (p2). Normally this is backed by previous data, but for now let's guess and dream big. We'll say a conversion rate of 30%, a 10% boost.We see we need 587 data points in total, or roughly 294 per condition.

**12. Let's practice!**

Let's get some more practice running power analyses.In Chapter 2 we'll get some results from our experiment and get to analyze them with a logistic regression in R.

## Randomized vs. sequential

You're designing a new experiment and you have two conditions. What's the best method for comparing your two conditions?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ Run control condition for a month and then test condition for a month.<br>
> ⬜ Use old data as control condition and run test condition for a month.<br>
> ✅ Run control and test conditions simultaneously for two months.<br>
> ⬜ Run control condition for a month, wait a year, then run test condition for a month.<br>

Running both conditions simultaneously is best because they are exposed to the same seasonal and other variables. Waiting a year would require too much time to analyze your experiment, and a lot of other variables can change in a year!

## SSizeLogisticBin() documentation

Let's take a moment to learn more about the `SSizeLogisticBin()` function from `powerMediation` that was introduced in the slides. Look at the documentation page for the `SSizeLogisticBin()` function by calling `help(SSizeLogisticBin)`. The `powerMediation` package is pre-loaded for you. What is another way to phrase what `p2` signifies?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ The probability when `X = 0` (the control condition).<br>
> ✅ The probability when `X = 1` (the test condition).<br>
> ⬜ The proportion of the dataset where `X = 1`.<br>
> ⬜ The Type I error rate.<br>
> ⬜ The power for testing if the odds ratio is equal to one.<br>

Correct! The argument `p2` is the expected value for the test condition.

## Power analysis August

In the video, we ran a power analysis assuming we were going to run the experiment in January. Run a new power analysis assuming we'll run the experiment in August. To compute the conversion rate for our control and test you'll need to look at the dataset. Be sure to round all values to the hundredth of a percent (e.g., `0.13453` to  `0.13`). The data `click_data_month` is available pre-loaded for you to look up the conversion rate for August.

**Steps**

1. Fill in the proportion of the sample from the test condition (`B`), the alpha (`alpha`), and the power (`power`).
2. Fill in the conversion rate for our control condition using the conversion rate for August (`p1`). Remember to round. 
3. Fill in the expected conversion rate for our test condition (`p2`), assuming a 10 percentage point increase (`p1` + 0.1). Remember to round.
4. Look at `total_sample_size` to see how many data points you need in total across both conditions.

```{r}
# Load powerMediation
library(powerMediation)

# Compute and look at sample size for experiment in August
total_sample_size <- SSizeLogisticBin(p1 = 0.54,
                                      p2 = 0.64,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

Great! We now know if we ran the experiment in August we would need at least 758 data points or 379 per group.

## Power analysis August 5 percentage point increase

Let's say you've reconsidered your expectations for running the experiment in August. 54% is already a really high conversion rate, so increasing it by 10 percentage points may be difficult. Rerun your power analysis assuming only a 5 percentage point increase in your conversion rate for the test condition.

**Steps**

1. Update your expected value to only be a 5 percentage point increase over conversion rates in August.
2. Fill in the argument names for proportion, alpha, and beta.
3. Take a look at your sample size to see how they've changed.

```{r}
# Load powerMediation
library(powerMediation)

# Compute and look at sample size for experiment in August with a 5 percentage point increase
total_sample_size <- SSizeLogisticBin(p1 = 0.54,
                                      p2 = 0.59,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

Wow! If we only expect a 5 percentage point conversion rate increase we need 3085 (about 1543 per group) to see a significant effect, much more than if we predicted a 10 percentage point increase.

# 2. Chapter 2: Mini case study in A/B Testing Part 2

In this chapter we'll continue with our case study, now moving to our statistical analysis. We'll also discuss how to do follow-up experiment planning.

## Analyzing results

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Analyzing Results**

Now that we've run our power analysis, we can run our experiment and get the chance to analyze our results!

**2. Experiment results**

Let's start by taking a look at the data.  As always we start by loading the `tidyverse`.And we'll load our data saved in the dataframe "experiment_data". If we take a look at the data, we see that there is one more column than we had Let's start by taking a look at the data.  As always we start by loading the `tidyverse`.And we'll load our data saved in the dataframe "experiment_data". If we take a look at the data,before: `condition`, which tells us whether the homepage picture was from the control or the test condition.Before we run our statistical test let's take a look at the conversion rates for the two conditions. We'll do this by

**3. Experiment results**

grouping by `condition` and computing the conversion rate, just as we have previously. The conversion rate for the control condition was about 17%, while for the test condition was about 38%. The difference looks pretty large, but let's see now if it's actually statistically different.

**4. Plotting results**

Before we statistically analyze our results though, it's useful to also plot them, to see if there are any visual differences. We already computed conversion rates over the entire length of the experiment, but what about by day of the experiment? Let's update our summary call to group by both `condition` and

**5. Plotting results**

`visit_date`. Let's also save this summary

**6. Plotting results**

to a new dataframe `experiment_data_sum` so that we can use it for plotting. Now we can start to build our plot. For the most part, this will look the same as past plots we've built.We initialize our `ggplot()`, plot `visit_date` on the x-axis and `conversion_rate` on the y-axis and add points and a line. To do so, we just have to add two more parameters to the aesthetics,

**7. Plotting results**

`color` and `group`, both of which are set to `condition`. This will make two lines on the plot, one for our control condition and one for our test condition, each a different color. By setting group we also make sure the points for the same condition are connected, rather than for example, the points for the same `visit_date` being connected.

**8. Plotting results**

If we take a look at our plot, we see that the test condition pretty consistently had higher conversion rates than our control condition on each day the experiment was run.Let's move to confirming this statistically.

**9. Analyzing results**

To start, in addition to loading `tidyverse` we'll also be using the package `broom`. We'll discuss why in a minute. First, we had decided earlier to analyze our results with a logistic regression. In R this is done with the `glm()` function.On the left side of the tilde is

**10. Analyzing results**

the dependent variable, in this case `clicked_adopt_today`, and on the right side is

**11. Analyzing results**

the independent variable, `condition`.

**12. Analyzing results**

We set family to `binomial` to make clear what kind of distribution to use,

**13. Analyzing results**

and then provide the dataframe to pull the data from. We'll also use the function

**14. Analyzing results**

`tidy` from the `broom` package to see a cleaned up version of our results. We can use a pipe to send the model result directly into `tidy`.Based on the results of the model there is a significant effect of condition. If you look at the row for `conditiontest` you'll see that the p dot value column has a value less than 0.05. Based on looking at the estimate column for that same row, we know that the test condition had a higher conversion rate. The model estimate is a positive estimate of roughly 1.14, showing that the test condition had a higher conversion rate than control. We can then say our experiment was a success!

**15. Let's practice!**

In the following exercises we'll get some practice taking these variables into account when designing and analyzing our experiment.

## Plotting results

It turns out there was a bug on January 10th and all of the data from that day was corrupted. A new data frame was created for you called `experiment_data_clean`, which drops the corrupted data. Re-build our plot showing the two conversion rates, one for the test condition and one for the control condition. The `tidyverse` and the updated data set are pre-loaded for you.

**Steps**

1. Create a new data frame called `experiment_data_clean_sum` that groups by both `condition` and `visit_date`.
2. Use the new data frame to build the plot, setting `color` and `group` to `condition`.

```{r}
# data
experiment_data <- read_csv("data/experiment_data.csv")
experiment_data_clean <- experiment_data |> filter(visit_date != "2018-01-10")

# Group and summarize data
experiment_data_clean_sum <- experiment_data_clean %>%
  group_by(visit_date, condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Make plot of conversion rates over time
ggplot(experiment_data_clean_sum,
       aes(x = visit_date,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point() +
  geom_line()
```

It looks like our results were pretty consistent over the month. The test condition almost always performed better.

## glm() documentation

To analyze our results, we used the function `glm()` and set `family` to `binomial`. Take a look at the documentation to using `?glm` to see what exactly the `family` argument is.

> *Question*
> ---
> ???<br>
> <br>
> ⬜ An optional vector of ‘prior weights’ to be used in the fitting process.<br>
> ⬜ A symbolic description of the model to be fitted.<br>
> ✅ A description of the error distribution and link function to be used in the model.<br>
> ⬜ A list of parameters for controlling the fitting process.<br>

Correct! The argument `family` can be used to express different error distributions.

## Practice with glm()

In the video, we analyzed our results with a logistic regression, using the function `tidy()` from the `broom` package to see a cleaned up version of our results. Run the same analysis using our new data frame `experiment_data_clean`.

**Steps**

1. Use the newly created `experiment_data_clean` to find the conversion rates for each `condition`.
2. Using the `glm()` function, run a logistic regression of `condition` by `clicked_adopt_today`.

```{r}
# Load package for cleaning model results
library(broom)

# View summary of results
experiment_data_clean %>%
  group_by(condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
experiment_results <- glm(clicked_adopt_today ~ condition,
                          family = "binomial",
                          data = experiment_data_clean) %>%
  tidy()
experiment_results
```

Great! Even dropping a day's worth of data our result was still significant. However, if this happened in real life we really should have run the experiment for another day to make sure we got the correct number of data points needed according to the power analysis.

## Designing follow-up experiments

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Designing Follow-up Experiments**

Congratulations! You've now successfully run your first experiment.

**2. Cycle of past steps**

Let's revisit our cycle from back in Chapter 1.First you thought of an idea to test.

**3. Cycle of past steps**

Next you ran the experiment.

**4. Cycle of past steps**

Then you statistically analyzed the results.

**5. Cycle of past steps**

Given that our cat in a hat resulted in a higher conversion rate, you should update the homepage to use the winning picture.

**6. Cycle of past steps**

And finally, since A/B testing is a continuous loop, it's time to repeat the process again and design a new experiment to see what else can increase conversion rates.Maybe you want to see if a kitten would drive higher conversion rates than an adult cat? What about two kittens? What about kittens wearing sweaters in addition to hats? All of these are great and testable ideas, but what's most important now is to remember to take it one step at a time. The worst thing people can do in a follow-up experiment is try and test too many ideas at once. If the experiment works, you don't know why, and if it fails you're not sure which variable or variables were worse than your control.

**7. Tips for designing a new experiment**

When thinking of follow-up experiments it's fine to brainstorm a bunch of ideas, but try and focus on small changes. Which change do you think is most important to try first? You can make a plan of several small experiments, but it's important that they really are unique experiments each with their own control group to compare to.One thought you may be having now is, "well in our previous experiment the only difference wasn't the cat wearing a hat, a lot of things changed about the photo". You're absolutely correct. In a real experimental context, our experiment wouldn't have be great because there were too many other differences, orconfounding variables. We'll talk more about confounding variables in a future lesson.We can't say for certain that the hat really made the difference. Maybe it was the way the cat was sitting, or the background colors. In a real world experiment we'd probably start with a much smaller change if we want to say that "hats" are the key variable that boost conversion rates. We'll talk about this more in future chapters.

**8. Follow-up experiment #1**

For the sake the exercise though, let's say we are confident that the hat really made the difference. There's two things we want to test:one, using a picture of a kitten instead of an adult cat,and two, using a picture of two cats or kittens instead of one.

**9. Let's practice!**

Let's build our next experiments.

## Follow-up experiment 1 design

You're now designing your follow-up experiments. What's the best path forward?

> *Question*
> ---
> ???<br>
> <br>
> ✅ Build one experiment where your test condition is a kitten in a hat. If the experiment works, run a second experiment with a kitten in a hat as the control and two kittens in hats as the test.<br>
> ⬜ Build one experiment where your control is the cat in a hat and the test is two kittens in hats.<br>
> ⬜ Build one experiment but with three conditions. Control is the cat in a hat, test one is a kitten in a hat, and test two is two kittens in hats.<br>
> ⬜ Build one experiment but with three conditions. Control is the cat in a hat, test one is a kitten in a hat, and test two is two adult cats in hats.<br>

Running two sequential experiments is preferred because you can be confident in which variable drove the effect, and be sure to have one control group per test condition.

## Follow-up experiment 1 power analysis

Let's start your kitten experiment. The hat already increased conversion rates a lot, but you think making the photo a kitten will really make the difference, so you think conversion rates will go up to 59%. Let's run a power analysis to see how much data you need to see a significant effect.

**Steps**

1. Load the powerMediation package.
2. Set `p1` to the conversion rate you found in the previous experiment (39%).
3. Set `p2` to the expected conversion rate with the picture of a kitten in a hat.
4. Name the third, fourth, and fifth arguments that has values of 0.5, 0.05, and 0.8 respectively.

```{r}
# Load package for running power analysis
library(powerMediation)

# Run logistic regression power analysis
total_sample_size <- SSizeLogisticBin(p1 = 0.39,
                                      p2 = 0.59,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

Good job! Turns out we only need 194 data points in total (97 per group) since our expected effect is so large.

## Follow-up experiment 1 analysis

Based on your power analysis, you have decided to run your experiment and now have data to analyze.

The `tidyverse` packages that you need for the exercise (`readr` and `dplyr`) and `broom` have been pre-loaded for you.

**Steps**

1. Read in the data from your new experiment `followup_experiment_data.csv`. \n**Note**: instead of `condition` having `control` and `test`, the levels are `cat_hat` and `kitten_hat`.
2. **Note**: instead of `condition` having `control` and `test`, the levels are `cat_hat` and `kitten_hat`.
3. Compute the conversion rate for the two conditions. Notice anything unexpected? (Hint, look at `visit_date`.)
4. Run the logistic regression to see if there is an effect.

```{r}
# Package
library(tidyr)

# Read in data for follow-up experiment
followup_experiment_data <- read_csv("data/followup_experiment_data.csv")

# View conversion rates by condition
followup_experiment_data %>%
  group_by(condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
followup_experiment_results <- glm(clicked_adopt_today ~ condition,
                                   family = "binomial",
                                   data = followup_experiment_data) %>%
  tidy()
followup_experiment_results
```

You correctly found that the follow-up experiment didn't work (our p-value was about 0.24, which is not less than 0.05). This could be because kittens aren't actually that desirable, or because we went in with bad assumptions. We found our conversion results in our first experiment in January, but ran our second experiment in August, when conversion rates were already high. Remember to always consider what 'control' really means when building your follow-up experiments.

## Pre-follow-up experiment assumptions

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Pre-follow-up Experiment Assumptions**

We successfully ran a second experiment, but we went into it with the wrong assumptions. We ran our power analysis and expectations for the condition with a cat in a hat (our old test condition) based on data from January, but our follow-up experiment was run in August. Let's take some time to rethink how we can run our follow-up experiment.

**2. Old conversion rates**

Let's bring back our plot on conversion rates from our original control. As discussed before, the conversion rate for August is very different from the conversion rate for January. When we ran our our follow-up experiment we ran it in August rather than in January, but didn't update our assumptions.Remember though, this is for our original control condition, the picture what a cat with no hat. How does this compare to the updated homepage, our new control of the cat with a hat, that we've been running for the last six months?

**3. Let's practice!**

Let's spend some time looking at our new data from the last six months, before jumping into the second experiment.

## Plot 8 months data

Before starting the next experiment, let's take a second to look at the data so far from our original two conditions. Even though the cat in the hat did better, you decided to keep running both versions so you could see how results compared over more time. All necessary libraries and the data frame `eight_month_checkin_data` has been pre-loaded for you.

**Steps**

1. Make a new column called `month_text` so we have the months formatted more nicely. Use the `mutate()` function and the `visit_date` column to create `month_text`.
2. Use our newly created column in the `group_by()` to find out conversion rates per month for each `condition`.
3. Use our new column as the `x` value in the plot and update the `color` and `group` aesthetics to show different lines for each `condition`.

```{r}
# Data
eight_month_checkin_data <- read_csv("data/eight_month_checkin_data.csv")

# Compute monthly summary
eight_month_checkin_data_sum <- eight_month_checkin_data %>%
  mutate(month_text = month(visit_date, label = TRUE)) %>%
  group_by(month_text, condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Plot month-over-month results
ggplot(eight_month_checkin_data_sum,
       aes(x = month_text,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point() +
  geom_line()
```

It looks like conversion rates have been consistently higher for our cat in a hat condition.

## Plot styling 1

This plot actually looks pretty nice and could be useful to share with someone else either on or outside of your team. Let's take some time to clean it up a bit. Some of these functions should be familiar from earlier. The summarized data and packages have been pre-loaded for you.

**Steps**

1. Use the function `scale_y_continuous()` to modify how the y-axis is displayed.
2. Set the `limits`so the plot displays 0% to 100%.
3. Update the x-axis name with to say `Month`.
4. Update the y-axis name with to say `Conversion Rate`.

```{r}
# Plot monthly summary
ggplot(eight_month_checkin_data_sum,
       aes(x = month_text,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1),
                     labels = scales::percent) +
  labs(x = "Month",
       y = "Conversion Rate")
```

Our plot is looking much nicer now!

## Plot styling 2

Let's do just a couple more updates and then we'll be ready to hand off our plot to others to show how great our first experiment was.

**Steps**

1. Make the dots larger by setting the `size` parameter in `geom_point()`. It should be equal to `4`.
2. To make the lines thicker, use the `lwd` parameter in `geom_line()`. Set the line thickness to `1`.

```{r}
# Plot monthly summary
ggplot(eight_month_checkin_data_sum,
       aes(x = month_text,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point(size = 4) +
  geom_line(lwd = 1) +
  scale_y_continuous(limits = c(0, 1),
                     labels = scales::percent) +
  labs(x = "Month",
       y = "Conversion Rate")
```

Great! Now we can see a lot more information about what's going on before getting ready for our next experiment.

## Follow-up experiment assumptions

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Follow-up Experiment Assumptions**

We've now learned a lot about how our first experiment has been performing. Let's get a little more information and then we'll be ready to jump into our next experiment.

**2. 8 month checkin**

In the previous exercises, we built this plot, which showed that our conversion rates have been consistently higher with our new picture. But has it been higher by the same rate each month? Let's double check that our increased conversion rate really has been consistent.

**3. Computing conversion rate difference**

Earlier, we computed the conversion rate for each month for each condition. Now we want to see what the difference was between each condition for every month. We'll start by making a new data frame

**4. Computing conversion rate difference**

called `eight_month_checkin_data_diff`. Next we can use the `spread()` function from `tidyr` to change the format of our data.The `spread()` function will turn levels in a column into actual columns. So instead of having a column called `condition` with levels `cat_hat` and `no_hat`, we now have two columns (one for each level of the old column `condition`) and the values for each column are the conversion rates. So, instead of having two rows for January, one for the `cat_hat` condition, and one for the `no_hat` condition, we have one row.

**5. Computing conversion rate difference**

With our data reformatted we can make a new column.

**6. Computing conversion rate difference**

Our new column, called `condition_diff` subtracts the `cat_hat` and `no_hat` columns from each other to see how much our conversion rate increased. We see that the difference hasn't been exactly the same each month, which is to be expected.Okay, the last thing we want to do now, is to see: one, what the typical difference has been, and two, how consistent the effect was.

**7. Computing conversion rate difference**

If we take the mean and standard deviation of our new column we see that on average there has been a difference of 19%, and a standard deviation of about 4%, suggesting that the effect is pretty consistent.If our effect hadn't been consistent we should take some time to think about why that is. It would require some additional data exploration. Maybe a different feature was optimized through another A/B experiment? There's no clear cutoff for what to call a "consistent" effect. The more time you spend with your data though, the better sense you should have about when something is off and more digging is needed.

**8. Let's practice!**

Now that we know our increase in conversion rate has been pretty consistent, let's go back to our second experiment.

## Conversion rate between years

In the video, I computed the conversion rate between our no-hat condition and our hat condition for the most recent year. We should also check if conversion rates have changed **between** years for our no-hat condition. The dataset with both years worth of data summarized by month and year and any packages are pre-loaded for you. You can find the data in `no_hat_data_sum`.

**Steps**

1. Fill in `year` and `conversion_rate` in `spread()` to make one column per year.
2. Create a new column called `year_diff` that is the difference between the columns ``2017`` and ``2018``.
3. Take a look at the data. Notice that there are missing values for 2018.
4. Compute the `mean()` for the `year_diff` column and update the `sd()` function to be sure you're not including the months with missing data for 2018.

```{r}
no_hat_data_sum <- read_csv("data/no_hat_data_sum.csv")

# Compute difference over time
no_hat_data_diff <- no_hat_data_sum %>%
  spread(year, conversion_rate) %>%
  mutate(year_diff = `2018` - `2017`)
no_hat_data_diff

# Compute summary statistics
mean(no_hat_data_diff$year_diff, na.rm = TRUE)
sd(no_hat_data_diff$year_diff, na.rm = TRUE)
```

Looks like our conversion rates have been pretty consistent for the no_hat condition. This is good! It means we can reference last year's data for months that haven't happened yet.

## Re-run power analysis for follow-up

Let's rerun our power analysis for our new experiment now taking into consideration the time of year we're running our new experiment: September. To figure out our baseline assumptions, we'll give you some introductory information: 1) the conversion rate for the "no hat" condition in 2017 was 30% (or 0.3), and 2) the average difference between the "no hat" condition and the "cat hat" condition is 19% (0.19). Use this information to run an updated power analysis.

**Steps**

1. Fill in the value for `p1` (our expected conversion rate for "cat hat" condition in September). Do this by summing the conversion rate for "no hat" condition (`0.3`), and the difference between the conditions (`0.19`).
2. Fill in the value for `p2` (our expected conversion rate for "kitten hat" condition in September) assuming an increase of 15 percentage points (0.15).
3. Fill in the parameter names of other values. Remember you can always do `help(SSizeLogisticBin)` if you've forgotten.

```{r}
# Load package for power analysis
library(powerMediation)

# Run power analysis for logistic regression
total_sample_size <- SSizeLogisticBin(p1 = 0.49,
                                      p2 = 0.64,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

Now our power analysis says we need 341 data points in total, much higher than when we ran it before.

## Re-run glm() for follow-up

Now that we updated our power analysis, I ran our experiment for September, pulling 171 data points per condition. The data has been pre-loaded for you in the data frame `followup_experiment_data_sep`.

**Steps**

1. Compute the `conversion_rate` for the two conditions using the `summarize()` function.
2. Fill in the dependent and independent values for the `glm()`.
3. Use the `tidy()` function from `broom` to look at a cleaned up version of the result.

```{r}
# Data
followup_experiment_data_sep <- read_csv("data/followup_experiment_data_sep.csv")

# Load package to clean up model outputs
library(broom)

# View summary of data
followup_experiment_data_sep %>%
  group_by(condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
followup_experiment_sep_results <- glm(clicked_adopt_today ~ condition,
                                       family = "binomial",
                                       data = followup_experiment_data_sep) %>%
  tidy()
followup_experiment_sep_results
```

Our follow-up experiment was successful! Now that we pulled the correct number of data points, we can see that there is a boost by using a kitten over a cat.

# 3. Chapter 3: Experimental Design in A/B Testing

In this chapter we'll dive deeper into the core concepts of A/B testing. This will include discussing A/B testing research questions, assumptions and types of A/B testing, as well as what confounding variables and side effects are.

## A/B testing research questions

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. A/B Testing Research Questions**

In the last two chapters, we worked through a mini-example of A/B testing. Here we'll dive into the specifics of how A/B testing works and the assumptions going into it.

**2. What is A/B testing?**

First, what is A/B testing? There are many definitions out there for what A/B testing is, and at it's core it's a very simple idea. My definition is:A/B testing is the use of experimental design and statistics to compare two or more variants of a design, with the key words here

**3. What is A/B testing?**

experimental design, and

**4. What is A/B testing?**

statistics.If you have a strong understanding of the basics of each of these concepts, you are ready to run your own A/B testing experiments. Building more complicated variants of A/B testing will always come from these basic principles.

**5. Uses of A/B testing**

Most often when people think of A/B testing they are thinking of website modifications, but really any experimental design that compares two ideas can be thought of as A/B testing.Some common uses in industry though are:conversion rates, which we looked at previously and can include things like how often a button is clicked or whether someone goes through with a purchased,engagement with the website, such as sharing or liking something,dropoff rate,  which could be whether a visitor continues to the next page of a website,or finally, simply the amount of time a visitor spends on a given website.All of these can be easily testing via A/B testing.

**6. Data visualization website**

To show some examples where A/B testing can be useful, I have another generated dataset here for you that looks at an imaginary website about data visualization. For the last year you've been collecting, four types of data:

**7. Data visualization website**

one, how long people spent on the homepage,

**8. Data visualization website**

two, how often people clicked on one of the articles on the page,

**9. Data visualization website**

three, how often people clicked the "like" button,

**10. Data visualization website**

and four, how often people clicked the "share" button.Let's start by taking a look at the first type of data, how long anyone spent on the homepage.

**11. Time spent on homepage**

Here it is. Note, I'm using the `str()`, for structure, function to display the data so that columns are displayed vertically rather than horizontally, since some names are rather long.`visit_date` is for the day when a visitor came to the website.The four other columns describe:one, the amount of time spent on the home page (in seconds),two, whether the visitor clicked an article or not,three, whether the visitor clicked like or not,and four, whether the visitor clicked share or not.

**12. Time spent on homepage**

If we first look at the average time spent on the homepage we get 67.3 seconds.

**13. Time spent on homepage**

If we group by month we can see how the time differed throughout the year.

**14. Let's practice!**

Now it's your turn to explore other the three other types of variables you've been collecting.

## Article click frequency monthly

In the video, we saw there were four different types of variables we've been collecting for our website. We looked at one of them. Compute the monthly average for how often people click on one of the articles on the homepage.

The data has been pre-loaded for you in the data frame `viz_website_2017` and all packages have been pre-loaded.

**Steps**

1. Group by the `month` of `visit_date` to find monthly conversion rates.
2. Use the appropriate `dplyr` function to summarize the data.
3. Find the conversion rate for the `clicked_article` column.

```{r}
# Data
viz_website_2017 <- read_rds("data/viz_website_2017.rds")

# Compute summary by month
viz_website_2017 %>%
  group_by(month(visit_date)) %>%
  summarize(article_conversion_rate = mean(clicked_article))
```

Well done! We see that overall conversion rates are pretty high, meaning that if someone comes to the website, they are pretty likely to click an article.

## 'Like' click frequency plot

Let's get some practice computing another variable, this time also plotting the results. Let's look at the amount of time someone clicked 'like' on one of the articles.

**Steps**

1. Get the conversion rate for clicking 'like'.
2. Initialize the plot.
3. Set the y-axis to our computed column.
4. Use `percent` to ensure the y-axis is displayed as a percent.

```{r}
# Compute 'like' click summary by month
viz_website_2017_like_sum <- viz_website_2017 %>%
  mutate(month = month(visit_date, label = TRUE)) %>%
  group_by(month) %>%
  summarize(like_conversion_rate = mean(clicked_like))

# Plot 'like' click summary by month
ggplot(viz_website_2017_like_sum,
       aes(x = month, y = like_conversion_rate, group = 1)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent)
```

It looks like conversion rates are below 50% and peaked in July.

## 'Like' / 'Share' click frequency plot

Instead of just computing another conversion rate, here, we want to plot the 'like' conversion rate and the 'share' conversion rate side by side. I did some work ahead of time for you and created a new data frame `viz_website_2017_like_share_sum`. Take a look at it and you'll see there are two new columns: `action`, which refers to clicking 'like' or clicking 'share' and `conversion_rate` which is the percentage of the time the action was done for a given month.

**Steps**

1. Use the new data frame to build your plot, setting the `color` and `group` aesthetics to `action`.
2. Fill in the missing `geom_*()` we've used in previous exercises.
3. Make sure the y-axis ranges from `0` to `1`, so it's 0% to 100%.

```{r}
# CALC
# Compute 'share' click summary by month
viz_website_2017_share_sum <- viz_website_2017 %>%
  mutate(month = month(visit_date, label = TRUE)) %>%
  group_by(month) %>%
  summarize(share_conversion_rate = mean(clicked_share))

viz_website_2017_like_share_sum <- bind_rows(viz_website_2017_like_sum, viz_website_2017_share_sum, .id = "action") |> 
                                      mutate(action = case_when(action == 1 ~ "like",
                                                                action == 2 ~ "share")) |> 
                                      mutate(conversion_rate = coalesce(like_conversion_rate, share_conversion_rate)) |> 
                                      select(-c(like_conversion_rate, share_conversion_rate))


# Plot comparison of 'like'ing and 'sharing'ing an article
ggplot(viz_website_2017_like_share_sum,
       aes(x = month, y = conversion_rate, color = action, group = action)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent)
```

Great! It looks like people are even less likely to 'share' an article as they are to 'like' it.

## Assumptions and types of A/B testing

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Assumptions and Types of A/B Testing**

In the last set of exercises we looked over the different types of data we've been collecting. We're pretty close to being able to build our experiment, but let's first go over some basic A/B testing assumptions and learn about the different types of A/B testing. We've already talked a bit about assumptions, such as that you need a "control" and "test" condition. We'll start to build our next experiment and go over a few more assumptions and types.

**2. Data visualization website**

Here's our website again.There are various things we could change to improve some of our measures, but we're going to start by focusing on

**3. Data visualization website**

the title of the webpage. Specifically you think that rather than naming the website "Tips for Better Data Visualization", you think using the word

**4. Data visualization website**

"tools" in the title will be more interesting for people.Okay, now that we have the rough idea of the experiment in place, let's go over a few more key aspects.

**5. Within group vs. between group**

One phrase you may have heard before is "within" versus "between" group or participants.A "within" experiment is one in which each participant sees both conditions of an experiment, so you can see if that particular person behaved differently between the two conditions.A "between" experiment puts a participant in one of the two conditions, and then you compare how the two groups of participants behaved.Both methods can be used. Generally "within" experiments have higher power, but if you have a website where you don't know if a specific visitor will come to your website more than once, it makes more sense to do a "between" experiment. One assumption of a "between" experiment is that participants in each condition should come from the same random group of possible participants, so that there is nothing qualitatively different between the two groups, except which condition they saw.In the case of our new experiment we're going to run a "between" experiment, since we don't know how often a given person comes to the website.

**6. Types of A/B testing**

Within A/B testing, there are various types of experiments you can run based on different conditions. So far we've focused on what's classically known asA/B testing, where you have one control condition (generally the current version of the website, in our case the use of the word "Tips") versus a test condition (in our case the use of the word "Tools").Another type of testing though is A/A, where you actually have two identical control conditions, in our case two groups of "Tips". This type of testing can be useful to confirm that your control condition really is stable, or that the way you're building a between experiment is actually testing two similar groups of people. If you get a significant effect for an A/A experiment, something is wrong, because in theory you are running two of the exact same condition. For example, if you get a significant effect it could mean there is an error in how you are randomly assigning your participants, and the two groups of people in your experiment are different in some way, when they should be the same.A final type of testing is A/B/N where you can have a control condition and any number of test conditions. For example here we could have "tips" (our control condition) versus "tools" versus "strategies" (two separate test conditions).A/B/N can seem like an exciting fast way to test more things quickly, but the statistics are more complicated and you'll need more data points to be sure of an effect, so in general it's safer to just use A/B when starting out.

**7. Let's practice!**

Let's apply some of the knowledge we learned about A/B testing assumptions and types.

## Between vs. within

Which of these experiments describes a within-participant experiment?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ You randomly sample a group of people where a button color is blue. One month later you sample a new group where the button color is green.<br>
> ✅ You have a group of email users and for two weeks randomly show one of two "Inbox" designs whenever a person logs in.<br>
> ⬜ You run one condition of homepage images simultaneously. One is tested on people with IP addresses in the United States and one with IP addresses in Italy.<br>
> ⬜ You use a new design that speeds up loading times. Every visitor who shows up on the website either gets the old version or the new, faster version. You don't track who your visitors are.<br>

Since a given user sees both versions of the Inbox, this is a within-participant experiment.

## Plotting A/A data

Before running our experiment, let's say we ran an A/A experiment first to be sure our "random" assignment of visitors really was random. Remember, a significant effect in an A/A experiment can mean our two groups of participants actually are different in some way. For this experiment, we're going to look at clicking 'like' conversion. We'll start by summarizing and plotting our data.

All packages and the data are pre-loaded for you. The data is in the data frame `viz_website_2018_01`.

**Steps**

1. Create a new data frame called `viz_website_2018_01_sum` by grouping by the `condition` column. (**Note** we're not grouping by `visit_date` since this data was all collected in a single month.)
2. Set the `x` value in our plot to the same variable we grouped by.
3. To make a bar plot, use `geom_bar()`. Here use `stat = "identity"` so it plots our computed values, rather than make bars of counts.

```{r}
# Data
viz_website_2018_01 <- read_rds("data/viz_website_2018_01.rds")

# Compute conversion rates for A/A experiment
viz_website_2018_01_sum <- viz_website_2018_01 %>%
  group_by(condition) %>%
  summarize(like_conversion_rate = mean(clicked_like))
viz_website_2018_01_sum

# Plot conversion rates for two conditions
ggplot(viz_website_2018_01_sum,
       aes(x = condition, y = like_conversion_rate)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent)
```

Based on these bar plots the two A conditions look very similar. That's good!

## Analyzing A/A data

Just as for a normal A/B experiment, we need to statistically analyze our A/A results to be sure there really was no effect. Here we'll build a logistic regression again, this time hoping for a null effect.

**Steps**

1. Load the package we've used in the past to clean up modeling results.
2. Set our independent variable to `condition`.
3. Set the `family` argument to the expected value for a logistic regression.

```{r}
# Load library to clean up model outputs
library(broom)

# Run logistic regression
aa_experiment_results <- glm(clicked_like ~ condition,
                             family = "binomial",
                             data = viz_website_2018_01) %>%
  tidy()
aa_experiment_results
```

There was no statistical difference between our two conditions. We can now safely say we randomly selected two groups of participants.

## Confounding variables

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Confounding Variables**

In the last lesson we talked about some of the assumptions of A/B testing as well as some different types of A/B testing. Here we'll go over one aspect that could hurt your experiment: confounding variables.

**2. Confounding variables**

You may recall that the phase "confounding variables" was mentioned once in a previous lesson. I'm defining it here as"an element of the environment that could affect your ability to find out the *truth* of an A/B experiment".So what exactly do I mean by that?

**3. Confounding variables - internal**

Sometimes a confounding variable is internal to how the experiment was designed.Let's come back to our experiment for a second. Recall that we have two conditions. One with "Tips" in the title and one with "Tools" in the title, and we want to see how often someone clicks a 'like' button. Let's say we run the experiment for 1 month and we find that for

**4. Confounding variables - internal**

the original "tips" page 20% of people clicked 'like', but on the "tools" page only 10% of people clicked 'like'.Looks like people like the word "tips" better! Is it really specifically that word though? Other effects could be driving this.For example

**5. Confounding variables - internal**

what about word length or word frequency? Maybe people like "tips" because it's shorter, or because it is less frequent and more novel.For any future experiment comparing "tips" to another word, these kinds of things should be considered.

**6. Confounding variables - external**

Confounding variables can be external to your experiment too.Let's say you run your experiment for a second month and you find

**7. Confounding variables - external**

that in the second month "tools" was actually much more likely to result in "likes". What happened? Well let's say you were able to collect demographic information about who came to visit your website. In the first month

**8. Confounding variables - external**

your traffic was evenly split between people ages 20 to 35, and people over 35. However, at the beginning of the second month, your website was sent in an alumni listserv for a university class of 2000, so people over 35.

**9. Confounding variables - external**

Now your traffic is 90% people over 35. So it's possible your effect isn't because "tools" is a better word across all your typical user base, but preferred by a certain age group.This is why it's important to track information like who your users are and when they come to your website, to know if an effect is really driven by your design change, or some external factor about your user base. If this jump in 90% of visitors being over 35 isn't the norm for your website, and then you change the design only to have it go back to an even split (50% of users age 20 to 35, 50% of users age 35 plus), you've just hurt your website by not taking into account the confounding variable of visitor age.

**10. Let's practice!**

Let's get some practice looking at the effects of confounding variables in our own dataset.

## Examples of confounding variables

Let's go back to our cat adoption website for a second. You run an experiment where in one condition, the homepage is a tabby cat, and in another condition, it is a black cat. Both conditions are run at the same time. At the beginning of the experiment, a children's movie comes out starring a black cat. Which of these statements is true?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ If the black cat condition results in a higher conversion rate it may be due to lack of randomization.<br>
> ⬜ If the black cat condition results in a higher conversion rate it may be due to an internal confounding variable.<br>
> ✅ If the black cat condition results in a higher conversion rate it may be due to an external confounding variable.<br>
> ⬜ If the black cat condition results in a higher conversion rate it may be due to a lack of an independent variable.<br>

Correct! The movie coming out is an external factor that could confound your results.

## Confounding variable example analysis

You have decided to run your "Tips" versus "Tools" experiment and look at percentage of 'like's. You run the experiment over the month of February. In the later half of February, an article goes viral called "Tips Just Get you Started, but Tools Open all the Doors". Let's see how this article affected results.

The data frame is `viz_website_2018_02`. All packages have already been loaded for you.

**Steps**

1. Group by the `week` column and the column related to condition.
2. Get the mean of the column related to clicking like.
3. To compute the conversion rate depending on if the article was published or not, group by the column that codes if the article is published or not.
4. Name your summarized column the same as you did for the previous aggregation.

```{r}
# Data
viz_website_2018_02 <- read_rds("data/viz_website_2018_02.rds")

# Compute 'like' conversion rate by week and condition
viz_website_2018_02 %>%
  mutate(week = week(visit_date)) %>%
  group_by(week, condition) %>%
  summarize(like_conversion_rate = mean(clicked_like))

# Compute 'like' conversion rate by if article published and condition
viz_website_2018_02 %>%
  group_by(article_published, condition) %>%
  summarize(like_conversion_rate = mean(clicked_like))
```

Clearly there was an effect of the article coming out. First we saw a switch in 'like' rates in the second half of February and then saw it was directly related to whether the article was published or not.

## Confounding variable example plotting

Let's see if we can tell when 'like' rates really started to change by plotting daily like rates.

The data frame summarized by day is `viz_website_2018_02_sum`. All packages have already been loaded for you.

**Steps**

1. Fill in the missing two `geom_*()`s we've used in previous exercises.
2. Set `color` to `condition`.
3. Set `linetype` to `article_published`.
4. Find the date when the article was first published. Now add that date as the value for `xintercept` in `geom_vline()`. (**Note**: it should be of the format "YYYY-MM-DD".)

```{r}
# Summarize
viz_website_2018_02_sum <- viz_website_2018_02 |> 
                              group_by(visit_date, condition, article_published) |> 
                              summarise(like_conversion_rate = mean(clicked_like))

# Plot 'like' conversion rates by date for experiment
ggplot(viz_website_2018_02_sum,
       aes(x = visit_date,
           y = like_conversion_rate,
           color = condition,
           linetype = article_published,
           group = interaction(condition, article_published))) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = as.numeric(as.Date("2018-02-15"))) +
  scale_y_continuous(limits = c(0, 0.3), labels = scales::percent)
```

It looks like the article had a pretty clear effect, and the effect seemed to increase over time as it presumably got more hits.

## Side effects

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Side Effects**

In the last lesson we looked at how confounding variables can negatively affect experiment results. Here we'll talk about another variable to account for: side effects.

**2. Side effects**

When designing an experiment, I've talked about how it's important to only change one thing at a time to really be sure of what you're testing. However, some changes are out of our control, and this is where side effects come in.A side effect is an unintended consequence of a change you made.What's a potential example of this?

**3. Side effects**

Let's take our example again. We found that 'like' click rates dropped by 10% in the "tools" condition. We also noted that "tools" is one letter longer than "tips". Well what if I told you that one letter made a huge difference on load times, and while the "tips" homepage loads in

**4. Side effects**

5 seconds, the "tools" homepage takes

**5. Side effects**

7 seconds to load!So we don't really know if our drop in 'like' rates is due to the word, or due to the side effect of the longer load times.This is of course an exaggerated example, but often times, adding new features can slow down a website, which can negatively impact performance. One way to account for this side effect would be to add a delay to the original "tips" homepage to be sure both versions have the same load time.

**6. Examples of side effects**

Some common examples of side effects in A/B testing areload times, as already discussed andthe amount of information above the fold.The fold is a reference to newspapers, and in the context of websites, it's what a person sees without having to do any scrolling. Adding a larger header image for example may push down other content to a place that requires scrolling to get to. People may give up before scrolling, resulting in less use of the website.Other side effects will come up depending on your design. The key is to try and be aware of them and account for them in both conditions whenever possible. This is why spending time to carefully think about how you're going to design your experiment is incredibly important rather than just jumping in.

**7. Let's practice!**

Let's look at some possible side effects in our own data.

## Confounding variable vs. side effect

For our cat adoption website with an experiment comparing a tabby cat and a black cat, the photo of the tabby cat loads in 5 seconds and the black cat in 2 seconds. The tabby cat is 1 year old and the black cat is a 6 years old. Which of these statements is true?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ Both the load time and age of the cats are confounding variables.<br>
> ⬜ Both the load time and age of the cats are side effects.<br>
> ⬜ The load time is a confounding variable, the age of the cats is a side effect.<br>
> ✅ The load time is a side effect, the age of the cats is a confounding variable.<br>

Correct! When choosing your pictures you introduced a confounding variable of cat age, when adding the pictures you had a side effect of load times.

## Side effect load time plot

The viral article has died down, so you've decided to rerun your experiment on "tips" vs. "tools". However, after running it for a month you realized there was a larger load delay for the "tools" homepage than the "tips" homepage. You then added a delay to the "tips" homepage so that they even out. To start, let's visualize the effect the delay has on a 'like' rates on a daily basis.

**Steps**

1. First create the aggregated data frame. Group by `visit_date` and `condition`.
2. In addition to computing the 'like' rate, we also want to find out the mean page load time in a given day. Find the column in the data frame associated with page load time and fill it in.
3. In building your `ggplot()`, use the newly created page load column as your x-variable.
4. Use `geom_point()` to display dots for your data.

```{r}
# Data
viz_website_2018_03 <- read_rds("data/viz_website_2018_03.rds")

# Compute 'like' conversion rate and mean pageload time by day
viz_website_2018_03_sum <- viz_website_2018_03 %>%
  group_by(visit_date, condition) %>%
  summarize(mean_pageload_time = mean(pageload_time),
            like_conversion_rate = mean(clicked_like))

# Plot effect of 'like' conversion rate by pageload time
ggplot(viz_website_2018_03_sum,
       aes(x = mean_pageload_time, y = like_conversion_rate, color = condition)) +
  geom_point()
```

There's clearly a correlation here. Longer load times lead to lower conversion rates.

## Side effects experiment plot

Let's end by plotting our new conversion rate, seeing if we can find the effect of when the page load delay was added.

**Steps**

1. Set `linetype` the column that codes for if the delay was added or not.
2. Fill in the missing `geom_*()`s.
3. Set the argument to show a vertical line.

```{r}
# Data
viz_website_2018_03_sum <- viz_website_2018_03_sum |> 
                              mutate(pageload_delay_added = if_else(visit_date <= "2018-03-14", "no", "yes"))


# Plot 'like' conversion rate by day
ggplot(viz_website_2018_03_sum,
       aes(x = visit_date,
           y = like_conversion_rate,
           color = condition,
           linetype = pageload_delay_added,
           group = interaction(condition, pageload_delay_added))) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = as.numeric(as.Date("2018-03-15"))) +
  scale_y_continuous(limits = c(0, 0.3), labels = scales::percent)
```

Great job! We can clearly see the effect that adding the delay had on the two conditions.

# 4. Chapter 4: Statistical Analyses in A/B Testing

In the final chapter we'll go over more types of statistical tests and power analyses for different A/B testing designs. We'll also introduce the concepts of stopping rules, sequential analysis, and multivariate analysis.

## Power analyses

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Power Analyses**

Welcome to our final chapter of A/B testing in R. While in Chapter 3 we focused on experimental design and the steps to prepare for our experiment, here we'll go over some of the statistical principles involved in A/B testing, the first being power analyses.By the end of these two chapters you should have a good understanding of the core concepts of A/B testing.

**2. What are power analyses? - Cambridge Dictionary of Statistics**

We talked a little bit about power analyses in our mini case study, but here we'll really dig into what a power analysis is.When you're doing a power analysis it's generally because you want to know your sample size. In order to solve that, you need three statistics defined:power (or 1 minus beta),significance level (or alpha),and the effect size.

**3. What are power analyses? - Cambridge Dictionary of Statistics**

Power is defined here as the probability of rejecting the null hypothesis when it is false. It is also the basis of procedures for estimating the sample size needed to detect an effect of a particular magnitude.

**4. What are power analyses? - Cambridge Dictionary of Statistics**

Power is often closely tied to the significance level, or alpha, also referred to as the p-value.The significance level is the level of probability at which it is agreed that the null hypothesis will be rejected.

**5. What are power analyses? - Cambridge Dictionary of Statistics**

The final variable is effect size, which is basically the magnitude of the effect you're expecting.One common way to compute this is by taking the difference between the two groups and dividing by the standard deviation of the two groups.

**6. Power analysis relationships**

Let's look at the relationships of these variables to number of data points.Here I've plotted power and number of data points, holding the significance level and effect size constant. As power goes up, so does the number of data points needed.

**7. Power analysis relationships**

If we hold power constant, but let significance level change, we see that as the significance level goes up the number of data points needed goes down.

**8. Power analysis relationships**

And finally, if we hold both power and significance level constant, but modulate effect size, we see that as the effect size increases, our number of data points decreases. If the magnitude of the effect is very large, we need fewer data points to know that it is real.So, when running an experiment the higher power, lower significance level, and smaller effect size you want or have the more data points you will need.Now that we have a basis in how this works on a theoretical level, let's see some examples in R.

**9. Power analysis in R: T-Test**

In the previous exercises with power analysis, we were running a logistic regression. Here we'll go over an example for an experiment with that uses a t-test. We'll use the `pwr` package to run this analysis and the function `pwr dot t dot test()`. Using this function, we can solve for several different values. We could solve for power if we had already run our experiment to see if we hit 0.8. In our case though, we're solving for n, the number of data points we need. There are two other arguments in the function, `type` and `alternative`. Explaining these is beyond the scope of this course, which is meant to introduce the main concepts of A/B testing, but please look into them more before running an experiment.

**10. Power analysis in R: T-Test**

Let's say we set power to 0.8 and our significance level to 0.05, the most common values for experiments. How many data points do we need if our effect size is 0.6? I won't be going into the details here for computing your effect size, but other DataCamp courses on experimental design can be a resource for you.According to our power analysis, we would need roughly 44.5 data points per group.

**11. Power analysis in R: T-Test**

Okay let's say instead the effect we're looking at isn't that large, only 0.2. What happens now? Now our number of data points has dramatically increased to 393 per group.This is why A/B testing experiments often need many data points. The effects people are looking at tend to be pretty small, but even small effect sizes can dramatically affect a company in the long run!

**12. Let's practice!**

Let's try our hand at a few different power analyses.

## Logistic regression power analysis

In the previous chapters you got some practice with power analyses with logistic regression (you didn't think you got to forget about that did you?). Let's test your recall knowledge by running that kind of power analysis assuming we're running an experiment on whether or not someone clicked 'like' on an article.

**Steps**

1. Load the package you need to run the logistic regression power analysis.
2. Fill in `p1` and `p2` assuming a control value of 17% click 'like' (the conversion rate for April 2017) and a 10 percentage point increase in the test condition.
3. Fill in the names for the arguments that are set to `0.05` and `0.8`.

```{r}
# Load package to run power analysis
library(powerMediation)

# Run power analysis for logistic regression
total_sample_size <- SSizeLogisticBin(p1 = 0.17,
                                      p2 = 0.27,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

Great! We need 537 data points in total to see an increase in 'like' clicks by 10 percentage points.

## pwr.t.test() documentation

I mentioned in the video that there are additional arguments that can be provided to `pwr.t.test()`. Take a look at the documentation to see what the difference is between `type` and `alternative`. Note, you'll need to load the `pwr` package.

> *Question*
> ---
> ???<br>
> <br>
> ✅ `type` refers to number and type of samples; `alternative` to hypothesis.<br>
> ⬜ `type` relates to Type 1 error; `alternative` relates to Type 2 error.<br>
> ⬜ `type` refers to hypothesis; `alternative` to number and type of samples.<br>
> ⬜ `type` relates to Type 2 error; `alternative` relates to Type 1 error.<br>

Correct! You use `type` to describe how your data points and `alternative` to talk about the hypothesis of the experiment.

## T-test power analysis

Now that we know how to run power analyses for continuous data using a t-test, let's try running a power analysis for an experiment on time spent on the homepage.

**Steps**

1. Load the `pwr` package.
2. Set the effect size (`d`) to `0.3`.
3. Set our significance level to  `0.05` and power to `0.8`.

```{r}
# Load package to run power analysis
library(pwr)

# Run power analysis for t-test
sample_size <- pwr.t.test(d = 0.3,
                          sig.level = 0.05,
                          power = 0.8)
sample_size
```

At an effect size of 0.3, we need a sample size of 176 data points per group.

## Statistical tests

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Statistical Tests**

In the last lesson we dug deeper into power analyses. Here we'll look at the next step in analyzing our A/B testing data: statistical tests.

**2. Common statistical test for A/B testing**

Assuming you followed other steps in the process and ran your power analysis, by the time you get to actually analyzing your data you should already know what statistical test you're going to run.While the statistics behind A/B testing can get very complicated, in the beginning the two most common test you'll run are:one, a logistic regression, where you have a binary or categorical dependent variable,or a t-test, where you have a continuous dependent variable.We already went over logistic regression earlier in the course, so here we'll focus on running a t-test in R.

**3. T-tests**

Let's bring back our A/A testing data from Chapter 3.  As you'll recall we had two of the same conditions and recorded several variables. Let's run a t-test now to see if there was a difference in time spent on the webpage between the two conditions.The function in R is `t dot test()`. We can structure it a few different ways, but we'll use the equation format similar to how we did for logistic regression.

**4. T-tests**

First we specify our dependent variable, which is time spent on the homepage in seconds.

**5. T-tests**

Next we specify our independent variable after the tilde, which here is condition,

**6. T-tests**

And then finally we specify our data. Note, just as for the power analysis, there are several other arguments for the `t dot test()` function.I encourage you to learn more about the different kinds of t-tests to be sure you're using the correct one for your experiment.Now if we take a look at the result we see that indeed there was no significant effect. Our p-value is about 0.38, far from significant. The t-test also gives us the means for each group, and we can see that they are quite similar.

**7. T-test vs. linear regression**

You may have noticed when talking about different statistical tests that I'd written linear regression in parentheses. Linear regression can be thought of as an extension of a t-test. While a t-test only compares two groups, linear regression is a powerful tool that can allow you to test multiple independent variables, and more than two levels for a given variable. However, if you only have one independent variable and only two levels for the variable (for example a control and test condition) you will actually get out the same statistics as with a t-test.Let's show an example with our data.

**8. T-test vs. linear regression**

Here's the result again of our t-test on the A/A experiment. We could write this same equation as a linear regression using the `lm()` function in R and then use the `summary()` function just as we did for logistic regression. I've reduced it slightly to make room on the slide.The key things to note are the t-value and p-value for the condition variable, the row `conditionA2` in the linear model results. Both are the same as for the t-test, the only difference being the linear regression has a negative t-value to show the direction of the effect (which means a slightly smaller mean for condition A2 than A1).The point of this is when running an experiment like this you can use either a t-test or a linear regression. However, when your experiments get more complicated, you'll need to run linear regressions.

**9. Let's practice!**

Let's get some practice analyzing our data with different statistical tests!

## Logistic regression

In the previous lessons, we got some practice with logistic regression. Let's do the same here looking at our experiment to see if people clicked 'like' or not depending on if the homepage has "Tips" (the control) or "Tools" (the test) in the title. You can assume this is after all confounding and side effects were figured out.

The data frame is pre-loaded as `viz_website_2018_04`.

**Steps**

1. Fill in the rest of the logistic regression sample code provided.
2. Use the function `tidy()` to look at a cleaned up version of the results.

```{r}
# Data
viz_website_2018_04 <- read_csv("data/data_viz_website_2018_04.csv")

# Load package to clean up model outputs
library(broom)

# Run logistic regression
ab_experiment_results <- glm(clicked_like ~ condition,
                             family = "binomial",
                             data = viz_website_2018_04) %>%
  tidy()
ab_experiment_results
```

Based on our logistic regression there was a significant effect, however not in the direction we wanted. It look like 'Tools' actually had lower 'like' click rates than 'Tips'.

## T-test

Let's now analyze one of our dependent variables for a continuous dependent variable using a t-test. The data frame is pre-loaded for you as `viz_website_2018_04`.

**Steps**

1. Use the `t.test()` function, setting the dependent variable to the column referring to time spent on the homepage and the independent variable to the column for `condition`.

```{r}
# Run t-test
ab_experiment_results <- t.test(time_spent_homepage_sec ~ condition,
                                data = viz_website_2018_04)
ab_experiment_results
```

It looks like our experiment had no effect of time spent on the homepage, despite a lower 'like' click rate.

## Stopping rules and sequential analysis

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Stopping Rules and Sequential Analysis**

In the last few lessons we went over how to do a power analysis and then how to test our results. But what if your results aren't significant and you want to continue running the experiment? According to the power analysis you should stop, but what if you overestimated your effect size, there really is an effect, and you just didn't run the experiment long enough? Stopping rules are one way to help with that situation.

**2. What is a stopping rule? - Cambridge Dictionary of Statistics**

We've talked about experimental planning as running a single power analysis, to see how many data points we need for the experiment. Another way to run an experiment is with stopping rules.Stopping rules are a procedure that allows for interim analyses, letting you peak at your data. However, since it's at pre-defined points, it's still built into your experimental plan.

**3. What is a stopping rule? - Cambridge Dictionary of Statistics**

This is also referred to as sequential analysis. At your pre-defined points for looking at the data you can either:one, stop because the experiment worked,two, stop because the experiment didn't work,or three, continue since not enough data has been collected.Crucially, your p-value will be lowered to take into account the fact that you're looking at your data multiple times. This also discourages too many data peaks, as more peaks means a lower significance level.

**4. Why stopping rules are useful**

Before getting into how to apply stopping rules and sequential analysis to your experiment, let's go over why it's useful. For one, the idea of reanalyzing data several times without pre-planning to, or running an experiment longer than planned are examples ofp-hacking. P-hacking is playing around with data looking for an effect and finding one when there are none. Stopping rules are useful because they can help avoid this situation.Stopping rules are also useful because sometimes you don't know what your true effect size is. If you guessed too big, your power analysis will tell you to stop the experiment too soon, and you won't find out that your experiment worked, because it's at a smaller effect size than you originally expected.Finally, if you are running an experiment and trying to validate resources spent at a company level, these stopping points can be thought of as "check-ins" to provide status updates to other people at the company.

**5. Sequential analysis in R**

To run a sequential analysis in R, we'll use the package `gsDesign` and the function `gsDesign()`.

**6. Sequential analysis in R**

There are five main arguments we'll focus on here: `k`, `test dot type`, `alpha`, `beta`, and `sfu`.There are several other parameters that you could set, and I encourage you to explore the function more.

**7. Sequential analysis in R**

`k` is the number of times we want to look at the data. For now, we'll say 4.

**8. Sequential analysis in R**

`test dot type` refers to the hypothesis, similar to `alternative` in `pwr dot t dot test`. We'll set it to `1`, a one-sided test.

**9. Sequential analysis in R**

Our `alpha` is 0.05 as it has been in the past.

**10. Sequential analysis in R**

Our `beta` is set to 0.2. Note, power is 1 minus beta, so for a power of 0.8 we need a beta of 0.2.

**11. Sequential analysis in R**

Finally `sfu` is the spending function to figure out how to update our p-values. I've set it here to a method to get uniform p-value cutoffs at each look point. Going into the details of the math for this argument is beyond the scope of this course, but you are encouraged to learn outside of the course.Let's view our saved result.I'm not including all of it because I want to focus on the first part here, specifically the column "Nominal p".  Instead of significance at p less than 0.05, we reduced our p-value to roughly 0.019. This lowering of our p-value is what allows us to look at our data more than once.

**12. Sequential analysis in R**

We've seen how to update our p-value using sequential analysis, but not how many data points we need. There are a few ways to figure out your sample size for your stopping points. One is resource based. Figure out the maximum number of data points you're willing to collect. For this experiment we'll say 1000.We can then compute how many points per group, the max divided by 2.Within in our sequential analysis object there is `timing`, which saves the points at which to look at the data, currently four evenly spaced points. If we multiple our max number per group we get a list of data points for when to look at our data. In our case that's when we have 125, 250, 375, or 500 data points per group. At any of these points if our effect is significant we can stop collecting data.

**13. Let's practice!**

Now it's your turn to try your hand at running a sequential analysis.

## What is a sequential analysis?

What is "sequential analysis"?

> *Question*
> ---
> ???<br>
> <br>
> ✅ Sequential analysis is when you adjust your p-value to allow for looking at your data at various stopping points.<br>
> ⬜ Sequential analysis is running multiple experiments one after the other and use different p-value cutoffs for both.<br>
> ⬜ Sequential analysis is running two separate types of analyses on the same data set, but with different p-value cutoffs.<br>
> ⬜ Sequential analysis is running two experiments at the same time, with the same p-values cuttoffs.<br>

Correct! Sequential analysis builds in stopping points to look at your data.

## Sequential analysis three looks

In the video, we built a sequential analysis with four looks at the data. In this exercise, you will build a sequential analysis for only three looks.

**Steps**

1. Load the package `gsDesign`.
2. Look at the data three times.
3. Use `Pocock` as our method for the spending function.
4. Take a look at the result of only using three looks.

```{r}
# Load package to run sequential analysis
library(gsDesign)

# Run sequential analysis
seq_analysis_3looks <- gsDesign(k = 3,
                               test.type = 1,
                               alpha = 0.05,
                               beta = 0.2,
                               sfu = "Pocock")
seq_analysis_3looks
```

It looks like our p-value cuttoff is higher if we only look at the data three times.

## Sequential analysis sample sizes

Now that we've built our sequential analysis with three looks, let's see what our stopping points are.

**Steps**

1. You've been given permission to run the experiment to collect a grand total of 3000 data points. Fill in `max_n` with this number.
2. Compute the `max_n_per_group` by filling in the missing variable.
3. Call the `timing` element from our sequential analysis object.

```{r}
# Load package to run sequential analysis
library(gsDesign)

# Run sequential analysis
seq_analysis_3looks = gsDesign(k = 3,
                               test.type = 1,
                               alpha = 0.05,
                               beta = 0.2,
                               sfu = "Pocock")

# Fill in max number of points and compute points per group and find stopping points
max_n <- 3000
max_n_per_group <- max_n / 2
stopping_points = max_n_per_group * seq_analysis_3looks$timing
stopping_points
```

Based on this we should run our analysis at 500, 1000, and 1500 data points per group.

## Multivariate testing

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Multivariate Testing**

For the final topic of this course, we'll go over a simple example of when you have more than one independent variable in an experiment, also known as multivariate testing.

**2. Single vs. multivariate design**

So far I've stressed the importance of only looking at one experimental manipulation at a time. However, sometimes the goal of an experiment is to see how two different changes might affect each other.For example, in our current experiments we've been comparing two versions of the same website, one where the homepage says "Tips" and one where it says "Tools". So far it looks like our original "Tips" website is better. However, what if we wanted to change a second aspect of the title?

**3. Single vs. multivariate design**

Let's test a change from "Better" to "Amazing".Based on how we've designed experiments so far, we should run one experiment, choose a winner, and then run a second experiment. Imagine though we have a hunch based on some earlier testing that actually while "Tools" hasn't done that well, the combination of "Tools" *and* "Amazing" will do really well, suggesting we should test them at the same time. This would call for a multivariate design.Let's see what this would look like in R.

**4. Time spent on homepage multivariate analysis**

To start, let's look at the variable for time spent on the homepage.We'll start our linear regression off as we typically have, with  the `lm()` call and `time_spent_homepage_sec` as a our dependent variable.Since we now have two variables, I've renamed them to `word_one` and `word_two`. We can add

**5. Time spent on homepage multivariate analysis**

`word_one` to our model just as we've been adding condition.Now we add our second variable, `word_two`, as well as the interaction. Rather than adding it with a plus sign we use

**6. Time spent on homepage multivariate analysis**

the asterisks, or multiplication sign.Let's take a look at the result. Based on this there is a significant effect for everything except 'better' versus 'amazing'. You can see that by looking at the row for `word_twobetter` and looking over at the p value column which has a value of roughly 0.25.There are some very important things to know though when interpreting a multivariate analysis with an interaction. The first two effects, row `word_onetools` and row `word_twobetter` are only in reference to the *baseline* values of the model, not the entire data set. So when it says there is no effect for `word_twobetter`, that is specifically in reference to when `word_one` is `tips`. We don't know if there is an effect when `word_one` is `tools` from this model.How do you know what the baseline values are? By default R chooses an order alphabetically, so for `word_one` `tips` is our baseline, and for `word_two` `amazing`.This isn't ideal since our baseline values should be `tips` and `better`, the original version of the site.For this reason it's good to be explicit in our baseline values.There are several ways to do this, including one with the function `factor()`.

**7. Time spent on homepage multivariate analysis**

The order in which you put the words in `levels`, is the order in which the model will order them.With my levels set in the correct order, I can now

**8. Time spent on homepage multivariate analysis**

pipe my updated data frame directly into `lm()`.Now if we look at the results, we see that word two is labeled as `word_twoamazing` since `amazing` is now in reference to your baseline of `better`. We also see that `word_one` is no longer significant. So from this comparison we know that there is a difference between 'Tips' and 'Tools' when the second word is 'Amazing', but not when it is 'Better'.

**9. Let's practice!**

Let's do a few practice items with multivariate analysis.

## Plotting time homepage in multivariate experiment

In the video, I ran our statistical analysis but didn't plot our data. Remember, it's always important to plot your data first so you're sure you have a sense of what's going on. Let's plot the means for our four conditions for time spent on the homepage.

The data is preloaded in a data frame for you called `viz_website_2018_05`.

**Steps**

1. Group by the two columns that represent our two independent variables.
2. Name the parameter that `"identity"` is set to in the plot.
3. Set `position` to `"dodge"`.

```{r}
# Data
viz_website_2018_05 <- read_rds("data/viz_website_2018_05.rds")

# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>%
  group_by(word_one, word_two) %>%
  summarize(mean_time_spent_homepage_sec = mean(time_spent_homepage_sec))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum,
       aes(x = word_one,
           y = mean_time_spent_homepage_sec,
           fill = word_two)) +
  geom_bar(stat = "identity", position = "dodge")
```

The results of our regression look validated. Indeed time spent on the homepage was highest in the 'Tools'/'Amazing' combination.

## Plotting 'like' clicks in multivariate experiment

Now that we've seen that there was an interaction for time spent on the homepage, let's take look at the conversion rates for clicking 'like' to see if there was an additive effect.

All packages you need are pre-loaded. The data is preloaded in the data frame `viz_website_2018_05`.

**Steps**

1. In the plot set `x` to the first variable.
2. In the plot set `fill` to the second variable.
3. In the plot make sure the labels on the y-axis display as percents.

```{r}
# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>%
  group_by(word_one, word_two) %>%
  summarize(like_conversion_rate = mean(clicked_like))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum,
       aes(x = word_one,
           y = like_conversion_rate,
           fill = word_two)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent)
```

Looks like we have another interaction! This one appears to be even stronger. While the 'like' click rate decreased with 'Tools' with 'Better', it looks to be even higher than the control with 'Amazing'.

## Multivariate design statistical test

Let's also run a statistical analysis for 'like' conversion rate.

**Steps**

1. Be sure that `word_one` has the levels in order of `tips` and then `tools`.
2. Update the second variable.
3. In the model set the dependent variable to look at whether a person clicked 'like' or not.

```{r}
# Load package for cleaning model output
library(broom)

# Organize variables and run logistic regression
viz_website_2018_05_like_results <- viz_website_2018_05 %>%
  mutate(word_one = factor(word_one,
                           levels = c("tips", "tools"))) %>%
  mutate(word_two = factor(word_two,
                           levels = c("better", "amazing"))) %>%
  glm(clicked_like ~ word_one * word_two,
                                    family = "binomial",
                                    data = .) %>%
  tidy()
viz_website_2018_05_like_results
```

Once again we found a significant interaction. While there was no effect of `word_two` for the baseline of 'Tips', there likely was an effect for 'Tools'.

## A/B Testing Recap

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. A/B Testing Recap**

This brings us to the end of our course on A/B testing in R. This course was meant to introduce you to some of the basic concepts and give you the tools to go deeper into certain areas.

**2. A/B testing summary**

To end, I want to bring back the cycle we talked about in the beginning and add in various topics we learned throughout the course.

**3. A/B testing summary**

We learned about plotting baseline data before an experiment,

**4. A/B testing summary**

running a power analysis to determine sample size,

**5. A/B testing summary**

plotting experimental results,

**6. A/B testing summary**

running statistical tests,

**7. A/B testing summary**

and we also covered sequential analysis,

**8. A/B testing summary**

and multivariate analysis.I hope you found these topics useful and are ready to learn more about each as you go out to build your A/B testing experiments.

**9. Thank you!**

Thanks for taking the course! I hope you enjoyed it!

