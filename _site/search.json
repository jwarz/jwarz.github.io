[
  {
    "objectID": "slides/synthetic-data.html",
    "href": "slides/synthetic-data.html",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "",
    "text": "In the example guide for generating random numbers, we explored how to use a bunch of different statistical distributions to create variables that had reasonable values. However, each of the columns that we generated there were completely independent of each other. In the final example, we made some data with columns like age, education, and income, but none of those were related, though in real life they would be.\nGenerating random variables is fairly easy: choose some sort of distributional shape, set parameters like a mean and standard deviation, and let randomness take over. Forcing variables to be related is a little trickier and involves a little math. But don’t worry! That math is all just regression stuff!\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(scales)\nlibrary(ggdag)",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#basic-example",
    "href": "slides/synthetic-data.html#basic-example",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "1 Basic example",
    "text": "1 Basic example\n\nRelationships and regression\nLet’s pretend we want to predict someone’s happiness on a 10-point scale based on the number of cookies they’ve eaten and whether or not their favorite color is blue.\n\\[\n\\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon\n\\]\nWe can generate a fake dataset with columns for happiness (Beta distribution clustered around 7ish), cookies (Poisson distribution), and favorite color (binomial distribution for blue/not blue):\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_simple &lt;- tibble(\n  id = 1:n_people,\n  happiness = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness = round(happiness * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\"))\n\nhead(happiness_simple)\n\nWe have a neat dataset now, so let’s run a regression. Is eating more cookies or liking blue associated with greater happiness?\n\nmodel_happiness1 &lt;- lm(happiness ~ cookies + color_blue, data = happiness_simple)\ntidy(model_happiness1)\n\nNot really. The coefficients for both cookies and color_blueBlue are basically 0 and not statistically significant. That makes sense since the three columns are completely independent of each other. If there were any significant effects, that’d be strange and solely because of random chance.\nFor the sake of your final project, you can just leave all the columns completely independent of each other if you want. None of your results will be significant and you won’t see any effects anywhere, but you can still build models, run all the pre-model diagnostics, and create graphs and tables based on this data.\nHOWEVER, it will be far more useful to you if you generate relationships. The whole goal of this class is to find causal effects in observational, non-experimental data. If you can generate synthetic non-experimental data and bake in a known causal effect, you can know if your different methods for recovering that effect are working.\nSo how do we bake in correlations and causal effects?\n\n\nExplanatory variables linked to outcome; no connection between explanatory variables\nTo help with the intuition of how to link these columns, think about the model we’re building:\n\\[\n\\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon\n\\]\nThis model provides estimates for all those betas. Throughout the semester, we’ve used the analogy of sliders and switches to describe regression coefficients. Here we have both:\n\n\\(\\beta_0\\): The average baseline happiness.\n\\(\\beta_1\\): The additional change in happiness that comes from eating one cookie. This is a slider: move cookies up by one and happiness changes by \\(\\beta_1\\).\n\\(\\beta_2\\): The change in happiness that comes from having your favorite color be blue. This is a switch: turn on “blue” for someone and their happiness changes by \\(\\beta_2\\).\n\nWe can invent our own coefficients and use some math to build them into the dataset. Let’s use these numbers as our targets:\n\n\\(\\beta_0\\): Average happiness is 7\n\\(\\beta_1\\): Eating one more cookie boosts happiness by 0.25 points\n\\(\\beta_2\\): People who like blue have 0.75 higher happiness\n\nWhen generating the data, we can’t just use rbeta() by itself to generate happiness, since happiness depends on both cookies and favorite color (that’s why we call it a dependent variable). To build in this effect, we can add a new column that uses math and modifies the underlying rbeta()-based happiness score:\n\nhappiness_with_effect &lt;- happiness_simple %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite color\n  mutate(happiness_modified = happiness + (0.25 * cookies) + (0.75 * color_blue_binary))\nhead(happiness_with_effect)\n\nNow that we have a new happiness_modified column we can run a model using it as the outcome:\n\nmodel_happiness2 &lt;- lm(happiness_modified ~ cookies + color_blue, data = happiness_with_effect)\ntidy(model_happiness2)\n\nWhoa! Look at those coefficients! They’re exactly what we tried to build in! The baseline happiness (intercept) is ≈7, eating one cookie is associated with a ≈0.25 increase in happiness, and liking blue is associated with a ≈0.75 increase in happiness.\nHowever, we originally said that happiness was a 0-10 point scale. After boosting it with extra happiness for cookies and liking blue, there are some people who score higher than 10:\n\n# Original scale\nggplot(happiness_with_effect, aes(x = happiness)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:11) +\n  coord_cartesian(xlim = c(0, 11))\n\n\n# Scaled up\nggplot(happiness_with_effect, aes(x = happiness_modified)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:11) +\n  coord_cartesian(xlim = c(0, 11))\n\nTo fix that, we can use the rescale() function from the scales package to force the new happiness_modified variable to fit back in its original range:\n\nhappiness_with_effect &lt;- happiness_with_effect %&gt;%\n  mutate(happiness_rescaled = rescale(happiness_modified, to = c(3, 10)))\n\nggplot(happiness_with_effect, aes(x = happiness_rescaled)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:11) +\n  coord_cartesian(xlim = c(0, 11))\n\nEverything is back in the 3–10 range now. However, the rescaling also rescaled our built-in effects. Look what happens if we use the happiness_rescaled in the model:\n\nmodel_happiness3 &lt;- lm(happiness_rescaled ~ cookies + color_blue, data = happiness_with_effect)\ntidy(model_happiness3)\n\nNow the baseline happiness is 6.3, the cookies effect is 0.2, and the blue effect is 0.63. These effects shrunk because we shrunk the data back down to have a maximum of 10.\nThere are probably fancy mathy ways to rescale data and keep the coefficients the same size, but rather than figure that out (who even wants to do that?!), my strategy is just to play with numbers until the results look good. Instead of using a 0.25 cookie effect and 0.75 blue effect, I make those effects bigger so that the rescaled version is roughly what I really want. There’s no systematic way to do this—I ran this chunk below a bunch of times until the numbers worked.\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_real_effect &lt;- tibble(\n  id = 1:n_people,\n  happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness_baseline = round(happiness_baseline * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\")) %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite color\n  mutate(happiness_effect = happiness_baseline +\n           (0.31 * cookies) +  # Cookie effect\n           (0.91 * color_blue_binary)) %&gt;%  # Blue effect\n  # Rescale to 3-10, since that's what the original happiness column looked like\n  mutate(happiness = rescale(happiness_effect, to = c(3, 10)))\n\nmodel_does_this_work_yet &lt;- lm(happiness ~ cookies + color_blue, data = happiness_real_effect)\ntidy(model_does_this_work_yet)\n\nThere’s nothing magical about the 0.31 and 0.91 numbers I used here; I just kept changing those to different things until the regression coefficients ended up at ≈0.25 and ≈0.75. Also, I gave up on trying to make the baseline happiness 7. It’s possible to do—you’d just need to also shift the underlying Beta distribution up (like shape1 = 9, shape2 = 2 or something). But then you’d also need to change the coefficients more. You’ll end up with 3 moving parts and it can get complicated, so I don’t worry too much about it, since what we care about the most here is the effect of cookies and favorite color, not baseline levels of happiness.\nPhew. We successfully connected cookies and favorite color to happiness and we have effects that are measurable with regression! One last thing that I would do is get rid of some of the intermediate columns like color_blue_binary or happiness_effect—we only used those for the behind-the-scenes math of creating the effect. Here’s our final synthetic dataset:\n\nhappiness &lt;- happiness_real_effect %&gt;%\n  select(id, happiness, cookies, color_blue)\nhead(happiness)\n\nWe can save that as a CSV file with write_csv():\n\nwrite_csv(happiness, \"data/happiness_fake_data.csv\")\n\n\n\nExplanatory variables linked to outcome; connection between explanatory variables\nIn that cookie example, we assumed that both cookie consumption and favorite color are associated with happiness. We also assumed that cookie consumption and favorite color are not related to each other. But what if they are? What if people who like blue eat more cookies?\nWe’ve already used regression-based math to connect explanatory variables to outcome variables. We can use that same intuition to connect explanatory variables to each other.\nThe easiest way to think about this is with DAGs. Here’s the DAG for the model we just ran:\n\nhappiness_dag1 &lt;- dagify(hap ~ cook + blue,\n                         coords = list(x = c(hap = 3, cook = 1, blue = 2),\n                                       y = c(hap = 1, cook = 1, blue = 2)))\n\nggdag(happiness_dag1) +\n  theme_dag()\n\nBoth cookies and favorite color cause happiness, but there’s no link between them. Notice that dagify() uses the same model syntax that lm() does: hap ~ cook + blue. If we think of this just like a regression model, we can pretend that there are coefficients there too: hap ~ 0.25*cook + 0.75*blue. We don’t actually include any coefficients in the DAG or anything, but it helps with the intuition.\nBut what if people who like blue eat more cookies on average? For whatever reason, let’s pretend that liking blue causes you to eat 0.5 more cookies, on average. Here’s the new DAG:\n\nhappiness_dag2 &lt;- dagify(hap ~ cook + blue,\n                         cook ~ blue,\n                         coords = list(x = c(hap = 3, cook = 1, blue = 2),\n                                       y = c(hap = 1, cook = 1, blue = 2)))\n\nggdag(happiness_dag2) +\n  theme_dag()\n\nNow we have two different equations: hap ~ cook + blue and cook ~ blue. Conveniently, these both translate to models, and we know the coefficients we want!\n\nhap ~ 0.25*cook + 0.75*blue: This is what we built before—cookies boost happiness by 0.25 and liking blue boosts happiness by 0.75\ncook ~ 0.3*blue: This is what we just proposed—liking blue boosts cookies by 0.5\n\nWe can follow the same process we did when building the cookie and blue effects into happiness to also build a blue effect into cookies!\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_cookies_blue &lt;- tibble(\n  id = 1:n_people,\n  happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness_baseline = round(happiness_baseline * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\")) %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make blue have an effect on cookie consumption\n  mutate(cookies = cookies + (0.5 * color_blue_binary)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite color\n  mutate(happiness_effect = happiness_baseline +\n           (0.31 * cookies) +  # Cookie effect\n           (0.91 * color_blue_binary)) %&gt;%  # Blue effect\n  # Rescale to 3-10, since that's what the original happiness column looked like\n  mutate(happiness = rescale(happiness_effect, to = c(3, 10)))\nhead(happiness_cookies_blue)\n\nNotice now that people who like blue eat partial cookies, as expected. We can verify that there’s a relationship between liking blue and cookies by running a model:\n\nlm(cookies ~ color_blue, data = happiness_cookies_blue) %&gt;%\n  tidy()\n\nYep. Liking blue is associated with 0.46 more cookies on average (it’s not quite 0.5, but that’s because of randomness).\nNow let’s do some neat DAG magic. Let’s say we’re interested in the causal effect of cookies on happiness. We could run a naive model:\n\nmodel_happiness_naive &lt;- lm(happiness ~ cookies, data = happiness_cookies_blue)\ntidy(model_happiness_naive)\n\nBased on this, eating a cookie causes you to have 0.325 more happiness points. But that’s wrong! Liking the color blue is a confounder and opens a path between cookies and happiness. You can see the confounding both in the DAG (since blue points to both the cookie node and the happiness node) and in the math (liking blue boosts happiness + liking blue boosts cookie consumption, which boosts happiness).\nTo fix this confounding, we need to statistically adjust for liking blue and close the backdoor path. Ordinarily we’d do this with something like matching or inverse probability weighting, but here we can just include liking blue as a control variable (since it’s linearly related to both cookies and happiness):\n\nmodel_happiness_ate &lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_blue)\ntidy(model_happiness_ate)\n\nAfter adjusting for backdoor confounding, eating one additional cookie causes a 0.249 point increase in happiness. This is the effect we originally built into the data!\nIf you wanted, we could rescale the number of cookies just like we rescaled happiness before, since sometimes adding effects to columns changes their reasonable ranges.\nNow that we have a good working dataset, we can keep the columns we care about and save it as a CSV file for later use:\n\nhappiness &lt;- happiness_cookies_blue %&gt;%\n  select(id, happiness, cookies, color_blue)\nhead(happiness)\n\n\nwrite_csv(happiness, \"data/happiness_fake_data.csv\")\n\n\n\nAdding extra noise\nWe’ve got columns that follow specific distributions, and we’ve got columns that are statistically related to each other. We can add one more wrinkle to make our fake data even more fun (and even more reflective of real life). We can add some noise.\nRight now, the effects we’re finding are too perfect. When we used mutate() to add a 0.25 boost in happiness for every cookie people ate, we added exactly 0.25 happiness points. If someone ate 2 cookies, they got 0.5 more happiness; if they ate 5, they got 1.25 more.\nWhat if the cookie effect isn’t exactly 0.25, but somewhere around 0.25? For some people it’s 0.1, for others it’s 0.3, for others it’s 0.22. We can use the same ideas we talked about in the random numbers example to generate a distribution of an effect. For instance, let’s say that the average cookie effect is 0.25, but it can vary somewhat with a standard deviation of 0.15:\n\ntemp_data &lt;- tibble(x = rnorm(10000, mean = 0.25, sd = 0.15))\n\nggplot(temp_data, aes(x = x)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, color = \"white\")\n\nSometimes it can go as low as −0.25; sometimes it can go as high as 0.75; normally it’s around 0.25.\nNothing in the model explains why it’s higher or lower for some people—it’s just random noise. Remember that the model accounts for that! This random variation is what the \\(\\varepsilon\\) is for in this model equation:\n\\[\n\\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon\n\\]\nWe can build that uncertainty into the fake column! Instead of using 0.31 * cookies when generating happiness (which is technically 0.25, but shifted up to account for rescaling happiness back down after), we’ll make a column for the cookie effect and then multiply that by the number of cookies.\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_cookies_noisier &lt;- tibble(\n  id = 1:n_people,\n  happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  cookie_effect = rnorm(n_people, mean = 0.31, sd = 0.2),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness_baseline = round(happiness_baseline * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\")) %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make blue have an effect on cookie consumption\n  mutate(cookies = cookies + (0.5 * color_blue_binary)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite\n  # color. Importantly, instead of using 0.31 * cookies, we'll use the random\n  # cookie effect we generated earlier\n  mutate(happiness_effect = happiness_baseline +\n           (cookie_effect * cookies) +\n           (0.91 * color_blue_binary)) %&gt;%\n  # Rescale to 3-10, since that's what the original happiness column looked like\n  mutate(happiness = rescale(happiness_effect, to = c(3, 10)))\nhead(happiness_cookies_noisier)\n\nNow let’s look at the cookie effect in this noisier data:\n\nmodel_noisier &lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_noisier)\ntidy(model_noisier)\n\nThe effect is a little smaller now because of the extra noise, so we’d need to mess with the 0.31 and 0.91 coefficients more to get those numbers back up to 0.25 and 0.75.\nWhile this didn’t influence the findings too much here, it can have consequences for other variables. For instance, in the previous section we said that the color blue influences cookie consumption. If the blue effect on cookies isn’t precisely 0.5 but follows some sort of distribution (sometimes small, sometimes big, sometimes negative, sometimes zero), that will influence cookies differently. That random effect on cookie consumption will then work together with the random effect of cookies on happiness, resulting in multiple varied values.\nFor instance, imagine the average effect of liking blue on cookies is 0.5, and the average effect of cookies on happiness is 0.25. For one person, their blue-on-cookie effect might be 0.392, which changes the number of cookies they eat. Their cookie-on-happiness effect is 0.573, which changes their happiness. Both of those random effects work together to generate the final happiness.\nIf you want more realistic-looking synthetic data, it’s a good idea to add some random noise wherever you can.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#visualizing-variables-and-relationships",
    "href": "slides/synthetic-data.html#visualizing-variables-and-relationships",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "2 Visualizing variables and relationships",
    "text": "2 Visualizing variables and relationships\nGoing through this process requires a ton of trial and error. You will change all sorts of numbers to make sure the relationships you’re building work. This is especially the case if you rescale things, since that rescales your effects. There are a lot of moving parts and this is a complicated process.\nYou’ll run your data generation chunks lots and lots and lots of times, tinkering with the numbers as you go. This example makes it look easy, since it’s the final product, but I ran all these chunks over and over again until I got the causal effect and relationships just right.\nIt’s best if you also create plots and models to see what the relationships look like\n\nVisualizing one variable\nWe covered a bunch of distributions in the random number generation example, but it’s hard to think about what a standard deviation of 2 vs 10 looks like, or what happens when you mess with the shape parameters in a Beta distribution.\nIt’s best to visualize these variables. You could build the variable into your official dataset and then look at it, but I find it’s often faster to just look at what a general distribution looks like first. The easiest way to do this is generate a dataset with just one column in it and look at it, either with a histogram or a density plot.\nFor instance, what does a Beta distribution with shape1 = 3 and shape2 = 16 look like? The math says it should peak around 0.15ish (\\(\\frac{3}{3 + 16}\\)), and that looks like the case:\n\ntemp_data &lt;- tibble(x = rbeta(10000, shape1 = 3, shape2 = 16))\n\nplot1 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, color = \"white\")\n\nplot2 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_density()\n\nplot1 + plot2\n\nWhat if we want a normal distribution centered around 100, with most values range from 50 to 150. That’s range of ±50, but that doesn’t mean the sd will be 50—it’ll be much smaller than that, like 25ish. Tinker with the numbers until it looks right.\n\ntemp_data &lt;- tibble(x = rnorm(10000, mean = 100, sd = 25))\n\nplot1 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_histogram(binwidth = 10, boundary = 0, color = \"white\")\n\nplot2 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_density()\n\nplot1 + plot2\n\n\n\nVisualizing two continuous variables\nIf you have two continuous/numeric columns, it’s best to use a scatterplot. For instance, let’s make two columns based on the Beta and normal distributions above, and we’ll make it so that y goes up by 0.25 for every increase in x, along with some noise:\n\nset.seed(1234)\n\ntemp_data &lt;- tibble(\n  x = rnorm(1000, mean = 100, sd = 25)\n) %&gt;%\n  mutate(y = rbeta(1000, shape1 = 3, shape2 = 16) +  # Baseline distribution\n           (0.25 * x) +  # Effect of x\n           rnorm(1000, mean = 0, sd = 10))  # Add some noise\n\nggplot(temp_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe can confirm the effect with a model:\n\nlm(y ~ x, data = temp_data) %&gt;%\n  tidy()\n\n\n\nVisualizing a binary variable and a continuous variable\nIf you have one binary column and one continuous/numeric column, it’s generally best to not use a scatterplot. Instead, either look at the distribution of the continuous variable across the binary variable with a faceted histogram or overlaid density plot, or look at the average of the continuous variable across the different values of the binary variable with a point range.\nLet’s make two columns: a continuous outcome (y) and a binary treatment (x). Being in the treatment group causes an increase of 20 points, on average.\n\nset.seed(1234)\n\ntemp_data &lt;- tibble(\n  treatment = rbinom(1000, size = 1, prob = 0.5)  # Make 1000 0/1 values with 50% chance of each\n) %&gt;%\n  mutate(outcome = rbeta(1000, shape1 = 3, shape2 = 16) +  # Baseline distribution\n           (20 * treatment) +  # Effect of treatment\n           rnorm(1000, mean = 0, sd = 20)) %&gt;%   # Add some noise\n  mutate(treatment = factor(treatment))  # Make treatment a factor/categorical variable\n\nWe can check the numbers with a model:\n\nlm(outcome ~ treatment, data = temp_data) %&gt;% tidy()\n\nHere’s what that looks like as a histogram:\n\nggplot(temp_data, aes(x = outcome, fill = treatment)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0) +\n  guides(fill = \"none\") +  # Turn off the fill legend since it's redundant\n  facet_wrap(vars(treatment), ncol = 1)\n\nAnd as overlapping densities:\n\nggplot(temp_data, aes(x = outcome, fill = treatment)) +\n  geom_density(alpha = 0.5)\n\nAnd with a point range:\n\n# hahaha these error bars are tiny\nggplot(temp_data, aes(x = treatment, y = outcome, color = treatment)) +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\") +\n  guides(color = \"none\")  # Turn off the color legend since it's redundant",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#specific-examples",
    "href": "slides/synthetic-data.html#specific-examples",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "3 Specific examples",
    "text": "3 Specific examples\n\ntl;dr: The general process\nThose previous sections go into a lot of detail. In general, here’s the process you should follow when building relationships in synthetic data:\n\nDraw a DAG that maps out how all the columns you care about are related.\nSpecify how those nodes are measured.\nSpecify the relationships between the nodes based on the DAG equations.\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nVerify all relationships with plots and models.\nTry it out!\nSave the data.\n\n\n\nCreating an effect in an observational DAG\n\nDraw a DAG that maps out how all the columns you care about are related.\nHere’s a simple DAG that shows the causal effect of mosquito net usage on malaria risk. Income and health both influence and confound net use and malaria risk, and income also influences health.\n\nmosquito_dag &lt;- dagify(mal ~ net + inc + hlth,\n                       net ~ inc + hlth,\n                       hlth ~ inc,\n                       coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3),\n                                     y = c(mal = 1, net = 1, inc = 2, hlth = 2)))\nggdag(mosquito_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nFor the sake of this example, we’ll measure these nodes like so. See the random number example for more details about the distributions.\n\nMalaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution. However, since we want to use other variables that increase the likelihood of using a net, we’ll do some cool tricky stuff, explained later.\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\nHealth: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are three models in this DAG:\n\nhlth ~ inc: Income influences health. We’ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\nnet ~ inc + hlth: Income and health both increase the probability of net usage. This is where we do some cool tricky stuff.\nBoth income and health have an effect on the probability of bed net use, but bed net use is measured as a 0/1, TRUE/FALSE variable. If we run a regression with net as the outcome, we can’t really interpret the coefficients like “a 1 point increase in health is associated with a 0.42 point increase in bed net being TRUE.” That doesn’t even make sense.\nOrdinarily, when working with binary outcome variables, you use logistic regression models (see the crash course we had when talking about propensity scores here). In this kind of regression, the coefficients in the model represent changes in the log odds of using a net. As we discuss in the crash course section, log odds are typically impossible to interpet. If you exponentiate them, you get odds ratios, which let you say things like “a 1 point increase in health is associated with a 15% increase in the likelihood of using a net.” Technically we could include coefficients for a logistic regression model and simulate probabilities of using a net or not using log odds and odds ratios (and that’s what I do in the rain barrel data from Problem Set 3 (see code here)), but that’s really hard to wrap your head around since you’re dealing with strange uninterpretable coefficients. So we won’t do that here.\nInstead, we’ll do some fun trickery. We’ll create something called a “bed net score” that gets bigger as income and health increase. We’ll say that a 1 point increase in health score is associated with a 1.5 point increase in bed net score, and a 1 dollar increase in income is associated with a 0.5 point increase in bed net score. This results in a column that ranges all over the place, from 200 to 500 (in this case; that won’t always be true). This column definitely doesn’t look like a TRUE/FALSE binary column—it’s just a bunch of numbers. That’s okay!\nWe’ll then use the rescale() function from the scales package to take this bed net score and scale it down so that it goes from 0.05 to 0.95. This represents a person’s probability of using a bed net.\nFinally, we’ll use that probability in the rbinom() function to generate a 0 or 1 for each person. Some people will have a high probability because of their income and health, like 0.9, and will most likely use a net. Some people might have a 0.15 probability and will likely not use a net.\nWhen you generate binary variables like this, it’s hard to know the exact effect you’ll get, so it’s best to tinker with the numbers until you see relationships that you want.\nmal ~ net + inc + hlth: Finally net use, income, and health all have an effect on the risk of malaria. Building this relationship is easy since it’s just a regular linear regression model (since malaria risk is not binary). We’ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That’s the casual effect we’re building in to the DAG.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nHere we go! Let’s make some data. I’ll comment the code below so you can see what’s happening at each step.\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1138 people (just for fun)\nn_people &lt;- 1138\n\nnet_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %&gt;%\n  # Generate net variable based on income, health, and random noise\n  mutate(net_score = (0.5 * income) + (1.5 * health) + rnorm(n_people, mean = 0, sd = 15),\n         # Scale net score down to 0.05 to 0.95 to create a probability of using a net\n         net_probability = rescale(net_score, to = c(0.05, 0.95)),\n         # Randomly generate a 0/1 variable using that probability\n         net = rbinom(n_people, 1, net_probability)) %&gt;%\n  # Finally generate a malaria risk variable based on income, health, net use,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down by 10 when using a net. Because we rescale things,\n         # though, we have to make the effect a lot bigger here so it scales\n         # down to -10. Risk also decreases as health and income go up. I played\n         # with these numbers until they created reasonable coefficients.\n         malaria_effect = (-30 * net) + (-1.9 * health) + (-0.1 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70)))\n\n# Look at all these columns!\nhead(net_data)\n\nVerify all relationships with plots and models.\nLet’s see if we have the relationships we want. Income looks like it’s associated with health:\n\nggplot(net_data, aes(x = income, y = health)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nlm(health ~ income, data = net_data) %&gt;% tidy()\n\nIt looks like richer and healthier people are more likely to use nets:\n\nnet_income &lt;- ggplot(net_data, aes(x = income, fill = as.factor(net))) +\n  geom_density(alpha = 0.7) +\n  theme(legend.position = \"bottom\")\n\nnet_health &lt;- ggplot(net_data, aes(x = health, fill = as.factor(net))) +\n  geom_density(alpha = 0.7) +\n  theme(legend.position = \"bottom\")\n\nnet_income + net_health\n\nIncome increasing makes it 1% more likely to use a net; health increasing make it 2% more likely to use a net:\n\nglm(net ~ income + health, family = binomial(link = \"logit\"), data = net_data) %&gt;%\n  tidy(exponentiate = TRUE)\n\nTry it out!\nIs the effect in there? Let’s try finding it by controlling for our two backdoors: health and income. Ordinarily we should do something like matching or inverse probability weighting, but we’ll just do regular regression here (which is okay-ish, since all these variables are indeed linearly related with each other—we made them that way!)\nIf we just look at the effect of nets on malaria risk without any statistical adjustment, we see that net cause a decrease of 13 points in malaria risk. This is wrong though becuase there’s confounding.\n\n# Wrong correlation-is-not-causation effect\nmodel_net_naive &lt;- lm(malaria_risk ~ net, data = net_data)\ntidy(model_net_naive)\n\nIf we control for the confounders, we get the 10 point ATE. It works!\n\n# Correctly adjusted ATE effect\nmodel_net_ate &lt;- lm(malaria_risk ~ net + health + income, data = net_data)\ntidy(model_net_ate)\n\nSave the data.\nSince it works, let’s save it:\n\n# In the end, all we need is id, income, health, net, and malaria risk:\nnet_data_final &lt;- net_data %&gt;%\n  select(id, income, health, net, malaria_risk)\nhead(net_data_final)\n\n\n# Save it as a CSV file\nwrite_csv(net_data_final, \"data/bed_nets.csv\")\n\n\n\n\nBrief pep talk intermission\nGenerating data for a full complete observational DAG like the example above is complicated and hard. These other forms of causal inference are design-based (i.e. tied to specific contexts like before/after treatment/control or arbitrary cutoffs) instead of model-based, so they’re actually a lot easier to simulate! So don’t be scared away yet!\n\n\nCreating an effect for RCTs\n\nDraw a DAG that maps out how all the columns you care about are related.\nRCTs are great because they make DAGs really easy! In a well-randomized RCT, you get to delete all arrows going into the treatment node in a DAG. We’ll stick with the same mosquito net situation we just used, but make it randomized:\n\nrct_dag &lt;- dagify(mal ~ net + inc + hlth,\n                  hlth ~ inc,\n                  coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3),\n                                y = c(mal = 1, net = 1, inc = 2, hlth = 2)))\nggdag(rct_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nWe’ll measure these nodes the same way as before:\n\nMalaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution.\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\nHealth: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThis is where RCTs are great. Because we removed all the arrows going into net, we don’t need to build any relationships that influence net use. Net use is randomized! We don’t need to make strange “bed net scores” and give people boosts according to income or health or anything. There are only two models in this DAG:\n\nhlth ~ inc: Income influences health. We’ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\nmal ~ net + inc + hlth: Net use, income, and health all have an effect on the risk of malaria. We’ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That’s the casual effect we’re building in to the DAG.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet’s make this data. It’ll be a lot easier than the full DAG we did before. Again, I’ll comment the code below so you can see what’s happening at each step.\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 793 people (just for fun)\nn_people &lt;- 793\n\nrct_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %&gt;%\n  # Randomly assign people to use a net (this is nice and easy!)\n  mutate(net = rbinom(n_people, 1, 0.5)) %&gt;%\n  # Finally generate a malaria risk variable based on income, health, net use,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down by 10 when using a net. Because we rescale things,\n         # though, we have to make the effect a lot bigger here so it scales\n         # down to -10. Risk also decreases as health and income go up. I played\n         # with these numbers until they created reasonable coefficients.\n         malaria_effect = (-35 * net) + (-1.9 * health) + (-0.1 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70)))\n\n# Look at all these columns!\nhead(rct_data)\n\nVerify all relationships with plots and models.\nIncome still looks like it’s associated with health (which isn’t surprising, since it’s the same code we used for the full DAG earlier):\n\nggplot(net_data, aes(x = income, y = health)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nlm(health ~ income, data = net_data) %&gt;% tidy()\n\nTry it out!\nIs the effect in there? With an RCT, all we really need to do is compare the outcome across treatment and control groups—because there’s no confounding, we don’t need to control for anything. Ordinarily we should check for balance across characteristics like health and income (and maybe generate other demographic columns) like we did in the RCT example, but we’ll skip all that here since we’re just checking to see if the effect is there.\nIt looks like using nets causes an average decrease of 10 risk points. Great!\n\n# Correct RCT-based ATE\nmodel_rct &lt;- lm(malaria_risk ~ net, data = rct_data)\ntidy(model_rct)\n\nJust for fun, if we control for health and income, we’ll get basically the same effect, since they don’t actualy confound the relationship and don’t really explain anything useful.\n\n# Controlling for stuff even though we don't need to\nmodel_rct_controls &lt;- lm(malaria_risk ~ net + health + income, data = rct_data)\ntidy(model_rct_controls)\n\nSave the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file.\n\n# In the end, all we need is id, income, health, net, and malaria risk:\nrct_data_final &lt;- rct_data %&gt;%\n  select(id, income, health, net, malaria_risk)\nhead(rct_data_final)\n\n\n# Save it as a CSV file\nwrite_csv(rct_data_final, \"data/bed_nets_rct.csv\")\n\n\n\n\nCreating an effect for diff-in-diff\n\nDraw a DAG that maps out how all the columns you care about are related.\nDifference-in-differences approaches to causal inference are not based on models but on context or research design. You need comparable treatment and control groups before and after some policy or program is implemented.\nWe’ll keep with our mosquito net example and pretend that two cities in some country are dealing with malaria infections. City B rolls out a free net program in 2017; City A does not. Here’s what the DAG looks like:\n\ndid_dag &lt;- dagify(mal ~ net + year + city,\n                  net ~ year + city,\n                  coords = list(x = c(mal = 3, net = 1, year = 2, city = 2),\n                                y = c(mal = 2, net = 2, year = 3, city = 1)))\nggdag(did_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n\nMalaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don’t need to simulate it since it will only happen for people who are in the treatment city after the universal net rollout.\nYear: year ranging from 2013 to 2020. Best to use a uniform distribution.\nCity: binary 0/1, City A/City B variable. Best to use a binomial distribution.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n\nnet ~ year + city: Net use is determined by being in City B and being after 2017. We’ll assume perfect compliance here (but it’s fairly easy to simulate non-compliance and have some people in City A use nets after 2017, and some people in both cities use nets before 2017).\nmal ~ net + year + city: Malaria risk is determined by net use, year, and city. It’s determined by lots of other things too (like we saw in the previous DAGs), but since we’re assuming that the two cities are comparable treatment and control groups, we don’t need to worry about things like health, income, age, etc.\nWe’ll pretend that in general, City B has historicallly had a problem with malaria and people there have had higher risk: being in City B increases malaria risk by 5 points, on average. Over time, both cities have worked on mosquito abatement, so average malaria risk has decreased by 2 points per year (in both cities, because we believe in parallel trends). Using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nGeneration time! Heavily annotated code below:\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 2567 people (just for fun)\nn_people &lt;- 2567\n\ndid_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate year variable: uniform, between 2013 and 2020. Round so it's whole.\n  year = round(runif(n_people, min = 2013, max = 2020), 0),\n  # Generate city variable: binomial, 50% chance of being in a city. We'll use\n  # sample() instead of rbinom()\n  city = sample(c(\"City A\", \"City B\"), n_people, replace = TRUE)\n) %&gt;%\n  # Generate net variable. We're assuming perfect compliance, so this will only\n  # be TRUE for people in City B after 2017\n  mutate(net = ifelse(city == \"City B\" & year &gt; 2017, TRUE, FALSE)) %&gt;%\n  # Generate a malaria risk variable based on year, city, net use, and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 6, shape2 = 3) * 100,\n         # Risk goes up if you're in City B because they have a worse problem.\n         # We could just say \"city_effect = 5\" and give everyone in City A an\n         # exact 5-point boost, but to add some noise, we'll give people an\n         # average boost using rnorm(). Some people might go up 7, some might go\n         # up 1, some might go down 2\n         city_effect = ifelse(city == \"City B\", rnorm(n_people, mean = 5, sd = 2), 0),\n         # Risk goes down by 2 points on average every year. Creating this\n         # effect with regression would work fine (-2 * year), except the years\n         # are huge here (-2 * 2013 and -2 * 2020, etc.) So first we create a\n         # smaller year column where 2013 is year 1, 2014 is year 2, and so on,\n         # that way we can say -2 * 1 and -2 * 6, or whatever.\n         # Also, rather than make risk go down by *exactly* 2 every year, we'll\n         # add some noise with rnorm(), so for some people it'll go down by 1 or\n         # 4 or up by 1, and so on\n         year_smaller = year - 2012,\n         year_effect = rnorm(n_people, mean = -2, sd = 0.1) * year_smaller,\n         # Using a net causes a decrease of 10 points, on average. Again, rather\n         # than use exactly 10, we'll use a distribution around 10. People only\n         # get a net boost if they're in City B after 2017.\n         net_effect = ifelse(city == \"City B\" & year &gt; 2017,\n                             rnorm(n_people, mean = -10, sd = 1.5),\n                             0),\n         # Finally combine all these effects to create the malaria risk variable\n         malaria_risk = malaria_risk_base + city_effect + year_effect + net_effect,\n         # Rescale so it doesn't go below 0 or above 100\n         malaria_risk = rescale(malaria_risk, to = c(0, 100))) %&gt;%\n  # Make an indicator variable showing if the row is after 2017\n  mutate(after = year &gt; 2017)\n\nhead(did_data)\n\nVerify all relationships with plots and models.\nIs risk higher in City B? Yep.\n\nggplot(did_data, aes(x = city, y = malaria_risk, color = city)) +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\") +\n  guides(color = \"none\")\n\nDoes risk decrease over time? And are the trends parallel? There was a weird random spike in City B in 2017 for whatever reason, but in general, the trends in the two cities are pretty parallel from 2013 to 2017.\n\nplot_data &lt;- did_data %&gt;%\n  group_by(year, city) %&gt;%\n  summarize(mean_risk = mean(malaria_risk),\n            se_risk = sd(malaria_risk) / sqrt(n()),\n            upper = mean_risk + (1.96 * se_risk),\n            lower = mean_risk + (-1.96 * se_risk))\n\nggplot(plot_data, aes(x = year, y = mean_risk, color = city)) +\n  geom_vline(xintercept = 2017.5) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = city), alpha = 0.3, color = FALSE) +\n  geom_line() +\n  theme(legend.position = \"bottom\")\n\nTry it out!\nLet’s see if it works! For diff-in-diff we need to use this model:\n\\[\n\\text{Malaria risk} = \\alpha + \\beta\\ \\text{City B} + \\gamma\\ \\text{After 2017} + \\delta\\ (\\text{City B} \\times \\text{After 2017}) + \\varepsilon\n\\]\n\nmodel_did &lt;- lm(malaria_risk ~ city + after + city * after, data = did_data)\ntidy(model_did)\n\nIt works! Being in City B is associated with a 5-point higher risk on average; being after 2017 is associated with a 7.5-point lower risk on average, and being in City B after 2017 causes risk to drop by −10. The number isn’t exactly −10 here, since we rescaled the malaria_risk column a little, but still, it’s close. It’d probably be a good idea to build in some more noise and noncompliance, since the p-values are really really tiny here, but this is good enough for now.\nHere’s an obligatory diff-in-diff visualization:\n\nplot_data &lt;- did_data %&gt;%\n  group_by(after, city) %&gt;%\n  summarize(mean_risk = mean(malaria_risk),\n            se_risk = sd(malaria_risk) / sqrt(n()),\n            upper = mean_risk + (1.96 * se_risk),\n            lower = mean_risk + (-1.96 * se_risk))\n\n# Extract parts of the model results for adding annotations\nmodel_results &lt;- tidy(model_did)\nbefore_treatment &lt;- filter(model_results, term == \"(Intercept)\")$estimate +\n  filter(model_results, term == \"cityCity B\")$estimate\ndiff_diff &lt;- filter(model_results, term == \"cityCity B:afterTRUE\")$estimate\nafter_treatment &lt;- before_treatment + diff_diff +\n  filter(model_results, term == \"afterTRUE\")$estimate\n\nggplot(plot_data, aes(x = after, y = mean_risk, color = city, group = city)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper)) +\n  geom_line() +\n  annotate(geom = \"segment\", x = FALSE, xend = TRUE,\n           y = before_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dashed\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = 2.1, xend = 2.1,\n           y = after_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dotted\", color = \"blue\") +\n  theme(legend.position = \"bottom\")\n\nSave the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file.\n\ndid_data_final &lt;- did_data %&gt;%\n  select(id, year, city, net, malaria_risk)\nhead(did_data_final)\n\n\n# Save data\nwrite_csv(did_data_final, \"data/diff_diff.csv\")\n\n\n\n\nCreating an effect for regression discontinuity\n\nDraw a DAG that maps out how all the columns you care about are related.\nRegression discontinuity designs are also based on context instead of models, so the DAG is pretty simple. We’ll keep with our mosquito net example and pretend that families that earn less than $450 a week qualify for a free net. Here’s the DAG:\n\nrdd_dag &lt;- dagify(mal ~ net + inc,\n                  net ~ cut,\n                  cut ~ inc,\n                  coords = list(x = c(mal = 4, net = 1, inc = 3, cut = 2),\n                                y = c(mal = 1, net = 1, inc = 2, cut = 1.75)))\nggdag(rdd_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n\nMalaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don’t need to simulate it since it will only happen for people who below the cutoff.\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\nCutoff: binary 0/1, below/above $450 variable. This is technically binomial, but we don’t need to simulate it since it is entirely based on income.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are three models in the DAG:\n\ncut ~ inc: Being above or below the cutpoint is determined by income. We know the cutoff is 450, so we just make an indicator showing if people are below that.\nnet ~ cut: Net usage is determined by the cutpoint. If people are below the cutpoint, they’ll use a net; if not, they won’t. We can build in noncompliance here if we want and use fuzzy regression discontinuity. For the sake of this example, we’ll do it both ways, just so you can see both sharp and fuzzy synthetic data.\nmal ~ net + inc: Malaria risk is determined by both net usage and income. It’s also determined by lots of other things (age, education, city, etc.), but we don’t need to include those in the DAG because we’re using RDD to say that we have good treatment and control groups right around the cutoff.\nWe’ll pretend that a 1 dollar increase in income is associated with a drop in risk of 0.01, and that using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet’s fake some data! Heavily annotated code below:\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 5441 people (we need a lot bc we're throwing most away)\nn_people &lt;- 5441\n\nrdd_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate cutoff variable\n  mutate(below_cutoff = ifelse(income &lt; 450, TRUE, FALSE)) %&gt;%\n  # Generate net variable. We'll make two: one that's sharp and has perfect\n  # compliance, and one that's fuzzy\n  # Here's the sharp one. It's easy. If you're below the cutoff you use a net.\n  mutate(net_sharp = ifelse(below_cutoff == TRUE, TRUE, FALSE)) %&gt;%\n  # Here's the fuzzy one, which is a little trickier. If you're far away from\n  # the cutoff, you follow what you're supposed to do (like if your income is\n  # 800, you don't use the program; if your income is 200, you definitely use\n  # the program). But if you're close to the cutoff, we'll pretend that there's\n  # an 80% chance that you'll do what you're supposed to do.\n  mutate(net_fuzzy = case_when(\n    # If your income is between 450 and 500, there's a 20% chance of using the program\n    income &gt;= 450 & income &lt;= 500 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.2, 0.8)),\n    # If your income is above 500, you definitely don't use the program\n    income &gt; 500 ~ FALSE,\n    # If your income is between 400 and 450, there's an 80% chance of using the program\n    income &lt; 450 & income &gt;= 400 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.8, 0.2)),\n    # If your income is below 400, you definitely use the program\n    income &lt; 400 ~ TRUE\n  )) %&gt;%\n  # Finally we can make the malaria risk score, based on income, net use, and\n  # random noise. We'll make two outcomes: one using the sharp net use and one\n  # using the fuzzy net use. They have the same effect built in, we just have to\n  # use net_sharp and net_fuzzy separately.\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100) %&gt;%\n  # Make the sharp version. There's really a 10 point decrease, but because of\n  # rescaling, we use 15. I only chose 15 through lots of trial and error (i.e.\n  # I used -11, ran the RDD model, and the effect was too small; I used -20, ran\n  # the model, and the effect was too big; I kept changing numbers until landing\n  # on -15). Risk also goes down as income increases.\n  mutate(malaria_effect_sharp = (-15 * net_sharp) + (-0.01 * income),\n         malaria_risk_sharp = malaria_risk_base + malaria_effect_sharp + rnorm(n_people, 0, sd = 3),\n         malaria_risk_sharp = rescale(malaria_risk_sharp, to = c(5, 70))) %&gt;%\n  # Do the same thing, but with net_fuzzy\n  mutate(malaria_effect_fuzzy = (-15 * net_fuzzy) + (-0.01 * income),\n         malaria_risk_fuzzy = malaria_risk_base + malaria_effect_fuzzy + rnorm(n_people, 0, sd = 3),\n         malaria_risk_fuzzy = rescale(malaria_risk_fuzzy, to = c(5, 70))) %&gt;%\n  # Make a version of income that's centered at the cutpoint\n  mutate(income_centered = income - 450)\n\nhead(rdd_data)\n\nVerify all relationships with plots and models.\nIs there a cutoff in the running variable when we use the sharp net variable? Yep!\n\nggplot(rdd_data, aes(x = income, y = net_sharp, color = below_cutoff)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) +\n  guides(color = \"none\")\n\nIs there a cutoff in the running variable when we use the fuzzy net variable? Yep! There are some richer people using the program and some poorer people not using it.\n\nggplot(rdd_data, aes(x = income, y = net_fuzzy, color = below_cutoff)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) +\n  guides(color = \"none\")\n\nTry it out!\nLet’s test it! For sharp RDD we need to use this model:\n\\[\n\\text{Malaria risk} = \\beta_0 + \\beta_1 \\text{Income}_\\text{centered} + \\beta_2 \\text{Net} + \\varepsilon\n\\]\nWe’ll use a bandwidth of ±$50, because why not. In real life you’d be more careful about bandwidth selection (or use rdbwselect() from the rdrobust package to find the optimal bandwidth)\n\nggplot(rdd_data, aes(x = income, y = malaria_risk_sharp, color = net_sharp)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.2, size = 0.5) +\n  # Add lines for the full range\n  geom_smooth(data = filter(rdd_data, income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  # Add lines for bandwidth = 50\n  geom_smooth(data = filter(rdd_data, income_centered &gt;= -50 & income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 2) +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0 & income_centered &lt;= 50),\n              method = \"lm\", se = FALSE, size = 2) +\n  theme(legend.position = \"bottom\")\n\n\nmodel_sharp &lt;- lm(malaria_risk_sharp ~ income_centered + net_sharp,\n                  data = filter(rdd_data,\n                                income_centered &gt;= -50 & income_centered &lt;= 50))\ntidy(model_sharp)\n\nThere’s an effect! For people in the bandwidth, the local average treatment effect of nets is a 10.6 point reduction in malaria risk.\nLet’s check if it works with the fuzzy version where there are noncompliers:\n\nggplot(rdd_data, aes(x = income, y = malaria_risk_fuzzy, color = net_fuzzy)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.2, size = 0.5) +\n  # Add lines for the full range\n  geom_smooth(data = filter(rdd_data, income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  # Add lines for bandwidth = 50\n  geom_smooth(data = filter(rdd_data, income_centered &gt;= -50 & income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 2) +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0 & income_centered &lt;= 50),\n              method = \"lm\", se = FALSE, size = 2) +\n  theme(legend.position = \"bottom\")\n\nhere’s a gap, but it’s hard to measure since there are noncompliers on both sides. We can deal with the noncompliance if we use above/below the cutoff as an instrument (see the fuzzy regression discontinuity guide for a complete example). We should run this set of models:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Net}} &= \\gamma_0 + \\gamma_1 \\text{Income}_{\\text{centered}} + \\gamma_2 \\text{Below 450} + \\omega \\\\\\\\\n\\text{Malaria risk} &= \\beta_0 + \\beta_1 \\text{Income}\\_{\\text{centered}} + \\beta_2 \\widehat{\\text{Net}} + \\epsilon\n\\end{aligned}\n\\]\nInstead of doing these two stages by hand (ugh), we’ll do the 2SLS regression with the iv_robust() function from the estimatr package:\n\nlibrary(estimatr)\n\nmodel_fuzzy &lt;- iv_robust(malaria_risk_fuzzy ~ income_centered + net_fuzzy |\n                           income_centered + below_cutoff,\n                         data = filter(rdd_data,\n                                       income_centered &gt;= -50 & income_centered &lt;= 50))\ntidy(model_fuzzy)\n\nThe effect is slightly larger now (−11.2), but that’s because we are looking at a doubly local ATE: compliers in the bandwidth. But still, it’s close to −10, so that’s good. And we could probably get it closer if we did other mathy shenanigans like adding squared and cubed terms or using nonparametric estimation with rdrobust() in the rdrobust package.\nSave the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file. We’ll make two separate CSV files for fuzzy and sharp, just because.\n\nrdd_data_final_sharp &lt;- rdd_data %&gt;%\n  select(id, income, net = net_sharp, malaria_risk = malaria_risk_sharp)\nhead(rdd_data_final_sharp)\n\nrdd_data_final_fuzzy &lt;- rdd_data %&gt;%\n  select(id, income, net = net_fuzzy, malaria_risk = malaria_risk_fuzzy)\nhead(rdd_data_final_fuzzy)\n\n\n# Save data\nwrite_csv(rdd_data_final_sharp, \"data/rdd_sharp.csv\")\nwrite_csv(rdd_data_final_fuzzy, \"data/rdd_fuzzy.csv\")\n\n\n\n\nCreating an effect for instrumental variables\n\nDraw a DAG that maps out how all the columns you care about are related.\nAs with diff-in-diff and regression discontinuity, instrumental variables are a design-based approach to causal inference and thus don’t require complicated models (but you can still add control variables!), so their DAGs are simpler. Once again we’ll look at the effect of mosquito nets on malaria risk, but this time we’ll say that we cannot possibly measure all the confounding factors between net use and malaria risk, so we’ll use an instrument to extract the exogeneity from net use.\nAs we talked about in Session 11, good plausible instruments are hard to find: they have to cause bed net use and not be related to malaria risk except through bed net use.\nFor this example, we’ll pretend that free bed nets are distributed from town halls around the country. We’ll use “distance to town hall” as our instrument, since it could arguably maybe work perhaps. Being closer to a town hall makes you more likely to use a net, but being closer to a town halls doesn’t make put you at higher or lower risk for malaria on its own—it does that only because it changes your likelihood of getting a net.\nThis is where the story for the instrument falls apart, actually; in real life, if you live far away from a town hall, you probably live further from health services and live in more rural places with worse mosquito abatement policies, so you’re probably at higher risk of malaria. It’s probably a bad instrument, but just go with it.\nHere’s the DAG:\n\niv_dag &lt;- dagify(mal ~ net + U,\n                 net ~ dist + U,\n                 coords = list(x = c(mal = 4, net = 2, U = 3, dist = 1),\n                               y = c(mal = 1, net = 1, U = 2, dist = 1.5)),\n                 latent = \"U\")\n\nggdag_status(iv_dag) +\n  guides(color = \"none\") +\n  theme_dag()\n\nSpecify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n\nMalaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable. However, since we want to use other variables that increase the likelihood of using a net, we’ll do some cool tricky stuff with a bed net score, like we did in the observational DAG example earlier.\nDistance: distance to nearest town hall, measured in kilometers, mostly around 3, with a left skewed long tail (i.e. most people live fairly close, some people live far away). Best to use a Beta distribution (to get the skewed shape) that we then rescale.\nUnobserved: who knows?! There are a lot of unknown confounders. We could generate columns like income, age, education, and health, make them mathematically related to malaria risk and net use, and then throw those columns away in the final data so they’re unobserved. That would be fairly easy and intuitive.\nFor the sake of simplicity here, we’ll make a column called “risk factors,” kind of like we did with the “ability” column in the instrumental variables example—it’s a magical column that is unmeasurable, but it’ll open a backdoor path between net use and malaria risk and thus create endogeneity. It’ll be normally distributed around 50, with a standard deviation of 25.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n\nnet ~ dist + U: Net usage is determined by both distance and our magical unobserved risk factor column. Net use is technically binomial, but in order to change the likelihood of net use based on distance to town hall and unobserved stuff, we’ll do the fancy tricky stuff we did in the observational DAG section above: we’ll create a bed net score, increase or decrease that score based on risk factors and distance, scale that score to a 0-1 scale of probabilities, and then draw a binomial 0/1 outcome using those probabilities.\nWe’ll say that a one kilometer increase in the distance to a town halls reduces the bed net score and a one point increase in risk factors reduces the bed net score.\nmal ~ net + U: Malaria risk is determined by both net usage and unkown stuff, or the magical column we’re calling “risk factors.” We’ll say that a one point increase in risk factors increases malaria risk, and that using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nFake data time! Here’s some heavily annotated code:\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1578 people (just for fun)\nn_people &lt;- 1578\n\niv_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate magical unobserved risk factor variable: normal, 500 ± 300\n  risk_factors = rnorm(n_people, mean = 100, sd = 25),\n  # Generate distance to town hall variable\n  distance = rbeta(n_people, shape1 = 1, shape2 = 4)\n) %&gt;%\n  # Scale up distance to be 0.1-15 instead of 0-1\n  mutate(distance = rescale(distance, to = c(0.1, 15))) %&gt;%\n  # Generate net variable based on distance, risk factors, and random noise\n  # Note: These -40 and -2 effects are entirely made up and I got them through a\n  # lot of trial and error and rerunning this stupid chunk dozens of times\n  mutate(net_score = 0 +\n           (-40 * distance) +  # Distance effect\n           (-2 * risk_factors) +  # Risk factor effect\n           rnorm(n_people, mean = 0, sd = 50),  # Random noise\n        net_probability = rescale(net_score, to = c(0.15, 1)),\n        # Randomly generate a 0/1 variable using that probability\n        net = rbinom(n_people, 1, net_probability)) %&gt;%\n  # Generate malaria risk variable based on net use, risk factors, and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 7, shape2 = 5) * 100,\n         # We're aiming for a -10 net effect, but need to boost it because of rescaling\n         malaria_effect = (-20 * net) + (0.5 * risk_factors),\n         # Make the final malaria risk score\n         malaria_risk = malaria_risk_base + malaria_effect,\n         # Rescale so it doesn't go below 0\n         malaria_risk = rescale(malaria_risk, to = c(5, 80)))\niv_data\n\nVerify all relationships with plots and models.\nIs there a relationship between unobserved risk factors and malaria risk? Yep.\n\nggplot(iv_data, aes(x = risk_factors, y = malaria_risk)) +\n  geom_point(aes(color = as.factor(net))) +\n  geom_smooth(method = \"lm\")\n\nIs there a relationship between distance to town hall and net use? Yeah, those who live further away are less likely to use a net.\n\nggplot(iv_data, aes(x = distance, fill = as.factor(net))) +\n  geom_density(alpha = 0.7)\n\nIs there a relationship between net use and malaria risk? Haha, yeah, that’s a huge highly significant effect. Probably too perfect. We could increase those error bars if we tinker with some of the numbers in the code, but for the sake of this example, we’ll leave them like this.\n\nggplot(iv_data, aes(x = as.factor(net), y = malaria_risk, color = as.factor(net))) +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\")\n\nTry it out!\nCool, let’s see if this works. Remember, we can’t actually use the risk_factors column in real life, but we will here just to make sure the effect we built in exists. Here’s the true effect, where using a net causes a decrease of 10.9 malaria risk points\n\nmodel_forbidden &lt;- lm(malaria_risk ~ net + risk_factors, data = iv_data)\ntidy(model_forbidden)\n\nSince we can’t actually use that column, we’ll use distance to town hall as an instrument. We should run this set of models:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Net}} &= \\gamma_0 + \\gamma_1 \\text{Distance to town hall} + \\omega \\\\\\\\\n\\text{Malaria risk} &= \\beta_0 + \\beta_1 \\widehat{\\text{Net}} + \\epsilon\n\\end{aligned}\n\\]\nWe’ll run this 2SLS model with the iv_robust() function from the estimatr package:\n\nlibrary(estimatr)\n\nmodel_iv &lt;- iv_robust(malaria_risk ~ net | distance, data = iv_data)\ntidy(model_iv)\n\n…and it’s relatively close, I guess, at −8.2. Getting instrumental variables to find exact causal effects is tricky, but I’m fine with this for simulated data.\nSave the data.\nThe data works well enough, so we’ll get rid of the extra intermediate columns and save it as a CSV file. We’ll keep the forbidden risk_factors column just for fun.\n\niv_data_final &lt;- iv_data %&gt;%\n  select(id, net, distance, malaria_risk, risk_factors)\n\nhead(iv_data_final)\n\n\n# Save data\nwrite_csv(iv_data_final, \"data/bed_nets_iv.csv\")",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#use-synthetic-data-packages",
    "href": "slides/synthetic-data.html#use-synthetic-data-packages",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "4 Use synthetic data packages",
    "text": "4 Use synthetic data packages\nThere are several R packages that let you generate synthetic data with built-in relationships in a more automatic way. They all work a little differently, and if you’re interested in trying them out, make sure you check the documentation for details.\n\nfabricatr\nThe fabricatr package is a very powerful package for simulating data. It was invented specifically for using in preregistered studies, so it can handle a ton of different data structures like panel data and time series data. You can build in causal effects and force columns to be correlated with each other.\nfabricatr has exceptionally well-written documentation with like a billion detailed examples (see the right sidebar here). This is a gold standard package and you should most definitely check it out.\nHere’s a simple example of simulating a bunch of voters and making older ones more likely to vote:\n\nlibrary(fabricatr)\n\nset.seed(1234)\n\nfake_voters &lt;- fabricate(\n  # Make 100 people\n  N = 100,\n  # Age uniformly distributed between 18 and 85\n  age = round(runif(N, 18, 85)),\n  # Older people more likely to vote\n  turnout = draw_binary(prob = ifelse(age &lt; 40, 0.4, 0.7), N = N)\n)\n\nhead(fake_voters)\n\nAnd here’s an example of country-year panel data where there are country-specific and year-specific effects on GDP:\n\nset.seed(1234)\n\npanel_global_data &lt;- fabricate(\n  years = add_level(\n    N = 10,\n    ts_year = 0:9,\n    year_shock = rnorm(N, 0, 0.3)\n  ),\n  countries = add_level(\n    N = 5,\n    base_gdp = runif(N, 15, 22),\n    growth_units = runif(N, 0.25, 0.5),\n    growth_error = runif(N, 0.15, 0.5),\n    nest = FALSE\n  ),\n  country_years = cross_levels(\n    by = join_using(years, countries),\n    gdp_measure = base_gdp + year_shock + (ts_year * growth_units) +\n      rnorm(N, sd = growth_error)\n  )\n) %&gt;%\n  # Scale up the years to be actual years instead of 1, 2, 3, etc.\n  mutate(year = ts_year + 2010)\n\nhead(panel_global_data)\n\n\nggplot(panel_global_data, aes(x = year, y = gdp_measure, color = countries)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Log GDP\", color = \"Countries\")\n\nThat all just scratches the surface of what fabricatr can do. Again, check the examples and documentation and play around with it to see what else it can do.\n\n\nwakefield\nThe wakefield package is jokingly named after Andrew Wakefield, the British researcher who invented fake data to show that the MMR vaccine causes autism. This package lets you quickly generate random fake datasets. It has a bunch of pre-set column possibilities, like age, color, Likert scales, political parties, religion, and so on, and you can also use standard R functions like rnorm(), rbinom(), or rbeta(). It also lets you create repeated measures (1st grade score, 2nd grade score, 3rd grade score, etc.) and build correlations between variables.\nYou should definitely look at the documentation to see a ton of examples of how it all works. Here’s a basic example:\n\nlibrary(wakefield)\n\nset.seed(1234)\n\nwakefield_data &lt;- r_data_frame(\n  n = 500,\n  id,\n  treatment = rbinom(1, 0.3),  # 30% chance of being in treatment\n  outcome = rnorm(mean = 500, sd = 100),\n  race,\n  age = age(x = 18:45),\n  sex = sex_inclusive(),\n  survey_question_1 = likert(),\n  survey_question_2 = likert()\n)\nhead(wakefield_data)\n\n\n\nfaux\nThe faux package does some really neat things. We can create data that has built-in correlations without going through all the math. For instance, let’s say we have 3 variables A, B, and C that are normally distributed with these parameters:\n\nA: mean = 10, sd = 2\nB: mean = 5, sd = 1\nC: mean = 20, sd = 5\n\nWe want A to correlate with B at r = 0.8 (highly correlated), A to correlate with C at r = 0.3 (less correlated), and B to correlate with C at r = 0.4 (moderately correlated). Here’s how to create that data with faux:\n\nlibrary(faux)\n\nset.seed(1234)\n\nfaux_data &lt;- rnorm_multi(n = 100,\n                         mu = c(10, 5, 20),\n                         sd = c(2, 1, 5),\n                         r = c(0.8, 0.3, 0.4),\n                         varnames = c(\"A\", \"B\", \"C\"),\n                         empirical = FALSE)\nhead(faux_data)\n\n# Check averages and standard deviations\nfaux_data %&gt;%\n  # Convert to long/tidy so we can group and summarize\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  group_by(variable) %&gt;%\n  summarize(mean = mean(value),\n            sd = sd(value))\n\n# Check correlations\ncor(faux_data$A, faux_data$B)\ncor(faux_data$A, faux_data$C)\ncor(faux_data$B, faux_data$C)\n\nfaux can do a ton of other things too, so make sure you check out the documentation and all the articles with examples here.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/slides_2023.html",
    "href": "slides/slides_2023.html",
    "title": "2024",
    "section": "",
    "text": "Druid24\n\n\n\n\n\nDiss - Current Status\n\n\n\n\nCoreSignal Analysis\n\n\n\nCoreSignal Analysis",
    "crumbs": [
      "Presentations",
      "2023",
      "Diss"
    ]
  },
  {
    "objectID": "slides/slides_2021.html",
    "href": "slides/slides_2021.html",
    "title": "2022 - January",
    "section": "",
    "text": "First presentation\n\n\n\nSecond presentation",
    "crumbs": [
      "Presentations",
      "2022",
      "January"
    ]
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Presentations overview",
    "section": "",
    "text": "Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!",
    "crumbs": [
      "Presentations",
      "Overview"
    ]
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#table-of-contents",
    "href": "revealjs/slides/2023/cs/cs_test.html#table-of-contents",
    "title": "CoreSignal Analysis",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#input-for-cs-domain-linkedin-handle-list-from-cb",
    "href": "revealjs/slides/2023/cs/cs_test.html#input-for-cs-domain-linkedin-handle-list-from-cb",
    "title": "CoreSignal Analysis",
    "section": "1. Input for CS: Domain & LinkedIn handle list (from CB)",
    "text": "1. Input for CS: Domain & LinkedIn handle list (from CB)\nObjective: Create list with domains and/or LinkedIn handles\n\nSeperated by organizations & employees\nBased mostly on our crunchbase dataset (I have added some angellist data as well)\n\n\n\n\nScript and File locations\n\n\nData 01_data_sources/06_coresignal/01_data/01_input_for_cs/final_selection/version_3_cijs/\n\norgs_cs.csv\nusr_cs.csv\n\nScripts 01_data_sources/06_coresignal/02_scripts/01_input_for_cs/\n\ncs_org_select_v2.R"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#output-from-cs-company-personal-linkedin-profile-date",
    "href": "revealjs/slides/2023/cs/cs_test.html#output-from-cs-company-personal-linkedin-profile-date",
    "title": "CoreSignal Analysis",
    "section": "2. Output from CS: Company & Personal LinkedIn Profile Date",
    "text": "2. Output from CS: Company & Personal LinkedIn Profile Date\nInstructions:\n\nMap provided domains with company profiles\nProvide entire employed user data (including company profiles)\n\nData was proided as json data:\n\n5 files for companies (~160MB)\n1662 files for employees (~175GB)\n\n\n\n\nScript and File locations\n\n\nData 01_data_sources/06_coresignal/01_data/02_raw/\n\ncompany/202203_custom/...\nmember/202203_custom/..."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#extract-convert-relevant-data",
    "href": "revealjs/slides/2023/cs/cs_test.html#extract-convert-relevant-data",
    "title": "CoreSignal Analysis",
    "section": "3. Extract & convert relevant data",
    "text": "3. Extract & convert relevant data\nObjectives: Extract data that is relevant (variables) for our analyses and convert it to .rds / .parquet files\n\n\n\nScript and File locations\n\n\nScripts 01_data_sources/06_coresignal/02_scripts/02_build_tables/\n\n01_cs_build_table_company.R\n02_cs_build_tables_member_V1_bd_exp_skills.R\n03_cs_build_tables_member_V2_edu_exp.R\n\nData 01_data_sources/06_coresignal/01_data/03_extracted/\n\ncompany/cs_companies_base.rds\nmember/01_basic_data/...\nmember/02_experience/... (non-deduplicated)\nmember/03_skills/...\nmember/04_education/... (non-deduplicated)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#deduplicate-data",
    "href": "revealjs/slides/2023/cs/cs_test.html#deduplicate-data",
    "title": "CoreSignal Analysis",
    "section": "4. Deduplicate data",
    "text": "4. Deduplicate data\nEverytime a user changes something on their profile a new record is being created (date, typos, names, …). The column deleted is not useful.\nObjectives: Deduplicate the member data: Experiences & Education Tables\n\n\n\nScript and File locations\n\n\nScripts 01_data_sources/06_coresignal/02_scripts/02_build_tables/\n\n04_cs_build_tables_member_exp_dist.R\n05_cs_build_tables_member_edu_dist.R\n\nData 01_data_sources/06_coresignal/01_data/04_wrangled/\n\ncompanies/cs_companies_base_slct.rds (just relevant columns selected)\nmember/02_experience/me_dist8/... (deduplicated)\nmember/04_education/02_wrangled_dist_chunked/... (deduplicated)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#overview---orgs",
    "href": "revealjs/slides/2023/cs/cs_test.html#overview---orgs",
    "title": "CoreSignal Analysis",
    "section": "1. Overview - Orgs",
    "text": "1. Overview - Orgs\nCoreSignal did not provide a matching table but provided only the resulting data. Hence, backmapping to our CrunchBase / Pitchbook Data via domains is necessary:\n\nMap Companies\n\n\n\n\nScript and File locations\n\n\nScripts 02_data_mapping/10_cbpb_cs/01_scripts/\n\n06_cbpb_cs_matching_companies.R\n\nData (Input) 02_data_mapping/10_cbpb_cs/02_data/\n\nfunded_companies.rds (created by Christoph)\n\nData (Output)\n\ncbpb_cs_joined.rds (companies joined)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#overview---employees",
    "href": "revealjs/slides/2023/cs/cs_test.html#overview---employees",
    "title": "CoreSignal Analysis",
    "section": "2. Overview - Employees",
    "text": "2. Overview - Employees\nMap Crunchbase / Pitchbook data to CoreSignal profiles (via mapped companies (1))\n\nMap employees\n\n\n\n\nScript and File locations\n\n\nScripts 02_data_mapping/10_cbpb_cs/01_scripts/\n\n07_cs_cb_matching_employees.R\n\nData (Input) 01_data_sources/06_coresignal/01_data/04_wrangled/member/02_experience/me_dist8/02_unnested/ 02_data_mapping/10_cbpb_cs/02_data/\n\ncs_me_dist8_unest_prqt (CoreSignal distinct Member Experiences)\ncbpb_cs_joined.rds (Joined CoreSignal / CrunchbasePitchbook Org Data)\n\nData (Output)\n\ncs_me_dist8_unest_fc_joined.parquet (intermediate data)\n\n\n\n\nThere are some further explanations about the org join and empployee join in the next two section (click on each tabset. There are also some fragments –&gt; You have to hit enter/arrow to slide them in). You can skip to the next chapter (VAR I) though."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#details-load-init-companies-crunchbase-pitchbook-coresignal",
    "href": "revealjs/slides/2023/cs/cs_test.html#details-load-init-companies-crunchbase-pitchbook-coresignal",
    "title": "CoreSignal Analysis",
    "section": "3. Details: Load & Init Companies (CrunchBase, PitchBook & CoreSignal)",
    "text": "3. Details: Load & Init Companies (CrunchBase, PitchBook & CoreSignal)\n\nCBPB: SELECTCBPB: WRANGLECBPB: CLEANCS: SELECTCS: WRANGLECS: CLEAN\n\n\nCrunchbase data contains 150,838 startups with a valid funding trajectory.\n\np_load(arrow, dplyr, tidyr)\n\nfunded_companies_prqt &lt;- open_dataset(\"funded_companies_identifiers.parquet\") \nfunded_companies_prqt\n\n\n\n#&gt; # A tibble: 150,838 × 3\n#&gt;   company_id            domain linkedin_url                                     \n#&gt;        &lt;int&gt; &lt;list&lt;character&gt;&gt; &lt;chr&gt;                                            \n#&gt; 1          1               [1] &lt;NA&gt;                                             \n#&gt; 2          2               [1] https://www.linkedin.com/company/luna-pharmaceut…\n#&gt; 3          3               [1] http://www.linkedin.com/company/chainsync        \n#&gt; # ℹ 150,835 more rows\n\n\n\nMultiple domains (Unnesting via Arrow not possible. Options: Spark & sparklyr.nested):\n\nfc_unnested_tbl &lt;- funded_companies_prqt |&gt; collect() |&gt; \n                      # 1. Allow multiple domains per company. No multiple linkedin handles.\n                      unnest(domain) \nfc_unnested_tbl\n\n#&gt; # A tibble: 155,413 × 3\n#&gt;   company_id domain              linkedin_url                                   \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                                          \n#&gt; 1          1 zana.io             &lt;NA&gt;                                           \n#&gt; 2          2 premamawellness.com https://www.linkedin.com/company/luna-pharmace…\n#&gt; 3          3 chainsync.com       http://www.linkedin.com/company/chainsync      \n#&gt; # ℹ 155,410 more rows\n\n\n\n\n\n\nMust have identifier (domain, linkedin)\nClean identifiers\nRemove duplicates\n\n\nlibrary(stringr)\nfc_unnested_tbl |&gt; \n  \n  # 1. At least 1 identifier: 4.518 observations are filtered out\n  filter(if_any(c(domain, linkedin_url), ~!is.na(.))) |&gt;\n  \n  # 2. Extract linkedin handle & clean domains\n  mutate(linkedin_handle = linkedin_url |&gt; str_extract(\"(?&lt;=linkedin\\\\.com/company/).*?(?=(?:\\\\?|$|/))\")) |&gt;\n  mutate(domain          = domain |&gt; clean_domain()) |&gt;\n\n  # 3. Remove 532 duplicates\n  distinct()\n\n\n\n#&gt; # A tibble: 150,363 × 3\n#&gt;   company_id domain              linkedin_handle          \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                    \n#&gt; 1          1 zana.io             &lt;NA&gt;                     \n#&gt; 2          2 premamawellness.com luna-pharmaceuticals-inc-\n#&gt; 3          3 chainsync.com       chainsync                \n#&gt; # ℹ 150,360 more rows\n\n\n–&gt; 145.991 distinct examineable companies.\n\n\nIssue: Some extracted domains are not unique and associated with multiple companies. Manual Cleaning: Domains with a count exceeding two were analyzed and set to NA if they do not correspond to the actual one.\n\n# ANALYZE\n# fc_wrangled_tbl |&gt; \n#   distinct(company_id, domain) |&gt; \n#   count(domain, sort = T) |&gt; \n#   filter(n&gt;2)`\n\nunwanted_domains_cb &lt;- c(\"webflow.io\", \"angel.co\", \"weebly.com\", \"wordpress.com\", \"wixsite.com\", \"squarespace.com\", \n                         \"webflow.io\", \"crypt2esports.com\", \"myshopify.com\", \"business.site\", \"mystrikingly.com\", \n                         \"launchrock.com\", \"square.site\", \"google.com\", \"sites.google.com\", \"t.co\", \"linktr.ee\",\n                         \"netlify.app\", \"itunes.apple.com\", \"apple.com\", \"crunchb.com\", \"tumblr.com\", \"linkedin.com\",\n                         \"godaddysites.com\", \"mit.edu\", \"paloaltonetworks.com\", \" wpengine.com\", \"facebook.com\",\n                         \"intuit.com\", \"medium.com\", \"salesforce.com\", \"strikingly.com\", \"wix.com\", \"cisco.com\",\n                         \"digi.me\", \"apps.apple.com\", \"bit.ly\", \"fleek.co\", \"harvard.edu\", \"ibm.com\", \"jimdo.com\",\n                         \"myftpupload.com\", \"odoo.com\", \"storenvy.com\", \"twitter.com\", \"umd.edu\", \"umich.edu\", \"vmware.com\", \"webs.com\")\n\n# Not all observations with unwanted domains are bad per se:\nwanted_ids_cb &lt;- c(angel = 128006, `catapult-centres-uk` = 115854, digime1 = 140904, digimi2 = 95430, fleek = 50738, \n                   jimdo = 108655, medium = 113415, storenvy = 85742, strikingly = 95831, substack = 34304, \n                   tumblr = 84838, twitter = 53139, weebly = 91365, wpengine = 91720)\n\n# Set misleading domains to NA\nfunded_companies_clnd &lt;- fc_wrangled_tbl |&gt; \n                              \n  mutate(domain = if_else(\n    domain %in% unwanted_domains_cb & !(company_id %in% wanted_ids_cb), \n    NA_character_, domain))\n\n\n\nIt appears that CoreSignal has been able to locate 45.026 companies within our gathered data.\n\n# Selection & Wrangle has been done already\ncs_companies_base_slct &lt;- readRDS(\"cs_companies_base_slct.rds\") \ncs_companies_base_slct\n\n\n\n#&gt; # A tibble: 45,362 × 4\n#&gt;      id name                                domain               linkedin_handle\n#&gt;   &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;                &lt;chr&gt;          \n#&gt; 1   305 Blueprint, a David's Bridal Company blueprintregistry.c… blueprint-regi…\n#&gt; 2   793 BookingLive                         bookinglive.com      bookinglive    \n#&gt; 3  2425 Brandvee                            momentum.ai          brandvee       \n#&gt; # ℹ 45,359 more rows\n\n\n\n\ncs_companies_base_slct$id |&gt; n_distinct()\n\n#&gt; [1] 45026\n\n\n\n\n\ncs_companies_base_slct |&gt; janitor::get_dupes(id)\n\n#&gt; # A tibble: 672 × 5\n#&gt;        id dupe_count name          domain           linkedin_handle\n#&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;          \n#&gt; 1  596494          2 Vi            vi.co            vitrainer      \n#&gt; 2  596494          2 Vi            vi.co            vi             \n#&gt; 3 1324413          2 Patch Lending patchlending.com patch-of-land  \n#&gt; # ℹ 669 more rows\n\n\n\n\n\nNothing to wrangle …\n\ncs_companies_base_wrangled &lt;- cs_companies_base_slct |&gt; select(-name) |&gt; \n  \n                                        # Add suffixes to col names\n                                        rename_with(~ paste(., \"cs\", sep = \"_\"))\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMore cleaning necessary (same as CBPB)! The task was undertaken with a limited degree of enthusiasm.\n\n\n\n\nunwanted_domains_cs    &lt;- c(\"bit.ly\", \"linktr.ee\", \"facebook.com\", \"linkedin.com\", \"twitter.com\", \"crunchbase.com\")\nwanted_ids_cs          &lt;- c(crunchbase = 1634413, linkedin = 8568581, twitter = 24745469)\n\ncs_companies_base_clnd &lt;- cs_companies_base_wrangled |&gt; \n  \n  mutate(domain_cs = if_else(\n    domain_cs %in% unwanted_domains_cs & !(id_cs %in% wanted_ids_cs), \n    NA_character_, \n    domain_cs)\n    )"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#details-join-companies-member-experiences-and-funding-information",
    "href": "revealjs/slides/2023/cs/cs_test.html#details-join-companies-member-experiences-and-funding-information",
    "title": "CoreSignal Analysis",
    "section": "4. Details: Join companies, member experiences and funding information",
    "text": "4. Details: Join companies, member experiences and funding information\n\nCompaniesJobs (all)Jobs (dist)Jobs (focal)FundingConversionJoin\n\n\nWe were able to match 37.287 CS & CB/PB companies.\n\ncb_cs_joined &lt;- funded_companies_clnd |&gt; \n\n    # Leftjoins\n    left_join(cs_companies_base_clnd |&gt; select(id_cs, domain_cs),          by = c(domain          = \"domain_cs\"),          na_matches = \"never\") |&gt; \n    left_join(cs_companies_base_clnd |&gt; select(id_cs, linkedin_handle_cs), by = c(linkedin_handle = \"linkedin_handle_cs\"), na_matches = \"never\") |&gt; \n\n    # Remove obs with no cs_id\n    filter(!is.na(id_cs)) |&gt;\n    \n    # Remove matches, that matched different domains, but same company (e.g. company_id: 83060, id_cs: 4507928) block.xyz & squareup.com\n    select(company_id, id_cs) |&gt; \n    distinct()\n    \ncb_cs_joined\n\n\n\n#&gt; # A tibble: 38,118 × 2\n#&gt;   company_id    id_cs\n#&gt;        &lt;int&gt;    &lt;int&gt;\n#&gt; 1          2  8345218\n#&gt; 2          5 28149599\n#&gt; 3          8  4469271\n#&gt; 4         11  5349023\n#&gt; 5         12  9364263\n#&gt; # ℹ 38,113 more rows\n\n\n\n\ncb_cs_joined |&gt; distinct(company_id) |&gt; nrow()\n\n#&gt; [1] 37287\n\n\n\n\n\nWe got over 460 million employment observations from CoreSignal.\n\n# Other data versions\n# 1. Complete: \nmember_experience_dt \n#&gt; {462.711.794}  \n\n# 2. Distinct1: \nmember_experience_dist_dt &lt;- unique(member_experience_dt) \n#&gt; {432.368.479}\n\n# 3. Distinct2: \nunique(member_experience_dist_dt[order(id)], by = setdiff(names(member_experience_dist_dt), \"id\")) \n#&gt; {431.899.547}\n\n\n\nBut only ~50 Mil distinct employments\n\n# Load distinct member experiences\nme_dist8_prqt &lt;- arrow::open_dataset(\"cs_me_dist8_unest_empl_hist.parquet\") \nme_dist8_prqt |&gt; glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 51,621,196 rows x 10 columns\n#&gt; $ id_tie                 &lt;int32&gt; 16615559, 16615560, 16615561, 16615562, 1661556…\n#&gt; $ id                    &lt;double&gt; 2244288231, 254049663, 948937291, 254049667, 25…\n#&gt; $ member_id              &lt;int32&gt; 179313066, 179313066, 179313066, 179313066, 179…\n#&gt; $ company_id             &lt;int32&gt; 865089, 9098713, 9098713, NA, 865089, 9020540, …\n#&gt; $ company_name          &lt;string&gt; \"heritage community bank\", \"aurora bank fsb\", \"…\n#&gt; $ title                 &lt;string&gt; \"AVP Chief Compliance/BSA Officer\", \"AVP Compli…\n#&gt; $ date_from_parsed &lt;date32[day]&gt; 2010-02-01, 2012-07-01, 2011-11-01, 1997-07-01,…\n#&gt; $ date_to_parsed   &lt;date32[day]&gt; 2011-11-01, 2013-06-01, 2012-07-01, 2006-05-01,…\n#&gt; $ date_from_parsed_year  &lt;int32&gt; 2010, 2012, 2011, 1997, 2006, 2019, 2017, 2021,…\n#&gt; $ date_to_parsed_year    &lt;int32&gt; 2011, 2013, 2012, 2006, 2010, 2021, 2018, NA, 1…\n#&gt; Call `print()` for full schema details\n\n\nExample\n\nme_orig &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_orig/\")\nme_dist &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_dist/\")\n\nme_orig |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed) |&gt; print(n=19)\nme_dist |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed)\n\n\n\nOver 10 million (valid: must have starting date) employments at our crunchbase / pitchbook data set companies. 385.100 with a title containing the string founder.\n\n# Distinct company ids\ncb_cs_joined_cs_ids &lt;- cb_cs_joined |&gt; distinct(id_cs) |&gt; pull(id_cs)\nme_wrangled_prqt    &lt;- me_dist8_prqt |&gt; \n  \n                          # Select features\n                          select(member_id, company_id, exp_id = \"id\", date_from_parsed) |&gt; \n                          \n                          # Select observations\n                          filter(company_id %in% cb_cs_joined_cs_ids) |&gt; \n                          # - 967.080 observations (date_to not considered yet)\n                          filter(!is.na(date_from_parsed)) |&gt; \n\n                          # Add suffix to col names\n                          rename_with(~ paste(., \"cs\", sep = \"_\")) |&gt; \n                          compute()\n\nme_wrangled_prqt |&gt; \n  glimpse()\n\n#&gt; Table\n#&gt; 11,050,164 rows x 4 columns\n#&gt; $ member_id_cs              &lt;int32&gt; 9897605, 9897605, 9897605, 9897605, 9897928,…\n#&gt; $ company_id_cs             &lt;int32&gt; 1105483, 5181133, 5181133, 5181133, 5025265,…\n#&gt; $ exp_id_cs                &lt;double&gt; 1665233144, 12744849, 995032176, 1665233146,…\n#&gt; $ date_from_parsed_cs &lt;date32[day]&gt; 2018-03-01, 2010-06-01, 2014-09-01, 2011-01-…\n#&gt; Call `print()` for full schema details\n\n\n\n\nMultiple Funding Dates –&gt; Take oldest\n\nfc_wrangled_tbl &lt;- funded_companies_tbl |&gt; \n  \n  # Consider multiple founding dates: Take oldest founding date\n  unnest(founded_on) |&gt; \n  arrange(company_id, founded_on) |&gt; \n  group_by(company_id) |&gt; slice(1) |&gt; ungroup()\n\n\n\n\nExample of funding round data:\n\nfc_wrangled_tbl$funding_rounds[[1]] |&gt;  \n  glimpse()\n\n\n\n#&gt; Rows: 15\n#&gt; Columns: 14\n#&gt; $ round_id             &lt;int&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n#&gt; $ round_uuid_pb        &lt;chr&gt; NA, \"47208-70T\", NA, \"58843-18T\", NA, NA, NA, \"78…\n#&gt; $ round_uuid_cb        &lt;chr&gt; \"a6d3bfd9-5afa-47ce-86de-30a3abad6c9b\", NA, \"ea3b…\n#&gt; $ announced_on         &lt;date&gt; 2013-01-01, 2014-04-01, 2015-06-01, 2015-10-07, …\n#&gt; $ round_new            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 12, 13\n#&gt; $ round                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n#&gt; $ exit_cycle           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n#&gt; $ last                 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1…\n#&gt; $ round_type_new       &lt;fct&gt; Seed, Series A, Series B, Series C, Series D, Ser…\n#&gt; $ round_type           &lt;list&gt; \"angel\", \"angel\", \"early_vc\", \"early_vc\", \"conver…\n#&gt; $ round_types          &lt;list&gt; &lt;\"angel\", \"angel_group\", \"investor\", \"company\", \"…\n#&gt; $ raised_amount        &lt;dbl&gt; NA, 520000, NA, 1399999, NA, NA, NA, 3250000, NA,…\n#&gt; $ post_money_valuation &lt;dbl&gt; NA, NA, NA, 3399998, NA, NA, NA, 10249998, NA, N…\n#&gt; $ investors_in_round   &lt;list&gt; [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df…\n\n\n\n\n\nJoining via dplyr due to memory constraint not possible.\nJoining via Arrow due to structure constraints not possible.\n–&gt; Joining via data.table most efficient.\n\nConversion to data.tables necessary:\n\n# 1.  Funding Data\n# 1.1 Level 1\nfc_wrangled_dt |&gt; setDT()\n\n# 1.2 Funding Data Level 2 (funding_rounds)\npurrr::walk(fc_wrangled_dt$funding_rounds, setDT)\n\n# 1.3 Remove unnecessary columns + initialize dummy for before_join\npurrr::walk(fc_wrangled_dt$funding_rounds, ~ .x[, \n          `:=`(round_uuid_pb = NULL, round_uuid_cb        = NULL, round_new          = NULL, round          = NULL,\n               exit_cycle    = NULL, last                 = NULL, round_type         = NULL, round_type_new = NULL, \n               round_types   = NULL, post_money_valuation = NULL, investors_in_round = NULL, before_join    = NA)\n          ]\n        )\n\n# 2. Matching Table\ncb_cs_joined_slct_dt |&gt; setDT()\n\n# 3. Member experiences\nme_wrangled_dt &lt;- me_wrangled_prqt |&gt; collect()\n\n\n\nWorking data.table solution (efficiency increase through join by reference possible).\n\n# 1. Add company_id from funded_companies to member experiences\nme_joined_dt &lt;- cb_cs_joined_slct_dt[me_wrangled_dt, on = .(id_cs = company_id_cs), allow.cartesian = TRUE]\n#&gt; 12.978.226\n\n# 2. Add funding data from funded_companies\nme_joined_dt &lt;- fc_wrangled_dt[me_joined_dt, on = .(company_id)]\n#&gt; 12.270.572\n\n# 3. Remove duplicates (why are there any?)\nme_joined_dt &lt;- unique(me_joined_dt, by = setdiff(names(me_joined_dt), \"funding_rounds\"))\n#&gt; 12.270.572 .... No duplicates anymore. Removed from cb_cs_joined_slct_dt\n\nNot working dplyr solution\n\nme_joined_dt_dplyr &lt;- me_wrangled_dt |&gt;\n\n  # Add company_id from funded_companies\n  left_join(cb_cs_joined_slct_dt,\n            by = c(company_id_cs = \"id_cs\")) |&gt;\n\n  # Add data from funded_companies\n  left_join(funded_companies_wrangled_dt,\n            by = \"company_id\")  |&gt;\n  distinct()\n\nArrow because of nested funding data not possible."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#overview",
    "href": "revealjs/slides/2023/cs/cs_test.html#overview",
    "title": "CoreSignal Analysis",
    "section": "1. Overview",
    "text": "1. Overview\nFrom here on almost everything is in\n\n\n\nScript and File locations\n\n\nScripts 05_analyses/03_cbpbcs/01_scripts\n\n01_founding_vs_employment (Company Funding vs. Time of Employment (I. Time, II. Capital, III. Rounds))\n02_stage_affiliation (stages based on Age and funding. was discarded later on –&gt; see 04_funding_history)\n03_employment_history (Fortune500, Startup, Founding, Research Experiences)\n04_funding_history (1. prior raised amount (person), 2. further funding (company), 3. funding per round, 4. Dataset based on series B)\n05_education_history (merge with rankings, extract degrees)\n06_skills\n07_analyses/02_per_company/ (last plots)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#list-of-all-variables-not-up-to-date",
    "href": "revealjs/slides/2023/cs/cs_test.html#list-of-all-variables-not-up-to-date",
    "title": "CoreSignal Analysis",
    "section": "2. List of all variables (not up to date)",
    "text": "2. List of all variables (not up to date)\n\nAllGeneralEDAExp (dummy)Exp (quant)EduFund\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 67\n#&gt; $ company_id_cbpb                      &lt;int&gt; 90591, 152845, 90440, 138208, 116…\n#&gt; $ funding_after_mid                    &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"…\n#&gt; $ funding_after_early                  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\",…\n#&gt; $ member_id                            &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005…\n#&gt; $ id_tie                               &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175…\n#&gt; $ exp_id_cs                            &lt;dbl&gt; 2481733250, 1423977093, 2638, 263…\n#&gt; $ exp_corporate                        &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.000…\n#&gt; $ exp_funded_startup                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0,…\n#&gt; $ exp_founder                          &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0…\n#&gt; $ exp_f500                             &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.0000…\n#&gt; $ exp_research                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ company_id_cs                        &lt;int&gt; 140537, 10644128, 6068905, 606890…\n#&gt; $ company_name_cs                      &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"…\n#&gt; $ company_name_cbpb                    &lt;chr&gt; \"receptos\", \"HERE Technologies Ch…\n#&gt; $ founded_on_cbpb                      &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-…\n#&gt; $ closed_on_cbpb                       &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-…\n#&gt; $ title_cs                             &lt;chr&gt; \"Key Account Manager\", \"GIS Analy…\n#&gt; $ date_from_parsed_cs                  &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_to_parsed_cs                    &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-…\n#&gt; $ tjoin_tfound                         &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, …\n#&gt; $ raised_amount_before_join_company    &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333…\n#&gt; $ num_rounds_before_join               &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, …\n#&gt; $ is_f500                              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, F…\n#&gt; $ is_founder                           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research                          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research_ivy                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ date_1st_founder_exp                 &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_f500_exp                    &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010…\n#&gt; $ date_1st_funded_startup_exp          &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_1st_research_exp                &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_research_ivy_exp            &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_corporate_exp               &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, …\n#&gt; $ time_since_1st_corporate_exp         &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40,…\n#&gt; $ time_since_1st_founder_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_f500_exp              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_funded_startup_exp    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, N…\n#&gt; $ time_since_1st_research_exp          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_research_ivy_exp      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_experience            &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176…\n#&gt; $ raised_amount_before_founder_member  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ raised_amount_before_all_member      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA,…\n#&gt; $ was_corporate_before                 &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, …\n#&gt; $ was_founder_before                   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_f500_before                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_fc_before                        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_uni_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_ivy_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ stage_mid                            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ stage_late                           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, FALSE, NA…\n#&gt; $ date_from_stage                      &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\",…\n#&gt; $ company_start_mid                    &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-…\n#&gt; $ company_start_late                   &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n#&gt; $ num_rounds_cumulated_founder         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ num_rounds_cumulated_all             &lt;int&gt; NA, NA, NA, NA, NA, NA, 1, 1, NA,…\n#&gt; $ announced_on_sB                      &lt;date&gt; 2012-02-03, 2018-01-04, 2011-03-…\n#&gt; $ round_type_new_next                  &lt;fct&gt; Series C, Series C, Series C, Ser…\n#&gt; $ raised_amount_cumsum_sB              &lt;dbl&gt; 46043054, 0, 1905000, 11022796, 2…\n#&gt; $ raised_amount_cumsum_sB_next         &lt;dbl&gt; 76043054, 0, 8712306, 13854868, 4…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(id_tie, member_id, exp_id_cs, company_id_cbpb, company_name_cbpb, company_id_cs, company_name_cs, \n         founded_on_cbpb, closed_on_cbpb,\n         title_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ id_tie            &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175, 209, 243, 321, 37…\n#&gt; $ member_id         &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005, 4224, 4224, 4317,…\n#&gt; $ exp_id_cs         &lt;dbl&gt; 2481733250, 1423977093, 2638, 2638, 1736317868, 3084…\n#&gt; $ company_id_cbpb   &lt;int&gt; 90591, 152845, 90440, 138208, 116099, 97810, 40123, …\n#&gt; $ company_name_cbpb &lt;chr&gt; \"receptos\", \"HERE Technologies Chicago\", \"crowdtwist…\n#&gt; $ company_id_cs     &lt;int&gt; 140537, 10644128, 6068905, 6068905, 11825305, 194148…\n#&gt; $ company_name_cs   &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"Oracle\", \"Oracle\", …\n#&gt; $ founded_on_cbpb   &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-01, 2006-01-01, 201…\n#&gt; $ closed_on_cbpb    &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-09, NA, NA, NA, NA,…\n#&gt; $ title_cs          &lt;chr&gt; \"Key Account Manager\", \"GIS Analyst I\", \"QA\", \"QA\", …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_parsed_cs, date_to_parsed_cs, \n         tjoin_tfound, raised_amount_before_join_company, num_rounds_before_join) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 5\n#&gt; $ date_from_parsed_cs               &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_to_parsed_cs                 &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-01,…\n#&gt; $ tjoin_tfound                      &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, 44,…\n#&gt; $ raised_amount_before_join_company &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333333…\n#&gt; $ num_rounds_before_join            &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, 2, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(starts_with(\"is_\"),\n         starts_with(\"was_\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ is_f500              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FAL…\n#&gt; $ is_founder           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research_ivy      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_corporate_before &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRU…\n#&gt; $ was_founder_before   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_f500_before      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_fc_before        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n#&gt; $ was_uni_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_ivy_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(\n    starts_with(\"date_1st_\"),\n    starts_with(\"time_since_1st_\"),\n    starts_with(\"exp_\"), -exp_id_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 19\n#&gt; $ date_1st_founder_exp              &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_f500_exp                 &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010-01…\n#&gt; $ date_1st_funded_startup_exp       &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_1st_research_exp             &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_research_ivy_exp         &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_corporate_exp            &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, 200…\n#&gt; $ time_since_1st_corporate_exp      &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40, 17…\n#&gt; $ time_since_1st_founder_exp        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_f500_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_funded_startup_exp &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, NA, …\n#&gt; $ time_since_1st_research_exp       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_research_ivy_exp   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_experience         &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176, 4…\n#&gt; $ exp_corporate                     &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.00000,…\n#&gt; $ exp_funded_startup                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0,…\n#&gt; $ exp_founder                       &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.00…\n#&gt; $ exp_f500                          &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.00000, …\n#&gt; $ exp_research                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(score_global_2023_best,\n         starts_with(\"rank\"),\n         starts_with(\"degree\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 8\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_stage, company_start_mid, company_start_late,\n         raised_amount_before_founder_member, raised_amount_before_all_member,\n         funding_after_mid, funding_after_early) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 7\n#&gt; $ date_from_stage                     &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\", …\n#&gt; $ company_start_mid                   &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-0…\n#&gt; $ company_start_late                  &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-0…\n#&gt; $ raised_amount_before_founder_member &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n#&gt; $ raised_amount_before_all_member     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA, …\n#&gt; $ funding_after_mid                   &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"y…\n#&gt; $ funding_after_early                 &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", …"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#creative-destruction-out-with-the-old-in-with-the-new",
    "href": "revealjs/slides/2023/beyond/evomap.html#creative-destruction-out-with-the-old-in-with-the-new",
    "title": "Beyond the Beginning",
    "section": "Creative Destruction: Out with the Old, in with the New",
    "text": "Creative Destruction: Out with the Old, in with the New\n\n\n\n\n\n\nSchumpeter characterized creative destruction as innovations in the manufacturing process that increase productivity, describing it as the …\n\n\n… «process of industrial mutation that incessantly revolutionizes the economic structure from within, incessantly destroying the old one, incessantly creating a new one.»\n\n\n\n\n— Joseph Schumpeter (1942)"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#digital-disrupt-how-netflixs-success-demonstrates-the-effectiveness-of-business-model-innovation",
    "href": "revealjs/slides/2023/beyond/evomap.html#digital-disrupt-how-netflixs-success-demonstrates-the-effectiveness-of-business-model-innovation",
    "title": "Beyond the Beginning",
    "section": "Digital Disrupt: How Netflix’s Success demonstrates the Effectiveness of Business Model Innovation",
    "text": "Digital Disrupt: How Netflix’s Success demonstrates the Effectiveness of Business Model Innovation"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#securing-the-future-navigating-the-evolutionary-path-of-business-models-for-sustainable-growth",
    "href": "revealjs/slides/2023/beyond/evomap.html#securing-the-future-navigating-the-evolutionary-path-of-business-models-for-sustainable-growth",
    "title": "Beyond the Beginning",
    "section": "Securing the Future: Navigating the Evolutionary Path of Business Models for sustainable Growth",
    "text": "Securing the Future: Navigating the Evolutionary Path of Business Models for sustainable Growth"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#blockbusters-digital-demise-how-failing-to-adapt-led-to-bankruptcy-dynamics-of-competitive-structures",
    "href": "revealjs/slides/2023/beyond/evomap.html#blockbusters-digital-demise-how-failing-to-adapt-led-to-bankruptcy-dynamics-of-competitive-structures",
    "title": "Beyond the Beginning",
    "section": "Blockbuster’s Digital Demise: How Failing to Adapt Led to Bankruptcy (Dynamics of competitive structures)",
    "text": "Blockbuster’s Digital Demise: How Failing to Adapt Led to Bankruptcy (Dynamics of competitive structures)"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#constructive-destruction-what-has-to-be-changed",
    "href": "revealjs/slides/2023/beyond/evomap.html#constructive-destruction-what-has-to-be-changed",
    "title": "Beyond the Beginning",
    "section": "Constructive destruction: What has to be changed?",
    "text": "Constructive destruction: What has to be changed?\n\n\n\n\n\n\n\n[…] it is not that kind of competition [price, ed] which counts but the competition from the new commodity, the new technology, the new source of supply, the new type of organization – competition which commands a decisive cost or quality advantage and which strikes not at the margins of the profits and the outputs of the existing firms but at their foundations and their very lives.»\n\n\n\n\n— Joseph Schumpeter (1942)\n\n\n\n\n\n\n\nBusiness Model Canvas\n\n\n\n\n\n\n\n\n\n\n\nValue Proposition Canvas"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#research-objective-an-evolutionary-approach-to-business-model-innovation-value-proposition-development",
    "href": "revealjs/slides/2023/beyond/evomap.html#research-objective-an-evolutionary-approach-to-business-model-innovation-value-proposition-development",
    "title": "Beyond the Beginning",
    "section": "Research Objective: An evolutionary approach to Business Model Innovation & Value Proposition Development",
    "text": "Research Objective: An evolutionary approach to Business Model Innovation & Value Proposition Development\n\n\n\nMarket structure map: Spatial representation of firms’ competitive positions relative to one another based on some measure of their competitive relationships (DeSarbo et al. 1993).\n\n\n\nSuch maps typically capture static snapshots in time. Yet, competitive positions tend to change.\n\n\n\nFirms’ trajectories: Evolutionary paths of firms’ positions over time relative to all other firms in a market based on their value proposition"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#defining-a-startups-unique-value-proposition-how-to-identify-core-value",
    "href": "revealjs/slides/2023/beyond/evomap.html#defining-a-startups-unique-value-proposition-how-to-identify-core-value",
    "title": "Beyond the Beginning",
    "section": "Defining a Startup’s unique Value Proposition: How to identify Core Value?",
    "text": "Defining a Startup’s unique Value Proposition: How to identify Core Value?\n \n \n\n\n\n\n\nApproach\n\n\n\nStatic: Venture Capital Databases (Crunchbase, Pitchbook, … )\n\n\n\n\n\n\n\nDynamic: Natural language processing and historical websites (WayBack Machine)"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#how-they-faced-a-digital-transformation-changes-in-netflixs-business-model-over-time",
    "href": "revealjs/slides/2023/beyond/evomap.html#how-they-faced-a-digital-transformation-changes-in-netflixs-business-model-over-time",
    "title": "Beyond the Beginning",
    "section": "How they faced a Digital Transformation: Changes in Netflix’s business model over time",
    "text": "How they faced a Digital Transformation: Changes in Netflix’s business model over time"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-1-extracting-key-noun-phrases-for-essential-insight",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-1-extracting-key-noun-phrases-for-essential-insight",
    "title": "Beyond the Beginning",
    "section": "Handling Noisy Data Part 1:Extracting Key Noun Phrases for Essential Insight",
    "text": "Handling Noisy Data Part 1:Extracting Key Noun Phrases for Essential Insight\n \n\n\nUnlike simplistic keywords, nounphrases transcend single words, comprising compound expressions that encapsulate the essence of the text more comprehensively\n\n\n\nTools: SpaCy (nounchunk library) & Custom Algorithmus"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-2-leveraging-embeddings-for-similarity-calculations",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-2-leveraging-embeddings-for-similarity-calculations",
    "title": "Beyond the Beginning",
    "section": "Handling Noisy Data Part 2:Leveraging Embeddings for Similarity Calculations",
    "text": "Handling Noisy Data Part 2:Leveraging Embeddings for Similarity Calculations"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-3-clustering-with-candidate-keyphrases",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-3-clustering-with-candidate-keyphrases",
    "title": "Beyond the Beginning",
    "section": "Handling Noisy Data Part 3:Clustering with Candidate Keyphrases",
    "text": "Handling Noisy Data Part 3:Clustering with Candidate Keyphrases\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster Crunchbase & Pitchbook Description Embeddings, which are describing value propositions\n\n\n\n\nUse corresponding medoids/centroids (k=300) as ultimate truth anchors\n\n\n\n\nFind appropriate similarity threshold as cutoff for website embeddings"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-4-consolidating-unique-embeddings-for-value-propositions",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-part-4-consolidating-unique-embeddings-for-value-propositions",
    "title": "Beyond the Beginning",
    "section": "Handling Noisy Data Part 4:Consolidating Unique Embeddings for Value Propositions",
    "text": "Handling Noisy Data Part 4:Consolidating Unique Embeddings for Value Propositions"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#mapping-moments-unveiling-competitive-positions-through-time-indexed-pairwise-similarity-measures",
    "href": "revealjs/slides/2023/beyond/evomap.html#mapping-moments-unveiling-competitive-positions-through-time-indexed-pairwise-similarity-measures",
    "title": "Beyond the Beginning",
    "section": "Mapping Moments: Unveiling competitive positions through Time-Indexed Pairwise Similarity Measures",
    "text": "Mapping Moments: Unveiling competitive positions through Time-Indexed Pairwise Similarity Measures"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#proof-of-concept-exploring-competitive-landscapes-with-the-evomap-algorithm",
    "href": "revealjs/slides/2023/beyond/evomap.html#proof-of-concept-exploring-competitive-landscapes-with-the-evomap-algorithm",
    "title": "Beyond the Beginning",
    "section": "Proof of Concept: Exploring Competitive Landscapes with the EvoMap Algorithm",
    "text": "Proof of Concept: Exploring Competitive Landscapes with the EvoMap Algorithm\n \n\n\n\n\n\n\nSequence of Maps\n\n\n\n\n\n\n\nDynamic Map"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#exploring-the-role-of-pivotal-shifts-driving-force-or-outcome-variable",
    "href": "revealjs/slides/2023/beyond/evomap.html#exploring-the-role-of-pivotal-shifts-driving-force-or-outcome-variable",
    "title": "Beyond the Beginning",
    "section": "Exploring the Role of Pivotal Shifts: Driving Force or Outcome Variable?",
    "text": "Exploring the Role of Pivotal Shifts: Driving Force or Outcome Variable?"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#fueling-startup-success-pivotal-shifts-as-driving-force-behind-securing-funding",
    "href": "revealjs/slides/2023/beyond/evomap.html#fueling-startup-success-pivotal-shifts-as-driving-force-behind-securing-funding",
    "title": "Beyond the Beginning",
    "section": "Fueling Startup Success: Pivotal Shifts as Driving Force Behind Securing Funding",
    "text": "Fueling Startup Success: Pivotal Shifts as Driving Force Behind Securing Funding"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#b1-the-evolutionary-path",
    "href": "revealjs/slides/2023/beyond/evomap.html#b1-the-evolutionary-path",
    "title": "Beyond the Beginning",
    "section": "B1: The Evolutionary Path",
    "text": "B1: The Evolutionary Path"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#b2-websites-perceived-value-proposition",
    "href": "revealjs/slides/2023/beyond/evomap.html#b2-websites-perceived-value-proposition",
    "title": "Beyond the Beginning",
    "section": "B2: Websites: Perceived value proposition",
    "text": "B2: Websites: Perceived value proposition\n \n\n\n\n\n\nValue propositions are derived from the descriptions that firms provide to the market through their websites\nThis contrasts with existing industry classification schemes, which often rely on external assessments\nBy this, we capture a more dynamic and internally generated perspective of their industry affiliations, potentially offering a richer understanding of their positioning and evolution over time"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#start-up-roadmap-the-myth-of-the-linear-path",
    "href": "revealjs/slides/2023/beyond/evomap.html#start-up-roadmap-the-myth-of-the-linear-path",
    "title": "Beyond the Beginning",
    "section": "Start-up Roadmap: The Myth of the Linear Path",
    "text": "Start-up Roadmap: The Myth of the Linear Path"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#st-goal-keyphrase-extraction",
    "href": "revealjs/slides/2023/beyond/evomap.html#st-goal-keyphrase-extraction",
    "title": "Beyond the Beginning",
    "section": "1st Goal: Keyphrase Extraction",
    "text": "1st Goal: Keyphrase Extraction\n\n\n\n\n\n\nJoschka Schwarz - Beyond the Beginning"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#quarto",
    "href": "revealjs/slides/2022/first/first.html#quarto",
    "title": "First",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#bullets",
    "href": "revealjs/slides/2022/first/first.html#bullets",
    "title": "First",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#code",
    "href": "revealjs/slides/2022/first/first.html#code",
    "title": "First",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#why",
    "href": "revealjs/slides/2022/first/first.html#why",
    "title": "First",
    "section": "Why",
    "text": "Why\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#same",
    "href": "revealjs/slides/2022/first/first.html#same",
    "title": "First",
    "section": "Same",
    "text": "Same\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#different",
    "href": "revealjs/slides/2022/first/first.html#different",
    "title": "First",
    "section": "Different",
    "text": "Different\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects overview",
    "section": "",
    "text": "Getting started with paged.js\n\n\nClassical themed CV\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced paged.js\n\n\nSpotify themed CV\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Getting started with paged.js\n\n\nClassical themed CV\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced paged.js\n\n\nSpotify themed CV\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html",
    "title": "Joining Data in SQL",
    "section": "",
    "text": "create database countries;\npsql -d countries -U jschwarz -f countries.sql\nShort Description\nDiscover all the fundamental in PostgreSQL, like inner, outer self, semi, anti, and cross joins, as well as unions, intersections, and except clauses.\nLong Description\nNow that you’ve learned the basics of SQL in our Introduction to SQL course, it’s time to supercharge your queries using joins and relational set theory. In this course, you’ll learn all about the power of joining tables while exploring interesting features of countries and their cities throughout the world. You will master inner and outer joins, as well as self joins, semi joins, anti joins and cross joins—fundamental tools in any PostgreSQL wizard’s toolbox. Never fear set theory again after learning all about unions, intersections, and except clauses through easy-to-understand diagrams and examples. Lastly, you’ll be introduced to the challenging topic of subqueries. You will be able to visually grasp these ideas by using Venn diagrams and other linking illustrations.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#introduction-to-inner-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#introduction-to-inner-join",
    "title": "Joining Data in SQL",
    "section": "1.1 Introduction to INNER JOIN",
    "text": "1.1 Introduction to INNER JOIN\nTheory. Coming soon …\n1. Welcome to the course!\nHi, my name is Chester Ismay and I’ll be your instructor for this course on Joining Data in PostgreSQL. As the name suggests, the focus of this course is using SQL to join two or more database tables together into a single table, an essential skill for data scientists. In this chapter, you’ll learn about the INNER JOIN, which along with LEFT JOIN are probably the two most common JOINs. You’ll see diagrams throughout this course that are designed to help you understand the mechanics of the different joins. Let’s begin with a diagram showing the layout of some data and then how an INNER JOIN can be applied to that data.\n2. Initial data diagram\nIn the videos in this chapter and the next, we’ll often work with two tables named left and right. You can see that matching values of the id field are colored with the same color. The id field is known as a KEY field since it can be used to reference one table to another. Both the left and right tables also have another field named val. This will be useful in helping you see specifically which records and values are included in each join.\n3. INNER JOIN diagram\nAn INNER JOIN only includes records in which the key is in both tables. You can see here that the id field matches for values of 1 and 4 only. With inner joins we look for matches in the right table corresponding to all entries in the key field in the left table.\n4. INNER JOIN diagram (2)\nSo the focus here shifts to only those records with a match in terms of the id field. The records not of interest to INNER JOIN have been faded.\n5. INNER JOIN diagram (3)\nHere’s a resulting single table from the INNER JOIN clause that gives the val field from the right table with records corresponding to only those with id value of 1 or 4, which are colored as yellow and purple. Now that you have a sense for how INNER JOIN works, let’s try an example in SQL.\n6. prime_ministers table\nThe prime_ministers table is one of the tables in the leaders database. It is displayed here. Note the countries that are included. Suppose you were interested in determining nations that have both a prime minister and a president AND putting the results into a single table. Next you’ll see the presidents table.\n7. presidents table\nHow did I display all of the prime_ministers table in the previous slide? Recall the use of SELECT and FROM clauses as is shown for the presidents table here.Which countries appear in both tables? With small tables like these, it is easy to notice that Egypt, Portugal, Vietnam, and Haiti appear in both tables. For larger tables, it isn’t as simple as just picking these countries out visually. So what does the syntax look like for SQL to get the results of countries with a prime minister and a president from these two tables into one?\n8. INNER JOIN in SQL\nThe syntax for completing an INNER JOIN from the prime_ministers table to the presidents table based on a key field of country is shown. Note the use of aliases for prime_ministers as p1 and presidents as p2. This helps to simplify your code, especially with longer table names like prime_ministers and presidents. A SELECT statement is used to select specific fields from the two tables. In this case, since country exists in both tables, we must write p1 and the period to avoid a SQL error. Next we list the table on the left of the inner join after FROM and then we list the table on the right after INNER JOIN. Lastly, we specify the keys in the two tables that we would like to match on.\n9. Let’s practice!\nYou’ll now practice applying an inner join to two tables and to three tables. Let’s get to it!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join",
    "title": "Joining Data in SQL",
    "section": "1.2 Inner join",
    "text": "1.2 Inner join\nAlthough this course focuses on PostgreSQL, you’ll find that these joins and the material here applies to different forms of SQL as well.\nThroughout this course, you’ll be working with the countries database containing information about the most populous world cities as well as country-level economic data, population data, and geographic data. This countries database also contains information on languages spoken in each country.\nYou can see the different tables in this database by clicking on the corresponding tabs. Click through them to get a sense for the types of data that each table contains before you continue with the course! Take note of the fields that appear to be shared across the tables.\nRecall from the video the basic syntax for an INNER JOIN, here including all columns in both tables:\n\nSELECT *\nFROM left_table\nINNER JOIN right_table\nON left_table.id = right_table.id;\n\nYou’ll start off with a SELECT statement and then build up to an INNER JOIN with the cities and countries tables. Let’s get to it!\nSteps\n\nBegin by selecting all columns from the cities table.\n\n\n-- Select all columns from cities\nSELECT *\nFROM cities;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nname\ncountry_code\ncity_proper_pop\nmetroarea_pop\nurbanarea_pop\n\n\n\n\nAbidjan\nCIV\n4765000\nNA\n4765000\n\n\nAbu Dhabi\nARE\n1145000\nNA\n1145000\n\n\nAbuja\nNGA\n1235880\n6000000\n1235880\n\n\nAccra\nGHA\n2070463\n4010054\n2070463\n\n\nAddis Ababa\nETH\n3103673\n4567857\n3103673\n\n\nAhmedabad\nIND\n5570585\nNA\n5570585\n\n\nAlexandria\nEGY\n4616625\nNA\n4616625\n\n\nAlgiers\nDZA\n3415811\n5000000\n3415811\n\n\nAlmaty\nKAZ\n1703481\nNA\n1703481\n\n\nAnkara\nTUR\n5271000\n4585000\n5271000\n\n\n\n\n\n\nInner join the cities table on the left to the countries table on the right, keeping all of the fields in both tables.\nYou should match the tables on the country_code field in cities and the code field in countries.\nDo not alias your tables here or in the next step. Using cities and countries is fine for now.\n\n\nSELECT * \nFROM cities\n  -- Inner join to countries\n  INNER JOIN countries\n    -- Match on the country codes\n    ON cities.country_code = countries.code;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ncountry_code\ncity_proper_pop\nmetroarea_pop\nurbanarea_pop\ncode\nname..7\ncontinent\nregion\nsurface_area\nindep_year\nlocal_name\ngov_form\ncapital\ncap_long\ncap_lat\n\n\n\n\nAbidjan\nCIV\n4765000\nNA\n4765000\nCIV\nCote d’Ivoire\nAfrica\nWestern Africa\n322463\n1960\nCote dIvoire\nRepublic\nYamoussoukro\n-4.03050\n5.33200\n\n\nAbu Dhabi\nARE\n1145000\nNA\n1145000\nARE\nUnited Arab Emirates\nAsia\nMiddle East\n83600\n1971\nAl-Imarat al-´Arabiya al-Muttahida\nEmirate Federation\nAbu Dhabi\n54.37050\n24.47640\n\n\nAbuja\nNGA\n1235880\n6000000\n1235880\nNGA\nNigeria\nAfrica\nWestern Africa\n923768\n1960\nNigeria\nFederal Republic\nAbuja\n7.48906\n9.05804\n\n\nAccra\nGHA\n2070463\n4010054\n2070463\nGHA\nGhana\nAfrica\nWestern Africa\n238533\n1957\nGhana\nRepublic\nAccra\n-0.20795\n5.57045\n\n\nAddis Ababa\nETH\n3103673\n4567857\n3103673\nETH\nEthiopia\nAfrica\nEastern Africa\n1104300\n-1000\nYeItyop´iya\nRepublic\nAddis Ababa\n38.74680\n9.02274\n\n\nAhmedabad\nIND\n5570585\nNA\n5570585\nIND\nIndia\nAsia\nSouthern and Central Asia\n3287260\n1947\nBharat/India\nFederal Republic\nNew Delhi\n77.22500\n28.63530\n\n\nAlexandria\nEGY\n4616625\nNA\n4616625\nEGY\nEgypt\nAfrica\nNorthern Africa\n1001450\n1922\nMisr\nRepublic\nCairo\n31.24610\n30.09820\n\n\nAlgiers\nDZA\n3415811\n5000000\n3415811\nDZA\nAlgeria\nAfrica\nNorthern Africa\n2381740\n1962\nAl-Jazair/Algerie\nRepublic\nAlgiers\n3.05097\n36.73970\n\n\nAlmaty\nKAZ\n1703481\nNA\n1703481\nKAZ\nKazakhstan\nAsia\nSouthern and Central Asia\n2724900\n1991\nQazaqstan\nRepublic\nAstana\n71.43820\n51.18790\n\n\nAnkara\nTUR\n5271000\n4585000\n5271000\nTUR\nTurkey\nAsia\nMiddle East\n774815\n1923\nTurkiye\nRepublic\nAnkara\n32.36060\n39.71530\n\n\n\n\n\n\nModify the SELECT statement to keep only the name of the city, the name of the country, and the name of the region the country resides in.\nAlias the name of the city AS city and the name of the country AS country.\n\n\n-- Select name fields (with alias) and region \nSELECT cities.name AS city, countries.name AS country, region\nFROM cities\n  INNER JOIN countries\n    ON cities.country_code = countries.code;\n\n\nDisplaying records 1 - 10\n\n\ncity\ncountry\nregion\n\n\n\n\nAbidjan\nCote d’Ivoire\nWestern Africa\n\n\nAbu Dhabi\nUnited Arab Emirates\nMiddle East\n\n\nAbuja\nNigeria\nWestern Africa\n\n\nAccra\nGhana\nWestern Africa\n\n\nAddis Ababa\nEthiopia\nEastern Africa\n\n\nAhmedabad\nIndia\nSouthern and Central Asia\n\n\nAlexandria\nEgypt\nNorthern Africa\n\n\nAlgiers\nAlgeria\nNorthern Africa\n\n\nAlmaty\nKazakhstan\nSouthern and Central Asia\n\n\nAnkara\nTurkey\nMiddle East\n\n\n\n\n\nGreat work! In the next exercise you’ll explore how you can do more aliasing to limit the amount of writing.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-2",
    "title": "Joining Data in SQL",
    "section": "1.3 Inner join (2)",
    "text": "1.3 Inner join (2)\nInstead of writing the full table name, you can use table aliasing as a shortcut. For tables you also use AS to add the alias immediately after the table name with a space. Check out the aliasing of cities and countries below.\n\nSELECT c1.name AS city, c2.name AS country\nFROM cities AS c1\nINNER JOIN countries AS c2\nON c1.country_code = c2.code;\n\nNotice that to select a field in your query that appears in multiple tables, you’ll need to identify which table/table alias you’re referring to by using a . in your SELECT statement.\nYou’ll now explore a way to get data from both the countries and economies tables to examine the inflation rate for both 2010 and 2015.\nSometimes it’s easier to write SQL code out of order: you write the SELECT statement after you’ve done the JOIN.\nSteps\n\nJoin the tables countries (left) and economies (right) aliasing countries AS c and economies AS e.\nSpecify the field to match the tables ON.\nFrom this join, SELECT:c.code, aliased as country_code.name, year, and inflation_rate, not aliased.\n\n\nc.code, aliased as country_code.\nname, year, and inflation_rate, not aliased.\n\n\n-- Select fields with aliases\nSELECT c.code AS country_code, name, year, inflation_rate\nFROM countries AS c\n  -- Join to economies (alias e)\n  INNER JOIN economies AS e\n    -- Match on code\n    ON c.code = e.code;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\nname\nyear\ninflation_rate\n\n\n\n\nAFG\nAfghanistan\n2010\n2.179\n\n\nAFG\nAfghanistan\n2015\n-1.549\n\n\nAGO\nAngola\n2010\n14.480\n\n\nAGO\nAngola\n2015\n10.287\n\n\nALB\nAlbania\n2010\n3.605\n\n\nALB\nAlbania\n2015\n1.896\n\n\nARE\nUnited Arab Emirates\n2010\n0.878\n\n\nARE\nUnited Arab Emirates\n2015\n4.070\n\n\nARG\nArgentina\n2010\n10.461\n\n\nARG\nArgentina\n2015\nNA\n\n\n\n\n\nNicely done! Using this short aliases takes some getting used to, but it will save you a lot of typing.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-3",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-3",
    "title": "Joining Data in SQL",
    "section": "1.4 Inner join (3)",
    "text": "1.4 Inner join (3)\nThe ability to combine multiple joins in a single query is a powerful feature of SQL, e.g:\n\nSELECT *\nFROM left_table\n  INNER JOIN right_table\n    ON left_table.id = right_table.id\n  INNER JOIN another_table\n    ON left_table.id = another_table.id;\n\nAs you can see here it becomes tedious to continually write long table names in joins. This is when it becomes useful to alias each table using the first letter of its name (e.g. countries AS c)! It is standard practice to alias in this way and, if you choose to alias tables or are asked to specifically for an exercise in this course, you should follow this protocol.\nNow, for each country, you want to get the country name, its region, the fertility rate, and the unemployment rate for both 2010 and 2015.\nNote that results should work throughout this course with or without table aliasing unless specified differently.\nSteps\n\nInner join countries (left) and populations (right) on the code and country_code fields respectively.\nAlias countries AS c and populations AS p.\nSelect code, name, and region from countries and also select year and fertility_rate from populations (5 fields in total).\n\n\n-- Select fields\nSELECT c.code, name, region, year, fertility_rate\n  -- From countries (alias as c)\n  FROM countries AS c\n  -- Join with populations (as p)\n  INNER JOIN populations AS p\n    -- Match on country code\n    ON c.code = p.country_code;\n\n\nDisplaying records 1 - 10\n\n\ncode\nname\nregion\nyear\nfertility_rate\n\n\n\n\nABW\nAruba\nCaribbean\n2010\n1.704\n\n\nABW\nAruba\nCaribbean\n2015\n1.647\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2010\n5.746\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2015\n4.653\n\n\nAGO\nAngola\nCentral Africa\n2010\n6.416\n\n\nAGO\nAngola\nCentral Africa\n2015\n5.996\n\n\nALB\nAlbania\nSouthern Europe\n2010\n1.663\n\n\nALB\nAlbania\nSouthern Europe\n2015\n1.793\n\n\nAND\nAndorra\nSouthern Europe\n2010\n1.270\n\n\nAND\nAndorra\nSouthern Europe\n2015\nNA\n\n\n\n\n\n\nAdd an additional INNER JOIN with economies to your previous query by joining on code.\nInclude the unemployment_rate column that became available through joining with economies.\nNote that year appears in both populations and economies, so you have to explicitly use e.year instead of year as you did before.\n\n\n-- Select fields\nSELECT c.code, name, region, e.year, fertility_rate, unemployment_rate\n  -- From countries (alias as c)\n  FROM countries AS c\n  -- Join to populations (as p)\n  INNER JOIN populations AS p\n    -- Match on country code\n    ON c.code = p.country_code\n  -- Join to economies (as e)\n  INNER JOIN economies AS e\n    -- Match on country code\n    ON c.code = e.code;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\ncode\nname\nregion\nyear\nfertility_rate\nunemployment_rate\n\n\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2015\n5.746\nNA\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2010\n5.746\nNA\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2015\n4.653\nNA\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2010\n4.653\nNA\n\n\nAGO\nAngola\nCentral Africa\n2015\n6.416\nNA\n\n\nAGO\nAngola\nCentral Africa\n2010\n6.416\nNA\n\n\nAGO\nAngola\nCentral Africa\n2015\n5.996\nNA\n\n\nAGO\nAngola\nCentral Africa\n2010\n5.996\nNA\n\n\nALB\nAlbania\nSouthern Europe\n2015\n1.663\n17.1\n\n\nALB\nAlbania\nSouthern Europe\n2010\n1.663\n14.0\n\n\n\n\n\n\nScroll down the query result and take a look at the results for Albania from your previous query. Does something seem off to you?\nThe trouble with doing your last join on c.code = e.code and not also including year is that e.g. the 2010 value for fertility_rate is also paired with the 2015 value for unemployment_rate.\nFix your previous query: in your last ON clause, use AND to add an additional joining condition. In addition to joining on code in c and e, also join on year in e and p.\n\n\n-- Select fields\nSELECT c.code, name, region, e.year, fertility_rate, unemployment_rate\n  -- From countries (alias as c)\n  FROM countries AS c\n  -- Join to populations (as p)\n  INNER JOIN populations AS p\n    -- Match on country code\n    ON c.code = p.country_code\n  -- Join to economies (as e)\n  INNER JOIN economies AS e\n    -- Match on country code and year\n    ON c.code = e.code AND e.year = p.year;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\ncode\nname\nregion\nyear\nfertility_rate\nunemployment_rate\n\n\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2010\n5.746\nNA\n\n\nAFG\nAfghanistan\nSouthern and Central Asia\n2015\n4.653\nNA\n\n\nAGO\nAngola\nCentral Africa\n2010\n6.416\nNA\n\n\nAGO\nAngola\nCentral Africa\n2015\n5.996\nNA\n\n\nALB\nAlbania\nSouthern Europe\n2010\n1.663\n14.00\n\n\nALB\nAlbania\nSouthern Europe\n2015\n1.793\n17.10\n\n\nARE\nUnited Arab Emirates\nMiddle East\n2010\n1.868\nNA\n\n\nARE\nUnited Arab Emirates\nMiddle East\n2015\n1.767\nNA\n\n\nARG\nArgentina\nSouth America\n2010\n2.370\n7.75\n\n\nARG\nArgentina\nSouth America\n2015\n2.308\nNA\n\n\n\n\n\nGood work! Time to learn something new!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-via-using",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-via-using",
    "title": "Joining Data in SQL",
    "section": "1.5 INNER JOIN via USING",
    "text": "1.5 INNER JOIN via USING\nTheory. Coming soon …\n1. INNER JOIN via USING\nCongratulations on making it through the first set of exercises on using INNER JOIN to combine two or three tables into one! You’ll next learn about the USING keyword in SQL and how it can be used in joins.\n2. The INNER JOIN diagram again\nRecall the INNER JOIN diagram you saw in the last video. Think about the SQL code needed to complete this diagram. Let’s check it out. We select and alias three fields and use the left table on the left of the join and the right table on the right of the join matching based on the entries for the id key field.\n3. The INNER JOIN diagram with USING\nWhen the key field you’d like to join on is the same name in both tables, you can use a USING clause instead of the ON clause you have seen so far.Since id is the same name in both the left table and the right table we can specify USING instead of ON here. Note that the parentheses are required around the key field with USING. Let’s revisit the example of joining the prime_ministers table\n4. Countries with prime ministers and presidents\nto the presidents table to determine countries with both types of leaders. How could you fill in the blanks to get the result with USING? (Pause for a few seconds)Did you get it? (PAUSE) Ah, I played a bit of a trick on you here. But why does this work? Since an INNER JOIN includes entries in both tables and both tables contain the countries listed, it doesn’t matter the order in which we place the tables in the join if we SELECT these columns. You’ll be told in the exercises which table to use on the left and on the right to avoid this confusion. Note again the use of the parentheses around country after USING.\n5. Let’s practice!\nNow you’ll test your understanding of INNER JOINs before we delve into an exercise with USING. Go get ’em!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#review-inner-join-using-on",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#review-inner-join-using-on",
    "title": "Joining Data in SQL",
    "section": "1.6 Review inner join using on",
    "text": "1.6 Review inner join using on\n\n1.7 Question\nWhy does the following code result in an error?  ⬜ The languages table has more rows than the countries table. ⬜ There are multiple languages spoken in many countries. ✅ INNER JOIN requires a specification of the key field (or fields) in each table. ⬜ Join queries may not be followed by a semi-colon.\n\n\nSELECT c.name AS country, l.name AS language\nFROM countries AS c\n  INNER JOIN languages AS l;",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-with-using",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-join-with-using",
    "title": "Joining Data in SQL",
    "section": "1.8 Inner join with using",
    "text": "1.8 Inner join with using\nWhen joining tables with a common field name, e.g.\n\nSELECT *\nFROM countries\n  INNER JOIN economies\n    ON countries.code = economies.code\n\nYou can use USING as a shortcut:\n\nSELECT *\nFROM countries\n  INNER JOIN economies\n    USING(code)\n\nYou’ll now explore how this can be done with the countries and languages tables.\nSteps\n\nInner join countries on the left and languages on the right with USING(code).\nSelect the fields corresponding to:\n\n\ncountry name AS country,\ncontinent name,\nlanguage name AS language, and\nwhether or not the language is official.\n\nRemember to alias your tables using the first letter of their names.\n\n-- Select fields\nSELECT c.name AS country, continent, l.name AS language, official\n  -- From countries (alias as c)\n  FROM countries AS c\n  -- Join to languages (as l)\n  INNER JOIN languages AS l\n    -- Match using code\n    USING(code);\n\n\nDisplaying records 1 - 10\n\n\ncountry\ncontinent\nlanguage\nofficial\n\n\n\n\nAfghanistan\nAsia\nDari\nTRUE\n\n\nAfghanistan\nAsia\nPashto\nTRUE\n\n\nAfghanistan\nAsia\nTurkic\nFALSE\n\n\nAfghanistan\nAsia\nOther\nFALSE\n\n\nAlbania\nEurope\nAlbanian\nTRUE\n\n\nAlbania\nEurope\nGreek\nFALSE\n\n\nAlbania\nEurope\nOther\nFALSE\n\n\nAlbania\nEurope\nunspecified\nFALSE\n\n\nAlgeria\nAfrica\nArabic\nTRUE\n\n\nAlgeria\nAfrica\nFrench\nFALSE\n\n\n\n\n\nWell done! Another technique to save you some typing!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#self-ish-joins-just-in-case",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#self-ish-joins-just-in-case",
    "title": "Joining Data in SQL",
    "section": "1.9 Self-ish joins, just in CASE",
    "text": "1.9 Self-ish joins, just in CASE\nTheory. Coming soon …\n1. Self-ish joins, just in CASE\nYou’ll now dive into inner joins where a table is joined with itself. Sounds a little selfish, doesn’t it? These types of joins, as you may have guessed, are called self joins. You’ll also explore how to slice a numerical field into categories using the CASE command. Joining a table to\n2. Join a table to itself?\nitself may seem like a bit of a crazy, strange thing to ever want to do. Self-joins are used to compare values in a field to other values of the same field from within the same table. Let’s further explore this with an example. Recall the prime_ministers table from earlier. What if you wanted to create a new table showing countries that are in the same continent matched as pairs? Let’s explore a chunk of INNER JOIN code using the prime_ministers table.\n3. Join prime_ministers to itself?\nYou might want to pause the video and think about what the resulting tablewill look like. The country column is selected twice as well as continent. The prime_ministers table is on both the left and the right. The vital step here is setting the key columns by which we match the table to itself. For each country, we will have a match if the country in the “right table” (that is also prime_ministers) is in the same continent. Lastly, since the results of this query are more than can fit on the slide, you’ll only see the first 14 records. See how we have exactly this in the result! It’s a pairing of each country with every other country in its same continent. But do you see a problem here? We don’t want to list the country with itself after all. In the next slide, you’ll see a way to do this. Pause to think about how to get around this before continuing. We don’t want to include rows\n4. Finishing off the self-join on prime_ministers\nwhere the country is the same in the country1 and country2 fields. The AND clause can check that multiple conditions are met. Here a match will not be made between prime_ministers and itself if the countries match.You, thus, have the correct table now; the results here are again limited in order for them to fit on the slide. Notice that self-join doesn’t have a syntax quite as simple as INNER JOIN (You can’t just write SELF JOIN in SQL code).\n5. CASE WHEN and THEN\nThe next command isn’t a join, but is a useful tool in your repertoire. You’ll be introduced to using CASE with another table in the leaders database. The states table contains numeric data about different countries in the six inhabited world continents. We’ll focus on the field indep_year now. Suppose we’d like to group the year of independence into categories of before 1900, between 1900 and 1930, and after 1930. CASE will get us there! CASE is a way to do multiple if-then-else statements in a simplified way in SQL.\n6. Preparing indep_year_group in states\nYou can now see the basic layout for creating a new field containing the groupings. How might we fill them in? After the first WHEN should specify that we want to check for indep_year being less than 1900. Next we want indep_year_group to contain ‘between 1900 and 1930’ in the next blank. Lastly any other record not matching these conditions will be assigned the value of ‘after 1930’ for indep_year_group.\n7. Creating indep_year_group in states\nCheck out the completed query with completed blanks.Notice how the values of indep_year are grouped in indep_year_group. Also observe how continent relates to indep_year_group.\n8. Let’s practice!\nYou’ll now work on a couple of exercises for practice, then complete a challenge testing your knowledge of the Chapter 1 material.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#self-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#self-join",
    "title": "Joining Data in SQL",
    "section": "1.10 Self-join",
    "text": "1.10 Self-join\nIn this exercise, you’ll use the populations table to perform a self-join to calculate the percentage increase in population from 2010 to 2015 for each country code!\nSince you’ll be joining the populations table to itself, you can alias populations as p1 and also populations as p2. This is good practice whenever you are aliasing and your tables have the same first letter. Note that you are required to alias the tables with self-joins.\nSteps\n\nJoin populations with itself ON country_code.\nSelect the country_code from p1 and the size field from both p1 and p2. SQL won’t allow same-named fields, so alias p1.size as size2010 and p2.size as size2015.\n\n\n-- Select fields with aliases\nSELECT p1.country_code,\n       p1.size AS size2010,\n       p2.size AS size2015\n-- From populations (alias as p1)\nFROM populations AS p1\n  -- Join to itself (alias as p2)\n  INNER JOIN populations AS p2\n    -- Match on country code\n    ON  p1.country_code = p2.country_code;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\nsize2010\nsize2015\n\n\n\n\nABW\n101597\n103889\n\n\nABW\n101597\n101597\n\n\nABW\n103889\n103889\n\n\nABW\n103889\n101597\n\n\nAFG\n27962208\n32526562\n\n\nAFG\n27962208\n27962208\n\n\nAFG\n32526562\n32526562\n\n\nAFG\n32526562\n27962208\n\n\nAGO\n21219954\n25021974\n\n\nAGO\n21219954\n21219954\n\n\n\n\n\n\nNotice from the result that for each country_code you have four entries laying out all combinations of 2010 and 2015.\nExtend the ON in your query to include only those records where the p1.year (2010) matches with p2.year - 5 (2015 - 5 = 2010). This will omit the three entries per country_code that you aren’t interested in.\n\n\n-- Select fields with aliases\nSELECT p1.country_code,\n       p1.size AS size2010,\n       p2.size AS size2015\n-- From populations (alias as p1)\nFROM populations AS p1\n  -- Join to itself (alias as p2)\n  INNER JOIN populations AS p2\n    -- Match on country code\n    ON p1.country_code = p2.country_code\n        -- and year (with calculation)\n        AND p1.year = p2.year - 5;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\nsize2010\nsize2015\n\n\n\n\nABW\n101597\n103889\n\n\nAFG\n27962208\n32526562\n\n\nAGO\n21219954\n25021974\n\n\nALB\n2913021\n2889167\n\n\nAND\n84419\n70473\n\n\nARE\n8329453\n9156963\n\n\nARG\n41222876\n43416756\n\n\nARM\n2963496\n3017712\n\n\nASM\n55636\n55538\n\n\nATG\n87233\n91818\n\n\n\n\n\n\nAs you just saw, you can also use SQL to calculate values like p2.year - 5 for you. With two fields like size2010 and size2015, you may want to determine the percentage increase from one field to the next:two numeric fields \\(A\\) and \\(B\\), the percentage growth from \\(A\\) to \\(B\\) can be calculated as \\((B - A) / A * 100.0\\).a new field to SELECT, aliased as growth_perc, that calculates the percentage population growth from 2010 to 2015 for each country, using p2.size and p1.size.\n\n\n-- Select fields with aliases\nSELECT p1.country_code,\n       p1.size AS size2010, \n       p2.size AS size2015,\n       -- Calculate growth_perc\n       ((p2.size - p1.size)/p1.size * 100.0) AS growth_perc\n-- From populations (alias as p1)\nFROM populations AS p1\n  -- Join to itself (alias as p2)\n  INNER JOIN populations AS p2\n    -- Match on country code\n    ON p1.country_code = p2.country_code\n        -- and year (with calculation)\n        AND p1.year = p2.year - 5;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\nsize2010\nsize2015\ngrowth_perc\n\n\n\n\nABW\n101597\n103889\n2.2559721\n\n\nAFG\n27962208\n32526562\n16.3232967\n\n\nAGO\n21219954\n25021974\n17.9171920\n\n\nALB\n2913021\n2889167\n-0.8188750\n\n\nAND\n84419\n70473\n-16.5199772\n\n\nARE\n8329453\n9156963\n9.9347457\n\n\nARG\n41222876\n43416756\n5.3219963\n\n\nARM\n2963496\n3017712\n1.8294608\n\n\nASM\n55636\n55538\n-0.1761449\n\n\nATG\n87233\n91818\n5.2560385\n\n\n\n\n\nNice!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#case-when-and-then",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#case-when-and-then",
    "title": "Joining Data in SQL",
    "section": "1.11 Case when and then",
    "text": "1.11 Case when and then\nOften it’s useful to look at a numerical field not as raw data, but instead as being in different categories or groups.\nYou can use CASE with WHEN, THEN, ELSE, and END to define a new grouping field.\nSteps\n\nUsing the countries table, create a new field AS geosize_group that groups the countries into three groups:\n\nIf surface_area is greater than 2 million, geosize_group is 'large'.\nIf surface_area is greater than 350 thousand but not larger than 2 million, geosize_group is 'medium'.\nOtherwise, geosize_group is 'small'.\n\n\n\nSELECT name, continent, code, surface_area,\n    -- First case\n    CASE WHEN surface_area &gt; 2000000 THEN 'large'\n        -- Second case\n        WHEN surface_area &gt; 350000 THEN 'medium'\n        -- Else clause + end\n        ELSE 'small' END\n        -- Alias name\n        AS geosize_group\n-- From table\nFROM countries;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nname\ncontinent\ncode\nsurface_area\ngeosize_group\n\n\n\n\nAfghanistan\nAsia\nAFG\n652090\nmedium\n\n\nNetherlands\nEurope\nNLD\n41526\nsmall\n\n\nAlbania\nEurope\nALB\n28748\nsmall\n\n\nAlgeria\nAfrica\nDZA\n2381740\nlarge\n\n\nAmerican Samoa\nOceania\nASM\n199\nsmall\n\n\nAndorra\nEurope\nAND\n468\nsmall\n\n\nAngola\nAfrica\nAGO\n1246700\nmedium\n\n\nAntigua and Barbuda\nNorth America\nATG\n442\nsmall\n\n\nUnited Arab Emirates\nAsia\nARE\n83600\nsmall\n\n\nArgentina\nSouth America\nARG\n2780400\nlarge\n\n\n\n\n\nWell done! Time for the last exercise of this chapter!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-challenge",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#inner-challenge",
    "title": "Joining Data in SQL",
    "section": "1.12 Inner challenge",
    "text": "1.12 Inner challenge\nThe table you created with the added geosize_group field has been loaded for you here with the name countries_plus. Observe the use of (and the placement of) the INTO command to create this countries_plus table:\n\nSELECT name, continent, code, surface_area,\n    CASE WHEN surface_area &gt; 2000000\n            THEN 'large'\n       WHEN surface_area &gt; 350000\n            THEN 'medium'\n       ELSE 'small' END\n       AS geosize_group\nINTO countries_plus\nFROM countries;\n\nYou will now explore the relationship between the size of a country in terms of surface area and in terms of population using grouping fields created with CASE.\nBy the end of this exercise, you’ll be writing two queries back-to-back in a single script. You got this!\nSteps\n\nUsing the populations table focused only for the year 2015, create a new field aliased as popsize_group to organize population size into\n\n'large' (&gt; 50 million),\n'medium' (&gt; 1 million), and\n'small' groups.\n\nSelect only the country code, population size, and this new popsize_group as fields.\n\n\nSELECT country_code, size,\n    -- First case\n    CASE WHEN size &gt; 50000000 THEN 'large'\n        -- Second case\n        WHEN size &gt; 1000000 THEN 'medium'\n        -- Else clause + end\n        ELSE 'small' END\n        -- Alias name (popsize_group)\n        AS popsize_group\n-- From table\nFROM populations\n-- Focus on 2015\nWHERE year = 2015;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\nsize\npopsize_group\n\n\n\n\nABW\n103889\nsmall\n\n\nAFG\n32526562\nmedium\n\n\nAGO\n25021974\nmedium\n\n\nALB\n2889167\nmedium\n\n\nAND\n70473\nsmall\n\n\nARE\n9156963\nmedium\n\n\nARG\n43416756\nmedium\n\n\nARM\n3017712\nmedium\n\n\nASM\n55538\nsmall\n\n\nATG\n91818\nsmall\n\n\n\n\n\n\nUse INTO to save the result of the previous query as pop_plus. You can see an example of this in the countries_plus code in the assignment text. Make sure to include a ; at the end of your WHERE clause!\n\n\nSELECT country_code, size,\n    CASE WHEN size &gt; 50000000 THEN 'large'\n        WHEN size &gt; 1000000 THEN 'medium'\n        ELSE 'small' END\n        AS popsize_group\n-- Into table\nINTO pop_plus\nFROM populations\nWHERE year = 2015;\n\n\nThen, include another query below your first query to display all the records in pop_plus usingSELECT * FROM pop_plus; so that you generate results and this will display pop_plus in the query result.\n\n\n-- Select all columns of pop_plus\nSELECT * FROM pop_plus;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\nsize\npopsize_group\n\n\n\n\nABW\n103889\nsmall\n\n\nAFG\n32526562\nmedium\n\n\nAGO\n25021974\nmedium\n\n\nALB\n2889167\nmedium\n\n\nAND\n70473\nsmall\n\n\nARE\n9156963\nmedium\n\n\nARG\n43416756\nmedium\n\n\nARM\n3017712\nmedium\n\n\nASM\n55538\nsmall\n\n\nATG\n91818\nsmall\n\n\n\n\n\n\nWrite a query to join countries_plus AS c on the left with pop_plus AS p on the right matchingthe country code fields.\nSort the data based on geosize_group, in ascending order so that large appears on top.\nSelect the name, continent, geosize_group, and popsize_group fields.\n\n\n-- Select fields\nSELECT name, continent, geosize_group, popsize_group\n-- From countries_plus (alias as c)\nFROM countries_plus AS c\n  -- Join to pop_plus (alias as p)\n  INNER JOIN pop_plus AS p\n    -- Match on country code\n    ON c.code = p.country_code\n-- Order the table    \nORDER BY geosize_group;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nname\ncontinent\ngeosize_group\npopsize_group\n\n\n\n\nIndia\nAsia\nlarge\nlarge\n\n\nUnited States\nNorth America\nlarge\nlarge\n\n\nSaudi Arabia\nAsia\nlarge\nmedium\n\n\nChina\nAsia\nlarge\nlarge\n\n\nKazakhstan\nAsia\nlarge\nmedium\n\n\nSudan\nAfrica\nlarge\nmedium\n\n\nArgentina\nSouth America\nlarge\nmedium\n\n\nAlgeria\nAfrica\nlarge\nmedium\n\n\nCongo, The Democratic Republic of the\nAfrica\nlarge\nlarge\n\n\nCanada\nNorth America\nlarge\nmedium\n\n\n\n\n\nThis concludes chapter 1 and you now know the INs of JOINs. Off to chapter 2 to learn the OUTs!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-and-right-joins",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-and-right-joins",
    "title": "Joining Data in SQL",
    "section": "2.1 LEFT and RIGHT JOINs",
    "text": "2.1 LEFT and RIGHT JOINs\nTheory. Coming soon …\n1. LEFT and RIGHT JOINs\nCongratulations on completing Chapter 1 on INNER JOINs. Welcome to Chapter 2 on OUTER JOINs! You can remember outer joins as reaching OUT to another table while keeping all of the records of the original table. Inner joins keep only the records IN both tables. You’ll begin this chapter by exploring (1) LEFT JOINs, (2) RIGHT JOINs, and (3) FULL JOINs, which are the three types of OUTER JOINs. Let’s begin by exploring how a LEFT JOIN differs from an INNER JOIN via a diagram.\n2. INNER JOIN diagram\nRecall the inner join diagram from Chapter 1. The only records that were included in the resulting table of the INNER JOIN query were those in which the id field had matching values.\n3. LEFT JOIN initial diagram\nIn contrast, a LEFT JOIN notes those records in the left table that do not have a match on the key field in the right table. This is denoted in the diagram by the open circles remaining close to the left table for id values of 2 and 3. These values of 2 and 3 do not appear in the id field of the right table.\n4. LEFT JOIN diagram\nYou now see the result of the LEFT JOIN query. Whereas the INNER JOIN kept just the records corresponding to id values of 1 and 4, a LEFT JOIN keeps all of the original records in the left table but then marks the values as missing in the right table for those that don’t have a match. The missing values are marked with dark gray boxes here for clarity. Note that the values of 5 and 6 for id in the right table are not found in the result of LEFT JOIN in any way.\n5. Multiple INNER JOIN diagram\nIt isn’t always the case that each key value in the left table corresponds to exactly one record in the key column of the right table. In these examples, we have this layout. Missing entries still occur for ids of 2 and 3 and the value of R3 is brought into the join from right2 since it matches on id 4. Duplicate rows are shown in the LEFT JOIN for id 1 since it has two matches corresponding to the values of R1 and R2 in the right2 table.\n6. The syntax of a LEFT JOIN\nThe syntax of the LEFT JOIN is similar to that of the INNER JOIN. Let’s explore the same code you used before to determine the countries with a prime minister and a president, but let’s use a LEFT JOIN instead of an INNER JOIN. Further, let’s remove continent to save space on the screen. The first four records in this table are the same as those from the INNER JOIN. The last six correspond to the countries that do not have a president and thus their president values are missing.\n7. RIGHT JOIN\nThe RIGHT JOIN is much less common than the LEFT JOIN so we won’t spend as much time on it here. The diagram will help you to understand how it works. Instead of matching entries in the id column on the left table TO the id column of the right table, a RIGHT JOIN does the reverse. Therefore, we see open circles on the ids of 5 and 6 in the right table since they are not found in the left table. The resulting table from the RIGHT JOIN shows these missing entries in the L_val field.As you can see in SQL the right table appears after RIGHT JOIN and the left table appears after FROM.\n8. Let’s practice!\nI’ll see you again soon to introduce FULL JOINs after you complete the next few exercises.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-join",
    "title": "Joining Data in SQL",
    "section": "2.2 Left Join",
    "text": "2.2 Left Join\nNow you’ll explore the differences between performing an inner join and a left join using the cities and countries tables.\nYou’ll begin by performing an inner join with the cities table on the left and the countries table on the right. Remember to alias the name of the city field as city and the name of the country field as country.\nYou will then change the query to a left join. Take note of how many records are in each query here!\nSteps\n\nFill in the code based on the instructions in the code comments to complete the inner join. Note how many records are in the result of the join in the query result.\n\n\n-- Select the city name (with alias), the country code,\n-- the country name (with alias), the region,\n-- and the city proper population\nSELECT c1.name AS city, code, c2.name AS country,\n       region, city_proper_pop\n-- From left table (with alias)\nFROM cities AS c1\n  -- Join to right table (with alias)\n  INNER JOIN countries AS c2\n    -- Match on country code?\n    ON c1.country_code = c2.code\n-- Order based on descending country code\nORDER BY code DESC;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\ncity\ncode\ncountry\nregion\ncity_proper_pop\n\n\n\n\nHarare\nZWE\nZimbabwe\nEastern Africa\n1606000\n\n\nLusaka\nZMB\nZambia\nEastern Africa\n1742979\n\n\nCape Town\nZAF\nSouth Africa\nSouthern Africa\n3740026\n\n\nJohannesburg\nZAF\nSouth Africa\nSouthern Africa\n4434827\n\n\nDurban\nZAF\nSouth Africa\nSouthern Africa\n3442361\n\n\nEkurhuleni\nZAF\nSouth Africa\nSouthern Africa\n3178470\n\n\nSana’a\nYEM\nYemen\nMiddle East\n1937451\n\n\nHo Chi Minh City\nVNM\nVietnam\nSoutheast Asia\n7681700\n\n\nHanoi\nVNM\nVietnam\nSoutheast Asia\n6844100\n\n\nCaracas\nVEN\nVenezuela\nSouth America\n1943901\n\n\n\n\n\n\nChange the code to perform a LEFT JOIN instead of an INNER JOIN. After executing this query, note how many records the query result contains.\n\n\nSELECT c1.name AS city, code, c2.name AS country,\n       region, city_proper_pop\nFROM cities AS c1\n  -- Join right table (with alias)\n  LEFT JOIN countries AS c2\n    -- Match on country code\n    ON c1.country_code = c2.code\n-- Order by descending country code\nORDER BY code DESC;\n\n\nDisplaying records 1 - 10\n\n\ncity\ncode\ncountry\nregion\ncity_proper_pop\n\n\n\n\nTaichung\nNA\nNA\nNA\n2752413\n\n\nTainan\nNA\nNA\nNA\n1885252\n\n\nKaohsiung\nNA\nNA\nNA\n2778918\n\n\nBucharest\nNA\nNA\nNA\n1883425\n\n\nTaipei\nNA\nNA\nNA\n2704974\n\n\nNew Taipei City\nNA\nNA\nNA\n3954929\n\n\nHarare\nZWE\nZimbabwe\nEastern Africa\n1606000\n\n\nLusaka\nZMB\nZambia\nEastern Africa\n1742979\n\n\nCape Town\nZAF\nSouth Africa\nSouthern Africa\n3740026\n\n\nEkurhuleni\nZAF\nSouth Africa\nSouthern Africa\n3178470\n\n\n\n\n\nPerfect! Notice that the INNER JOIN version resulted in 230 records. The LEFT JOIN version returned 236 rows.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-join-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-join-2",
    "title": "Joining Data in SQL",
    "section": "2.3 Left join (2)",
    "text": "2.3 Left join (2)\nNext, you’ll try out another example comparing an inner join to its corresponding left join. Before you begin though, take note of how many records are in both the countries and languages tables below.\nYou will begin with an inner join on the countries table on the left with the languages table on the right. Then you’ll change the code to a left join in the next bullet.\nNote the use of multi-line comments here using /* and */.\nSteps\n\nPerform an inner join and alias the name of the country field as country and the name of the language field as language.\nSort based on descending country name.\n\n\n/*\nSelect country name AS country, the country's local name,\nthe language name AS language, and\nthe percent of the language spoken in the country\n*/\nSELECT c.name AS country, local_name, l.name AS language, percent\n-- From left table (alias as c)\nFROM countries AS c\n  -- Join to right table (alias as l)\n  INNER JOIN languages AS l\n    -- Match on fields\n    ON c.code = l.code\n-- Order by descending country\nORDER BY country DESC;\n\n\nDisplaying records 1 - 10\n\n\ncountry\nlocal_name\nlanguage\npercent\n\n\n\n\nZimbabwe\nZimbabwe\nShona\nNA\n\n\nZimbabwe\nZimbabwe\nTonga\nNA\n\n\nZimbabwe\nZimbabwe\nTswana\nNA\n\n\nZimbabwe\nZimbabwe\nVenda\nNA\n\n\nZimbabwe\nZimbabwe\nXhosa\nNA\n\n\nZimbabwe\nZimbabwe\nSotho\nNA\n\n\nZimbabwe\nZimbabwe\nsign\nNA\n\n\nZimbabwe\nZimbabwe\nShangani\nNA\n\n\nZimbabwe\nZimbabwe\nNdau\nNA\n\n\nZimbabwe\nZimbabwe\nNambya\nNA\n\n\n\n\n\n\nPerform a left join instead of an inner join. Observe the result, and also note the change in the number of records in the result.\nCarefully review which records appear in the left join result, but not in the inner join result.\n\n\n/*\nSelect country name AS country, the country's local name,\nthe language name AS language, and\nthe percent of the language spoken in the country\n*/\nSELECT c.name AS country, local_name, l.name AS language, percent\n-- From left table (alias as c)\nFROM countries AS c\n  -- Join to right table (alias as l)\n  LEFT JOIN languages AS l\n    -- Match on fields\n    ON c.code = l.code\n-- Order by descending country\nORDER BY country DESC;\n\n\nDisplaying records 1 - 10\n\n\ncountry\nlocal_name\nlanguage\npercent\n\n\n\n\nZimbabwe\nZimbabwe\nChibarwe\nNA\n\n\nZimbabwe\nZimbabwe\nShona\nNA\n\n\nZimbabwe\nZimbabwe\nNdebele\nNA\n\n\nZimbabwe\nZimbabwe\nEnglish\nNA\n\n\nZimbabwe\nZimbabwe\nChewa\nNA\n\n\nZimbabwe\nZimbabwe\nXhosa\nNA\n\n\nZimbabwe\nZimbabwe\nVenda\nNA\n\n\nZimbabwe\nZimbabwe\nTswana\nNA\n\n\nZimbabwe\nZimbabwe\nTonga\nNA\n\n\nZimbabwe\nZimbabwe\nSotho\nNA\n\n\n\n\n\nPerfect! Notice that the INNER JOIN version resulted in 909 records. The LEFT JOIN version returned 916 rows.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-join-3",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#left-join-3",
    "title": "Joining Data in SQL",
    "section": "2.4 Left join (3)",
    "text": "2.4 Left join (3)\nYou’ll now revisit the use of the AVG() function introduced in our introductory SQL course. You will use it in combination with left join to determine the average gross domestic product (GDP) per capita by region in 2010.\nSteps\n\nBegin with a left join with the countries table on the left and the economies table on the right.\nFocus only on records with 2010 as the year.\n\n\n-- Select name, region, and gdp_percapita\nSELECT name, region, gdp_percapita\n-- From countries (alias as c)\nFROM countries AS c\n  -- Left join with economies (alias as e)\n  LEFT JOIN economies AS e\n    -- Match on code fields\n    ON c.code = e.code\n-- Focus on 2010\nWHERE year = 2010;\n\n\nDisplaying records 1 - 10\n\n\nname\nregion\ngdp_percapita\n\n\n\n\nAfghanistan\nSouthern and Central Asia\n539.667\n\n\nAngola\nCentral Africa\n3599.270\n\n\nAlbania\nSouthern Europe\n4098.130\n\n\nUnited Arab Emirates\nMiddle East\n34628.630\n\n\nArgentina\nSouth America\n10412.950\n\n\nArmenia\nMiddle East\n3121.780\n\n\nAntigua and Barbuda\nCaribbean\n13531.780\n\n\nAustralia\nAustralia and New Zealand\n56362.840\n\n\nAustria\nWestern Europe\n46757.130\n\n\nAzerbaijan\nMiddle East\n5847.260\n\n\n\n\n\n\nModify your code to calculate the average GDP per capita AS avg_gdp for each region in 2010.\nSelect the region and avg_gdp fields.\n\n\n-- Select fields\nSELECT region, AVG(gdp_percapita) AS avg_gdp\n-- From countries (alias as c)\nFROM countries AS c\n  -- Left join with economies (alias as e)\n  LEFT JOIN economies AS e\n    -- Match on code fields\n    ON c.code = e.code\n-- Focus on 2010\nWHERE year = 2010\n-- Group by region\nGROUP BY region;\n\n\nDisplaying records 1 - 10\n\n\nregion\navg_gdp\n\n\n\n\nSouthern Africa\n5051.598\n\n\nCaribbean\n11413.339\n\n\nEastern Africa\n1757.348\n\n\nSouthern Europe\n22926.411\n\n\nEastern Asia\n26205.851\n\n\nSouth America\n7420.599\n\n\nBaltic Countries\n12631.030\n\n\nNorth America\n47911.510\n\n\nAustralia and New Zealand\n44792.385\n\n\nSouthern and Central Asia\n2797.155\n\n\n\n\n\n\nArrange this data on average GDP per capita for each region in 2010 from highest to lowest average GDP per capita.\n\n\n-- Select fields\nSELECT region, AVG(gdp_percapita) AS avg_gdp\n-- From countries (alias as c)\nFROM countries AS c\n  -- Left join with economies (alias as e)\n  LEFT JOIN economies AS e\n    -- Match on code fields\n    ON c.code = e.code\n-- Focus on 2010\nWHERE year = 2010\n-- Group by region\nGROUP BY region\n-- Order by descending avg_gdp\nORDER BY avg_gdp DESC;\n\n\nDisplaying records 1 - 10\n\n\nregion\navg_gdp\n\n\n\n\nWestern Europe\n58130.96\n\n\nNordic Countries\n57074.00\n\n\nNorth America\n47911.51\n\n\nAustralia and New Zealand\n44792.38\n\n\nBritish Islands\n43588.33\n\n\nEastern Asia\n26205.85\n\n\nSouthern Europe\n22926.41\n\n\nMiddle East\n18204.64\n\n\nBaltic Countries\n12631.03\n\n\nCaribbean\n11413.34\n\n\n\n\n\nWell done. Notice how gradually you’re adding more and more building blocks to your SQL vocabulary. This enables you to answer questions of ever-increasing complexity!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#right-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#right-join",
    "title": "Joining Data in SQL",
    "section": "2.5 Right join",
    "text": "2.5 Right join\nRight joins aren’t as common as left joins. One reason why is that you can always write a right join as a left join.\nSteps\n\nThe left join code is commented out here. Your task is to write a new query using rights joins that produces the same result as what the query using left joins produces. Keep this left joins code commented as you write your own query just below it using right joins to solve the problem.\n\nNote the order of the joins matters in your conversion to using right joins!\n\n-- convert this code to use RIGHT JOINs instead of LEFT JOINs\n/*\nSELECT cities.name AS city, urbanarea_pop, countries.name AS country,\n       indep_year, languages.name AS language, percent\nFROM cities\n  LEFT JOIN countries\n    ON cities.country_code = countries.code\n  LEFT JOIN languages\n    ON countries.code = languages.code\nORDER BY city, language;\n*/\n\nSELECT cities.name AS city, urbanarea_pop, countries.name AS country,\n       indep_year, languages.name AS language, percent\nFROM languages\n  RIGHT JOIN countries\n    ON languages.code = countries.code\n  RIGHT JOIN cities\n    ON countries.code = cities.country_code\nORDER BY city, language;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\ncity\nurbanarea_pop\ncountry\nindep_year\nlanguage\npercent\n\n\n\n\nAbidjan\n4765000\nCote d’Ivoire\n1960\nFrench\nNA\n\n\nAbidjan\n4765000\nCote d’Ivoire\n1960\nOther\nNA\n\n\nAbu Dhabi\n1145000\nUnited Arab Emirates\n1971\nArabic\nNA\n\n\nAbu Dhabi\n1145000\nUnited Arab Emirates\n1971\nEnglish\nNA\n\n\nAbu Dhabi\n1145000\nUnited Arab Emirates\n1971\nHindi\nNA\n\n\nAbu Dhabi\n1145000\nUnited Arab Emirates\n1971\nPersian\nNA\n\n\nAbu Dhabi\n1145000\nUnited Arab Emirates\n1971\nUrdu\nNA\n\n\nAbuja\n1235880\nNigeria\n1960\nEnglish\nNA\n\n\nAbuja\n1235880\nNigeria\n1960\nFulani\nNA\n\n\nAbuja\n1235880\nNigeria\n1960\nHausa\nNA\n\n\n\n\n\nCorrect; everything should be reversed!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-joins",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-joins",
    "title": "Joining Data in SQL",
    "section": "2.6 FULL JOINs",
    "text": "2.6 FULL JOINs\nTheory. Coming soon …\n1. FULL JOINs\nThe last of the three types of OUTER JOINs is the FULL JOIN. In this video, you’ll see the differences between a FULL JOIN and the other joins you’ve learned about. In particular, the instruction will focus on comparing them to INNER JOINs and LEFT JOINs and then to LEFT JOINs and RIGHT JOINs.Let’s first review how the diagram changes between an INNER JOIN and a LEFT JOIN for our basic example using the left and right tables. Then we’ll delve into the FULL JOIN diagram and its SQL code.\n2. INNER JOIN vs LEFT JOIN\nRecall that an INNER JOIN keeps only the records that have matching key field values in both tables. A LEFT JOIN keeps all of the records in the left table while bringing in missing values for those key field values that don’t appear in the right table. Let’s next review the differences between a LEFT JOIN and a RIGHT JOIN.\n3. LEFT JOIN vs RIGHT JOIN\nNow you can see the differences between a LEFT JOIN and a RIGHT JOIN. The id values of 2 and 3 in the left table do not match with the id values in the right table, so missing values are brought in for them in the LEFT JOIN. Likewise for the RIGHT JOIN, missing values are brought in for id values of 5 and 6.\n4. FULL JOIN initial diagram\nA FULL JOIN combines a LEFT JOIN and a RIGHT JOIN as you can see by looking at this diagram. So it will bring in all records from both the left and the right table and keep track of the missing values accordingly.\n5. FULL JOIN diagram\nNote the missing values here and that all six of the values of id are included in the table. You can also see from the SQL code to produce this FULL JOIN result that the general format aligns closely with the SQL syntax you’ve seen for both an INNER JOIN and a LEFT JOIN. You’ll next explore an example from the leaders database.\n6. FULL JOIN example using leaders database\nLet’s revisit the example of looking at countries with prime ministers and/or presidents. We’ll walk through the code line by line to do this using a FULL JOIN. The SELECT statement starts us off by including the country field from both of our tables of interest and also the prime_minister and president fields.\n7. FULL JOIN example using leaders database\nNext, the left table is specified as prime_ministers. Note that the order matters here and if you switched the two tables you’d get slightly different output.\n8. FULL JOIN example using leaders database\nThe right table is specified as presidents with the alias of p2. prime_ministers was aliased as p1 in the previous line.\n9. FULL JOIN example using leaders database\nLastly, the join is done based on the key field of country in both tables.\n10. FULL JOIN example results using leaders\nTime for some practice!\n11. Let’s practice!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-join",
    "title": "Joining Data in SQL",
    "section": "2.7 Full join",
    "text": "2.7 Full join\nIn this exercise, you’ll examine how your results differ when using a full join versus using a left join versus using an inner join with the countries and currencies tables.\nYou will focus on the North American region and also where the name of the country is missing. Dig in to see what we mean!\nBegin with a full join with countries on the left and currencies on the right. The fields of interest have been SELECTed for you throughout this exercise.\nThen complete a similar left join and conclude with an inner join.\nSteps\n\nChoose records in which region corresponds to North America or is NULL.\n\n\nSELECT name AS country, code, region, basic_unit\n-- From countries\nFROM countries\n  -- Join to currencies\n  FULL JOIN currencies\n    -- Match on code\n    USING (code)\n-- Where region is North America or null\nWHERE region = 'North America' OR region IS NULL\n-- Order by region\nORDER BY region;\n\n\nDisplaying records 1 - 10\n\n\ncountry\ncode\nregion\nbasic_unit\n\n\n\n\nCanada\nCAN\nNorth America\nCanadian dollar\n\n\nUnited States\nUSA\nNorth America\nUnited States dollar\n\n\nBermuda\nBMU\nNorth America\nBermudian dollar\n\n\nGreenland\nGRL\nNorth America\nNA\n\n\nNA\nTMP\nNA\nUnited States dollar\n\n\nNA\nFLK\nNA\nFalkland Islands pound\n\n\nNA\nAIA\nNA\nEast Caribbean dollar\n\n\nNA\nNIU\nNA\nNew Zealand dollar\n\n\nNA\nROM\nNA\nRomanian leu\n\n\nNA\nSHN\nNA\nSaint Helena pound\n\n\n\n\n\n\nRepeat the same query as before, using a LEFT JOIN instead of a FULL JOIN. Note what has changed compared to the FULL JOIN result!\n\n\nSELECT name AS country, code, region, basic_unit\n-- From countries\nFROM countries\n  -- Join to currencies\n  LEFT JOIN currencies\n    -- Match on code\n    USING (code)\n-- Where region is North America or null\nWHERE region = 'North America' OR region IS NULL\n-- Order by region\nORDER BY region;\n\n\n4 records\n\n\ncountry\ncode\nregion\nbasic_unit\n\n\n\n\nBermuda\nBMU\nNorth America\nBermudian dollar\n\n\nCanada\nCAN\nNorth America\nCanadian dollar\n\n\nUnited States\nUSA\nNorth America\nUnited States dollar\n\n\nGreenland\nGRL\nNorth America\nNA\n\n\n\n\n\n\nRepeat the same query again but use an INNER JOIN instead of a FULL JOIN.what has changed compared to the FULL JOIN and LEFT JOIN results!\n\n\nSELECT name AS country, code, region, basic_unit\n-- From countries\nFROM countries\n  -- Join to currencies\n  INNER JOIN currencies\n    -- Match on code\n    USING (code)\n-- Where region is North America or null\nWHERE region = 'North America' OR region IS NULL\n-- Order by region\nORDER BY region;\n\n\n3 records\n\n\ncountry\ncode\nregion\nbasic_unit\n\n\n\n\nBermuda\nBMU\nNorth America\nBermudian dollar\n\n\nCanada\nCAN\nNorth America\nCanadian dollar\n\n\nUnited States\nUSA\nNorth America\nUnited States dollar\n\n\n\n\n\nHave you kept an eye out on the different numbers of records these queries returned? The FULL JOIN query returned 18 rows, the OUTER JOIN returned 4 rows, and the INNER JOIN only returned 3 rows. Do these results make sense to you?",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-join-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-join-2",
    "title": "Joining Data in SQL",
    "section": "2.8 Full join (2)",
    "text": "2.8 Full join (2)\nYou’ll now investigate a similar exercise to the last one, but this time focused on using a table with more records on the left than the right. You’ll work with the languages and countries tables.\nBegin with a full join with languages on the left and countries on the right. Appropriate fields have been selected for you again here.\nSteps\n\nChoose records in which countries.name starts with the capital letter 'V' or is NULL.\nArrange by countries.name in ascending order to more clearly see the results.\n\n\nSELECT countries.name, code, languages.name AS language\n-- From languages\nFROM languages\n  -- Join to countries\n  FULL JOIN countries\n    -- Match on code\n    USING (code)\n-- Where countries.name starts with V or is null\nWHERE countries.name LIKE 'V%' OR countries.name IS NULL\n-- Order by ascending countries.name\nORDER BY countries.name;\n\n\nDisplaying records 1 - 10\n\n\nname\ncode\nlanguage\n\n\n\n\nVanuatu\nVUT\nTribal Languages\n\n\nVanuatu\nVUT\nEnglish\n\n\nVanuatu\nVUT\nFrench\n\n\nVanuatu\nVUT\nOther\n\n\nVanuatu\nVUT\nBislama\n\n\nVenezuela\nVEN\nSpanish\n\n\nVenezuela\nVEN\nindigenous\n\n\nVietnam\nVNM\nVietnamese\n\n\nVietnam\nVNM\nEnglish\n\n\nVietnam\nVNM\nOther\n\n\n\n\n\n\nRepeat the same query as before, using a LEFT JOIN instead of a FULL JOIN. Note what has changed compared to the FULL JOIN result!\n\n\nSELECT countries.name, code, languages.name AS language\n-- From languages\nFROM languages\n  -- Join to countries\n  LEFT JOIN countries\n    -- Match using code\n    USING (code)\n-- Where countries.name starts with V or is null\nWHERE countries.name LIKE 'V%' OR countries.name IS NULL\n-- Order by ascending countries.name\nORDER BY countries.name;\n\n\nDisplaying records 1 - 10\n\n\nname\ncode\nlanguage\n\n\n\n\nVanuatu\nVUT\nEnglish\n\n\nVanuatu\nVUT\nOther\n\n\nVanuatu\nVUT\nFrench\n\n\nVanuatu\nVUT\nTribal Languages\n\n\nVanuatu\nVUT\nBislama\n\n\nVenezuela\nVEN\nindigenous\n\n\nVenezuela\nVEN\nSpanish\n\n\nVietnam\nVNM\nEnglish\n\n\nVietnam\nVNM\nVietnamese\n\n\nVietnam\nVNM\nOther\n\n\n\n\n\n\nRepeat once more, but use an INNER JOIN instead of a LEFT JOIN. Note what has changed compared to the FULL JOIN and LEFT JOIN results.\n\n\nSELECT countries.name, code, languages.name AS language\n-- From languages\nFROM languages\n  -- Join to countries\n  INNER JOIN countries\n  -- Match using code\n    USING (code)\n-- Where countries.name starts with V or is null\nWHERE countries.name LIKE 'V%' OR countries.name IS NULL\n-- Order by ascending countries.name\nORDER BY countries.name;\n\n\nDisplaying records 1 - 10\n\n\nname\ncode\nlanguage\n\n\n\n\nVanuatu\nVUT\nTribal Languages\n\n\nVanuatu\nVUT\nBislama\n\n\nVanuatu\nVUT\nEnglish\n\n\nVanuatu\nVUT\nFrench\n\n\nVanuatu\nVUT\nOther\n\n\nVenezuela\nVEN\nSpanish\n\n\nVenezuela\nVEN\nindigenous\n\n\nVietnam\nVNM\nVietnamese\n\n\nVietnam\nVNM\nEnglish\n\n\nVietnam\nVNM\nOther\n\n\n\n\n\nWell done. Again, make sure to compare the number of records the different types of joins return and try to verify whether the results make sense.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-join-3",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#full-join-3",
    "title": "Joining Data in SQL",
    "section": "2.9 Full join (3)",
    "text": "2.9 Full join (3)\nYou’ll now explore using two consecutive full joins on the three tables you worked with in the previous two exercises.\nSteps\n\nComplete a full join with countries on the left and languages on the right.\nNext, full join this result with currencies on the right.\nUse LIKE to choose the Melanesia and Micronesia regions (Hint: 'M%esia').\nSelect the fields corresponding to the country name AS country, region, language name AS language, and basic and fractional units of currency.\n\n\n-- Select fields (with aliases)\nSELECT c1.name AS country, region, l.name AS language,\n       basic_unit, frac_unit\n-- From countries (alias as c1)\nFROM countries AS c1\n  -- Join with languages (alias as l)\n  FULL JOIN languages AS l\n    -- Match on code\n    USING (code)\n  -- Join with currencies (alias as c2)\n  FULL JOIN currencies AS c2\n    -- Match on code\n    USING (code)\n-- Where region like Melanesia and Micronesia\nWHERE region LIKE 'M%esia';\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\ncountry\nregion\nlanguage\nbasic_unit\nfrac_unit\n\n\n\n\nKiribati\nMicronesia\nEnglish\nAustralian dollar\nCent\n\n\nKiribati\nMicronesia\nKiribati\nAustralian dollar\nCent\n\n\nMarshall Islands\nMicronesia\nOther\nUnited States dollar\nCent\n\n\nMarshall Islands\nMicronesia\nMarshallese\nUnited States dollar\nCent\n\n\nNauru\nMicronesia\nOther\nAustralian dollar\nCent\n\n\nNauru\nMicronesia\nEnglish\nAustralian dollar\nCent\n\n\nNauru\nMicronesia\nNauruan\nAustralian dollar\nCent\n\n\nNew Caledonia\nMelanesia\nOther\nCFP franc\nCentime\n\n\nNew Caledonia\nMelanesia\nFrench\nCFP franc\nCentime\n\n\nPalau\nMicronesia\nOther\nUnited States dollar\nCent\n\n\n\n\n\nWell done! How many countries are in the regions you filtered on?",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#review-outer-joins",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#review-outer-joins",
    "title": "Joining Data in SQL",
    "section": "2.10 Review outer joins",
    "text": "2.10 Review outer joins\nA(n) ___ join is a join combining the results of a ___ join and a ___ join.\n\n2.11 Question\nFill out!  ⬜ left, full, right ⬜ right, full, left ⬜ inner, left, right ✅ None of the above are true\n\nCorrect!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#crossing-the-rubicon",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#crossing-the-rubicon",
    "title": "Joining Data in SQL",
    "section": "2.12 CROSSing the rubicon",
    "text": "2.12 CROSSing the rubicon\nTheory. Coming soon …\n1. CROSSing the Rubicon\nNow that you’ve worked with INNER JOINs and OUTER JOINs it’s time to check out the CROSS JOIN. CROSS JOINs create all possible combinations of two tables. Let’s explore the diagram for a CROSS JOIN next.\n2. CROSS JOIN diagram\nIn this diagram we have two tables named table1 and table2. Each table only has one field, both with the name of id. The result of the CROSS JOIN is all nine combinations of the id values of 1, 2, and 3 in table1 with the id values of A, B, and C for table2. Next you’ll explore an example from the leaders database and look over the SQL syntax for a CROSS JOIN.\n3. Pairing prime ministers with presidents\nSuppose that all prime ministers in North America and Oceania in the prime_ministers table are scheduled for individual meetings with all presidents in the presidents table. You can look at all of these combinations by using a CROSS JOIN. The syntax here remains similar to what you’ve seen earlier in the course. We use a WHERE clause to focus on only prime ministers in North America and Oceania in the prime_ministers table. The results of the query give us the pairings for the two prime ministers in North America and Oceania from the prime_ministers table with the seven presidents in the presidents table.\n4. Let’s practice!\nYou’ll now hop into an exercise focusing on a couple of cities in a tribute to the author Charles Dickens. This chapter closes with a challenge to test your comprehension of the content covered here. Good luck!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#a-table-of-two-cities",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#a-table-of-two-cities",
    "title": "Joining Data in SQL",
    "section": "2.13 A table of two cities",
    "text": "2.13 A table of two cities\nThis exercise looks to explore languages potentially and most frequently spoken in the cities of Hyderabad, India and Hyderabad, Pakistan.\nSteps\n\nCreate a CROSS JOIN with cities AS c on the left and languages AS l on the right.\nMake use of LIKE and Hyder% to choose Hyderabad in both countries.\nSelect only the city name AS city and language name AS language.\n\n\n-- Select fields\nSELECT c.name AS city, l.name AS language\n-- From cities (alias as c)\nFROM cities AS c        \n  -- Join to languages (alias as l)\n  CROSS JOIN languages AS l\n-- Where c.name like Hyderabad\nWHERE c.name LIKE 'Hyder%';\n\n\nDisplaying records 1 - 10\n\n\ncity\nlanguage\n\n\n\n\nHyderabad (India)\nDari\n\n\nHyderabad\nDari\n\n\nHyderabad (India)\nPashto\n\n\nHyderabad\nPashto\n\n\nHyderabad (India)\nTurkic\n\n\nHyderabad\nTurkic\n\n\nHyderabad (India)\nOther\n\n\nHyderabad\nOther\n\n\nHyderabad (India)\nAlbanian\n\n\nHyderabad\nAlbanian\n\n\n\n\n\n\nUse an INNER JOIN instead of a CROSS JOIN. Think about what the difference will be in the results for this INNER JOIN result and the one for the CROSS JOIN.\n\n\n-- Select fields\nSELECT c.name AS city, l.name AS language\n-- From cities (alias as c)\nFROM cities AS c        \n  -- Join to languages (alias as l)\n  INNER JOIN languages AS l\n    -- Match on country code\n    ON c.country_code = l.code\n-- Where c.name like Hyderabad\nWHERE c.name LIKE 'Hyder%';\n\n\nDisplaying records 1 - 10\n\n\ncity\nlanguage\n\n\n\n\nHyderabad (India)\nHindi\n\n\nHyderabad (India)\nBengali\n\n\nHyderabad (India)\nTelugu\n\n\nHyderabad (India)\nMarathi\n\n\nHyderabad (India)\nTamil\n\n\nHyderabad (India)\nUrdu\n\n\nHyderabad (India)\nGujarati\n\n\nHyderabad (India)\nKannada\n\n\nHyderabad (India)\nMalayalam\n\n\nHyderabad (India)\nOriya\n\n\n\n\n\nGood one! Can you see the difference between a CROSS JOIN and a INNER JOIN?",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#outer-challenge",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#outer-challenge",
    "title": "Joining Data in SQL",
    "section": "2.14 Outer challenge",
    "text": "2.14 Outer challenge\nNow that you’re fully equipped to use OUTER JOINs, try a challenge problem to test your knowledge!\nIn terms of life expectancy for 2010, determine the names of the lowest five countries and their regions.\nSteps\n\nSelect country name AS country, region, and life expectancy AS life_exp.\nMake sure to use LEFT JOIN, WHERE, ORDER BY, and LIMIT.\n\n\n-- Select fields\nSELECT c.name AS country,\n       region,\n       life_expectancy AS life_exp\n-- From countries (alias as c)\nFROM countries AS c\n  -- Join to populations (alias as p)\n  LEFT JOIN populations AS p\n    -- Match on country code\n    ON c.code = p.country_code\n-- Focus on 2010\nWHERE year = 2010\n-- Order by life_exp\nORDER BY life_exp\n-- Limit to 5 records\nLIMIT 5;\n\n\n5 records\n\n\ncountry\nregion\nlife_exp\n\n\n\n\nLesotho\nSouthern Africa\n47.48341\n\n\nCentral African Republic\nCentral Africa\n47.62532\n\n\nSierra Leone\nWestern Africa\n48.22895\n\n\nSwaziland\nSouthern Africa\n48.34576\n\n\nZimbabwe\nEastern Africa\n49.57466\n\n\n\n\n\nThis was the last exercise of this chapter on outer joins and cross joins. In the next chapter, you’ll learn about set theory clauses!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#state-of-the-union",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#state-of-the-union",
    "title": "Joining Data in SQL",
    "section": "3.1 State of the UNION",
    "text": "3.1 State of the UNION\nTheory. Coming soon …\n1. State of the UNION\nWelcome to Chapter 3! You’ve made it through some challenging material so far. Keep up the great work! Next are set theory clauses. In this video, we’ll focus on the operations of UNION and UNION ALL. In addition to joining diagrams, in this chapter, you’ll also see how Venn diagrams can be used to represent set operations. Let’s begin with these Venn diagrams now.\n2. Set Theory Venn Diagrams\nYou can think of each circle as representing a table of data. The shading represents what is included in the result of the set operation from each table. Beginning in the top left, UNION includes every record in both tables but DOES NOT double count those that are in both tables. UNION ALL includes every record in both tables and DOES replicate those that are in both tables. This is why the center is shaded black. The two diagrams on the bottom represent only subsets of data being selected. INTERSECT results in only those records found in both of the two tables. EXCEPT results in only those records in one table BUT NOT the other. Let’s investigate what UNION looks like as a joining diagram.\n3. UNION diagram\nIn this diagram, you have two tables with names left_one and right_one. The “one” here corresponds to each table only having one field. If you run a UNION on these two fields you get each record appearing in either table, but notice that the id values of 1 and 4 in right_one are not included again in the UNION since they were already found in the left_one table.\n4. UNION ALL diagram\nBy contrast (with the same two tables left_one and right_one), UNION ALL includes all duplicates in its result. So left_one and right_one both having four records yields eight records for the result of the UNION ALL. If it were the case that right_one had these same four values and also one more value of 1 for id, you’d see three entries for 1 in the resulting UNION ALL. Let’s check out the SQL syntax using the leaders database for both UNION and UNION ALL, but first you’ll see one more table in the leaders database.\n5. monarchs table\nCheck out the monarchs table in the leaders database that we will use in examples here. The table lists the country, continent, and the name of the monarch for that country. Do some of these names look familiar based on the other tables you’ve seen? They should! We’ll come back to this.\n6. All prime ministers and monarchs\nYou can use UNION on the prime_ministers and monarchs table to show all of the different prime ministers and monarchs in these two tables. The country field is also included here for reference. Note that the prime_minister field has been aliased as leader. In fact, the resulting field from the UNION will have the name of leader. That’s an important property of the set theory clauses you will see in this chapter. The fields included in the operation must be of the same data type since they come back as just a single field. You can’t stack a number on top of a character field in other words.\n7. Resulting table from UNION\nOur resulting table from the UNION gives all the leaders and their corresponding country. Does something stand out to you here? (PAUSE)\n8. UNION ALL with leaders\nThe countries of Brunei and Oman were listed only once in the UNION table. These countries have monarchs that also act as prime ministers. This can be seen in the UNION ALL results. You’ve seen\n9. Let’s practice!\nthat UNION and UNION ALL clauses do not do the lookup step that JOINs do. They simply stack records on top of each other from one table to the next.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#union",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#union",
    "title": "Joining Data in SQL",
    "section": "3.2 Union",
    "text": "3.2 Union\nYou have two new tables, economies2010 and economies2015, available to you. The economies table is also included for reference.\nSteps\n\nCombine the two new tables into one table containing all of the fields in economies2010.\nSort this resulting single table by country code and then by year, both in ascending order.\n\n\n-- Select fields from 2010 table\nSELECT *\n  -- From 2010 table\n  FROM economies2010\n    -- Set theory clause\n    UNION\n-- Select fields from 2015 table\nSELECT *\n  -- From 2015 table\n  FROM economies2015\n-- Order by code and year\nORDER BY code, year;\n\n\nDisplaying records 1 - 10\n\n\ncode\nyear\nincome_group\ngross_savings\n\n\n\n\nAFG\n2010\nLow income\n37.133\n\n\nAFG\n2015\nLow income\n21.466\n\n\nAGO\n2010\nUpper middle income\n23.534\n\n\nAGO\n2015\nUpper middle income\n-0.425\n\n\nALB\n2010\nUpper middle income\n20.011\n\n\nALB\n2015\nUpper middle income\n13.840\n\n\nARE\n2010\nHigh income\n27.073\n\n\nARE\n2015\nHigh income\n34.106\n\n\nARG\n2010\nUpper middle income\n17.361\n\n\nARG\n2015\nUpper middle income\n14.111\n\n\n\n\n\nWhat a beauty!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#union-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#union-2",
    "title": "Joining Data in SQL",
    "section": "3.3 Union (2)",
    "text": "3.3 Union (2)\nUNION can also be used to determine all occurrences of a field across multiple tables. Try out this exercise with no starter code.\nSteps\n\nDetermine all (non-duplicated) country codes in either the cities or the currencies table. The result should be a table with only one field called country_code.\nSort by country_code in alphabetical order.\n\n\n-- Select field\nSELECT country_code\n  -- From cities\n  FROM cities\n  -- Set theory clause\n  UNION\n-- Select field\nSELECT code\n  -- From currencies\n  FROM currencies\n-- Order by country_code\nORDER BY country_code;\n\n\nDisplaying records 1 - 10\n\n\ncountry_code\n\n\n\n\nABW\n\n\nAFG\n\n\nAGO\n\n\nAIA\n\n\nALB\n\n\nAND\n\n\nARE\n\n\nARG\n\n\nARM\n\n\nATG\n\n\n\n\n\nWell done! Let’s take it up a notch!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#union-all",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#union-all",
    "title": "Joining Data in SQL",
    "section": "3.4 Union all",
    "text": "3.4 Union all\nAs you saw, duplicates were removed from the previous two exercises by using UNION.\nTo include duplicates, you can use UNION ALL.\nSteps\n\nDetermine all combinations (include duplicates) of country code and year that exist in either the economies or the populations tables. Order by code then year.\nThe result of the query should only have two columns/fields. Think about how many records this query should result in.\n\nYou’ll use code very similar to this in your next exercise after the video. Make note of this code after completing it.\n\n\n-- Select fields\nSELECT code, year\n  -- From economies\n  FROM economies\n  -- Set theory clause\n  UNION ALL\n-- Select fields\nSELECT country_code, year\n  -- From populations\n  FROM populations\n-- Order by code, year\nORDER BY code, year;\n\n\nDisplaying records 1 - 10\n\n\ncode\nyear\n\n\n\n\nABW\n2010\n\n\nABW\n2015\n\n\nAFG\n2010\n\n\nAFG\n2010\n\n\nAFG\n2015\n\n\nAFG\n2015\n\n\nAGO\n2010\n\n\nAGO\n2010\n\n\nAGO\n2015\n\n\nAGO\n2015\n\n\n\n\n\nCan you spot some duplicates in the query result?",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#intersectional-data-science",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#intersectional-data-science",
    "title": "Joining Data in SQL",
    "section": "3.5 INTERSECTional data science",
    "text": "3.5 INTERSECTional data science\nTheory. Coming soon …\n1. INTERSECTional data science\nYou saw with UNION and UNION ALL via examples that they do not do quite the same thing as what a join does. They only bind fields on top of one another in the two tables. The set theory clause INTERSECT works in a similar fashion to UNION and UNION ALL, but remember from the Venn diagram that INTERSECT only includes those records in common to both tables and fields selected. Let’s investigate the diagram for INTERSECT and the corresponding SQL code to achieve it.\n2. INTERSECT diagram and SQL code\nThe result of the INTERSECT on left_one and right_one is only the records in common to both left_one and right_one: 1 and 4. Let’s next see how you could use INTERSECT to determine all countries having both a prime minister and a president.\n3. Prime minister and president countries\nThe code for each of these set operations has a similar layout. You first select which fields you’d like to include in your first table, and then you specify the name of the first table. Next you specify the set operation to perform. Lastly, you denote which fields in the second table you’d like to include and then the name of the second table.The result of the query is the four countries with both a prime minister and a president in the leaders database.\n4. INTERSECT on two fields\nNext, let’s think about what would happen if we tried to select two columns instead of one from our previous example. The code shown does just that. What will be the result of this query? Will this also give you the names of the countries that have both a prime minister and a president? Hmmm [PAUSE]The actual result is an empty table. Why is that? When INTERSECT looks at two columns it includes both columns in the search. So it didn’t find any countries with prime ministers AND presidents having the same name. INTERSECT looks for RECORDS in common, not individual key fields like what a join does to match. This is an important distinction.\n5. Let’s practice!\nLet’s get some practice!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#intersect",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#intersect",
    "title": "Joining Data in SQL",
    "section": "3.6 Intersect",
    "text": "3.6 Intersect\nUNION ALL will extract all records from two tables, while INTERSECT will only return records that both tables have in common. In this exercise, you will create a similar query as before, however, this time you will look at the records in common for country code and year for the economies and populations tables.\nNote the number of records from the result of this query compared to the similar UNION ALL query result of 814 records.\nSteps\n\nUse INTERSECT to determine the records in common for country code and year for the economies and populations tables.\nAgain, order by code and then by year, both in ascending order.\n\n\n-- Select fields\nSELECT code, year\n  -- From economies\n  FROM economies\n  -- Set theory clause\n  INTERSECT\n-- Select fields\nSELECT country_code, year\n  -- From populations\n  FROM populations\n-- Order by code and year\nORDER BY code, year;\n\n\nDisplaying records 1 - 10\n\n\ncode\nyear\n\n\n\n\nAFG\n2010\n\n\nAFG\n2015\n\n\nAGO\n2010\n\n\nAGO\n2015\n\n\nALB\n2010\n\n\nALB\n2015\n\n\nARE\n2010\n\n\nARE\n2015\n\n\nARG\n2010\n\n\nARG\n2015\n\n\n\n\n\nBoom!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#intersect-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#intersect-2",
    "title": "Joining Data in SQL",
    "section": "3.7 Intersect (2)",
    "text": "3.7 Intersect (2)\nAs you think about major world cities and their corresponding country, you may ask which countries also have a city with the same name as their country name?\nSteps\n\nUse INTERSECT to answer this question with countries and cities!\n\n\n-- Select fields\nSELECT name\n  -- From countries\n  FROM countries\n  -- Set theory clause\n  INTERSECT\n-- Select fields\nSELECT name\n  -- From cities\n  FROM cities;\n\n\n2 records\n\n\nname\n\n\n\n\nSingapore\n\n\nHong Kong\n\n\n\n\n\nNice one! It looks as though Singapore is the only country that has a city with the same name!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#review-union-and-intersect",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#review-union-and-intersect",
    "title": "Joining Data in SQL",
    "section": "3.8 Review union and intersect",
    "text": "3.8 Review union and intersect\n\n3.9 Question\nWhich of the following combinations of terms and definitions is correct?  ⬜ UNION: returns all records (potentially duplicates) in both tables ⬜ UNION ALL: returns only unique records ✅ INTERSECT: returns only records appearing in both tables ⬜ None of the above are matched correctly\n\nCorrect!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#exceptional",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#exceptional",
    "title": "Joining Data in SQL",
    "section": "3.10 EXCEPTional",
    "text": "3.10 EXCEPTional\nTheory. Coming soon …\n1. EXCEPTional\nWay to go! You’ve made it to the last of the four set theory clauses in this course. EXCEPT allows you to include only the records that are in one table, but not the other. Let’s mix things up and look into the SQL code and result first and then dive into the diagram.\n2. Monarchs that aren’t prime ministers\nYou saw earlier that there are some monarchs that also act as the prime minister for their country. One way to determine those monarchs in the monarchs table that do not also hold the title of prime minister is to use the EXCEPT clause. [CLICK]This SQL query selects the monarch field from monarchs and then looks for common entries with the prime_ministers field, while also keeping track of the country for each leader. [CLICK] You can see in the resulting query that only the two European monarchs are not also prime ministers in the leaders database.\n3. EXCEPT diagram\nThis diagram gives the structure of EXCEPT clauses. Only the records that appear in the left table BUT DO NOT appear in the right table are included.\n4. Let’s practice!\nAfter a couple exercises on using EXCEPT clauses, you’ll check out the last two types of joins for the course: semi-joins and anti-joins. I’ll see you in the next video for them!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#except",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#except",
    "title": "Joining Data in SQL",
    "section": "3.11 Except",
    "text": "3.11 Except\nGet the names of cities in cities which are not noted as capital cities in countries as a single field result.\nNote that there are some countries in the world that are not included in the countries table, which will result in some cities not being labeled as capital cities when in fact they are.\nSteps\n\nOrder the resulting field in ascending order.\nCan you spot the city/cities that are actually capital cities which this query misses?\n\n\n-- Select field\nSELECT name\n  -- From cities\n  FROM cities\n  -- Set theory clause\n  EXCEPT\n-- Select field\nSELECT capital\n  -- From countries\n  FROM countries\n-- Order by result\nORDER BY name;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nAbidjan\n\n\nAhmedabad\n\n\nAlexandria\n\n\nAlmaty\n\n\nAuckland\n\n\nBandung\n\n\nBarcelona\n\n\nBarranquilla\n\n\nBasra\n\n\nBelo Horizonte\n\n\n\n\n\nEXCEPTional!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#except-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#except-2",
    "title": "Joining Data in SQL",
    "section": "3.12 Except (2)",
    "text": "3.12 Except (2)\nNow you will complete the previous query in reverse!\nDetermine the names of capital cities that are not listed in the cities table.\nSteps\n\nOrder by capital in ascending order.\nThe cities table contains information about 236 of the world’s most populous cities. The result of your query may surprise you in terms of the number of capital cities that do not appear in this list!\n\n\n-- Select field\nSELECT capital\n  -- From countries\n  FROM countries\n  -- Set theory clause\n  EXCEPT\n-- Select field\nSELECT name\n  -- From cities\n  FROM cities\n-- Order by ascending capital\nORDER BY capital;\n\n\nDisplaying records 1 - 10\n\n\ncapital\n\n\n\n\nAgana\n\n\nAmman\n\n\nAmsterdam\n\n\nAndorra la Vella\n\n\nAntananarivo\n\n\nApia\n\n\nAshgabat\n\n\nAsmara\n\n\nAstana\n\n\nAsuncion\n\n\n\n\n\nWell done. Is this query surprising, as the instructions suggested?",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#semi-joins-and-anti-joins",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#semi-joins-and-anti-joins",
    "title": "Joining Data in SQL",
    "section": "3.13 Semi-joins and Anti-joins",
    "text": "3.13 Semi-joins and Anti-joins\nTheory. Coming soon …\n1. Semi-joins and Anti-joins\nYou’ll now close this chapter by returning to joins. The six joins you’ve worked with so far are all additive joins in that they add columns to the original “left” table. Can you name all six? (1. INNER JOIN, 2. self-join, 3. LEFT JOIN, 4. RIGHT JOIN, 5. FULL JOIN, and 6. CROSS JOIN).\n2. Building up to a semi-join\nThe last two joins we will cover use a right table to determine which records to keep in the left table. In other words, you use these last two joins (semi-join and anti-join) in a way similar to a WHERE clause dependent on the values of a second table. Let’s try out some examples of semi-joins and anti-joins and then return to the diagrams for each.Suppose that you are interested in determining the presidents of countries that gained independence before 1800. Let’s first determine which countries this corresponds to in the states table. Recall from your knowledge of SQL before you knew anything about JOINs how this could be done. To get only the countries meeting this condition you can use the WHERE clause. We’ll next set up the other part of the query to get the\n3. Another step towards the semi-join\npresidents we want. What code is needed to retrieve the president, country, and continent columns from the presidents table in that order? [PAUSE]Now we need to use this result with the one in the previous slide to further filter the country field in the presidents table to give us the correct result. Let’s see how this might be done next.\n4. Finish the semi-join (an intro to subqueries)\nIn the first query of this example, we determined that Portugal and Spain were both independent before 1800. In the second query, we determined how to display the table in a nice form to answer our question. In order to combine the two tables together we will again use a WHERE clause and then use the first query as the condition to check in the WHERE clause. Check it out!This is your first example of a subquery: a query that sits inside of another query. You’ll explore these more in Chapter 4. What does this give as a result? Is it the presidents of Spain and of Portugal? Since Spain does not have a president, it is not included here and only the Portuguese president is listed. The semi-join chooses records in the first table where a condition IS met in a second table. An anti-join chooses records in the first table where a condition IS NOT met in the second table. How might you determine countries in the Americas founded after 1800?\n5. An anti-join\nUsing the code from the previous example, you only need to add a few pieces of code. So what goes in the blanks? [PAUSE]Fill in the WHERE clause by choosing only those continents ending in America and then fill in the other space with a NOT to exclude those countries in the subquery. The presidents of\n6. The result of the anti-join\ncountries in the Americas founded after 1800 are given in the table.\n7. Semi-join and anti-join diagrams\nThe semi-join matches records by key field in the right table with those in the left. It then picks out only the rows in the left table that match that condition. The anti-join picks out those columns in the left table that do not match the condition on the right table. Semi-joins and anti-joins don’t have the same built-in SQL syntax that INNER JOIN and LEFT JOIN have. They are useful tools in filtering one table’s records on the records of another table.\n8. Let’s practice!\nThis chapter’s challenge exercise will ask you to combine set theory clauses with semi-joins. Before you get to that, you’ll try out some exercises on semi-joins and anti-joins. See you again in Chapter 4!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#semi-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#semi-join",
    "title": "Joining Data in SQL",
    "section": "3.14 Semi-join",
    "text": "3.14 Semi-join\nYou are now going to use the concept of a semi-join to identify languages spoken in the Middle East.\nSteps\n\nBegin by selecting all country codes in the Middle East as a single field result using SELECT, FROM, and WHERE.\n\n\n-- Select code\nSELECT code\n  -- From countries\n  FROM countries\n-- Where region is Middle East\nWHERE region = 'Middle East';\n\n\nDisplaying records 1 - 10\n\n\ncode\n\n\n\n\nARE\n\n\nARM\n\n\nAZE\n\n\nBHR\n\n\nGEO\n\n\nIRQ\n\n\nISR\n\n\nYEM\n\n\nJOR\n\n\nKWT\n\n\n\n\n\n\nBelow the commented code, select only unique languages by name appearing in the languages table.\nOrder the resulting single field table by name in ascending order.\n\n\n-- Query from step 1:\n/*\nSELECT code\n  FROM countries\nWHERE region = 'Middle East';\n*/\n\n-- Select field\nSELECT DISTINCT name\n  -- From languages\n  FROM languages\n-- Order by name\nORDER BY name;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nAfar\n\n\nAfrikaans\n\n\nAkyem\n\n\nAlbanian\n\n\nAlsatian\n\n\nAmerindian\n\n\nAmharic\n\n\nAngolar\n\n\nAntiguan creole\n\n\nArabic\n\n\n\n\n\n\nCombine the previous two queries into one query by adding a WHERE IN statement to the SELECT DISTINCT query to determine the unique languages spoken in the Middle East.\nOrder the result by name in ascending order.\n\n\n-- Query from step 2\nSELECT DISTINCT name\n  FROM languages\n-- Where in statement\nWHERE code IN\n  -- Query from step 1\n  -- Subquery\n  (SELECT code\n   FROM countries\n   WHERE region = 'Middle East')\n-- Order by name\nORDER BY name;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nArabic\n\n\nAramaic\n\n\nArmenian\n\n\nAzerbaijani\n\n\nAzeri\n\n\nBaluchi\n\n\nBulgarian\n\n\nCircassian\n\n\nEnglish\n\n\nFarsi\n\n\n\n\n\nYour first subquery is a fact! Let’s dive a little deeper into the concept.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#relating-semi-join-to-a-tweaked-inner-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#relating-semi-join-to-a-tweaked-inner-join",
    "title": "Joining Data in SQL",
    "section": "3.15 Relating semi-join to a tweaked inner join",
    "text": "3.15 Relating semi-join to a tweaked inner join\nLet’s revisit the code from the previous exercise, which retrieves languages spoken in the Middle East.\n\nSELECT DISTINCT name\nFROM languages\nWHERE code IN\n  (SELECT code\n   FROM countries\n   WHERE region = 'Middle East')\nORDER BY name;\n\nSometimes problems solved with semi-joins can also be solved using an inner join.\n\nSELECT languages.name AS language\nFROM languages\nINNER JOIN countries\nON languages.code = countries.code\nWHERE region = 'Middle East'\nORDER BY language;\n\n\n3.16 Question\nThis inner join isn’t quite right. What is missing from this second code block to get it to match with the correct answer produced by the first block?  ⬜ HAVING instead of WHERE ✅ DISTINCT ⬜ UNIQUE",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#diagnosing-problems-using-anti-join",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#diagnosing-problems-using-anti-join",
    "title": "Joining Data in SQL",
    "section": "3.17 Diagnosing problems using anti-join",
    "text": "3.17 Diagnosing problems using anti-join\nAnother powerful join in SQL is the anti-join. It is particularly useful in identifying which records are causing an incorrect number of records to appear in join queries.\nYou will also see another example of a subquery here, as you saw in the first exercise on semi-joins. Your goal is to identify the currencies used in Oceanian countries!\nSteps\n\nBegin by determining the number of countries in countries that are listed in Oceania using SELECT, FROM, and WHERE.\n\n\n-- Select statement\nSELECT COUNT(*)\n  -- From countries\n  FROM countries\n-- Where continent is Oceania\nWHERE continent = 'Oceania';\n\n\n1 records\n\n\ncount\n\n\n\n\n19\n\n\n\n\n\n\nComplete an inner join with countries AS c1 on the left and currencies AS c2 on the right to get the different currencies used in the countries of Oceania.\nMatch ON the code field in the two tables.\nInclude the country code, country name, and basic_unit AS currency.\n\n\n-- Select fields (with aliases)\nSELECT c1.code, name, basic_unit AS currency\n  -- From countries (alias as c1)\n  FROM countries AS c1\n    -- Join with currencies (alias as c2)\n    INNER JOIN currencies AS c2\n    -- Match on code\n    ON c1.code = c2.code\n-- Where continent is Oceania\nWHERE c1.continent = 'Oceania';\n\n\nDisplaying records 1 - 10\n\n\ncode\nname\ncurrency\n\n\n\n\nAUS\nAustralia\nAustralian dollar\n\n\nPYF\nFrench Polynesia\nCFP franc\n\n\nKIR\nKiribati\nAustralian dollar\n\n\nMHL\nMarshall Islands\nUnited States dollar\n\n\nNRU\nNauru\nAustralian dollar\n\n\nNCL\nNew Caledonia\nCFP franc\n\n\nNZL\nNew Zealand\nNew Zealand dollar\n\n\nPLW\nPalau\nUnited States dollar\n\n\nPNG\nPapua New Guinea\nPapua New Guinean kina\n\n\nWSM\nSamoa\nSamoan tala\n\n\n\n\n\n\nNote that not all countries in Oceania were listed in the resulting inner join with currencies. Use an anti-join to determine which countries were not included!\n\nUse NOT IN and (SELECT code FROM currencies) as a subquery to get the country code and country name for the Oceanian countries that are not included in the currencies table.\n\n\n\n-- Select fields\nSELECT code, name\n  -- From Countries\n  FROM countries\n  -- Where continent is Oceania\n  WHERE continent = 'Oceania'\n    -- And code not in\n    AND code NOT IN\n    -- Subquery\n    (SELECT code\n     FROM currencies);\n\n\n5 records\n\n\ncode\nname\n\n\n\n\nASM\nAmerican Samoa\n\n\nFJI\nFiji Islands\n\n\nGUM\nGuam\n\n\nFSM\nMicronesia, Federated States of\n\n\nMNP\nNorthern Mariana Islands\n\n\n\n\n\nNice! Can you tell which countries were not included now?",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#set-theory-challenge",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#set-theory-challenge",
    "title": "Joining Data in SQL",
    "section": "3.18 Set theory challenge",
    "text": "3.18 Set theory challenge\nCongratulations! You’ve now made your way to the challenge problem for this third chapter. Your task here will be to incorporate two of UNION/UNION ALL/INTERSECT/EXCEPT to solve a challenge involving three tables.\nIn addition, you will use a subquery as you have in the last two exercises! This will be great practice as you hop into subqueries more in Chapter 4!\nSteps\n\nIdentify the country codes that are included in either economies or currencies but not in populations.\nUse that result to determine the names of cities in the countries that match the specification in the previous instruction.\n\n\n-- Select the city name\nSELECT name\n  -- Alias the table where city name resides\n  FROM cities AS c1\n  -- Choose only records matching the result of multiple set theory clauses\n  WHERE country_code IN\n(\n    -- Select appropriate field from economies AS e\n    SELECT e.code\n    FROM economies AS e\n    -- Get all additional (unique) values of the field from currencies AS c2   \n    UNION\n    SELECT c2.code\n    FROM currencies AS c2\n    -- Exclude those appearing in populations AS p  \n    EXCEPT\n    SELECT p.country_code\n    FROM populations AS p\n);\n\n\n6 records\n\n\nname\n\n\n\n\nBucharest\n\n\nKaohsiung\n\n\nNew Taipei City\n\n\nTaichung\n\n\nTainan\n\n\nTaipei\n\n\n\n\n\nSuccess! Head over to the final chapter of this course to feel the power of subqueries at your fingertips!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subqueries-inside-where-and-select-clauses",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subqueries-inside-where-and-select-clauses",
    "title": "Joining Data in SQL",
    "section": "4.1 Subqueries inside WHERE and SELECT clauses",
    "text": "4.1 Subqueries inside WHERE and SELECT clauses\nTheory. Coming soon …\n1. Subqueries inside WHERE and SELECT clauses\nYou’ve made it all the way to the last chapter of this course. Congratulations and keep up the excellent work. This last chapter is focused on embedding queries inside of queries. These are called nested queries and also known as subqueries as you saw in Chapter 3. The most common type of subquery is one inside of a WHERE statement. Let’s check out another one of these now with a little bit of setting up to do first, of course.\n2. Subquery inside WHERE clause set-up\nYou’ve seen many examples of using a subquery inside a WHERE clause already with the semi-join and anti-join examples and exercises you just completed. With the WHERE clause being the most common place for a subquery to be found, it’s important that you see just one more example of doing so. With this being the final chapter, it’s time to unveil the remaining fields in the states table. Note that the continent field is not shown to display all the fields here.The fert_rate field gives an estimate for the average number of babies born per woman in each country. The women_parli_perc field gives the percentage of women in the elected federal parliament for each country. Across these 13 countries, how would you determine the average fertility rate?\n3. Average fert_rate\nWe will use the average fertility rate as part of a subquery. Recall how this is done. The average babies born to women across these countries is 2-point-28 children.\n4. Asian countries below average fert_rate\nLet’s use the previous slide’s query as a subquery to determine Asian countries that fall under this average. You’ll see the code in a couple steps. First we select the country name and the fertility rate for Asian countries.\n5. Asian countries below average fert_rate\nNext, we want to choose records where fert_rate is smaller than What comes next?\n6. Asian countries below average fert_rate\nThe subquery is to get the average fertility rate! And now we can check out our result to make sure it makes sense.\n7. Asian countries below average fert_rate\nIt appears so. These are the two Asian countries we were looking for with fertility rates below 2-point-28 babies per woman.\n8. Subqueries inside SELECT clauses - setup\nThe second most common type of a subquery is inside of a SELECT clause. The task here is to count the number of countries listed in the states table for each continent in the prime_ministers table. Let’s again take the stepwise approach to setting up the problem. What does this code do? [PAUSE] It gives each of the five continents in the prime_ministers table. Let’s keep building our answer in the next slide.\n9. Subquery inside SELECT clause - complete\nNext is determining the counts of the number of countries in states for each of the continents in the last slide. Combining a COUNT clause with a WHERE statement matching the continent fields in the two tables gets us there. Let’s check out the code and then discuss a bit further. The subquery involving states also can reference the prime_ministers table in the main query. Any time you do a subquery inside a SELECT statement like this, you need to give the subquery an alias like countries_num here. Please pause the video here and carefully review this code. The result of this query comes next. [PAUSE] It’s kinda like magic that this works, huh?! If you haven’t discovered it already, there are often many different ways to solve problems with SQL queries. You could use a carefully constructed JOIN to achieve this same result, for example.\n10. Let’s practice!\nTest out your subquery expertise with a few exercises. I’ll see you back soon in the subqueries inside FROM clauses video!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-where",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-where",
    "title": "Joining Data in SQL",
    "section": "4.2 Subquery inside where",
    "text": "4.2 Subquery inside where\nYou’ll now try to figure out which countries had high average life expectancies (at the country level) in 2015.\nSteps\n\nBegin by calculating the average life expectancy across all countries for 2015.\n\n\n-- Select average life_expectancy\nSELECT AVG(life_expectancy)\n  -- From populations\n  FROM populations\n-- Where year is 2015\nWHERE year = 2015;\n\n\n1 records\n\n\navg\n\n\n\n\n71.67634\n\n\n\n\n\n\nRecall that you can use SQL to do calculations for you. Suppose we wanted only records that were above1.15 * 100 in terms of life expectancy for 2015:\n\n\nSELECT *\n  FROM populations\nWHERE life_expectancy &gt; 1.15 * 100\n  AND year = 2015;\n\nSelect all fields from populations with records corresponding to larger than 1.15 timesaverage you calculated in the first task for 2015. In other words, change the 100 in the example above with a subquery.\n\n-- Select fields\nSELECT *\n  -- From populations\n  FROM populations\n-- Where life_expectancy is greater than\nWHERE life_expectancy &gt;\n  -- 1.15 * subquery\n  1.15 * (SELECT AVG(life_expectancy)\n   FROM populations\n   WHERE year = 2015) AND\n  year = 2015;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\npop_id\ncountry_code\nyear\nfertility_rate\nlife_expectancy\nsize\n\n\n\n\n21\nAUS\n2015\n1.833\n82.45122\n23789752\n\n\n376\nCHE\n2015\n1.540\n83.19756\n8281430\n\n\n356\nESP\n2015\n1.320\n83.38049\n46443992\n\n\n134\nFRA\n2015\n2.010\n82.67073\n66538392\n\n\n170\nHKG\n2015\n1.195\n84.27805\n7305700\n\n\n174\nISL\n2015\n1.930\n82.86098\n330815\n\n\n190\nITA\n2015\n1.370\n83.49024\n60730584\n\n\n194\nJPN\n2015\n1.460\n83.84366\n126958470\n\n\n340\nSGP\n2015\n1.240\n82.59512\n5535002\n\n\n374\nSWE\n2015\n1.880\n82.55122\n9799186\n\n\n\n\n\nGood work! Let’s see how you do on a more high-level question in one go.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-where-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-where-2",
    "title": "Joining Data in SQL",
    "section": "4.3 Subquery inside where (2)",
    "text": "4.3 Subquery inside where (2)\nUse your knowledge of subqueries in WHERE to get the urban area population for only capital cities.\nSteps\n\nMake use of the capital field in the countries table in your subquery.\nSelect the city name, country code, and urban area population fields.\n\n\n-- Select fields\nSELECT name, country_code, urbanarea_pop\n  -- From cities\n  FROM cities\n-- Where city name in the field of capital cities\nWHERE name IN\n  -- Subquery\n  (SELECT capital\n   FROM countries)\nORDER BY urbanarea_pop DESC;\n\n\nDisplaying records 1 - 10\n\n\nname\ncountry_code\nurbanarea_pop\n\n\n\n\nBeijing\nCHN\n21516000\n\n\nDhaka\nBGD\n14543124\n\n\nTokyo\nJPN\n13513734\n\n\nMoscow\nRUS\n12197596\n\n\nCairo\nEGY\n10230350\n\n\nKinshasa\nCOD\n10130000\n\n\nJakarta\nIDN\n10075310\n\n\nSeoul\nKOR\n9995784\n\n\nMexico City\nMEX\n8974724\n\n\nLima\nPER\n8852000\n\n\n\n\n\nAlright. You’ve got some practice on subqueries inside WHERE now. Time to see how you do when these subqueries are in the SELECT statement!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-select",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-select",
    "title": "Joining Data in SQL",
    "section": "4.4 Subquery inside select",
    "text": "4.4 Subquery inside select\nIn this exercise, you’ll see how some queries can be written using either a join or a subquery.\nYou have seen previously how to use GROUP BY with aggregate functions and an inner join to get summarized information from multiple tables.\nThe code given in the first query selects the top nine countries in terms of number of cities appearing in the cities table. Recall that this corresponds to the most populous cities in the world. Your task will be to convert the second query to get the same result as the provided code.\nSteps\n\nSubmit the code to view the result of the provided query.\n\n\nSELECT countries.name AS country, COUNT(*) AS cities_num\n  FROM cities\n    INNER JOIN countries\n    ON countries.code = cities.country_code\nGROUP BY country\nORDER BY cities_num DESC, country\nLIMIT 9;\n\n/* \nSELECT ___ AS ___,\n  (SELECT ___\n   FROM ___\n   WHERE countries.code = cities.country_code) AS cities_num\nFROM ___\nORDER BY ___ ___, ___\nLIMIT 9;\n*/\n\n\n9 records\n\n\ncountry\ncities_num\n\n\n\n\nChina\n36\n\n\nIndia\n18\n\n\nJapan\n11\n\n\nBrazil\n10\n\n\nPakistan\n9\n\n\nUnited States\n9\n\n\nIndonesia\n7\n\n\nRussian Federation\n7\n\n\nSouth Korea\n7\n\n\n\n\n\n\nConvert the GROUP BY code to use a subquery inside of SELECT by filling in the blanks to get a result that matches the one given using the GROUP BY code in the first query.\nAgain, sort the result by cities_num descending and then by country ascending.\n\n\n/*\nSELECT countries.name AS country, COUNT(*) AS cities_num\n  FROM cities\n    INNER JOIN countries\n    ON countries.code = cities.country_code\nGROUP BY country\nORDER BY cities_num DESC, country\nLIMIT 9;\n*/\n\nSELECT countries.name AS country,\n  -- Subquery\n  (SELECT COUNT(*)\n   FROM cities\n   WHERE countries.code = cities.country_code) AS cities_num\nFROM countries\nORDER BY cities_num DESC, country\nLIMIT 9;\n\n\n9 records\n\n\ncountry\ncities_num\n\n\n\n\nChina\n36\n\n\nIndia\n18\n\n\nJapan\n11\n\n\nBrazil\n10\n\n\nPakistan\n9\n\n\nUnited States\n9\n\n\nIndonesia\n7\n\n\nRussian Federation\n7\n\n\nSouth Korea\n7\n\n\n\n\n\nGreat! The next video will introduce you to using subqueries in the FROM clause. Exciting stuff!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-from-clause",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-from-clause",
    "title": "Joining Data in SQL",
    "section": "4.5 Subquery inside FROM clause",
    "text": "4.5 Subquery inside FROM clause\nTheory. Coming soon …\n1. Subquery inside the FROM clause\nThe last basic type of a subquery exists inside of a FROM clause. A motivating example pertaining to the percentage of women in parliament will be used now to help you understand this style of subquery. Let’s dig in!\n2. Build-up\nFirst, let’s determine the maximum percentage of women in parliament for each continent listed in states. Recall that this query will only work if you include continent as one of the fields in the SELECT clause since we are grouping based on that field. Let’s check out the result. We see that Europe has the largest value and North America has the smallest value for the countries listed in the states table.\n3. Focusing on records in monarchs\nWhat if you weren’t interested in all continents, but specifically those in the monarchs table. You haven’t seen this yet in the course but you can include multiple tables in a FROM clause by adding a comma between them. Let’s investigate a way to get the continents only in monarchs using this new trick. We have at least part of our answer here, but how do we get rid of those duplicate entries? And what about the maximum column?\n4. Finishing off the subquery\nTo get Asia and Europe to appear only once, use the DISTINCT command in your SELECT statement. But now how do you get that maximum column to also come along with Asia and Europe? Instead of including states in the FROM clause, include the subquery instead and alias it with a name like subquery. There you have it! That’s how to include a subquery as a temporary table in your FROM clause.\n5. Let’s practice!\nYou are very close to the end of the course. Awesome work on making it this far! The last remaining exercises are designed to really test your understanding of the material. You may be a bit frustrated but stick with it and you’ll take the knowledge gained in these problems with you as you continue to develop your SQL skills. See you in a bit for the course review video!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-from",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-inside-from",
    "title": "Joining Data in SQL",
    "section": "4.6 Subquery inside from",
    "text": "4.6 Subquery inside from\nThe last type of subquery you will work with is one inside of FROM.\nYou will use this to determine the number of languages spoken for each country, identified by the country’s local name! (Note this may be different than the name field and is stored in the local_name field.)\nSteps\n\nBegin by determining for each country code how many languages are listed in thelanguages table using SELECT, FROM, and GROUP BY.\nAlias the aggregated field as lang_num.\n\n\n-- Select fields (with aliases)\nSELECT code, COUNT(*) AS lang_num\n  -- From languages\n  FROM languages\n-- Group by code\nGROUP BY code;\n\n\nDisplaying records 1 - 10\n\n\ncode\nlang_num\n\n\n\n\nPRY\n2\n\n\nNRU\n3\n\n\nMDG\n3\n\n\nASM\n5\n\n\nTZA\n4\n\n\nPLW\n6\n\n\nNLD\n1\n\n\nVEN\n2\n\n\nBMU\n2\n\n\nMSR\n1\n\n\n\n\n\n\nInclude the previous query (aliased as subquery) as a subquery in the FROM clause of a new query.\nSelect the local name of the country from countries.\nAlso, select lang_num from subquery.\nMake sure to use WHERE appropriately to match code in countries and in subquery.\nSort by lang_num in descending order.\n\n\nSELECT local_name, subquery.lang_num\n  FROM countries,\n    (SELECT code, COUNT(*) AS lang_num\n     FROM languages\n     GROUP BY code) AS subquery\n  WHERE countries.code = subquery.code\nORDER BY lang_num DESC;\n\n\nDisplaying records 1 - 10\n\n\nlocal_name\nlang_num\n\n\n\n\nZambia\n19\n\n\nYeItyop´iya\n16\n\n\nZimbabwe\n16\n\n\nBharat/India\n14\n\n\nNepal\n14\n\n\nMali\n13\n\n\nFrance\n13\n\n\nSouth Africa\n13\n\n\nMalawi\n12\n\n\nAngola\n12\n\n\n\n\n\nThis one wasn’t easy!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#advanced-subquery",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#advanced-subquery",
    "title": "Joining Data in SQL",
    "section": "4.7 Advanced subquery",
    "text": "4.7 Advanced subquery\nYou can also nest multiple subqueries to answer even more specific questions.\nIn this exercise, for each of the six continents listed in 2015, you’ll identify which country had the maximum inflation rate, and how high it was, using multiple subqueries. The table result of your final query should look something like the following, where anything between &lt; &gt; will be filled in with appropriate values:\n\n+------------+---------------+-------------------+\n| name       | continent     | inflation_rate    |\n|------------+---------------+-------------------|\n| &lt;country1&gt; | North America | &lt;max_inflation1&gt;  |\n| &lt;country2&gt; | Africa        | &lt;max_inflation2&gt;  |\n| &lt;country3&gt; | Oceania       | &lt;max_inflation3&gt;  |\n| &lt;country4&gt; | Europe        | &lt;max_inflation4&gt;  |\n| &lt;country5&gt; | South America | &lt;max_inflation5&gt;  |\n| &lt;country6&gt; | Asia          | &lt;max_inflation6&gt;  |\n+------------+---------------+-------------------+\n\nAgain, there are multiple ways to get to this solution using only joins, but the focus here is on showing you an introduction into advanced subqueries.\nSteps\n\nCreate an INNER JOIN with countries on the left and economies on the right with USING, without aliasing your tables or columns.\nRetrieve the country’s name, continent, and inflation rate for 2015.\n\n\n-- Select fields\nSELECT name, continent, inflation_rate\n  -- From countries\n  FROM countries\n    -- Join to economies\n    INNER JOIN economies\n    -- Match on code\n    USING (code)\n-- Where year is 2015\nWHERE year = 2015;\n\n\nDisplaying records 1 - 10\n\n\nname\ncontinent\ninflation_rate\n\n\n\n\nAfghanistan\nAsia\n-1.549\n\n\nAngola\nAfrica\n10.287\n\n\nAlbania\nEurope\n1.896\n\n\nUnited Arab Emirates\nAsia\n4.070\n\n\nArgentina\nSouth America\nNA\n\n\nArmenia\nAsia\n3.731\n\n\nAntigua and Barbuda\nNorth America\n0.969\n\n\nAustralia\nOceania\n1.461\n\n\nAustria\nEurope\n0.810\n\n\nAzerbaijan\nAsia\n4.049\n\n\n\n\n\n\nSelect the maximum inflation rate in 2015 AS max_inf grouped by continent using the previous step’s query as a subquery in the FROM clause.\n\n\nThus, in your subquery you should:\n\nCreate an inner join with countries on the left and economies on the right with USING (without aliasing your tables or columns).\nRetrieve the country name, continent, and inflation rate for 2015.\nAlias the subquery as subquery.\n\n\n\nThis will result in the six maximum inflation rates in 2015 for the six continents as one field table. Make sure to not include continent in the outer SELECT statement.\n\n\n-- Select the maximum inflation rate as max_inf\nSELECT MAX(inflation_rate) AS max_inf\n  -- Subquery using FROM (alias as subquery)\n  FROM (\n      SELECT name, continent, inflation_rate\n      FROM countries\n      INNER JOIN economies\n      USING (code)\n      WHERE year = 2015) AS subquery\n-- Group by continent\nGROUP BY continent;\n\n\n6 records\n\n\nmax_inf\n\n\n\n\n21.858\n\n\n39.403\n\n\n121.738\n\n\n7.524\n\n\n48.684\n\n\n9.784\n\n\n\n\n\n\nNow it’s time to append your second query to your first query using AND and IN to obtain the name of the country, its continent, and the maximum inflation rate for each continent in 2015.\nFor the sake of practice, change all joining conditions to use ON instead of USING.\n\n\n-- Select fields\nSELECT name, continent, inflation_rate\n  -- From countries\n  FROM countries\n  -- Join to economies\n  INNER JOIN economies\n  -- Match on code\n  ON countries.code = economies.code\n  -- Where year is 2015\n  WHERE year = 2015\n    -- And inflation rate in subquery (alias as subquery)\n    AND inflation_rate IN (\n        SELECT MAX(inflation_rate) AS max_inf\n        FROM (\n             SELECT name, continent, inflation_rate\n             FROM countries\n             INNER JOIN economies\n             ON countries.code = economies.code\n             WHERE year = 2015) AS subquery\n      -- Group by continent\n        GROUP BY continent);\n\n\n6 records\n\n\nname\ncontinent\ninflation_rate\n\n\n\n\nHaiti\nNorth America\n7.524\n\n\nMalawi\nAfrica\n21.858\n\n\nNauru\nOceania\n9.784\n\n\nUkraine\nEurope\n48.684\n\n\nVenezuela\nSouth America\n121.738\n\n\nYemen\nAsia\n39.403\n\n\n\n\n\nWow! Well done! This code works since each of the six maximum inflation rate values occur only once in the 2015 data. Think about whether this particular code involving subqueries would work in cases where there are ties for the maximum inflation rate values.",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-challenge",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-challenge",
    "title": "Joining Data in SQL",
    "section": "4.8 Subquery challenge",
    "text": "4.8 Subquery challenge\nLet’s test your understanding of the subqueries with a challenge problem! Use a subquery to get 2015 economic data for countries that do not have\n\ngov_form of 'Constitutional Monarchy' or\n'Republic' in their gov_form. Here, gov_form stands for the form of the government for each country. Review the different entries for gov_form in the countries table.\n\nSteps\n\nSelect the country code, inflation rate, and unemployment rate.\nOrder by inflation rate ascending.\nDo not use table aliasing in this exercise.\n\n\n-- Select fields\nSELECT code, inflation_rate, unemployment_rate\n  -- From economies\n  FROM economies\n  -- Where year is 2015 and code is not in\n  WHERE year = 2015 AND code NOT IN\n    -- Subquery\n    (SELECT code\n     FROM countries\n     WHERE (gov_form = 'Constitutional Monarchy' OR gov_form LIKE '%Republic%'))\n-- Order by inflation rate\nORDER BY inflation_rate;\n\n\nDisplaying records 1 - 10\n\n\ncode\ninflation_rate\nunemployment_rate\n\n\n\n\nAFG\n-1.549\nNA\n\n\nCHE\n-1.140\n3.178\n\n\nPRI\n-0.751\n12.000\n\n\nROU\n-0.596\n6.812\n\n\nBRN\n-0.423\n6.900\n\n\nTON\n-0.283\nNA\n\n\nOMN\n0.065\nNA\n\n\nTLS\n0.553\nNA\n\n\nBEL\n0.620\n8.492\n\n\nCAN\n1.132\n6.900\n\n\n\n\n\nSuperb! Let’s review subqueries before you head off to the last video of this course!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-review",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#subquery-review",
    "title": "Joining Data in SQL",
    "section": "4.9 Subquery review",
    "text": "4.9 Subquery review\n\n4.10 Question\nWithin which SQL clause are subqueries most frequently found?  ✅ WHERE ⬜ FROM ⬜ SELECT ⬜ IN\n\nCorrect!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#course-review",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#course-review",
    "title": "Joining Data in SQL",
    "section": "4.11 Course review",
    "text": "4.11 Course review\nTheory. Coming soon …\n1. Course Review\nOnly the challenge problems remain! Way to go! You’re on your way to being a SQL query warrior using PostgreSQL! Before you tackle the three challenge problems, let’s review the main topics covered throughout the course.\n2. Types of joins\nIn SQL, a join combines columns from one or more tables in a relational database via a lookup process.There are four different types of joins you learned about in this course. First, an INNER JOIN is also denoted as just JOIN in SQL. A special case of an INNER JOIN you explored is called a self-join.Second, there are three OUTER JOINs denoted as LEFT JOIN (or LEFT OUTER JOIN), RIGHT JOIN (or RIGHT OUTER JOIN), and FULL JOIN (or FULL OUTER JOIN).Third, you worked with CROSS JOINs to create all possible combinations between two tables. Lastly, you investigated semi-joins and anti-joins. Remember that words appearing in ALL capital letters correspond to the joins having simple SQL syntax. Self-joins, semi-joins, and anti-joins don’t have built-in SQL syntax.\n3. INNER JOIN vs LEFT JOIN\nAn INNER JOIN keeps only the records in which the key field (or fields) is in both tables. A LEFT JOIN keeps all the records in fields specified in the left table and includes the matches in the right table based on the key field or fields. Key field values that don’t match in the right table are included as missing data in the resulting table of a LEFT JOIN.\n4. RIGHT JOIN vs FULL JOIN\nA RIGHT JOIN keeps all the records specified in the right table and includes the matches from the key field (or fields) in the left table. Those that don’t match are included as missing values in the resulting table from the RIGHT JOIN query. A FULL JOIN is a combination of a LEFT JOIN and a RIGHT JOIN showing exactly which values appear in both tables and those that appear in only one or the other table.\n5. CROSS JOIN with code\nA CROSS JOIN matches all records from fields specified in one table with all records from fields specified in another table. Remember that a CROSS JOIN does not have an ON or USING clause, but otherwise looks very similar to the code for an INNER JOIN, LEFT JOIN, RIGHT JOIN, or FULL JOIN.\n6. Set Theory Clauses\nRecall that UNION includes every record in both tables but DOES NOT double count those that are in both tables whereas UNION ALL DOES replicate those that are in both tables. INTERSECT gives only those records found in both of the two tables. EXCEPT gives only those records in one table BUT NOT the other.\n7. Semi-joins and Anti-joins\nWhen you’d like to filter your first table based on conditions set on a second table, you should use a semi-join to accomplish your task. If instead you’d like to filter your first table based on conditions NOT being met on a second table, you should use an anti-join. Anti-joins are particularly useful in diagnosing problems with other joins in terms of getting fewer or more records than you expected.\n8. Types of basic subqueries\nThe most common type of subquery is done inside of a WHERE clause. The next most frequent types of subqueries are inside SELECT clauses and inside FROM clauses. As you’ll see in the challenge exercises, subqueries can also find their way into the ON statement of a join in ways similar to what you’ve seen inside WHERE clauses too.\n9. Own the challenge problems! You got this!\nWell, you are only three exercises away from mastering the content in this course. You are a true SQL ninja. Now take down these last three problems using all the skills you’ve built up in this course!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#final-challenge",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#final-challenge",
    "title": "Joining Data in SQL",
    "section": "4.12 Final challenge",
    "text": "4.12 Final challenge\nWelcome to the end of the course! The next three exercises will test your knowledge of the content covered in this course and apply many of the ideas you’ve seen to difficult problems. Good luck!\nRead carefully over the instructions and solve them step-by-step, thinking about how the different clauses work together.\nIn this exercise, you’ll need to get the country names and other 2015 data in the economies table and the countries table for Central American countries with an official language.\nSteps\n\nSelect unique country names. Also select the total investment and imports fields.\nUse a left join with countries on the left. (An inner join would also work, but please use a left join here.)\nMatch on code in the two tables AND use a subquery inside of ON to choose the appropriate languages records.\nOrder by country name ascending.\nUse table aliasing but not field aliasing in this exercise.\n\n\n-- Select fields\nSELECT DISTINCT name, total_investment, imports\n  -- From table (with alias)\n  FROM countries AS c\n    -- Join with table (with alias)\n    LEFT JOIN economies AS e\n      -- Match on code\n      ON (c.code = e.code\n        -- and code in Subquery\n        AND c.code IN (\n          SELECT l.code\n          FROM languages AS l\n          WHERE official = 'true'\n        ) )\n  -- Where region and year are correct\n  WHERE region = 'Central America' AND year = 2015\n-- Order by field\nORDER BY name;\n\n\n7 records\n\n\nname\ntotal_investment\nimports\n\n\n\n\nBelize\n22.014\n6.743\n\n\nCosta Rica\n20.218\n4.629\n\n\nEl Salvador\n13.983\n8.193\n\n\nGuatemala\n13.433\n15.124\n\n\nHonduras\n24.633\n9.353\n\n\nNicaragua\n31.862\n11.665\n\n\nPanama\n46.557\n5.898\n\n\n\n\n\nOne down, two to go!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#final-challenge-2",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#final-challenge-2",
    "title": "Joining Data in SQL",
    "section": "4.13 Final challenge (2)",
    "text": "4.13 Final challenge (2)\nWhoofta! That was challenging, huh?\nLet’s ease up a bit and calculate the average fertility rate for each region in 2015.\nSteps\n\nInclude the name of region, its continent, and average fertility rate aliased as avg_fert_rate.\nSort based on avg_fert_rate ascending.\nRemember that you’ll need to GROUP BY all fields that aren’t included in the aggregate function of SELECT.\n\n\n-- Select fields\nSELECT region, continent, AVG(fertility_rate) AS avg_fert_rate\n  -- From left table\n  FROM countries AS c\n    -- Join to right table\n    INNER JOIN populations AS p\n      -- Match on join condition\n      ON c.code = p.country_code\n  -- Where specific records matching some condition\n  WHERE year = 2015\n-- Group appropriately?\nGROUP BY region, continent\n-- Order appropriately\nORDER BY avg_fert_rate;\n\n\nDisplaying records 1 - 10\n\n\nregion\ncontinent\navg_fert_rate\n\n\n\n\nSouthern Europe\nEurope\n1.426100\n\n\nEastern Europe\nEurope\n1.490889\n\n\nBaltic Countries\nEurope\n1.603333\n\n\nEastern Asia\nAsia\n1.620714\n\n\nWestern Europe\nEurope\n1.632500\n\n\nNorth America\nNorth America\n1.765750\n\n\nBritish Islands\nEurope\n1.875000\n\n\nNordic Countries\nEurope\n1.893333\n\n\nAustralia and New Zealand\nOceania\n1.911500\n\n\nCaribbean\nNorth America\n1.950571\n\n\n\n\n\nInteresting. It seems that the average fertility rate is lowest in Southern Europe and highest in Central Africa. Two down, one to go!",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/joining_data_in_sql/joining_data_in_sql.html#final-challenge-3",
    "href": "content/sql/joining_data_in_sql/joining_data_in_sql.html#final-challenge-3",
    "title": "Joining Data in SQL",
    "section": "4.14 Final challenge (3)",
    "text": "4.14 Final challenge (3)\nWelcome to the last challenge problem. By now you’re a query warrior! Remember that these challenges are designed to take you to the limit to solidify your SQL knowledge! Take a deep breath and solve this step-by-step.\nYou are now tasked with determining the top 10 capital cities in Europe and the Americas in terms of a calculated percentage using city_proper_pop and metroarea_pop in cities.\nDo not use table aliasing in this exercise.\nSteps\n\nSelect the city name, country code, city proper population, and metro area population.\nCalculate the percentage of metro area population composed of city proper population for each city in cities, aliased as city_perc.\n\nFocus only on capital cities in Europe and the Americas in a subquery.\nMake sure to exclude records with missing data on metro area population.\nOrder the result by city_perc descending.\nThen determine the top 10 capital cities in Europe and the Americas in terms of this city_perc percentage.\n\n\n-- Select fields\nSELECT name, country_code, city_proper_pop, metroarea_pop,\n    -- Calculate city_perc\n      city_proper_pop / metroarea_pop * 100 AS city_perc\n  -- From appropriate table    \n  FROM cities\n  -- Where\n  WHERE name IN\n    -- Subquery\n    (SELECT capital\n     FROM countries\n     WHERE (continent = 'Europe'\n        OR continent LIKE '%America'))\n       AND metroarea_pop IS NOT NULL\n-- Order appropriately\nORDER BY city_perc DESC\n-- Limit amount\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nname\ncountry_code\ncity_proper_pop\nmetroarea_pop\ncity_perc\n\n\n\n\nLima\nPER\n8852000\n10750000\n82.34419\n\n\nBogota\nCOL\n7878783\n9800000\n80.39575\n\n\nMoscow\nRUS\n12197596\n16170000\n75.43349\n\n\nVienna\nAUT\n1863881\n2600000\n71.68773\n\n\nMontevideo\nURY\n1305082\n1947604\n67.00962\n\n\nCaracas\nVEN\n1943901\n2923959\n66.48182\n\n\nRome\nITA\n2877215\n4353775\n66.08552\n\n\nBrasilia\nBRA\n2556149\n3919864\n65.21015\n\n\nLondon\nGBR\n8673713\n13879757\n62.49182\n\n\nBudapest\nHUN\n1759407\n2927944\n60.09018",
    "crumbs": [
      "SQL",
      "Joining Data in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html",
    "title": "Introduction to Relational Databases in SQL",
    "section": "",
    "text": "Short Description\nLearn how to create tables and specify their relationships in SQL, as well as how to enforce data integrity and other unique features of database systems.\nLong Description\nYou’ve already used SQL to query data from databases. But did you know that there’s a lot more you can do with databases? You can model different phenomena in your data, as well as the relationships between them. This gives your data structure and consistency, which results in better data quality. In this course, you’ll experience this firsthand by working with a real-life dataset that was used to investigate questionable university affiliations. Column by column, table by table, you’ll get to unlock and admire the full potential of databases. You’ll learn how to create tables and specify their relationships, as well as how to enforce data integrity. You’ll also discover other unique features of database systems, such as constraints.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#introduction-to-relational-databases",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#introduction-to-relational-databases",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.1 Introduction to relational databases",
    "text": "1.1 Introduction to relational databases\nTheory. Coming soon …\n1. Your first database\nWelcome to this course on Introduction to Relational Databases. My name is Timo Grossenbacher, and I work as a data journalist in Switzerland. In this course, you will see why using relational databases has many advantages over using flat files like CSVs or Excel sheets. You’ll learn how to create such databases, and bring into force their most prominent features.\n2. Investigating universities in Switzerland\nLet me tell you a little story first. As a data journalist, I try to uncover corruption, misconduct and other newsworthy stuff with data. A couple of years ago I researched secondary employment of Swiss university professors. It turns out a lot of them have more than one side job besides their university duty, being paid by big companies like banks and insurances. So I discovered more than 1500 external employments and visualized them in an interactive graphic, shown on the left. For this story, I had to compile data from various sources with varying quality. Also, I had to account for certain specialties, for example, that a professor can work for different universities; or that a third-party company can have multiple professors working for them. In order to analyze the data, I needed to make sure its quality was good and stayed good throughout the process. That’s why I stored my data in a database, whose quite complex design you can see in the right graphic. All these rectangles were turned into database tables.\n3. A relational database:\nBut why did I use a database? A database models real-life entities like professors and universities by storing them in tables. Each table only contains data from a single entity type. This reduces redundancy by storing entities only once – for example, there only needs to be one row of data containing the details of a certain company. Lastly, a database can be used to model relationships between entities. You can define exactly how entities relate to each other. For instance, a professor can work at multiple universities and companies, while a company can employ more than one professor.\n4. Throughout this course you will:\nThroughout this course, you will actually work with the same real-life data used during my investigation. You’ll start from a single table of data and build a full-blown relational database from it, column by column, table by table. By doing so, you’ll get to know constraints, keys, and referential integrity. These three concepts help preserve data quality in databases. By the end of the course, you’ll know how to use them. In order to get going, you’ll just need a basic understanding of SQL – which can also be used to build and maintain databases, not just for querying data.\n5. Your first duty: Have a look at the PostgreSQL database\nI’ve already created a single PostgreSQL database table containing all the raw data for this course. In the next few exercises, I want you to have a look at that table. For that, you’ll need to retrieve your SQL knowledge and query the “information_schema” database, which is available in PostgreSQL by default. “information_schema” is actually some sort of meta-database that holds information about your current database. It’s not PostgreSQL specific and also available in other database management systems like MySQL or SQL Server. This “information_schema” database holds various information in different tables, for example in the “tables” table.\n6. Have a look at the columns of a certain table\n“information_schema” also holds information about columns in the “columns” table. Once you know the name of a table, you can query its columns by accessing the “columns” table. Here, for example, you see that the system table “pg_config” has only two columns – supposedly for storing name-value pairs.\n7. Let’s do this.\nOkay, let’s have a look at your first database.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#attributes-of-relational-databases",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#attributes-of-relational-databases",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.2 Attributes of relational databases",
    "text": "1.2 Attributes of relational databases\n\n1.3 Question\nWhich of the following statements does not hold true for databases? Relational databases …  ⬜ … store different real-world entities in different tables. ⬜ … allow to establish relationships between entities. ✅ … are called “relational” because they store data only about people. ⬜ … use constraints, keys and referential integrity in order to assure data quality.\n\nCorrect! Of course, databases can also store information about any other kind of entities, e.g. spare car parts.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#query-information_schema-with-select",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#query-information_schema-with-select",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.4 Query information_schema with SELECT",
    "text": "1.4 Query information_schema with SELECT\ninformation_schema is a meta-database that holds information about your current database. information_schema has multiple tables you can query with the known SELECT * FROM syntax:\n\ntables: information about all tables in your current database\ncolumns: information about all columns in all of the tables in your current database\n…\n\nIn this exercise, you’ll only need information from the 'public' schema, which is specified as the column table_schema of the tables and columns tables. The 'public' schema holds information about user-defined tables and databases. The other types of table_schema hold system information – for this course, you’re only interested in user-defined stuff.\nSteps\n\nGet information on all table names in the current database, while limiting your query to the 'public' table_schema.\n\n\n-- Query the right table in information_schema\nSELECT table_name \nFROM information_schema.tables\n-- Specify the correct table_schema value\nWHERE table_schema = 'public';\n\n\n1 records\n\n\ntable_name\n\n\n\n\nuniversity_professors\n\n\n\n\n\n\nNow have a look at the columns in university_professors by selecting all entries in information_schema.columns that correspond to that table.\n\n\n-- Query the right table in information_schema to get columns\nSELECT column_name, data_type \nFROM information_schema.columns \nWHERE table_name = 'university_professors' AND table_schema = 'public';\n\n\n8 records\n\n\ncolumn_name\ndata_type\n\n\n\n\nfirstname\ntext\n\n\nlastname\ntext\n\n\nuniversity\ntext\n\n\nuniversity_shortname\ntext\n\n\nuniversity_city\ntext\n\n\nfunction\ntext\n\n\norganization\ntext\n\n\norganization_sector\ntext\n\n\n\n\n\n\n1.5 Question\nHow many columns does the table university_professors have?  ⬜ 12 ⬜ 9 ✅ 8 ⬜ 5\n\n\nFinally, print the first five rows of the university_professors table.\n\n\n-- Query the first five rows of our table\nSELECT * \nFROM university_professors \nLIMIT 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nfirstname\nlastname\nuniversity\nuniversity_shortname\nuniversity_city\nfunction\norganization\norganization_sector\n\n\n\n\nKarl\nAberer\nETH Lausanne\nEPF\nLausanne\nChairman of L3S Advisory Board\nL3S Advisory Board\nEducation & research\n\n\nKarl\nAberer\nETH Lausanne\nEPF\nLausanne\nMember Conseil of Zeno-Karl Schindler Foundation\nZeno-Karl Schindler Foundation\nEducation & research\n\n\nKarl\nAberer\nETH Lausanne\nEPF\nLausanne\nMember of Conseil Fondation IDIAP\nFondation IDIAP\nEducation & research\n\n\nKarl\nAberer\nETH Lausanne\nEPF\nLausanne\nPanel Member\nSNF Ambizione Program\nEducation & research\n\n\nReza Shokrollah\nAbhari\nETH Zürich\nETH\nZurich\nAufsichtsratsmandat\nPNE Wind AG\nEnergy, environment & mobility\n\n\n\n\n\nGreat work! You’re now familiar with the pre-existing university_professors table, which holds information on all kinds of entities. You’ll migrate data from this table to other tables in the upcoming lessons.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#tables-at-the-core-of-every-database",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#tables-at-the-core-of-every-database",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.6 Tables: At the core of every database",
    "text": "1.6 Tables: At the core of every database\nTheory. Coming soon …\n1. Tables: At the core of every database\nNow that you’ve had a first look at your database, let’s delve into one of the most important concepts behind databases: tables.\n2. Redundancy in the university_professors table\nYou might have noticed that there’s some redundancy in the “university_professors” table. Let’s have a look at the first three records, for example.\n3. Redundancy in the university_professors table\nAs you can see, this professor is repeated in the first three records. Also, his university, the “ETH Lausanne”, is repeated a couple of times – because he only works for this university. However, he seems to have affiliations with at least three different organizations. So, there’s a certain redundancy in that table. The reason for this is that the table actually contains entities of at least three different types. Let’s have a look at these entity types.\n4. Redundancy in the university_professors table\nActually the table stores professors, highlighted in blue, universities, highlighted in green, and organizations, highlighted in brown. There’s also this column called “function” which denotes the role the professor plays at a certain organization. More on that later.\n5. Currently: One “entity type” in the database\nLet’s look at the current database once again. The graphic used here is called an entity-relationship diagram. Squares denote so-called entity types, while circles connected to these denote attributes (or columns). So far, we have only modeled one so-called entity type – “university_professors”. However, we discovered that this table actually holds many different entity types…\n6. A better database model with three entity types\n…so this updated entity-relationship model on the right side would be better suited. It represents three entity types, “professors”, “universities”, and “organizations” in their own tables, with respective attributes. This reduces redundancy, as professors, unlike now, need to be stored only once. Note that, for each professor, the respective university is also denoted through the “university_shortname” attribute. However, one original attribute, the “function”, is still missing.\n7. A better database model with four entity types\nAs you know, this database contains affiliations of professors with third-party organizations. The attribute “function” gives some extra information to that affiliation. For instance, somebody might act as chairman for a certain third-party organization. So the best idea at the moment is to store these affiliations in their own table – it connects professors with their respective organizations, where they have a certain function.\n8. Create new tables with CREATE TABLE\nThe first thing you need to do now is to create four empty tables for professors, universities, organizations, and affiliations. This is quite easy with SQL – you’ll use the “CREATE TABLE” command for that. At the minimum, this command requires a table name and one or more columns with their respective data types.\n9. Create new tables with CREATE TABLE\nFor example, you could create a “weather” table with three aptly named columns. After each column name, you must specify the data type. There are many different types, and you will discover some in the remainder of this course. For example, you could specify a text column, a numeric column, and a column that requires fixed-length character strings with 5 characters each. These data types will be explained in more detail in the next chapter.\n10. Let’s practice!\nFor now, you will first create the four tables and then migrate data from the original table to them. Let’s do this.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#create-your-first-few-tables",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#create-your-first-few-tables",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.7 CREATE your first few TABLEs",
    "text": "1.7 CREATE your first few TABLEs\nYou’ll now start implementing a better database model. For this, you’ll create tables for the professors and universities entity types. The other tables will be created for you.\nThe syntax for creating simple tables is as follows:\n\nCREATE TABLE table_name (\n column_a data_type,\n column_b data_type,\n column_c data_type\n);\n\nAttention: Table and columns names, as well as data types, don’t need to be surrounded by quotation marks.\nSteps\n\nCreate a table professors with two text columns: firstname and lastname.\n\n\n-- Create a table for the professors entity type\nCREATE TABLE professors (\n firstname text,\n lastname text\n);\n\n\n-- Print the contents of this table\nSELECT * \nFROM professors\n\n\n0 records\n\n\nfirstname\nlastname\n\n\n\n\n\n\n\n\nCreate a table universities with three text columns: university_shortname, university, and university_city.\n\n\n-- Create a table for the universities entity type\nCREATE TABLE universities (\n university_shortname text,\n university text,\n university_city text\n);\n\n\n-- Print the contents of this table\nSELECT * \nFROM universities\n\n\n0 records\n\n\nuniversity_shortname\nuniversity\nuniversity_city\n\n\n\n\n\n\n\nGreat job. The other two tables, affiliations and organizations, will be created for you.\n\nCREATE TABLE affiliations (\n firstname text,\n lastname text,\n university_shortname text,\n function text,\n organisation text\n);\n\n\nCREATE TABLE organizations (\n organization text,\n organization_sector text\n);",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-a-column-with-alter-table",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-a-column-with-alter-table",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.8 ADD a COLUMN with ALTER TABLE",
    "text": "1.8 ADD a COLUMN with ALTER TABLE\nOops! We forgot to add the university_shortname column to the professors table. You’ve probably already noticed:\n\nIn chapter 4 of this course, you’ll need this column for connecting the professors table with the universities table.\nHowever, adding columns to existing tables is easy, especially if they’re still empty.\nTo add columns you can use the following SQL query:\n\nALTER TABLE table_name\nADD COLUMN column_name data_type;\n\nSteps\n\nAlter professors to add the text column university_shortname.\n\n\n-- Add the university_shortname column\nALTER TABLE professors\nADD COLUMN university_shortname text;\n\n\n-- Print the contents of this table\nSELECT * \nFROM professors\n\n\n0 records\n\n\nfirstname\nlastname\nuniversity_shortname\n\n\n\n\n\n\n\nPerfect – now your first sample database model is complete. Time to fill these tables with data!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#update-your-database-as-the-structure-changes",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#update-your-database-as-the-structure-changes",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.9 Update your database as the structure changes",
    "text": "1.9 Update your database as the structure changes\nTheory. Coming soon …\n1. Update your database as the structure changes\nWell done so far. You now have a database consisting of five different tables. Now it’s time to migrate the data.\n2. The current database model\nHere’s the current entity-relationship diagram, showing the five tables.\n3. The current database model\nAt this moment, only the “university_professors” table holds data. The other four, shown in red, are still empty. In the remainder of this chapter, you will migrate data from the green part of this diagram to the red part, moving the respective entity types to their appropriate tables. In the end, you’ll be able to delete the “university_professors” table.\n4. Only store DISTINCT data in the new tables\nOne advantage of splitting up “university_professors” into several tables is the reduced redundancy. As of now, “university_professors” holds 1377 entries. However, there are only 1287 distinct organizations, as this query shows. Therefore, you only need to store 1287 distinct organizations in the new “organizations” table.\n5. INSERT DISTINCT records INTO the new tables\nIn order to copy data from an existing table to a new one, you can use the “INSERT INTO SELECT DISTINCT” pattern. After “INSERT INTO”, you specify the name of the target table – “organizations” in this case. Then you select the columns that should be copied over from the source table – “unviversity_professors” in this case. You use the “DISTINCT” keyword to only copy over distinct organizations. As the output shows, only 1287 records are inserted into the “organizations” table. If you just used “INSERT INTO SELECT”, without the “DISTINCT” keyword, duplicate records would be copied over as well. In the following exercises, you will migrate your data to the four new tables.\n6. The INSERT INTO statement\nBy the way, this is the normal use case for “INSERT INTO” – where you insert values manually. “INSERT INTO” is followed by the table name and an optional list of columns which should be filled with data. Then follows the “VALUES” keyword and the actual values you want to insert.\n7. RENAME a COLUMN in affiliations\nBefore you start migrating the table, you need to fix some stuff! In the last lesson, I created the “affiliations” table for you. Unfortunately, I made a mistake in this process. Can you spot it? The way the “organisation” column is spelled is not consistent with the American-style spelling of this table, using an “s” instead of a “z”. In the first exercise after the video, you will correct this with the known “ALTER TABLE” syntax. You do this with the RENAME COLUMN command by specifying the old column name first and then the new column name, i.e., “RENAME COLUMN old_name TO new_name”.\n8. DROP a COLUMN in affiliations\nAlso, the “university_shortname” column is not even needed here. So I want you to delete it. The syntax for this is again very simple, you use a “DROP COLUMN” command followed by the name of the column. Dropping columns is straightforward when the tables are still empty, so it’s not too late to fix this error. But why is it an error in the first place?\n9. A professor is uniquely identified by firstname, lastname only\nWell, I queried the “university_professors” table and saw that there are 551 unique combinations of first names, last names, and associated universities. I then queried the table again and only looked for unique combinations of first and last names. Turns out, this is also 551 records. This means that the columns “firstname” and “lastname” uniquely identify a professor.\n10. A professor is uniquely identified by firstname, lastname only\nSo the “university_shortname” column is not needed in order to reference a professor in the affiliations table. You can remove it, and this will reduce the redundancy in your database again. In other words: The columns “firstname”, “lastname”, “function”, and “organization” are enough to store the affiliation a professor has with a certain organization.\n11. Let’s get to work!\nTime to prepare the database for data migration. After this, you will migrate the data.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#rename-and-drop-columns-in-affiliations",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#rename-and-drop-columns-in-affiliations",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.10 RENAME and DROP COLUMNs in affiliations",
    "text": "1.10 RENAME and DROP COLUMNs in affiliations\nAs mentioned in the video, the still empty affiliations table has some flaws. In this exercise, you’ll correct them as outlined in the video.\nYou’ll use the following queries:\n\nTo rename columns:\n\n\nALTER TABLE table_name\nRENAME COLUMN old_name TO new_name;\n\n\nTo delete columns:\n\n\nALTER TABLE table_name\nDROP COLUMN column_name;\n\nSteps\n\nRename the organisation column to organization in affiliations.\n\n\n-- Rename the organisation column\nALTER TABLE affiliations\nRENAME COLUMN organisation TO organization;\n\n\nDelete the university_shortname column in affiliations.\n\n\n-- Delete the university_shortname column\nALTER TABLE affiliations\nDROP COLUMN university_shortname;\n\nGreat work! Now the tables are finally ready for data migration.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#migrate-data-with-insert-into-select-distinct",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#migrate-data-with-insert-into-select-distinct",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.11 Migrate data with INSERT INTO SELECT DISTINCT",
    "text": "1.11 Migrate data with INSERT INTO SELECT DISTINCT\nNow it’s finally time to migrate the data into the new tables. You’ll use the following pattern:\n\nINSERT INTO ... \nSELECT DISTINCT ... \nFROM ...;\n\nIt can be broken up into two parts:\nFirst part:\n\nSELECT DISTINCT column_name1, column_name2, ... \nFROM table_a;\n\nThis selects all distinct values in table table_a – nothing new for you.\nSecond part:\n\nINSERT INTO table_b ...;\n\nTake this part and append it to the first, so it inserts all distinct rows from table_a into table_b.\nOne last thing: It is important that you run all of the code at the same time once you have filled out the blanks.\nSteps\n\nInsert all DISTINCT professors from university_professors into professors.\nPrint all the rows in professors.\n\n\n-- Insert unique professors into the new table\nINSERT INTO professors \nSELECT DISTINCT firstname, lastname, university_shortname \nFROM university_professors;\n\n\n-- Doublecheck the contents of professors\nSELECT * \nFROM professors;\n\n\nDisplaying records 1 - 10\n\n\nfirstname\nlastname\nuniversity_shortname\n\n\n\n\nMichel\nRappaz\nEPF\n\n\nHilal\nLashuel\nEPF\n\n\nJeffrey\nHuang\nEPF\n\n\nPierre\nMagistretti\nEPF\n\n\nPaolo\nIenne\nEPF\n\n\nFrédéric\nKaplan\nEPF\n\n\nOlivier\nHari\nUNE\n\n\nChristian\nHesse\nUBE\n\n\nMajed\nChergui\nEPF\n\n\nDouglas\nHanahan\nEPF\n\n\n\n\n\n\nInsert all DISTINCT affiliations into affiliations from university_professors.\n\n\n-- Insert unique affiliations into the new table\nINSERT INTO affiliations \nSELECT DISTINCT firstname, lastname, function, organization \nFROM university_professors;\n\n\n-- Doublecheck the contents of affiliations\nSELECT * \nFROM affiliations;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nfirstname\nlastname\nfunction\norganization\n\n\n\n\nDimos\nPoulikakos\nVR-Mandat\nScrona AG\n\n\nFrancesco\nStellacci\nCo-editor in Chief, Nanoscale\nRoyal Chemistry Society, UK\n\n\nAlexander\nFust\nFachexperte und Coach für Designer Startups\nCreative Hub\n\n\nJürgen\nBrugger\nProposal reviewing HEPIA\nHES Campus Biotech, Genève\n\n\nHervé\nBourlard\nDirector\nIdiap Research Institute\n\n\nIoannis\nPapadopoulos\nMandat\nSchweizerischer Nationalfonds (SNF)\n\n\nOlaf\nBlanke\nProfesseur à 20%\nUniversité de Genève\n\n\nLeo\nStaub\nPräsident Verwaltungsrat\nGenossenschaft Migros Ostschweiz\n\n\nPascal\nPichonnaz\nVize-Präsident\nEKK (Eidgenössische Konsumenten Kommission)\n\n\nDietmar\nGrichnik\nPräsident\nSwiss Startup Monitor Stiftung\n\n\n\n\n\n\n-- Doublecheck the contents of affiliations\nSELECT COUNT(*) \nFROM affiliations;\n\n\n1 records\n\n\ncount\n\n\n\n\n1377\n\n\n\n\n\nPerfect. You can see that there are 1377 distinct combinations of professors and organisations in the dataset. We’ll migrate the other two tables universities and organisations for you.\nMigrate data to the universities and organizations tables\n\nINSERT INTO universities\nSELECT DISTINCT university_shortname, university, university_city\nFROM university_professors;\n\n\nSELECT *\nFROM universities\n\n\nDisplaying records 1 - 10\n\n\nuniversity_shortname\nuniversity\nuniversity_city\n\n\n\n\nUGE\nUni Genf\nGeneva\n\n\nUSI\nUSI Lugano\nLugano\n\n\nUFR\nUni Freiburg\nFribourg\n\n\nUSG\nUni St. Gallen\nSaint Gallen\n\n\nULA\nUni Lausanne\nLausanne\n\n\nEPF\nETH Lausanne\nLausanne\n\n\nUBE\nUni Bern\nBern\n\n\nETH\nETH Zürich\nZurich\n\n\nUNE\nUni Neuenburg\nNeuchâtel\n\n\nUBA\nUni Basel\nBasel\n\n\n\n\n\n\nINSERT INTO organizations\nSELECT DISTINCT organization, organization_sector\nFROM university_professors;\n\n\nSELECT *\nFROM organizations\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\norganization\norganization_sector\n\n\n\n\nStiftung-Sammlung Schweizer Rechtsquellen\nNot classifiable\n\n\nResponsAbility\nFinancial industry & insurances\n\n\nFondation IQRGC\nNot classifiable\n\n\nu-blox AG\nIndustry, construction & agriculture\n\n\nDepartement für Entwicklungshilfe und -zusammenarbeit DEZA\nPolitics, administration, justice system & security sector\n\n\nub-invent GmbH\nPharma & health\n\n\nQuiartet Medicine, Boston, USA\nTechnology\n\n\nCSSI Bern\nPolitics, administration, justice system & security sector\n\n\nSchweizerische Akademie der Naturwissenschaften SCNAT Bern\nEducation & research\n\n\nAvenir Suisse\nSociety, Social, Culture & Sports\n\n\n\n\n\nThe last thing to do in this chapter is to delete the no longer needed university_professors table.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#delete-tables-with-drop-table",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#delete-tables-with-drop-table",
    "title": "Introduction to Relational Databases in SQL",
    "section": "1.12 Delete tables with DROP TABLE",
    "text": "1.12 Delete tables with DROP TABLE\nObviously, the university_professors table is now no longer needed and can safely be deleted.\nFor table deletion, you can use the simple command:\n\nDROP TABLE table_name;\n\nSteps\n\nDelete the university_professors table.\n\n\n-- Delete the university_professors table\nDROP TABLE university_professors;\n\nPerfect! Now it’s finally time to delve into the real advantages of databases. In the following chapters, you will discover many cool features that ultimately lead to better data consistency and quality, such as domain constraints, keys, and referential integrity. See you soon!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#better-data-quality-with-constraints",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#better-data-quality-with-constraints",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.1 Better data quality with constraints",
    "text": "2.1 Better data quality with constraints\nTheory. Coming soon …\n1. Better data quality with constraints\nSo far you’ve learned how to set up a simple database that consists of multiple tables. Apart from storing different entity types, such as professors, in different tables, you haven’t made much use of database features. In the end, the idea of a database is to push data into a certain structure – a pre-defined model, where you enforce data types, relationships, and other rules. Generally, these rules are called integrity constraints, although different names exist.\n2. Integrity constraints\nIntegrity constraints can roughly be divided into three types. The most simple ones are probably the so-called attribute constraints. For example, a certain attribute, represented through a database column, could have the integer data type, allowing only for integers to be stored in this column. They’ll be the subject of this chapter. Secondly, there are so-called key constraints. Primary keys, for example, uniquely identify each record, or row, of a database table. They’ll be discussed in the next chapter. Lastly, there are referential integrity constraints. In short, they glue different database tables together. You’ll learn about them in the last chapter of this course.\n3. Why constraints?\nSo why should you know about constraints? Well, they press the data into a certain form. With good constraints in place, people who type in birthdates, for example, have to enter them in always the same form. Data entered by humans is often very tedious to pre-process. So constraints give you consistency, meaning that a row in a certain table has exactly the same form as the next row, and so forth. All in all, they help to solve a lot of data quality issues. While enforcing constraints on human-entered data is difficult and tedious, database management systems can be a great help. In the next chapters and exercises, you’ll explore how.\n4. Data types as attribute constraints\nYou’ll start with attribute constraints in this chapter. In its simplest form, attribute constraints are data types that can be specified for each column of a table. Here you see the beginning of a list of all data types in PostgreSQL. There are basic data types for numbers, such as “bigint”, or strings of characters, such as “character varying”. There are also more high-level data types like “cidr”, which can be used for IP addresses. Implementing such a type on a column would disallow anything that doesn’t fit the structure of an IP.\n5. Dealing with data types (casting)\nData types also restrict possible SQL operations on the stored data. For example, it is impossible to calculate a product from an integer and a text column, as shown here in the example. The text column “wind_speed” may store numbers, but PostgreSQL doesn’t know how to use text in a calculation. The solution for this is type casts, that is, on-the-fly type conversions. In this case, you can use the “CAST” function, followed by the column name, the AS keyword, and the desired data type, and PostgreSQL will turn “wind_speed” into an integer right before the calculation.\n6. Let’s practice!\nOkay, let’s look into this first type of constraints!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#types-of-database-constraints",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#types-of-database-constraints",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.2 Types of database constraints",
    "text": "2.2 Types of database constraints\n\n2.3 Question\nWhich of the following is not used to enforce a database constraint?  ⬜ Foreign keys ✅ SQL aggregate functions ⬜ The BIGINT data type ⬜ Primary keys\n\nExactly! SQL aggregate functions are not used to enforce constraints, but to do calculations on data.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#conforming-with-data-types",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#conforming-with-data-types",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.4 Conforming with data types",
    "text": "2.4 Conforming with data types\nFor demonstration purposes, I created a fictional database table that only holds three records. The columns have the data types date, integer, and text, respectively.\n\nCREATE TABLE transactions (\n transaction_date date, \n amount integer,\n fee text\n);\n\nHave a look at the contents of the transactions table.\nThe transaction_date accepts date values. According to the PostgreSQL documentation, it accepts values in the form of YYYY-MM-DD, DD/MM/YY, and so forth.\nBoth columns amount and fee appear to be numeric, however, the latter is modeled as text – which you will account for in the next exercise.\nSteps\n\nExecute the given sample code.\nAs it doesn’t work, have a look at the error message and correct the statement accordingly – then execute it again.\n\n\n-- Let's add a record to the table\nINSERT INTO transactions (transaction_date, amount, fee) \nVALUES ('2018-24-09', 5454, '30');\n\n#&gt; Error: Failed to prepare query : ERROR:  date/time field value out of range: \"2018-24-09\"\n#&gt; LINE 3: VALUES ('2018-24-09', 5454, '30');\n#&gt;                 ^\n#&gt; HINT:  Perhaps you need a different \"datestyle\" setting.\n\n\n\n-- Let's add a record to the table\nINSERT INTO transactions (transaction_date, amount, fee) \nVALUES ('2018-09-24', 5454, '30');\n\n\n-- Doublecheck the contents\nSELECT *\nFROM transactions;\n\n\nDisplaying records 1 - 10\n\n\ntransaction_date\namount\nfee\n\n\n\n\n1999-01-08\n500\n20\n\n\n2001-02-20\n403\n15\n\n\n2001-03-20\n3430\n35\n\n\n2018-09-24\n5454\n30\n\n\n1999-01-08\n500\n20\n\n\n2001-02-20\n403\n15\n\n\n2001-03-20\n3430\n35\n\n\n2018-09-24\n5454\n30\n\n\n1999-01-08\n500\n20\n\n\n2001-02-20\n403\n15\n\n\n\n\n\nGood work. You can see that data types provide certain restrictions on how data can be entered into a table. This may be tedious at the moment of insertion, but saves a lot of headache in the long run.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#type-casts",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#type-casts",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.5 Type CASTs",
    "text": "2.5 Type CASTs\nIn the video, you saw that type casts are a possible solution for data type issues. If you know that a certain column stores numbers as text, you can cast the column to a numeric form, i.e. to integer.\n\nSELECT CAST(some_column AS integer)\nFROM table;\n\nNow, the some_column column is temporarily represented as integer instead of text, meaning that you can perform numeric calculations on the column.\nSteps\n\nExecute the given sample code.\nAs it doesn’t work, add an integer type cast at the right place and execute it again.\n\n\n-- Calculate the net amount as amount + fee\nSELECT transaction_date, amount + fee AS net_amount \nFROM transactions;\n\n#&gt; Error: Failed to prepare query : ERROR:  operator does not exist: double precision + text\n#&gt; LINE 2: SELECT transaction_date, amount + fee AS net_amount \n#&gt;                                         ^\n#&gt; HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n\n\n\n-- Calculate the net amount as amount + fee\nSELECT transaction_date, amount + CAST(fee AS integer) AS net_amount \nFROM transactions;\n\n\nDisplaying records 1 - 10\n\n\ntransaction_date\nnet_amount\n\n\n\n\n1999-01-08\n520\n\n\n2001-02-20\n418\n\n\n2001-03-20\n3465\n\n\n2018-09-24\n5484\n\n\n1999-01-08\n520\n\n\n2001-02-20\n418\n\n\n2001-03-20\n3465\n\n\n2018-09-24\n5484\n\n\n1999-01-08\n520\n\n\n2001-02-20\n418\n\n\n\n\n\nGood job! You saw how, sometimes, type casts are necessary to work with data. However, it is better to store columns in the right data type from the first place. You’ll learn how to do this in the next exercises.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#working-with-data-types",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#working-with-data-types",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.6 Working with data types",
    "text": "2.6 Working with data types\nTheory. Coming soon …\n1. Working with data types\nWorking with data types is straightforward in a database management system like PostgreSQL.\n2. Working with data types\nAs said before, data types are attribute constraints and are therefore implemented for single columns of a table. They define the so-called “domain” of values in a column, that means, what form these values can take – and what not. Therefore, they also define what operations are possible with the values in the column, as you saw in the previous exercises. Of course, through this, consistent storage is enforced, so a street number will always be an actual number, and a postal code will always have no more than 6 digits, according to your conventions. This greatly helps with data quality.\n3. The most common types\nHere are the most common types in PostgreSQL. Note that these types are specific to PostgreSQL but appear in many other database management systems as well, and they mostly conform to the SQL standard. The “text” type allows characters strings of any length, while the “varchar” and “char” types specify a maximum number of characters, or a character string of fixed length, respectively. You’ll use these two for your database. The “boolean” type allows for two boolean values, for example, “true” and “false” or “1” and “0”, and for a third unknown value, expressed through “NULL”.\n4. The most common types (cont’d.)\nThen there are various formats for date and time calculations, also with timezone support. “numeric” is a general type for any sort of numbers with arbitrary precision, while “integer” allows only whole numbers in a certain range. If that range is not enough for your numbers, there’s also “bigint” for larger numbers.\n5. Specifying types upon table creation\nHere’s an example of how types are specified upon table creation. Let’s say the social security number, “ssn”, should be stored as an integer as it only contains whole numbers. The name may be a string with a maximum of 64 characters, which might or might not be enough. The date of birth, “dob”, is naturally stored as a date, while the average grade is a numeric value with a precision of 3 and a scale of 2, meaning that numbers with a total of three digits and two digits after the fractional point are allowed. Lastly, the information whether the tuition of the student was paid is, of course, a boolean one, as it can be either true or false.\n6. Alter types after table creation\nAltering types after table creation is also straightforward, just use the shown “ALTER TABLE ALTER COLUMN” statement. In this case, the maximum name length is extended to 128 characters.Sometimes it may be necessary to truncate column values or transform them in any other way, so they fit with the new data type. Then you can use the “USING” keyword, and specify a transformation that should happen before the type is altered. Let’s say you’d want to turn the “average_grade” column into an integer type. Normally, PostgreSQL would just keep the part of the number before the fractional point. With “USING”, you can tell it to round the number to the nearest integer, for example.\n7. Let’s apply this!\nLet’s apply this to your database and add the proper types to your table columns.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#change-types-with-alter-column",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#change-types-with-alter-column",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.7 Change types with ALTER COLUMN",
    "text": "2.7 Change types with ALTER COLUMN\nThe syntax for changing the data type of a column is straightforward. The following code changes the data type of the column_name column in table_name to varchar(10):\n\nALTER TABLE table_name\nALTER COLUMN column_name\nTYPE varchar(10)\n\nNow it’s time to start adding constraints to your database.\nSteps 1. Have a look at the distinct university_shortname values in the professors table and take note of the length of the strings.\n\n-- Select the university_shortname column\nSELECT DISTINCT(university_shortname) \nFROM professors;\n\n\nDisplaying records 1 - 10\n\n\nuniversity_shortname\n\n\n\n\nULA\n\n\nUNE\n\n\nEPF\n\n\nUSG\n\n\nUBA\n\n\nUBE\n\n\nUZH\n\n\nUGE\n\n\nUFR\n\n\nUSI\n\n\n\n\n\n\nNow specify a fixed-length character type with the correct length for university_shortname.\n\n\n-- Specify the correct fixed-length character type\nALTER TABLE professors\nALTER COLUMN university_shortname\nTYPE char(3);\n\n\nChange the type of the firstname column to varchar(64).\n\n\n-- Change the type of firstname\nALTER TABLE professors\nALTER COLUMN firstname\nTYPE varchar(64);\n\nGood work. I’ve specified the types of the other tables for you.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#convert-types-using-a-function",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#convert-types-using-a-function",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.8 Convert types USING a function",
    "text": "2.8 Convert types USING a function\nIf you don’t want to reserve too much space for a certain varchar column, you can truncate the values before converting its type.\nFor this, you can use the following syntax:\n\nALTER TABLE table_name\nALTER COLUMN column_name\nTYPE varchar(x)\nUSING SUBSTRING(column_name FROM 1 FOR x)\n\nYou should read it like this: Because you want to reserve only x characters for column_name, you have to retain a SUBSTRING of every value, i.e. the first x characters of it, and throw away the rest. This way, the values will fit the varchar(x) requirement.\nSteps\n\nRun the sample code as is and take note of the error.\nNow use SUBSTRING() to reduce firstname to 16 characters so its type can be altered to varchar(16).\n\n\n-- Convert the values in firstname to a max. of 16 characters\nALTER TABLE professors \nALTER COLUMN firstname \nTYPE varchar(16) \nUSING SUBSTRING(firstname FROM 1 FOR 16);\n\nPerfect! However, it’s best not to truncate any values in your database, so we’ll revert this column to varchar(64). Now it’s time to move on to the next set of attribute constraints!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#the-not-null-and-unique-constraints",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#the-not-null-and-unique-constraints",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.9 The not-null and unique constraints",
    "text": "2.9 The not-null and unique constraints\nTheory. Coming soon …\n1. The not-null and unique constraints\nIn the last part of this chapter, you’ll get to know two special attribute constraints: the not-null and unique constraints.\n2. The not-null constraint\nAs the name already says, the not-null constraint disallows any “NULL” values on a given column. This must hold true for the existing state of the database, but also for any future state. Therefore, you can only specify a not-null constraint on a column that doesn’t hold any “NULL” values yet. And: It won’t be possible to insert “NULL” values in the future.\n3. What does NULL mean?\nBefore I go on explaining how to specify not-null constraints, I want you to think about “NULL” values. What do they actually mean to you? There’s no clear definition. “NULL” can mean a couple of things, for example, that the value is unknown, or does not exist at all. It can also be possible that a value does not apply to the column. Let’s look into an example.\n4. What does NULL mean? An example\nLet’s say we define a table “students”. The first two columns for the social security number and the last name cannot be “NULL”, which makes sense: this should be known and apply to every student. The “home_phone” and “office_phone” columns though should allow for null values – which is the default, by the way. Why? First of all, these numbers can be unknown, for any reason, or simply not exist, because a student might not have a phone. Also, some values just don’t apply: Some students might not have an office, so they don’t have an office phone, and so forth. So, one important take away is that two “NULL” values must not have the same meaning. This also means that comparing “NULL” with “NULL” always results in a “FALSE” value.\n5. How to add or remove a not-null constraint\nYou’ve just seen how to add a not-null constraint to certain columns when creating a table. Just add “not null” after the respective columns. But you can also add and remove not-null constraints to and from existing tables. To add a not-null constraint to an existing table, you can use the “ALTER COLUMN SET NOT NULL” syntax as shown here. Similarly, to remove a not-null constraint, you can use “ALTER COLUMN DROP NOT NULL”.\n6. The unique constraint\nThe unique constraint on a column makes sure that there are no duplicates in a column. So any given value in a column can only exist once. This, for example, makes sense for university short names, as storing universities more than once leads to unnecessary redundancy. However, it doesn’t make sense for university cities, as two universities can co-exist in the same city. Just as with the not-null constraint, you can only add a unique constraint if the column doesn’t hold any duplicates before you apply it.\n7. Adding unique constraints\nHere’s how to create columns with unique constraints. Just add the “UNIQUE” keyword after the respective table column. You can also add a unique constraint to an existing table. For that, you have to use the “ADD CONSTRAINT” syntax. This is different from adding a “NOT NULL” constraint. However, it’s a pattern that frequently occurs. You’ll see plenty of other examples of “ADD CONSTRAINT” in the remainder of this course.\n8. Let’s apply this to the database!\nOkay, time now to apply the not-null and unique constraints to some of the columns of your current university affiliations database.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#disallow-null-values-with-set-not-null",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#disallow-null-values-with-set-not-null",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.10 Disallow NULL values with SET NOT NULL",
    "text": "2.10 Disallow NULL values with SET NOT NULL\nThe professors table is almost ready now. However, it still allows for NULLs to be entered. Although some information might be missing about some professors, there’s certainly columns that always need to be specified.\nSteps\n\nAdd a not-null constraint for the firstname column.\n\n\n-- Disallow NULL values in firstname\nALTER TABLE professors \nALTER COLUMN firstname SET NOT NULL;\n\n\nAdd a not-null constraint for the lastname column.\n\n\n-- Disallow NULL values in lastname\nALTER TABLE professors \nALTER COLUMN lastname SET NOT NULL;\n\nGood job – it is no longer possible to add professors which have either their first or last name set to NULL. Likewise, it is no longer possible to update an existing professor and setting their first or last name to NULL.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#what-happens-if-you-try-to-enter-nulls",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#what-happens-if-you-try-to-enter-nulls",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.11 What happens if you try to enter NULLs?",
    "text": "2.11 What happens if you try to enter NULLs?\nExecute the following statement:\n\nINSERT INTO professors (firstname, lastname, university_shortname)\nVALUES (NULL, 'Miller', 'ETH');\n\n#&gt; Error: Failed to fetch row : ERROR:  null value in column \"firstname\" of relation \"professors\" violates not-null constraint\n#&gt; DETAIL:  Failing row contains (null, Miller, ETH).\n\n\n\n2.12 Question\nWhy does this throw an error?  ⬜ Professors without first names do not exist. ✅ Because a database constraint is violated. ⬜ Error? This works just fine. ⬜ NULL is not put in quotes.\n\nCorrect! This statement violates one of the not-null constraints you’ve just specified.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#make-your-columns-unique-with-add-constraint",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#make-your-columns-unique-with-add-constraint",
    "title": "Introduction to Relational Databases in SQL",
    "section": "2.13 Make your columns UNIQUE with ADD CONSTRAINT",
    "text": "2.13 Make your columns UNIQUE with ADD CONSTRAINT\nAs seen in the video, you add the UNIQUE keyword after the column_name that should be unique. This, of course, only works for new tables:\n\nCREATE TABLE table_name (\n column_name UNIQUE\n);\n\nIf you want to add a unique constraint to an existing table, you do it like that:\n\nALTER TABLE table_name\nADD CONSTRAINT some_name UNIQUE(column_name);\n\nNote that this is different from the ALTER COLUMN syntax for the not-null constraint. Also, you have to give the constraint a name some_name.\nSteps\n\nAdd a unique constraint to the university_shortname column in universities. Give it the name university_shortname_unq.\n\n\n-- Make universities.university_shortname unique\nALTER TABLE universities\nADD CONSTRAINT university_shortname_unq UNIQUE(university_shortname);\n\n\nAdd a unique constraint to the organization column in organizations. Give it the name organization_unq.\n\n\n-- Make organizations.organization unique\nALTER TABLE organizations\nADD CONSTRAINT organization_unq UNIQUE(organization);\n\nPerfect. Making sure universities.university_shortname and organizations.organization only contain unique values is a prerequisite for turning them into so-called primary keys – the subject of the next chapter!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#keys-and-superkeys",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#keys-and-superkeys",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.1 Keys and superkeys",
    "text": "3.1 Keys and superkeys\nTheory. Coming soon …\n1. Keys and superkeys\nWelcome back! Let’s discuss key constraints. They are a very important concept in database systems, so we’ll spend a whole chapter on them.\n2. The current database model\nLet’s have a look at your current database model first. In the last chapter, you specified attribute constraints, first and foremost data types. You also set not-null and unique constraints on certain attributes. This didn’t actually change the structure of the model, so it still looks the same.\n3. The database model with primary keys\nBy the end of this chapter, the database will look slightly different. You’ll add so-called primary keys to three different tables. You’ll name them “id”. In the entity-relationship diagram, keys are denoted by underlined attribute names. Notice that you’ll add a whole new attribute to the “professors” table, and you’ll modify existing columns of the “organizations” and “universities” tables.\n4. What is a key?\nBefore we go into the nitty-gritty of what a primary key actually is, let’s look at keys in general. Typically a database table has an attribute, or a combination of multiple attributes, whose values are unique across the whole table. Such attributes identify a record uniquely. Normally, a table, as a whole, only contains unique records, meaning that the combination of all attributes is a key in itself. However, it’s not called a key, but a superkey, if attributes from that combination can be removed, and the attributes still uniquely identify records. If all possible attributes have been removed but the records are still uniquely identifiable by the remaining attributes, we speak of a minimal superkey. This is the actual key. So a key is always minimal. Let’s look at an example.\n5. An example\nHere’s an example that I found in a textbook on database systems. Obviously, the table shows six different cars, so the combination of all attributes is a superkey. If we remove the “year” attribute from the superkey, the six records are still unique, so it’s still a superkey. Actually, there are a lot of possible superkeys in this example.\n6. An example (contd.)\nHowever, there are only four minimal superkeys, and these are “license_no”, “serial_no”, and “model”, as well as the combination of “make” and “year”. Remember that superkeys are minimal if no attributes can be removed without losing the uniqueness property. This is trivial for K1 to 3, as they only consist of a single attribute. Also, if we remove “year” from K4, “make” would contain duplicates, and would, therefore, be no longer suited as key. These four minimal superkeys are also called candidate keys. Why candidate keys? In the end, there can only be one key for the table, which has to be chosen from the candidates. More on that in the next video.\n7. Let’s discover some keys!\nFirst I want you to have a look at some tables in your database and think about keys.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#get-to-know-select-count-distinct",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#get-to-know-select-count-distinct",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.2 Get to know SELECT COUNT DISTINCT",
    "text": "3.2 Get to know SELECT COUNT DISTINCT\nYour database doesn’t have any defined keys so far, and you don’t know which columns or combinations of columns are suited as keys.\nThere’s a simple way of finding out whether a certain column (or a combination) contains only unique values – and thus identifies the records in the table.\nYou already know the SELECT DISTINCT query from the first chapter. Now you just have to wrap everything within the COUNT() function and PostgreSQL will return the number of unique rows for the given columns:\n\nSELECT COUNT(DISTINCT(column_a, column_b, ...))\nFROM table;\n\nSteps\n\nFirst, find out the number of rows in universities.\n\n\n-- Count the number of rows in universities\nSELECT COUNT(*) \nFROM universities;\n\n\n1 records\n\n\ncount\n\n\n\n\n11\n\n\n\n\n\n\nThen, find out how many unique values there are in the university_city column.\n\n\n-- Count the number of distinct values in the university_city column\nSELECT COUNT(DISTINCT(university_city)) \nFROM universities;\n\n\n1 records\n\n\ncount\n\n\n\n\n9\n\n\n\n\n\nGreat! So, obviously, the university_city column wouldn\\’t lend itself as a key. Why? Because there are only 9 distinct values, but the table has 11 rows.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#identify-keys-with-select-count-distinct",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#identify-keys-with-select-count-distinct",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.3 Identify keys with SELECT COUNT DISTINCT",
    "text": "3.3 Identify keys with SELECT COUNT DISTINCT\nThere’s a very basic way of finding out what qualifies for a key in an existing, populated table:\n\nCount the distinct records for all possible combinations of columns. If the resulting number x equals the number of all rows in the table for a combination, you have discovered a superkey.\nThen remove one column after another until you can no longer remove columns without seeing the number x decrease. If that is the case, you have discovered a (candidate) key.\n\nThe table professors has 551 rows. It has only one possible candidate key, which is a combination of two attributes. You might want to try different combinations using the “Run code” button. Once you have found the solution, you can submit your answer.\nSteps\n\nUsing the above steps, identify the candidate key by trying out different combination of columns.\n\n\n-- Try out different combinations\nSELECT COUNT(DISTINCT(firstname, lastname)) \nFROM professors;\n\n\n1 records\n\n\ncount\n\n\n\n\n551\n\n\n\n\n\nIndeed, the only combination that uniquely identifies professors is {firstname, lastname}. {firstname, lastname, university_shortname} is a superkey, and all other combinations give duplicate values. Hopefully, the concept of superkeys and keys is now a bit more clear. Let\\’s move on to primary keys!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#primary-keys",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#primary-keys",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.4 Primary keys",
    "text": "3.4 Primary keys\nTheory. Coming soon …\n1. Primary keys\nOkay, now it’s time to look at an actual use case for superkeys, keys, and candidate keys.\n2. Primary keys\nPrimary keys are one of the most important concepts in database design. Almost every database table should have a primary key – chosen by you from the set of candidate keys. The main purpose, as already explained, is uniquely identifying records in a table. This makes it easier to reference these records from other tables, for instance – a concept you will go through in the next and last chapter. You might have already guessed it, but primary keys need to be defined on columns that don’t accept duplicate or null values. Lastly, primary key constraints are time-invariant, meaning that they must hold for the current data in the table – but also for any future data that the table might hold. It is therefore wise to choose columns where values will always be unique and not null.\n3. Specifying primary keys\nSo these two tables accept exactly the same data, however, the latter has an explicit primary key specified. As you can see, specifying primary keys upon table creation is very easy. Primary keys can also be specified like so: This notation is necessary if you want to designate more than one column as the primary key. Beware, that’s still only one primary key, it is just formed by the combination of two columns. Ideally, though, primary keys consist of as few columns as possible!\n4. Specifying primary keys (contd.)\nAdding primary key constraints to existing tables is the same procedure as adding unique constraints, which you might remember from the last chapter. As with unique constraints, you have to give the constraint a certain name.\n5. Your database\nIn the exercises that follow, you will add primary keys to the tables “universities” and “organizations”. You will add a special type of primary key, a so-called surrogate key, to the table “professors” in the last part of this chapter.\n6. Let’s practice!\nLet’s go add these keys.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#identify-the-primary-key",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#identify-the-primary-key",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.5 Identify the primary key",
    "text": "3.5 Identify the primary key\nHave a look at the example table from the previous video. As the database designer, you have to make a wise choice as to which column should be the primary key.\n\n     license_no     | serial_no |    make    |  model  | year\n--------------------+-----------+------------+---------+------\n Texas ABC-739      | A69352    | Ford       | Mustang |    2\n Florida TVP-347    | B43696    | Oldsmobile | Cutlass |    5\n New York MPO-22    | X83554    | Oldsmobile | Delta   |    1\n California 432-TFY | C43742    | Mercedes   | 190-D   |   99\n California RSK-629 | Y82935    | Toyota     | Camry   |    4\n Texas RSK-629      | U028365   | Jaguar     | XJS     |    4\n\n\n3.6 Question\nWhich of the following column or column combinations could best serve as primary key?  ⬜ PK = {make} ⬜ PK = {model, year} ✅ PK = {license_no} ⬜ PK = {year, make}\n\nCorrect! A primary key consisting solely of “license_no” is probably the wisest choice, as license numbers are certainly unique across all registered cars in a country.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-key-constraints-to-the-tables",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-key-constraints-to-the-tables",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.7 ADD key CONSTRAINTs to the tables",
    "text": "3.7 ADD key CONSTRAINTs to the tables\nTwo of the tables in your database already have well-suited candidate keys consisting of one column each: organizations and universities with the organization and university_shortname columns, respectively.\nIn this exercise, you’ll rename these columns to id using the RENAME COLUMN command and then specify primary key constraints for them. This is as straightforward as adding unique constraints (see the last exercise of Chapter 2):\n\nALTER TABLE table_name\nADD CONSTRAINT some_name PRIMARY KEY (column_name)\n\nNote that you can also specify more than one column in the brackets.\nSteps\n\nRename the organization column to id in organizations.\nMake id a primary key and name it organization_pk.\n\n\n-- Rename the organization column to id\nALTER TABLE organizations\nRENAME COLUMN organization TO id;\n\n\n-- Make id a primary key\nALTER TABLE organizations\nADD CONSTRAINT organization_pk PRIMARY KEY (id);\n\n\nRename the university_shortname column to id in universities.\nMake id a primary key and name it university_pk.\n\n\n-- Rename the university_shortname column to id\nALTER TABLE universities\nRENAME COLUMN university_shortname TO id;\n\n\n-- Make id a primary key\nALTER TABLE universities\nADD CONSTRAINT university_pk PRIMARY KEY (id);\n\nGood job! That was easy, wasn’t it? Let’s tackle the last table that needs a primary key right now: professors. However, things are going to be different this time, because you’ll add a so-called surrogate key.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#surrogate-keys",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#surrogate-keys",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.8 Surrogate keys",
    "text": "3.8 Surrogate keys\nTheory. Coming soon …\n1. Surrogate keys\nSurrogate keys are sort of an artificial primary key. In other words, they are not based on a native column in your data, but on a column that just exists for the sake of having a primary key. Why would you need that?\n2. Surrogate keys\nThere are several reasons for creating an artificial surrogate key. As mentioned before, a primary key is ideally constructed from as few columns as possible. Secondly, the primary key of a record should never change over time. If you define an artificial primary key, ideally consisting of a unique number or string, you can be sure that this number stays the same for each record. Other attributes might change, but the primary key always has the same value for a given record.\n3. An example\nLet’s look back at the example in the first video of this chapter. I altered it slightly and added the “color” column. In this table, the “license_no” column would be suited as the primary key – the license number is unlikely to change over time, not like the color column, for example, which might change if the car is repainted. So there’s no need for a surrogate key here. However, let’s say there were only these three attributes in the table. The only sensible primary key would be the combination of “make” and “model”, but that’s two columns for the primary key.\n4. Adding a surrogate key with serial data type\nYou could add a new surrogate key column, called “id”, to solve this problem. Actually, there’s a special data type in PostgreSQL that allows the addition of auto-incrementing numbers to an existing table: the “serial” type. It is specified just like any other data type. Once you add a column with the “serial” type, all the records in your table will be numbered. Whenever you add a new record to the table, it will automatically get a number that does not exist yet. There are similar data types in other database management systems, like MySQL.\n5. Adding a surrogate key with serial data type (contd.)\nAlso, if you try to specify an ID that already exists, the primary key constraint will prevent you from doing so. So, after all, the “id” column uniquely identifies each record in this table – which is very useful, for example, when you want to refer to these records from another table. But this will be the subject of the next chapter.\n6. Another type of surrogate key\nAnother strategy for creating a surrogate key is to combine two existing columns into a new one. In this example, we first add a new column with the “varchar” data type. We then “UPDATE” that column with the concatenation of two existing columns. The “CONCAT” function glues together the values of two or more existing columns. Lastly, we turn that new column into a surrogate primary key.\n7. Your database\nIn the exercises, you’ll add a surrogate key to the “professors” table, because the existing attributes are not really suited as primary key. Theoretically, there could be more than one professor with the same name working for one university, resulting in duplicates. With an auto-incrementing “id” column as the primary key, you make sure that each professor can be uniquely referred to. This was not necessary for organizations and universities, as their names can be assumed to be unique across these tables. In other words: It is unlikely that two organizations with the same name exist, solely for trademark reasons. The same goes for universities.\n8. Let’s try this!\nLet’s try this out before you move on to the last chapter.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-a-serial-surrogate-key",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-a-serial-surrogate-key",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.9 Add a SERIAL surrogate key",
    "text": "3.9 Add a SERIAL surrogate key\nSince there’s no single column candidate key in professors (only a composite key candidate consisting of firstname, lastname), you’ll add a new column id to that table.\nThis column has a special data type serial, which turns the column into an auto-incrementing number. This means that, whenever you add a new professor to the table, it will automatically get an id that does not exist yet in the table: a perfect primary key!\nSteps\n\nAdd a new column id with data type serial to the professors table.\n\n\n-- Add the new column to the table\nALTER TABLE professors \nADD COLUMN id serial;\n\n\nMake id a primary key and name it professors_pkey.\n\n\n-- Make id a primary key\nALTER TABLE professors \nADD CONSTRAINT professors_pkey PRIMARY KEY (id);\n\n\nWrite a query that returns all the columns and 10 rows from professors.\n\n\n-- Have a look at the first 10 rows of professors\nSELECT * \nFROM professors \nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nfirstname\nlastname\nuniversity_shortname\nid\n\n\n\n\nMichel\nRappaz\nEPF\n1\n\n\nHilal\nLashuel\nEPF\n2\n\n\nJeffrey\nHuang\nEPF\n3\n\n\nPierre\nMagistretti\nEPF\n4\n\n\nPaolo\nIenne\nEPF\n5\n\n\nFrédéric\nKaplan\nEPF\n6\n\n\nOlivier\nHari\nUNE\n7\n\n\nChristian\nHesse\nUBE\n8\n\n\nMajed\nChergui\nEPF\n9\n\n\nDouglas\nHanahan\nEPF\n10\n\n\n\n\n\nWell done. As you can see, PostgreSQL has automatically numbered the rows with the id column, which now functions as a (surrogate) primary key – it uniquely identifies professors.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#concatenate-columns-to-a-surrogate-key",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#concatenate-columns-to-a-surrogate-key",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.10 CONCATenate columns to a surrogate key",
    "text": "3.10 CONCATenate columns to a surrogate key\nAnother strategy to add a surrogate key to an existing table is to concatenate existing columns with the CONCAT() function.\nLet’s think of the following example table:\n\nCREATE TABLE cars (\n make varchar(64) NOT NULL,\n model varchar(64) NOT NULL,\n mpg integer NOT NULL\n)\n\nThe table is populated with 10 rows of completely fictional data.\nUnfortunately, the table doesn’t have a primary key yet. None of the columns consists of only unique values, so some columns can be combined to form a key.\nIn the course of the following exercises, you will combine make and model into such a surrogate key.\nSteps\n\nCount the number of distinct rows with a combination of the make and model columns.\n\n\n-- Count the number of distinct rows with columns make, model\nSELECT COUNT(DISTINCT(make, model)) \nFROM cars;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\nAdd a new column id with the data type varchar(128).\n\n\n-- Count the number of distinct rows with columns make, model\nSELECT COUNT(DISTINCT(make, model)) \nFROM cars;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n-- Add the id column\nALTER TABLE cars\nADD COLUMN id varchar(128);\n\n\nConcatenate make and model into id using an UPDATE table_name SET column_name = ... query and the CONCAT() function.\n\n\n-- Update id with make + model\nUPDATE cars\nSET id = CONCAT(make, model);\n\n\nMake id a primary key and name it id_pk.\n\n\n-- Make id a primary key\nALTER TABLE cars\nADD CONSTRAINT id_pk PRIMARY KEY(id);\n\n\n-- Have a look at the table\nSELECT * FROM cars;\n\n\nDisplaying records 1 - 10\n\n\nmake\nmodel\nmpg\nid\n\n\n\n\nSubaru\nForester\n24\nSubaruForester\n\n\nOpel\nAstra\n45\nOpelAstra\n\n\nOpel\nVectra\n40\nOpelVectra\n\n\nFord\nAvenger\n30\nFordAvenger\n\n\nFord\nGalaxy\n30\nFordGalaxy\n\n\nToyota\nPrius\n50\nToyotaPrius\n\n\nToyota\nSpeedster\n30\nToyotaSpeedster\n\n\nToyota\nGalaxy\n20\nToyotaGalaxy\n\n\nMitsubishi\nForester\n10\nMitsubishiForester\n\n\nMitsubishi\nGalaxy\n30\nMitsubishiGalaxy\n\n\n\n\n\nGood job! These were quite some steps, but you’ve managed! Let’s look into another method of adding a surrogate key now.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#test-your-knowledge-before-advancing",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#test-your-knowledge-before-advancing",
    "title": "Introduction to Relational Databases in SQL",
    "section": "3.11 Test your knowledge before advancing",
    "text": "3.11 Test your knowledge before advancing\nBefore you move on to the next chapter, let’s quickly review what you’ve learned so far about attributes and key constraints. If you’re unsure about the answer, please quickly review chapters 2 and 3, respectively.\nLet’s think of an entity type “student”. A student has:\n\na last name consisting of up to 128 characters (required),\na unique social security number, consisting only of integers, that should serve as a key,\na phone number of fixed length 12, consisting of numbers and characters (but some students don’t have one). Steps\n\n\nGiven the above description of a student entity, create a table students with the correct column types.\nAdd a PRIMARY KEY for the social security number ssn.\n\nNote that there is no formal length requirement for the integer column. The application would have to make sure it’s a correct SSN!\n\n-- Create the table\nCREATE TABLE students (\n  last_name varchar(128) NOT NULL,\n  ssn integer PRIMARY KEY,\n  phone_no char(12)\n);\n\nGreat! Looks like you are ready for the last chapter of this course, where you\\’ll connect tables in your database.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#model-1n-relationships-with-foreign-keys",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#model-1n-relationships-with-foreign-keys",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.1 Model 1:N relationships with foreign keys",
    "text": "4.1 Model 1:N relationships with foreign keys\nTheory. Coming soon …\n1. Model 1:N relationships with foreign keys\nWelcome to the last chapter of this course. Now it’s time to make use of key constraints.\n2. The current database model\nHere’s your current database model. The three entity types “professors”, “organizations”, and “universities” all have primary keys – but “affiliations” doesn’t, for a specific reason that will be revealed in this chapter.\n3. The next database model\nNext up, you’ll model a so-called relationship type between “professors” and “universities”. As you know, in your database, each professor works for a university. In the ER diagram, this is drawn with a rhombus. The small numbers specify the cardinality of the relationship: a professor works for at most one university, while a university can have any number of professors working for it – even zero.\n4. Implementing relationships with foreign keys\nSuch relationships are implemented with foreign keys. Foreign keys are designated columns that point to a primary key of another table. There are some restrictions for foreign keys. First, the domain and the data type must be the same as one of the primary key. Secondly, only foreign key values are allowed that exist as values in the primary key of the referenced table. This is the actual foreign key constraint, also called “referential integrity”. You’ll dig into referential integrity at the end of this chapter. Lastly, a foreign key is not necessarily an actual key, because duplicates and “NULL” values are allowed. Let’s have a look at your database.\n5. A query\nAs you can see, the column “university_shortname” of “professors” has the same domain as the “id” column of the “universities” table. If you go through each record of “professors”, you can always find the respective “id” in the “universities” table. So both criteria for a foreign key in the table “professors” referencing “universities” are fulfilled. Also, you see that “university_shortname” is not really a key because there are duplicates. For example, the id “EPF” and “UBE” occur three times each.\n6. Specifying foreign keys\nWhen you create a new table, you can specify a foreign key similarly to a primary key. Let’s look at two example tables. First, we create a “manufacturers” table with a primary key called “name”. Then we create a table “cars”, that also has a primary key, called “model”. As each car is produced by a certain manufacturer, it makes sense to also add a foreign key to this table. We do that by writing the “REFERENCES” keyword, followed by the referenced table and its primary key in brackets. From now on, only cars with valid and existing manufacturers may be entered into that table. Trying to enter models with manufacturers that are not yet stored in the “manufacturers” table won’t be possible, thanks to the foreign key constraint.\n7. Specifying foreign keys to existing tables\nAgain, the syntax for adding foreign keys to existing tables is the same as the one for adding primary keys and unique constraints.\n8. Let’s implement this!\nOkay, let’s have a look at your database and implement a simple relationship between “professors” and “universities”.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#reference-a-table-with-a-foreign-key",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#reference-a-table-with-a-foreign-key",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.2 REFERENCE a table with a FOREIGN KEY",
    "text": "4.2 REFERENCE a table with a FOREIGN KEY\nIn your database, you want the professors table to reference the universities table. You can do that by specifying a column in professors table that references a column in the universities table.\nAs just shown in the video, the syntax for that looks like this:\n\nALTER TABLE a \nADD CONSTRAINT a_fkey FOREIGN KEY (b_id) REFERENCES b (id);\n\nTable a should now refer to table b, via b_id, which points to id. a_fkey is, as usual, a constraint name you can choose on your own.\nPay attention to the naming convention employed here: Usually, a foreign key referencing another primary key with name id is named x_id, where x is the name of the referencing table in the singular form.\nSteps\n\nRename the university_shortname column to university_id in professors.\n\n\n-- Rename the university_shortname column\nALTER TABLE professors\nRENAME COLUMN university_shortname TO university_id;\n\n\nAdd a foreign key on university_id column in professors that references the id column in universities.\nName this foreign key professors_fkey.\n\n\n-- Add a foreign key on professors referencing universities\nALTER TABLE professors \nADD CONSTRAINT professors_fkey FOREIGN KEY (university_id) REFERENCES universities (id);\n\nPerfect! Now, the professors table has a link to the universities table. Each professor belongs to exactly one university.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#explore-foreign-key-constraints",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#explore-foreign-key-constraints",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.3 Explore foreign key constraints",
    "text": "4.3 Explore foreign key constraints\nForeign key constraints help you to keep order in your database mini-world. In your database, for instance, only professors belonging to Swiss universities should be allowed, as only Swiss universities are part of the universities table.\nThe foreign key on professors referencing universities you just created thus makes sure that only existing universities can be specified when inserting new data. Let’s test this!\nSteps\n\nRun the sample code and have a look at the error message.\nWhat’s wrong? Correct the university_id so that it actually reflects where Albert Einstein wrote his dissertation and became a professor – at the University of Zurich (UZH)!\n\n\n-- Try to insert a new professor\nINSERT INTO professors (firstname, lastname, university_id)\nVALUES ('Albert', 'Einstein', 'MIT');\n\n#&gt; Error: Failed to fetch row : ERROR:  insert or update on table \"professors\" violates foreign key constraint \"professors_fkey\"\n#&gt; DETAIL:  Key (university_id)=(MIT) is not present in table \"universities\".\n\n\n\n-- Try to insert a new professor\nINSERT INTO professors (firstname, lastname, university_id)\nVALUES ('Albert', 'Einstein', 'UZH');\n\nGreat! As you can see, inserting a professor with non-existing university IDs violates the foreign key constraint you\\’ve just added. This also makes sure that all universities are spelled equally – adding to data consistency.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#join-tables-linked-by-a-foreign-key",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#join-tables-linked-by-a-foreign-key",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.4 JOIN tables linked by a foreign key",
    "text": "4.4 JOIN tables linked by a foreign key\nLet’s join these two tables to analyze the data further!\nYou might already know how SQL joins work from the Intro to SQL for Data Science course (last exercise) or from Joining Data in PostgreSQL.\nHere’s a quick recap on how joins generally work:\n\nSELECT ...\nFROM table_a\nJOIN table_b\nON ...\nWHERE ...\n\nWhile foreign keys and primary keys are not strictly necessary for join queries, they greatly help by telling you what to expect. For instance, you can be sure that records referenced from table A will always be present in table B – so a join from table A will always find something in table B. If not, the foreign key constraint would be violated.\nSteps\n\nJOIN professors with universities on professors.university_id = universities.id, i.e., retain all records where the foreign key of professors is equal to the primary key of universities.\nFilter for university_city = 'Zurich'.\n\n\n-- Select all professors working for universities in the city of Zurich\nSELECT professors.lastname, universities.id, universities.university_city\nFROM professors\nJOIN universities\nON professors.university_id = universities.id\nWHERE universities.university_city = 'Zurich';\n\n\nDisplaying records 1 - 10\n\n\nlastname\nid\nuniversity_city\n\n\n\n\nGrote\nETH\nZurich\n\n\nCarreira\nETH\nZurich\n\n\nWennemers\nETH\nZurich\n\n\nHafen\nETH\nZurich\n\n\nSaar\nETH\nZurich\n\n\nLoeliger\nETH\nZurich\n\n\nSchönsleben\nETH\nZurich\n\n\nBühler\nUZH\nZurich\n\n\nHellweg\nETH\nZurich\n\n\nGugerli\nUZH\nZurich\n\n\n\n\n\nThat’s a long query! First, the university belonging to each professor was attached with the JOIN operation. Then, only professors having \\“Zurich\\”” as university city were retained with the WHERE clause.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#model-more-complex-relationships",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#model-more-complex-relationships",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.5 Model more complex relationships",
    "text": "4.5 Model more complex relationships\nTheory. Coming soon …\n1. Model more complex relationships\nIn the last few exercises, you made your first steps in modeling and implementing 1:N-relationships. Now it’s time to look at more complex relationships between tables.\n2. The current database model\nSo you’ve added a 1:N-relationship between professors and universities. Such relationships have to be implemented with one foreign key in the table that has at most one foreign entity associated. In this case, that’s the “professors” table, as professors cannot have more than one university associated. Now, what about affiliations? We know that a professor can have more than one affiliation with organizations, for instance, as a chairman of a bank and as a president of a golf club. On the other hand, organizations can also have more than one professor connected to them. Let’s look at the entity-relationship diagram that models this.\n3. The final database model\nThere are a couple of things that are new. First of all, a new relationship between organizations and professors was added. This is an N:M relationship, not an 1:N relationship as with professors and universities. This depicts the fact that a professor can be affiliated with more than one organization and vice versa. Also, it has an own attribute, the function. Remember that each affiliation comes with a function, for instance, “chairman”. The second thing you’ll notice is that the affiliations entity type disappeared altogether. For clarity, I still included it in the diagram, but it’s no longer needed. However, you’ll still have four tables: Three for the entities “professors”, “universities” and “organizations”, and one for the N:M-relationship between “professors” and “organizations”.\n4. How to implement N:M-relationships\nSuch a relationship is implemented with an ordinary database table that contains two foreign keys that point to both connected entities. In this case, that’s a foreign key pointing to the “professors.id” column, and one pointing to the “organizations.id” column. Also, additional attributes, in this case “function”, need to be included. If you were to create that relationship table from scratch, you would define it as shown. Note that “professor_id” is stored as “integer”, as the primary key it refers to has the type “serial”, which is also an integer. On the other hand, “organization_id” has “varchar(256)” as type, conforming to the primary key in the “organizations” table. One last thing: Notice that no primary key is defined here because a professor can theoretically have multiple functions in one organization. One could define the combination of all three attributes as the primary key in order to have some form of unique constraint in that table, but that would be a bit over the top.\n5. Time to implement this!\nSince you already have a pre-populated affiliations table, things are not going to be so straightforward. You’ll need to link and migrate the data to a new table to implement this relationship. This will be the goal of the following exercises.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-foreign-keys-to-the-affiliations-table",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#add-foreign-keys-to-the-affiliations-table",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.6 Add foreign keys to the “affiliations” table",
    "text": "4.6 Add foreign keys to the “affiliations” table\nAt the moment, the affiliations table has the structure {firstname, lastname, function, organization}, as you can see in the preview at the bottom right. In the next three exercises, you’re going to turn this table into the form {professor_id, organization_id, function}, with professor_id and organization_id being foreign keys that point to the respective tables.\nYou’re going to transform the affiliations table in-place, i.e., without creating a temporary table to cache your intermediate results.\nSteps\n\nAdd a professor_id column with integer data type to affiliations, and declare it to be a foreign key that references the id column in professors.\n\n\n-- Add a professor_id column\nALTER TABLE affiliations\nADD COLUMN professor_id integer REFERENCES professors (id);\n\n\nRename the organization column in affiliations to organization_id.\n\n\n-- Rename the organization column to organization_id\nALTER TABLE affiliations\nRENAME organization TO organization_id;\n\n\nAdd a foreign key constraint on organization_id so that it references the id column in organizations.\n\n\n-- Add a foreign key on organization_id\nALTER TABLE affiliations\nADD CONSTRAINT affiliations_organization_fkey FOREIGN KEY (organization_id) REFERENCES organizations (id);\n\nPerfect! Making organization_id a foreign key worked flawlessly because these organizations actually exist in the organizations table. That was only the first part, though. Now it\\’s time to update professor_id in affiliations – so that it correctly refers to the corresponding professors.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#populate-the-professor_id-column",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#populate-the-professor_id-column",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.7 Populate the “professor_id” column",
    "text": "4.7 Populate the “professor_id” column\nNow it’s time to also populate professors_id. You’ll take the ID directly from professors.\nHere’s a way to update columns of a table based on values in another table:\n\nUPDATE table_a\nSET column_to_update = table_b.column_to_update_from\nFROM table_b\nWHERE condition1 AND condition2 AND ...;\n\nThis query does the following:\n\nFor each row in table_a, find the corresponding row in table_b where condition1, condition2, etc., are met.\nSet the value of column_to_update to the value of column_to_update_from (from that corresponding row).\n\nThe conditions usually compare other columns of both tables, e.g. table_a.some_column = table_b.some_column. Of course, this query only makes sense if there is only one matching row in table_b.\nSteps\n\nFirst, have a look at the current state of affiliations by fetching 10 rows and all columns.\n\n\n-- Have a look at the 10 first rows of affiliations\nSELECT * \nFROM affiliations \nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nfirstname\nlastname\nfunction\norganization_id\nprofessor_id\n\n\n\n\nDimos\nPoulikakos\nVR-Mandat\nScrona AG\nNA\n\n\nFrancesco\nStellacci\nCo-editor in Chief, Nanoscale\nRoyal Chemistry Society, UK\nNA\n\n\nAlexander\nFust\nFachexperte und Coach für Designer Startups\nCreative Hub\nNA\n\n\nJürgen\nBrugger\nProposal reviewing HEPIA\nHES Campus Biotech, Genève\nNA\n\n\nHervé\nBourlard\nDirector\nIdiap Research Institute\nNA\n\n\nIoannis\nPapadopoulos\nMandat\nSchweizerischer Nationalfonds (SNF)\nNA\n\n\nOlaf\nBlanke\nProfesseur à 20%\nUniversité de Genève\nNA\n\n\nLeo\nStaub\nPräsident Verwaltungsrat\nGenossenschaft Migros Ostschweiz\nNA\n\n\nPascal\nPichonnaz\nVize-Präsident\nEKK (Eidgenössische Konsumenten Kommission)\nNA\n\n\nDietmar\nGrichnik\nPräsident\nSwiss Startup Monitor Stiftung\nNA\n\n\n\n\n\n\nUpdate the professor_id column with the corresponding value of the id column in professors.\n\n“Corresponding” means rows in professors where the firstname and lastname are identical to the ones in affiliations.\n\n-- Update professor_id to professors.id where firstname, lastname correspond to rows in professors\nUPDATE affiliations\nSET professor_id = professors.id\nFROM professors\nWHERE affiliations.firstname = professors.firstname AND affiliations.lastname = professors.lastname;\n\n\nCheck out the first 10 rows and all columns of affiliations again. Have the professor_ids been correctly matched?\n\n\n-- Have a look at the 10 first rows of affiliations again\nSELECT * \nFROM affiliations \nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nfirstname\nlastname\nfunction\norganization_id\nprofessor_id\n\n\n\n\nFrançoise Gisou van der\nGoot Grunberg\nConseil de fondation et Conseil Scientifique de la Fondation Jeantet\nFondation Jeantet, Genève\nNA\n\n\nWalter Hans Jakob\nKaufmann\nVR-Mandat\ndsp Ingenieure & Planer AG\nNA\n\n\nJérôme Jean-Constant\nFaist\nVR-Mandat\nAlpes Lasers S.A.\nNA\n\n\nJohan Olof Anders\nRobertsson\nGeschäftsführer\nRobertsson Industrial Geoscience Innovation GmbH\nNA\n\n\nFrançoise Gisou van der\nGoot Grunberg\nEMBO YIP (Young Investigator Program) committee\nEMBO, Heidelberg\nNA\n\n\nAlain\nWegmann\nConseil informatique\nTAG Aviation, Genève\n462\n\n\nFrançoise Gisou van der\nGoot Grunberg\nConseil Suisse de la Science et de l’Innovation\nCSSI Bern\nNA\n\n\nFrançoise Gisou van der\nGoot Grunberg\nDiverses commissions du SNF\nSNF Bern\nNA\n\n\nFrançoise Gisou van der\nGoot Grunberg\nScientific Advisory Board Clinical Research Department\nUniv Bern\nNA\n\n\nJérôme Jean-Constant\nFaist\nNA\nCentre Suisse d’Electronique et de Microtechnique (CSEM)\nNA\n\n\n\n\n\nWow, that was a thing! As you can see, the correct professors.id has been inserted into professor_id for each record, thanks to the matching firstname and lastname in both tables.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#drop-firstname-and-lastname",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#drop-firstname-and-lastname",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.8 Drop “firstname” and “lastname”",
    "text": "4.8 Drop “firstname” and “lastname”\nThe firstname and lastname columns of affiliations were used to establish a link to the professors table in the last exercise – so the appropriate professor IDs could be copied over. This only worked because there is exactly one corresponding professor for each row in affiliations. In other words: {firstname, lastname} is a candidate key of professors – a unique combination of columns.\nIt isn’t one in affiliations though, because, as said in the video, professors can have more than one affiliation.\nBecause professors are referenced by professor_id now, the firstname and lastname columns are no longer needed, so it’s time to drop them. After all, one of the goals of a database is to reduce redundancy where possible.\nSteps\n\nDrop the firstname and lastname columns from the affiliations table.\n\n\n-- Drop the firstname column\nALTER TABLE affiliations\nDROP COLUMN firstname;\n\n\n-- Drop the lastname column\nALTER TABLE affiliations\nDROP COLUMN lastname;\n\nGood job! Now the affiliations table that models the N:M-relationship between professors and organizations is finally complete.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#referential-integrity",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#referential-integrity",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.9 Referential integrity",
    "text": "4.9 Referential integrity\nTheory. Coming soon …\n1. Referential integrity\nWe’ll now talk about one of the most important concepts in database systems: referential integrity. It’s a very simple concept…\n2. Referential integrity\n…that states that a record referencing another record in another table must always refer to an existing record. In other words: A record in table A cannot point to a record in table B that does not exist. Referential integrity is a constraint that always concerns two tables, and is enforced through foreign keys, as you’ve seen in the previous lessons of this chapter. So if you define a foreign key in the table “professors” referencing the table “universities”, referential integrity is held from “professors” to “universities”.\n3. Referential integrity violations\nReferential integrity can be violated in two ways. Let’s say table A references table B. So if a record in table B that is already referenced from table A is deleted, you have a violation. On the other hand, if you try to insert a record in table A that refers to something that does not exist in table B, you also violate the principle. And that’s the main reason for foreign keys – they will throw errors and stop you from accidentally doing these things.\n4. Dealing with violations\nHowever, throwing an error is not the only option. If you specify a foreign key on a column, you can actually tell the database system what should happen if an entry in the referenced table is deleted. By default, the “ON DELETE NO ACTION” keyword is automatically appended to a foreign key definition, like in the example here. This means that if you try to delete a record in table B which is referenced from table A, the system will throw an error. However, there are other options. For example, there’s the “CASCADE” option, which will first allow the deletion of the record in table B, and then will automatically delete all referencing records in table A. So that deletion is cascaded.\n5. Dealing with violations, contd.\nThere are even more options. The “RESTRICT” option is almost identical to the “NO ACTION” option. The differences are technical and beyond the scope of this course. More interesting is the “SET NULL” option. It will set the value of the foreign key for this record to “NULL”. The “SET DEFAULT” option only works if you have specified a default value for a column. It automatically changes the referencing column to a certain default value if the referenced record is deleted. Setting default values is also beyond the scope of this course, but this option is still good to know.\n6. Let’s look at some examples!\nLet’s practice this a bit and change the referential integrity behavior of your database.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#referential-integrity-violations",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#referential-integrity-violations",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.10 Referential integrity violations",
    "text": "4.10 Referential integrity violations\n\nDELETE FROM universities WHERE id = 'EPF';\n\n#&gt; Error: Failed to fetch row : ERROR:  update or delete on table \"universities\" violates foreign key constraint \"professors_fkey\" on table \"professors\"\n#&gt; DETAIL:  Key (id)=(EPF) is still referenced from table \"professors\".\n\n\n\n4.11 Question\nGiven the current state of your database, what happens if you execute the following SQL statement?  ⬜ It throws an error because the university with ID “EPF” does not exist. ⬜ The university with ID “EPF” is deleted. ⬜ It fails because referential integrity from universities to professors is violated. ✅ It fails because referential integrity from professors to universities is violated.\n\nCorrect! You defined a foreign key on professors.university_id that references universities.id, so referential integrity is said to hold from professors to universities.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#change-the-referential-integrity-behavior-of-a-key",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#change-the-referential-integrity-behavior-of-a-key",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.12 Change the referential integrity behavior of a key",
    "text": "4.12 Change the referential integrity behavior of a key\nSo far, you implemented three foreign key constraints:\n\nprofessors.university_id to universities.id\naffiliations.organization_id to organizations.id\naffiliations.professor_id to professors.id\n\nThese foreign keys currently have the behavior ON DELETE NO ACTION. Here, you’re going to change that behavior for the column referencing organizations from affiliations. If an organization is deleted, all its affiliations (by any professor) should also be deleted.\nAltering a key constraint doesn’t work with ALTER COLUMN. Instead, you have to DROP the key constraint and then ADD a new one with a different ON DELETE behavior.\nFor deleting constraints, though, you need to know their name. This information is also stored in information_schema.\nSteps\n\nHave a look at the existing foreign key constraints by querying table_constraints in information_schema.\n\n\n-- Identify the correct constraint name\nSELECT constraint_name, table_name, constraint_type\nFROM information_schema.table_constraints\nWHERE constraint_type = 'FOREIGN KEY';\n\n\n3 records\n\n\nconstraint_name\ntable_name\nconstraint_type\n\n\n\n\nprofessors_fkey\nprofessors\nFOREIGN KEY\n\n\naffiliations_professor_id_fkey\naffiliations\nFOREIGN KEY\n\n\naffiliations_organization_fkey\naffiliations\nFOREIGN KEY\n\n\n\n\n\n\nDelete the affiliations_organization_fkey foreign key constraint in affiliations.\n\n\n-- Drop the right foreign key constraint\nALTER TABLE affiliations\nDROP CONSTRAINT affiliations_organization_fkey;\n\n\nAdd a new foreign key to affiliations that CASCADEs deletion if a referenced record is deleted from organizations. Name it affiliations_organization_id_fkey.\n\n\n-- Add a new foreign key constraint from affiliations to organizations which cascades deletion\nALTER TABLE affiliations\nADD CONSTRAINT affiliations_organization_id_fkey FOREIGN KEY (organization_id) REFERENCES organizations (id) ON DELETE CASCADE;\n\n\nRun the DELETE and SELECT queries to double check that the deletion cascade actually works.\n\n\n-- Delete an organization \nDELETE FROM organizations \nWHERE id = 'CUREM';\n\n\n-- Check that no more affiliations with this organization exist\nSELECT * FROM affiliations\nWHERE organization_id = 'CUREM';\n\n\n0 records\n\n\nfunction\norganization_id\nprofessor_id\n\n\n\n\n\n\n\nGood job. As you can see, whenever an organization referenced by an affiliation is deleted, the affiliations also gets deleted. It is your job as database designer to judge whether this is a sensible configuration. Sometimes, setting values to NULL or to restrict deletion altogether might make more sense!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#roundup",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#roundup",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.13 Roundup",
    "text": "4.13 Roundup\nTheory. Coming soon …\n1. Roundup\nCongratulations, you’re almost done. Let’s quickly revise what you’ve done throughout this course.\n2. How you’ve transformed the database\nYou started with a simple table with a lot of redundancy. You might be used to such tables when working with flat files like CSVs or Excel files. Throughout the course, you transformed it step by step into the database schema on the right – only by executing SQL queries. You’ve defined column data types, added primary keys and foreign keys, and through that, specified relationships between tables. All these measures will guarantee better data consistency and therefore quality. This is especially helpful if you add new data to the database but also makes analyzing the data easier.\n3. The database ecosystem\nTo go further from here, it’s useful to look at the bigger picture for a minute. In this course, you’ve transformed a database. You did that with PostgreSQL, which is also called a “Database Management System”, or DBMS. The DBMS and one or more databases together form the “Database System”. The DBMS exposes a query interface where you can run ad-hoc analyses and queries with SQL. However, you can also access this interface through other client applications. You could, for example, program a Python script that connects to the database, loads data from it, and visualizes it.\n4. The database ecosystem\nIn the remainder of this course, you’ll no longer manipulate data in your database system, but employ some analysis queries on your database. This will be a quick repetition of what you’ve learned in previous SQL courses such as “Intro to SQL for Data Science”, but it will also demonstrate the advantages of having a database instead of a flat file in the first place.\n5. Thank you!\nFor me it’s time to say goodbye, thank you for taking this course, and I hope you will soon build your first relational database with the knowledge you’ve gained here.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#count-affiliations-per-university",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#count-affiliations-per-university",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.14 Count affiliations per university",
    "text": "4.14 Count affiliations per university\nNow that your data is ready for analysis, let’s run some exemplary SQL queries on the database. You’ll now use already known concepts such as grouping by columns and joining tables.\nIn this exercise, you will find out which university has the most affiliations (through its professors). For that, you need both affiliations and professors tables, as the latter also holds the university_id.\nAs a quick repetition, remember that joins have the following structure:\n\nSELECT table_a.column1, table_a.column2, table_b.column1, ... \nFROM table_a\nJOIN table_b \nON table_a.column = table_b.column\n\nThis results in a combination of table_a and table_b, but only with rows where table_a.column is equal to table_b.column.\nSteps\n\nCount the number of total affiliations by university.\nSort the result by that count, in descending order.\n\n\n-- Count the total number of affiliations per university\nSELECT COUNT(*), professors.university_id \nFROM affiliations\nJOIN professors\nON affiliations.professor_id = professors.id\n-- Group by the university ids of professors\nGROUP BY professors.university_id \nORDER BY count DESC;\n\n\nDisplaying records 1 - 10\n\n\ncount\nuniversity_id\n\n\n\n\n572\nEPF\n\n\n272\nUSG\n\n\n162\nUBE\n\n\n128\nETH\n\n\n75\nUBA\n\n\n40\nUFR\n\n\n36\nUNE\n\n\n35\nULA\n\n\n33\nUGE\n\n\n7\nUZH\n\n\n\n\n\nVery good. As you can see, the count of affiliations is completely different from university to university.",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#join-all-the-tables-together",
    "href": "content/sql/introduction_to_relational_databases_in_sql/introduction_to_relational_databases_in_sql.html#join-all-the-tables-together",
    "title": "Introduction to Relational Databases in SQL",
    "section": "4.15 Join all the tables together",
    "text": "4.15 Join all the tables together\nIn this last exercise, your task is to find the university city of the professor with the most affiliations in the sector “Media & communication”.\nFor this,\n\nyou need to join all the tables,\ngroup by some column,\nand then use selection criteria to get only the rows in the correct sector. Let’s do this in three steps!\n\nSteps\n\nJoin all tables in the database (starting with affiliations, professors, organizations, and universities) and look at the result.\n\n\n-- Join all tables\nSELECT *\nFROM affiliations\nJOIN professors\nON affiliations.professor_id = professors.id\nJOIN organizations\nON affiliations.organization_id = organizations.id\nJOIN universities\nON professors.university_id = universities.id;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction\norganization_id\nprofessor_id\nfirstname\nlastname\nuniversity_id\nid\nid..8\norganization_sector\nid..10\nuniversity\nuniversity_city\n\n\n\n\nConseil informatique\nTAG Aviation, Genève\n462\nAlain\nWegmann\nEPF\n462\nTAG Aviation, Genève\nEnergy, environment & mobility\nEPF\nETH Lausanne\nLausanne\n\n\nNA\nWHO, Geneva\n534\nMatthias\nEgger\nUBE\n534\nWHO, Geneva\nPharma & health\nUBE\nUni Bern\nBern\n\n\nNA\nSecuritas AG\n242\nArtur\nBaldauf\nUBE\n242\nSecuritas AG\nPolitics, administration, justice system & security sector\nUBE\nUni Bern\nBern\n\n\nNA\nFoundation of Talents\n407\nAchim\nConzelmann\nUBE\n407\nFoundation of Talents\nSociety, Social, Culture & Sports\nUBE\nUni Bern\nBern\n\n\nNA\nPH Bern, Schulrat\n407\nAchim\nConzelmann\nUBE\n407\nPH Bern, Schulrat\nEducation & research\nUBE\nUni Bern\nBern\n\n\nNA\nBüro Vatter AG\n386\nAdrian\nVatter\nUBE\n386\nBüro Vatter AG\nConsulting, public relations, legal & trust\nUBE\nUni Bern\nBern\n\n\nNA\nCurt Rommel Stiftung\n344\nAdriano\nMarantelli\nUBE\n344\nCurt Rommel Stiftung\nEducation & research\nUBE\nUni Bern\nBern\n\n\nNA\nVereinsvorstand BEWEST (Weiterbildungsverein)\n344\nAdriano\nMarantelli\nUBE\n344\nVereinsvorstand BEWEST (Weiterbildungsverein)\nEducation & research\nUBE\nUni Bern\nBern\n\n\nNA\nStiftung Archiv für schweizerisches Abgaberecht\n344\nAdriano\nMarantelli\nUBE\n344\nStiftung Archiv für schweizerisches Abgaberecht\nSociety, Social, Culture & Sports\nUBE\nUni Bern\nBern\n\n\nConseil informatique (non-payé)\nAeroport Genève\n462\nAlain\nWegmann\nEPF\n462\nAeroport Genève\nEnergy, environment & mobility\nEPF\nETH Lausanne\nLausanne\n\n\n\n\n\n\nNow group the result by organization sector, professor, and university city.\nCount the resulting number of rows.\n\n\n-- Group the table by organization sector, professor ID and university city\nSELECT COUNT(*), organizations.organization_sector, \nprofessors.id, universities.university_city\nFROM affiliations\nJOIN professors\nON affiliations.professor_id = professors.id\nJOIN organizations\nON affiliations.organization_id = organizations.id\nJOIN universities\nON professors.university_id = universities.id\nGROUP BY organizations.organization_sector, \nprofessors.id, universities.university_city;\n\n\nDisplaying records 1 - 10\n\n\ncount\norganization_sector\nid\nuniversity_city\n\n\n\n\n1\nTechnology\n135\nZurich\n\n\n5\nEducation & research\n113\nBasel\n\n\n1\nTechnology\n363\nLausanne\n\n\n1\nIndustry, construction & agriculture\n39\nZurich\n\n\n2\nSociety, Social, Culture & Sports\n182\nZurich\n\n\n1\nPharma & health\n377\nBern\n\n\n1\nMedia & communication\n50\nLausanne\n\n\n4\nEnergy, environment & mobility\n320\nSaint Gallen\n\n\n1\nEducation & research\n327\nFribourg\n\n\n1\nEnergy, environment & mobility\n194\nLausanne\n\n\n\n\n\n\nOnly retain rows with “Media & communication” as organization sector, and sort the table by count, in descending order.\n\n\n-- Filter the table and sort it\nSELECT COUNT(*), organizations.organization_sector, \nprofessors.id, universities.university_city\nFROM affiliations\nJOIN professors\nON affiliations.professor_id = professors.id\nJOIN organizations\nON affiliations.organization_id = organizations.id\nJOIN universities\nON professors.university_id = universities.id\nWHERE organizations.organization_sector = 'Media & communication'\nGROUP BY organizations.organization_sector, \nprofessors.id, universities.university_city\nORDER BY count DESC;\n\n\nDisplaying records 1 - 10\n\n\ncount\norganization_sector\nid\nuniversity_city\n\n\n\n\n4\nMedia & communication\n345\nLausanne\n\n\n3\nMedia & communication\n253\nSaint Gallen\n\n\n3\nMedia & communication\n322\nLausanne\n\n\n2\nMedia & communication\n453\nSaint Gallen\n\n\n2\nMedia & communication\n380\nSaint Gallen\n\n\n2\nMedia & communication\n290\nSaint Gallen\n\n\n2\nMedia & communication\n439\nLausanne\n\n\n2\nMedia & communication\n378\nLausanne\n\n\n2\nMedia & communication\n452\nZurich\n\n\n1\nMedia & communication\n115\nLausanne\n\n\n\n\n\nGood job! The professor with id 538 has the most affiliations in the “Media & communication” sector, and he or she lives in the city of Lausanne. Thanks to your database design, you can be sure that the data you’ve just queried is consistent. Of course, you could also put university_city and organization_sector in their own tables, making the data model even more formal. However, in database design, you have to strike a balance between modeling overhead, desired data consistency, and usability for queries like the one you\\’ve just wrote. Congratulations, you made it to the end!",
    "crumbs": [
      "SQL",
      "Introduction to Relational Databases in SQL"
    ]
  },
  {
    "objectID": "content/R/index.html",
    "href": "content/R/index.html",
    "title": "Data Science with R",
    "section": "",
    "text": "This section is centered around the use of the R programming language within the tidy data framework, and as such employs the most recent advances in data analysis coding. The chapter provide a sophisticated first introduction to the field of data science and provide a balanced mix of practical skills along with generalizable principles.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/R/index.html#chapter",
    "href": "content/R/index.html#chapter",
    "title": "Data Science with R",
    "section": "Chapter",
    "text": "Chapter\n\n1 Programming Basics\n\n1.1 Introduction to R\nMaster the basics of data analysis by manipulating common data structures such as vectors, matrices, and data frames.\n1.2 Intermediate R\nDiscover conditional statements, loops, and functions to power your own R scripts, and learn to make your R code more efficient using the apply functions.\n1.3 Introduction to the tidyverse\nGet started on the path to exploring and visualizing your own data with the tidyverse, a powerful and popular collection of data science tools within R. Discover the fundamentals of the Tidyverse, and learn all about renaming and reordering variables, while becoming familiar with binomial distribution.\n\n2 Importing Data\n\n2.1 Introduction to Importing Data\nLearn to read .xls, .csv, and text files in R using readxl and gdata, before learning how to use readr and data.table packages to import flat file data.\n2.2 Intermediate Importing Data\nParse data in any format. Whether it’s flat files, statistical software, databases, or data right from the web.\n2.3 Working with web Data\nLearn how to efficiently import data from the web into R. Discover how to work with APIs, build your own API client, and access data from Wikipedia and other sources by using R to scrape information from web pages.\n\n3 Data Wrangling\n\n3.1 Data Manipulation with dplyr\nDelve further into the Tidyverse by learning to transform and manipulate data with dplyr. Learn how to use dplyr to transform and aggregate data, then add, remove, or change variables. You’ll then apply your skills to a real-world case study.\n3.2 Joining data with dplyr\nLearn to combine data across multiple tables to answer more complex questions with dplyr. Learn to combine data across multiple tables to answer complex questions with dplyr. You’ll learn 6 different joins including inner, full, anti, and more.\n3.3 Exploratory Data Analysis\nLearn how to use graphical and numerical techniques for exploratory data analysis while generating insightful and beautiful graphics in R.\n3.4 Case Study: EDA\nUse data manipulation and visualization skills to explore the historical voting of the United Nations General Assembly.\n3.5 Cleaning Data\nDevelop the skills you need to go from raw data to awesome insights as quickly and accurately as possible.\n3.6 Data Manipulation with data.table\nMaster core concepts about data manipulation such as filtering, selecting and calculating groupwise statistics using data.table.\n3.7 Joining Data with data.table\nThis course will show you how to combine and merge datasets with data.table.\n\n4 Data Visualization\n\n4.1 Intermediate Data Visualization with ggplot2\nLearn to produce meaningful and beautiful data visualizations with ggplot2 by understanding the grammar of graphics.\n4.2 Intermediate Data Visualization with ggplot2\nLearn to use facets, coordinate systems and statistics in ggplot2 to create meaningful explanatory plots.\n\n5 Statistics\n\n5.1 Introduction to Statistics\nGrow your statistical skills and learn how to collect, analyze, and draw accurate conclusions from data. Learn how to work with variables, plotting, and standard deviation in R. It covers histograms, distributions and more.\n5.2 Foundations of Probability\nIn this course, you’ll learn about the concepts of random variables, distributions, and conditioning. Learn about random variables, distributions and conditioning, while gaining intuition for how to solve probability problems through random simulation.\n5.3 Introduction to Regression\nLearn how you can predict housing prices and ad click-through rate by implementing, analyzing, and interpreting linear and logistic regressions using R.\n5.4 Intermediate Regression\nLearn to perform linear and logistic regression with multiple explanatory variables. Discover how to include multiple explanatory variables in a model, how interactions affect predictions, and how linear and logistic regression work in R.\n5.5 Modeling with Data in the Tidyverse\nExplore Linear Regression in a tidy framework.Discover different types in data modeling, including for prediction, and learn how to conduct linear regression and model assement measures in the Tidyverse.\n5.6 Experimental Design\nIn this course you’ll learn about basic experimental design, a crucial part of any data analysis. Learn about basic experimental design, including block and factorial designs, and commonly used statistical tests, such as the t-tests and ANOVAs in R.\n5.7 A/B Testing\nLearn A/B testing: including hypothesis testing, experimental design, and confounding variables.\n5.8 Fundamentals of Bayesian Data Analysis\nLearn what Bayesian data analysis is, how it works, and why it is a useful tool to have in your data science toolbox.\n5.9 Factor Analysis\nExplore latent variables, such as personality, using exploratory and confirmatory factor analyses. Start this four-hour course today to discover exploratory factor analysis and confirmatory factor analysis in R to explore latent variables such as personality.\n\n7 Machine Learning\n\n7.1 Supervised Learning: Classification\nBasics of machine learning for classification. This beginner-level introduction to machine learning covers four of the most common classification algorithms. You will come away with a basic understanding of how each algorithm approaches a learning task, as well as learn the R functions needed to apply these tools to your own work.\n7.2 Supervised Learning: Regression\nIn this course you will learn how to predict future events using linear regression, generalized additive models, random forests, and xgboost.\n7.3 Unsupervised Learning\nThis course provides an intro to clustering and dimensionality reduction in R from a machine learning perspective.\n7.4 Machine Learning in the tidyverse\nLeverage the tools in the tidyverse to generate, explore and evaluate machine learning models.\n7.5 Cluster Analysis\nDevelop a strong intuition for how hierarchical and k-means clustering work and learn how to apply them to extract insights from your data.\n7.6 Cluster Analysis\nThis course teaches the big ideas in machine learning: how to build and evaluate predictive models, how to tune them for performance, and how to preprocess data.\n7.7 Modeling with tidymodels\nLearn to streamline your machine learning workflows with tidymodels.\n7.8 Machine Learning with tree-based Models\nLearn how to use tree-based models and ensembles to make classification and regression predictions with tidymodels.\n7.9 Support Vector Machines\nThis course will introduce the support vector machine (SVM) using an intuitive, visual approach.\n7.10 Topic Modeling\nLearn how to fit topic models using the Latent Dirichlet Allocation algorithm.\n7.11 Hyperparameter Tuning\nUse the caret, mlr and h2o packages to find optimal hyperparameters using grid search, random search, adaptive resampling and automatic machine learning.\n7.12 Bayesian Regression Modeling\nLearn how to leverage Bayesian estimation methods to make better inferences about linear regression models.\n7.13 Introduction to Spark\nLearn how to analyze huge datasets using Apache Spark and R using the sparklyr package.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/sql/index.html",
    "href": "content/sql/index.html",
    "title": "About Data Science with SQL",
    "section": "",
    "text": "This section is centered around the use of the SQL programming language…",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html",
    "title": "Introduction to SQL",
    "section": "",
    "text": "CREATE DATABASE films;\npsql -d films -U jschwarz -f films.sql\nShort Description\nLearn the basics of syntax in SQL, how to extract and wrangle data, and use querying tables in relational databases like MySQL, SQL Server, and PostgreSQL.\nLong Description\nThe role of a data scientist is to turn raw data into actionable insights. Much of the world’s raw data—from electronic medical records to customer transaction histories—lives in organized collections of tables called relational databases. To be an effective data scientist, you must know how to wrangle and extract data from these databases using a language called SQL . This course teaches syntax in SQL shared by many types of databases, such as PostgreSQL, MySQL, SQL Server, and Oracle. This course teaches you everything you need to know to begin working with databases today!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#welcome-to-the-course",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#welcome-to-the-course",
    "title": "Introduction to SQL",
    "section": "1.1 Welcome to the course!",
    "text": "1.1 Welcome to the course!\nTheory. Coming soon …\n1. Welcome to the course!\nHi, and welcome to Introduction to SQL for Data Science!Most of the world’s data live in databases, so learning how to access and unlock insights from these data is an essential skill for every data scientist. SQL, or ess-que-el, is the native language for interacting with databases and is designed for exactly this purpose.This course will give you a basic introduction to SQL. We hope you enjoy it.\n2. Let’s practice!\nNow let’s get started!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-tables",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-tables",
    "title": "Introduction to SQL",
    "section": "1.2 Onboarding | Tables",
    "text": "1.2 Onboarding | Tables\nThe DataCamp interface for SQL courses contains a few unique features you should be aware of.\nFor this course, you’ll be using a database containing information on almost 5000 films. To the right, underneath the editor, you can see the data in this database by clicking through the tabs.\nFrom looking at the tabs, who is the first person listed in the people table?\n\n1.3 Question\n???  ⬜ Kanye West ⬜ Biggie Smalls ✅ 50 Cent ⬜ Jay Z\n\nThat’s correct! Head over to the next exercise to see how we can answer a similar question with code!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-query-result",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-query-result",
    "title": "Introduction to SQL",
    "section": "1.4 Onboarding | Query Result",
    "text": "1.4 Onboarding | Query Result\nNotice the query result tab in the bottom right corner of your screen. This is where the results of your SQL queries will be displayed.\nRun this query in the editor and check out the resulting table in the query result tab!\n\nSELECT name FROM people;\n\nWho is the second person listed in the query result?\n\n1.5 Question\n???  ⬜ Kanye West ✅ A. Michael Baldwin ⬜ 50 Cent ⬜ Jay Z",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-errors",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-errors",
    "title": "Introduction to SQL",
    "section": "1.6 Onboarding | Errors",
    "text": "1.6 Onboarding | Errors\nIf you submit the code to the right, you’ll see that you get two types of errors.\nSQL errors are shown below the editor. These are errors returned by the SQL engine. You should see:\n\nsyntax error at or near \"'I &lt;3 SQL'\" LINE 2: 'I &lt;3 SQL' ^\n\nDataCamp errors are shown in the Instructions box. These will let you know in plain English where you went wrong in your code! You should see:\n\nYou need to add SELECT at the start of line 2!\n\nSteps\n\nSubmit the code to the right, check out the errors, then fix them!\n\n\n-- Try running me!\nSELECT 'I &lt;3 SQL'\nAS result;\n\n\n1 records\n\n\nresult\n\n\n\n\nI &lt;3 SQL\n\n\n\n\n\nExcellent error editing! You can feel safe experimenting with code in the editor – you’ll always get feedback if something goes wrong.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-multi-step-exercises",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#onboarding-multi-step-exercises",
    "title": "Introduction to SQL",
    "section": "1.7 Onboarding | Multi-step Exercises",
    "text": "1.7 Onboarding | Multi-step Exercises\nThe following multi-step exercise allows you to practice a new concept through repetition. Check it out!\nSteps\n\nSubmit the query in the editor! Don’t worry, you’ll learn how it works soon.\n\n\nSELECT 'SQL'\nAS result;\n\n\n1 records\n\n\nresult\n\n\n\n\nSQL\n\n\n\n\n\n\nNow change 'SQL' to 'SQL is' and click Submit!\n\n\nSELECT 'SQL is'\nAS result;\n\n\n1 records\n\n\nresult\n\n\n\n\nSQL is\n\n\n\n\n\n\nFinally, change 'SQL is' to 'SQL is cool' and click Submit!\n\n\nSELECT 'SQL is cool'\nAS result;\n\n\n1 records\n\n\nresult\n\n\n\n\nSQL is cool\n\n\n\n\n\nWell done! The time has come to actually fetch information from tables now!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#beginning-your-sql-journey",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#beginning-your-sql-journey",
    "title": "Introduction to SQL",
    "section": "1.8 Beginning your SQL journey",
    "text": "1.8 Beginning your SQL journey\nNow that you’re familiar with the interface, let’s get straight into it.\nSQL, which stands for Structured Query Language, is a language for interacting with data stored in something called a relational database.\nYou can think of a relational database as a collection of tables. A table is just a set of rows and columns, like a spreadsheet, which represents exactly one type of entity. For example, a table might represent employees in a company or purchases made, but not both.\nEach row, or record, of a table contains information about a single entity. For example, in a table representing employees, each row represents a single person. Each column, or field, of a table contains a single attribute for all rows in the table. For example, in a table representing employees, we might have a column containing first and last names for all employees.\nThe table of employees might look something like this:\n\n\n\nid\nname\nage\nnationality\n\n\n\n\n1\nJessica\n22\nIreland\n\n\n2\nGabriel\n48\nFrance\n\n\n3\nLaura\n36\nUSA\n\n\n\n\n1.9 Question\nHow many fields does the employees table above contain?  ⬜ 1 ⬜ 2 ⬜ 3 ✅ 4\n\nCorrect! The table contains four columns, or fields.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#selecting-single-columns",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#selecting-single-columns",
    "title": "Introduction to SQL",
    "section": "1.10 SELECTing single columns",
    "text": "1.10 SELECTing single columns\nWhile SQL can be used to create and modify databases, the focus of this course will be querying databases. A query is a request for data from a database table (or combination of tables). Querying is an essential skill for a data scientist, since the data you need for your analyses will often live in databases.\nIn SQL, you can select data from a table using a SELECT statement. For example, the following query selects the name column from the people table:\n\nSELECT name\nFROM people;\n\nIn this query, SELECT and FROM are called keywords. In SQL, keywords are not case-sensitive, which means you can write the same query as:\n\nselect name\nfrom people;\n\nThat said, it’s good practice to make SQL keywords uppercase to distinguish them from other parts of your query, like column and table names.\nIt’s also good practice (but not necessary for the exercises in this course) to include a semicolon at the end of your query. This tells SQL where the end of your query is!\nRemember, you can see the results of executing your query in the query tab!\nSteps\n\nSelect the title column from the films table.\n\n\nSELECT title\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ntitle\n\n\n\n\nIntolerance: Love’s Struggle Throughout the Ages\n\n\nOver the Hill to the Poorhouse\n\n\nThe Big Parade\n\n\nMetropolis\n\n\nPandora’s Box\n\n\nThe Broadway Melody\n\n\nHell’s Angels\n\n\nA Farewell to Arms\n\n\n42nd Street\n\n\nShe Done Him Wrong\n\n\n\n\n\n\nSelect the release_year column from the films table.\n\n\nSELECT release_year\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\n\n\n\n\n1916\n\n\n1920\n\n\n1925\n\n\n1927\n\n\n1929\n\n\n1929\n\n\n1930\n\n\n1932\n\n\n1933\n\n\n1933\n\n\n\n\n\n\nSelect the name of each person in the people table.\n\n\nSELECT name\nFROM people;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\n50 Cent\n\n\nA. Michael Baldwin\n\n\nA. Raven Cruz\n\n\nA.J. Buckley\n\n\nA.J. DeLucia\n\n\nA.J. Langer\n\n\nAaliyah\n\n\nAaron Ashmore\n\n\nAaron Hann\n\n\nAaron Hill",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#selecting-multiple-columns",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#selecting-multiple-columns",
    "title": "Introduction to SQL",
    "section": "1.11 SELECTing multiple columns",
    "text": "1.11 SELECTing multiple columns\nWell done! Now you know how to select single columns.\nIn the real world, you will often want to select multiple columns. Luckily, SQL makes this really easy. To select multiple columns from a table, simply separate the column names with commas!\nFor example, this query selects two columns, name and birthdate, from the people table:\n\nSELECT name, birthdate\nFROM people;\n\nSometimes, you may want to select all columns from a table. Typing out every column name would be a pain, so there’s a handy shortcut:\n\nSELECT *\nFROM people;\n\nIf you only want to return a certain number of results, you can use the LIMIT keyword to limit the number of rows returned:\n\nSELECT *\nFROM people\nLIMIT 10;\n\nBefore getting started with the instructions below, check out the column names in the films table!\nSteps\n\nGet the title of every film from the films table.\n\n\nSELECT title\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ntitle\n\n\n\n\nIntolerance: Love’s Struggle Throughout the Ages\n\n\nOver the Hill to the Poorhouse\n\n\nThe Big Parade\n\n\nMetropolis\n\n\nPandora’s Box\n\n\nThe Broadway Melody\n\n\nHell’s Angels\n\n\nA Farewell to Arms\n\n\n42nd Street\n\n\nShe Done Him Wrong\n\n\n\n\n\n\nGet the title and release year for every film.\n\n\nSELECT title, release_year\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\nIntolerance: Love’s Struggle Throughout the Ages\n1916\n\n\nOver the Hill to the Poorhouse\n1920\n\n\nThe Big Parade\n1925\n\n\nMetropolis\n1927\n\n\nPandora’s Box\n1929\n\n\nThe Broadway Melody\n1929\n\n\nHell’s Angels\n1930\n\n\nA Farewell to Arms\n1932\n\n\n42nd Street\n1933\n\n\nShe Done Him Wrong\n1933\n\n\n\n\n\n\nGet the title, release year and country for every film.\n\n\nSELECT title, release_year, country\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\ntitle\nrelease_year\ncountry\n\n\n\n\nIntolerance: Love’s Struggle Throughout the Ages\n1916\nUSA\n\n\nOver the Hill to the Poorhouse\n1920\nUSA\n\n\nThe Big Parade\n1925\nUSA\n\n\nMetropolis\n1927\nGermany\n\n\nPandora’s Box\n1929\nGermany\n\n\nThe Broadway Melody\n1929\nUSA\n\n\nHell’s Angels\n1930\nUSA\n\n\nA Farewell to Arms\n1932\nUSA\n\n\n42nd Street\n1933\nUSA\n\n\nShe Done Him Wrong\n1933\nUSA\n\n\n\n\n\n\nGet all columns from the films table.\n\n\nSELECT *\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n1\nIntolerance: Love’s Struggle Throughout the Ages\n1916\nUSA\n123\nNA\nNot Rated\nNA\n385907\n\n\n2\nOver the Hill to the Poorhouse\n1920\nUSA\n110\nNA\nNA\n3000000\n100000\n\n\n3\nThe Big Parade\n1925\nUSA\n151\nNA\nNot Rated\nNA\n245000\n\n\n4\nMetropolis\n1927\nGermany\n145\nGerman\nNot Rated\n26435\n6000000\n\n\n5\nPandora’s Box\n1929\nGermany\n110\nGerman\nNot Rated\n9950\nNA\n\n\n6\nThe Broadway Melody\n1929\nUSA\n100\nEnglish\nPassed\n2808000\n379000\n\n\n7\nHell’s Angels\n1930\nUSA\n96\nEnglish\nPassed\nNA\n3950000\n\n\n8\nA Farewell to Arms\n1932\nUSA\n79\nEnglish\nUnrated\nNA\n800000\n\n\n9\n42nd Street\n1933\nUSA\n89\nEnglish\nUnrated\n2300000\n439000\n\n\n10\nShe Done Him Wrong\n1933\nUSA\n66\nEnglish\nApproved\nNA\n200000",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#select-distinct",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#select-distinct",
    "title": "Introduction to SQL",
    "section": "1.12 SELECT DISTINCT",
    "text": "1.12 SELECT DISTINCT\nOften your results will include many duplicate values. If you want to select all the unique values from a column, you can use the DISTINCT keyword.\nThis might be useful if, for example, you’re interested in knowing which languages are represented in the films table:\n\nSELECT DISTINCT language\nFROM films;\n\nRemember, you can check out the data in the tables by clicking on the table name!\nSteps\n\nGet all the unique countries represented in the films table.\n\n\nSELECT DISTINCT country\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ncountry\n\n\n\n\nNA\n\n\nIndonesia\n\n\nCameroon\n\n\nCzech Republic\n\n\nSweden\n\n\nDominican Republic\n\n\nCambodia\n\n\nIreland\n\n\nFinland\n\n\nColombia\n\n\n\n\n\n\nGet all the different film certifications from the films table.\n\n\nSELECT DISTINCT certification\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ncertification\n\n\n\n\nNA\n\n\nUnrated\n\n\nNC-17\n\n\nR\n\n\nNot Rated\n\n\nX\n\n\nPG-13\n\n\nApproved\n\n\nM\n\n\nPG\n\n\n\n\n\n\nGet the different types of film roles from the roles table.\n\n\nSELECT DISTINCT role\nFROM roles;\n\n\n2 records\n\n\nrole\n\n\n\n\nactor\n\n\ndirector",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#learning-to-count",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#learning-to-count",
    "title": "Introduction to SQL",
    "section": "1.13 Learning to COUNT",
    "text": "1.13 Learning to COUNT\nWhat if you want to count the number of employees in your employees table? The COUNT() function lets you do this by returning the number of rows in one or more columns.\nFor example, this code gives the number of rows in the people table:\n\nSELECT COUNT(*)\nFROM people;\n\n\n1 records\n\n\ncount\n\n\n\n\n8397\n\n\n\n\n\n\n1.14 Question\nHow many records are contained in the reviews table?  ⬜ 9,468 ⬜ 8,397 ✅ 4,968 ⬜ 9,837 ⬜ 9,864",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#practice-with-count",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#practice-with-count",
    "title": "Introduction to SQL",
    "section": "1.15 Practice with COUNT",
    "text": "1.15 Practice with COUNT\nAs you’ve seen, COUNT(*) tells you how many rows are in a table. However, if you want to count the number of non-missing values in a particular column, you can call COUNT() on just that column.\nFor example, to count the number of birth dates present in the people table:\n\nSELECT COUNT(birthdate)\nFROM people;\n\nIt’s also common to combine COUNT() with DISTINCT to count the number of distinct values in a column.\nFor example, this query counts the number of distinct birth dates contained in the people table:\n\nSELECT COUNT(DISTINCT birthdate)\nFROM people;\n\nLet’s get some practice with COUNT()!\nSteps\n\nCount the number of rows in the people table.\n\n\nSELECT COUNT(*)\nFROM people;\n\n\n1 records\n\n\ncount\n\n\n\n\n8397\n\n\n\n\n\n\nCount the number of (non-missing) birth dates in the people table.\n\n\nSELECT COUNT(birthdate)\nFROM people;\n\n\n1 records\n\n\ncount\n\n\n\n\n6152\n\n\n\n\n\n\nCount the number of unique birth dates in the people table.\n\n\nSELECT COUNT(DISTINCT birthdate)\nFROM people;\n\n\n1 records\n\n\ncount\n\n\n\n\n5398\n\n\n\n\n\n\nCount the number of unique languages in the films table.\n\n\nSELECT COUNT(DISTINCT language)\nFROM films;\n\n\n1 records\n\n\ncount\n\n\n\n\n47\n\n\n\n\n\n\nCount the number of unique countries in the films table.\n\n\nSELECT COUNT(DISTINCT country)\nFROM films;\n\n\n1 records\n\n\ncount\n\n\n\n\n64",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#filtering-results",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#filtering-results",
    "title": "Introduction to SQL",
    "section": "2.1 Filtering results",
    "text": "2.1 Filtering results\nCongrats on finishing the first chapter! You now know how to select columns and perform basic counts. This chapter will focus on filtering your results.\nIn SQL, the WHERE keyword allows you to filter based on both text and numeric values in a table. There are a few different comparison operators you can use:\n\n= equal\n&lt;&gt; not equal\n&lt; less than\n&gt; greater than\n&lt;= less than or equal to\n&gt;= greater than or equal to\n\nFor example, you can filter text records such as title. The following code returns all films with the title 'Metropolis':\n\nSELECT title\nFROM films\nWHERE title = 'Metropolis';\n\nNotice that the WHERE clause always comes after the FROM statement!\nNote that in this course we will use &lt;&gt; and not != for the not equal operator, as per the SQL standard.\n\nSELECT title\nFROM films\nWHERE release_year &gt; 2000;\n\n\n2.2 Question\nWhat does the following query return?  ⬜ Films released before the year 2000 ✅ Films released after the year 2000 ⬜ Films released after the year 2001 ⬜ Films released in 2000",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#simple-filtering-of-numeric-values",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#simple-filtering-of-numeric-values",
    "title": "Introduction to SQL",
    "section": "2.3 Simple filtering of numeric values",
    "text": "2.3 Simple filtering of numeric values\nAs you learned in the previous exercise, the WHERE clause can also be used to filter numeric records, such as years or ages.\nFor example, the following query selects all details for films with a budget over ten thousand dollars:\n\nSELECT *\nFROM films\nWHERE budget &gt; 10000;\n\nNow it’s your turn to use the WHERE clause to filter numeric values!\nSteps\n\nGet all details for all films released in 2016.\n\n\nSELECT *\nFROM films\nWHERE release_year = 2016;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n4821\n10 Cloverfield Lane\n2016\nUSA\n104\nEnglish\nPG-13\n71897215\n15000000\n\n\n4822\n13 Hours\n2016\nUSA\n144\nEnglish\nR\n52822418\n50000000\n\n\n4823\nA Beginner’s Guide to Snuff\n2016\nUSA\n87\nEnglish\nNA\nNA\nNA\n\n\n4824\nAirlift\n2016\nIndia\n130\nHindi\nNA\nNA\n4400000\n\n\n4825\nAlice Through the Looking Glass\n2016\nUSA\n113\nEnglish\nPG\n76846624\n170000000\n\n\n4826\nAllegiant\n2016\nUSA\n120\nEnglish\nPG-13\n66002193\n110000000\n\n\n4827\nAlleluia! The Devil’s Carnival\n2016\nUSA\n97\nEnglish\nNA\nNA\n500000\n\n\n4828\nAntibirth\n2016\nUSA\n94\nEnglish\nNA\nNA\n3500000\n\n\n4829\nBad Moms\n2016\nUSA\n100\nEnglish\nR\n55461307\n20000000\n\n\n4830\nBad Moms\n2016\nUSA\n100\nEnglish\nR\n55461307\n20000000\n\n\n\n\n\n\nGet the number of films released before 2000.\n\n\nSELECT COUNT(*)\nFROM films\nWHERE release_year &lt; 2000;\n\n\n1 records\n\n\ncount\n\n\n\n\n1337\n\n\n\n\n\n\nGet the title and release year of films released after 2000.\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year &gt; 2000;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\n15 Minutes\n2001\n\n\n3000 Miles to Graceland\n2001\n\n\nA Beautiful Mind\n2001\n\n\nA Knight’s Tale\n2001\n\n\nA.I. Artificial Intelligence\n2001\n\n\nAli\n2001\n\n\nAlias Betty\n2001\n\n\nAll the Queen’s Men\n2001\n\n\nAlong Came a Spider\n2001\n\n\nAmélie\n2001\n\n\n\n\n\nGreat job! After filtering of numeric values, it’s time to explore filtering of text!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#simple-filtering-of-text",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#simple-filtering-of-text",
    "title": "Introduction to SQL",
    "section": "2.4 Simple filtering of text",
    "text": "2.4 Simple filtering of text\nRemember, the WHERE clause can also be used to filter text results, such as names or countries.\nFor example, this query gets the titles of all films which were filmed in China:\n\nSELECT title\nFROM films\nWHERE country = 'China';\n\nNow it’s your turn to practice using WHERE with text values!\nImportant: in PostgreSQL (the version of SQL we’re using), you must use single quotes with WHERE.\nSteps\n\nGet all details for all French language films.\n\n\nSELECT *\nFROM films\nWHERE language = 'French';\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n108\nUne Femme Mariée\n1964\nFrance\n94\nFrench\nNA\nNA\n120000\n\n\n111\nPierrot le Fou\n1965\nFrance\n110\nFrench\nNot Rated\nNA\n300000\n\n\n140\nMississippi Mermaid\n1969\nFrance\n123\nFrench\nR\n26893\n1600000\n\n\n423\nSubway\n1985\nFrance\n98\nFrench\nR\nNA\n17000000\n\n\n662\nLes visiteurs\n1993\nFrance\n107\nFrench\nR\n700000\n50000000\n\n\n801\nThe Horseman on the Roof\n1995\nFrance\n135\nFrench\nR\n1877179\nNA\n\n\n916\nWhen the Cat’s Away\n1996\nFrance\n91\nFrench\nR\n1652472\n300000\n\n\n1004\nThe Chambermaid on the Titanic\n1997\nFrance\n101\nFrench\nNA\n244465\nNA\n\n\n1026\nThe Swindle\n1997\nFrance\n101\nFrench\nNA\n231417\n60000000\n\n\n1088\nLes couloirs du temps: Les visiteurs II\n1998\nFrance\n118\nFrench\nNA\n146072\n140000000\n\n\n\n\n\n\nGet the name and birth date of the person born on November 11th, 1974. Remember to use ISO date format ('1974-11-11')!\n\n\nSELECT name, birthdate\nFROM people\nWHERE birthdate = '1974-11-11';\n\n\n1 records\n\n\nname\nbirthdate\n\n\n\n\nLeonardo DiCaprio\n1974-11-11\n\n\n\n\n\n\nGet the number of Hindi language films.\n\n\nSELECT COUNT(*)\nFROM films\nWHERE language = 'Hindi';\n\n\n1 records\n\n\ncount\n\n\n\n\n28\n\n\n\n\n\n\nGet all details for all films with an R certification.\n\n\nSELECT *\nFROM films\nWHERE certification = 'R';\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n76\nPsycho\n1960\nUSA\n108\nEnglish\nR\n32000000\n806947\n\n\n99\nA Fistful of Dollars\n1964\nItaly\n99\nItalian\nR\n3500000\n200000\n\n\n134\nRosemary’s Baby\n1968\nUSA\n136\nEnglish\nR\nNA\n2300000\n\n\n140\nMississippi Mermaid\n1969\nFrance\n123\nFrench\nR\n26893\n1600000\n\n\n145\nThe Wild Bunch\n1969\nUSA\n144\nEnglish\nR\nNA\n6244087\n\n\n149\nCatch-22\n1970\nUSA\n122\nEnglish\nR\nNA\n18000000\n\n\n150\nCotton Comes to Harlem\n1970\nUSA\n97\nEnglish\nR\nNA\n1200000\n\n\n153\nThe Ballad of Cable Hogue\n1970\nUSA\n121\nEnglish\nR\nNA\n3716946\n\n\n154\nThe Conformist\n1970\nItaly\n106\nItalian\nR\nNA\n750000\n\n\n158\nWoodstock\n1970\nUSA\n215\nEnglish\nR\n13300000\n600000\n\n\n\n\n\nWonderful! Let’s look at combining different conditions now!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#where-and",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#where-and",
    "title": "Introduction to SQL",
    "section": "2.5 WHERE AND",
    "text": "2.5 WHERE AND\nOften, you’ll want to select data based on multiple conditions. You can build up your WHERE queries by combining multiple conditions with the AND keyword.\nFor example,\n\nSELECT title\nFROM films\nWHERE release_year &gt; 1994\nAND release_year &lt; 2000;\n\ngives you the titles of films released between 1994 and 2000.\nNote that you need to specify the column name separately for every AND condition, so the following would be invalid:\n\nSELECT title\nFROM films\nWHERE release_year &gt; 1994 AND &lt; 2000;\n\nYou can add as many AND conditions as you need!\nSteps\n\nGet the title and release year for all Spanish language films released before 2000.\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year &lt; 2000\nAND language = 'Spanish';\n\n\n3 records\n\n\ntitle\nrelease_year\n\n\n\n\nEl Mariachi\n1992\n\n\nLa otra conquista\n1998\n\n\nTango\n1998\n\n\n\n\n\n\nGet all details for Spanish language films released after 2000.\n\n\nSELECT *\nFROM films\nWHERE release_year &gt; 2000\nAND language = 'Spanish';\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n1695\nY Tu Mamá También\n2001\nMexico\n106\nSpanish\nR\n13622333\n2000000\n\n\n1757\nEl crimen del padre Amaro\n2002\nMexico\n118\nSpanish\nR\n5709616\n1800000\n\n\n1807\nMondays in the Sun\n2002\nSpain\n113\nSpanish\nR\n146402\n4000000\n\n\n2173\nLive-In Maid\n2004\nArgentina\n83\nSpanish\nUnrated\nNA\n800000\n\n\n2175\nMaria Full of Grace\n2004\nColombia\n101\nSpanish\nR\n6517198\n3000000\n\n\n2246\nThe Holy Girl\n2004\nArgentina\n106\nSpanish\nR\n304124\n1400000\n\n\n2263\nThe Sea Inside\n2004\nSpain\n125\nSpanish\nPG-13\n2086345\n10000000\n\n\n2458\nThe Legend of Zorro\n2005\nUSA\n129\nSpanish\nPG\n45356386\n75000000\n\n\n2542\nCaptain Alatriste: The Spanish Musketeer\n2006\nSpain\n145\nSpanish\nNA\nNA\n24000000\n\n\n2646\nPan’s Labyrinth\n2006\nSpain\n112\nSpanish\nR\n37623143\n13500000\n\n\n\n\n\n\nGet all details for Spanish language films released after 2000, but before 2010.\n\n\nSELECT *\nFROM films\nWHERE release_year &gt; 2000\nAND release_year &lt; 2010\nAND language = 'Spanish';\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n1695\nY Tu Mamá También\n2001\nMexico\n106\nSpanish\nR\n13622333\n2000000\n\n\n1757\nEl crimen del padre Amaro\n2002\nMexico\n118\nSpanish\nR\n5709616\n1800000\n\n\n1807\nMondays in the Sun\n2002\nSpain\n113\nSpanish\nR\n146402\n4000000\n\n\n2173\nLive-In Maid\n2004\nArgentina\n83\nSpanish\nUnrated\nNA\n800000\n\n\n2175\nMaria Full of Grace\n2004\nColombia\n101\nSpanish\nR\n6517198\n3000000\n\n\n2246\nThe Holy Girl\n2004\nArgentina\n106\nSpanish\nR\n304124\n1400000\n\n\n2263\nThe Sea Inside\n2004\nSpain\n125\nSpanish\nPG-13\n2086345\n10000000\n\n\n2458\nThe Legend of Zorro\n2005\nUSA\n129\nSpanish\nPG\n45356386\n75000000\n\n\n2542\nCaptain Alatriste: The Spanish Musketeer\n2006\nSpain\n145\nSpanish\nNA\nNA\n24000000\n\n\n2646\nPan’s Labyrinth\n2006\nSpain\n112\nSpanish\nR\n37623143\n13500000\n\n\n\n\n\nGreat work! Being able to combine conditions with AND will prove to be very useful if you only want your query to return a specific subset of records!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#where-and-or",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#where-and-or",
    "title": "Introduction to SQL",
    "section": "2.6 WHERE AND OR",
    "text": "2.6 WHERE AND OR\nWhat if you want to select rows based on multiple conditions where some but not all of the conditions need to be met? For this, SQL has the OR operator.\nFor example, the following returns all films released in either 1994 or 2000:\n\nSELECT title\nFROM films\nWHERE release_year = 1994\nOR release_year = 2000;\n\nNote that you need to specify the column for every OR condition, so the following is invalid:\n\nSELECT title\nFROM films\nWHERE release_year = 1994 OR 2000;\n\nWhen combining AND and OR, be sure to enclose the individual clauses in parentheses, like so:\n\nSELECT title\nFROM films\nWHERE (release_year = 1994 OR release_year = 1995)\nAND (certification = 'PG' OR certification = 'R');\n\nOtherwise, due to SQL’s precedence rules, you may not get the results you’re expecting!\n\n2.7 Question\nWhat does the OR operator do?  ✅ Display only rows that meet at least one of the specified conditions. ⬜ Display only rows that meet all of the specified conditions. ⬜ Display only rows that meet none of the specified conditions.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#where-and-or-2",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#where-and-or-2",
    "title": "Introduction to SQL",
    "section": "2.8 WHERE AND OR (2)",
    "text": "2.8 WHERE AND OR (2)\nYou now know how to select rows that meet some but not all conditions by combining AND and OR.\nFor example, the following query selects all films that were released in 1994 or 1995 which had a rating of PG or R.\n\nSELECT title\nFROM films\nWHERE (release_year = 1994 OR release_year = 1995)\nAND (certification = 'PG' OR certification = 'R');\n\nNow you’ll write a query to get the title and release year of films released in the 90s which were in French or Spanish and which took in more than $2M gross.\nIt looks like a lot, but you can build the query up one step at a time to get comfortable with the underlying concept in each step. Let’s go!\nSteps\n\nGet the title and release year for films released in the 90s.\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year &gt;= 1990 AND release_year &lt; 2000;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\nArachnophobia\n1990\n\n\nBack to the Future Part III\n1990\n\n\nChild’s Play 2\n1990\n\n\nDances with Wolves\n1990\n\n\nDays of Thunder\n1990\n\n\nDick Tracy\n1990\n\n\nDie Hard 2\n1990\n\n\nEdward Scissorhands\n1990\n\n\nFlatliners\n1990\n\n\nGhost\n1990\n\n\n\n\n\n\nNow, build on your query to filter the records to only include French or Spanish language films.\n\n\nSELECT title, release_year\nFROM films\nWHERE (release_year &gt;= 1990 AND release_year &lt; 2000)\nAND (language = 'French' OR language = 'Spanish');\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\nEl Mariachi\n1992\n\n\nLes visiteurs\n1993\n\n\nThe Horseman on the Roof\n1995\n\n\nWhen the Cat’s Away\n1996\n\n\nThe Chambermaid on the Titanic\n1997\n\n\nThe Swindle\n1997\n\n\nLa otra conquista\n1998\n\n\nLes couloirs du temps: Les visiteurs II\n1998\n\n\nTango\n1998\n\n\nThe Red Violin\n1998\n\n\n\n\n\n\nFinally, restrict the query to only return films that took in more than $2M gross.\n\n\nSELECT title, release_year\nFROM films\nWHERE (release_year &gt;= 1990 AND release_year &lt; 2000)\nAND (language = 'French' OR language = 'Spanish')\nAND gross &gt; 2000000;\n\n\n2 records\n\n\ntitle\nrelease_year\n\n\n\n\nEl Mariachi\n1992\n\n\nThe Red Violin\n1998\n\n\n\n\n\nThat was pretty involved!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#between",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#between",
    "title": "Introduction to SQL",
    "section": "2.9 BETWEEN",
    "text": "2.9 BETWEEN\nAs you’ve learned, you can use the following query to get titles of all films released in and between 1994 and 2000:\n\nSELECT title\nFROM films\nWHERE release_year &gt;= 1994\nAND release_year &lt;= 2000;\n\nChecking for ranges like this is very common, so in SQL the BETWEEN keyword provides a useful shorthand for filtering values within a specified range. This query is equivalent to the one above:\n\nSELECT title\nFROM films\nWHERE release_year\nBETWEEN 1994 AND 2000;\n\nIt’s important to remember that BETWEEN is inclusive, meaning the beginning and end values are included in the results!\n\n2.10 Question\nWhat does the BETWEEN keyword do?  ⬜ Filter numeric values ⬜ Filter text values ⬜ Filter values in a specified list ✅ Filter values in a specified range",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#between-2",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#between-2",
    "title": "Introduction to SQL",
    "section": "2.11 BETWEEN (2)",
    "text": "2.11 BETWEEN (2)\nSimilar to the WHERE clause, the BETWEEN clause can be used with multiple AND and OR operators, so you can build up your queries and make them even more powerful!\nFor example, suppose we have a table called kids. We can get the names of all kids between the ages of 2 and 12 from the United States:\n\nSELECT name\nFROM kids\nWHERE age BETWEEN 2 AND 12\nAND nationality = 'USA';\n\nTake a go at using BETWEEN with AND on the films data to get the title and release year of all Spanish language films released between 1990 and 2000 (inclusive) with budgets over $100 million. We have broken the problem into smaller steps so that you can build the query as you go along!\nSteps\n\nGet the title and release year of all films released between 1990 and 2000 (inclusive).\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year BETWEEN 1990 AND 2000;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\nArachnophobia\n1990\n\n\nBack to the Future Part III\n1990\n\n\nChild’s Play 2\n1990\n\n\nDances with Wolves\n1990\n\n\nDays of Thunder\n1990\n\n\nDick Tracy\n1990\n\n\nDie Hard 2\n1990\n\n\nEdward Scissorhands\n1990\n\n\nFlatliners\n1990\n\n\nGhost\n1990\n\n\n\n\n\n\nNow, build on your previous query to select only films that have budgets over $100 million.\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year BETWEEN 1990 AND 2000\nAND budget &gt; 100000000;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\nTerminator 2: Judgment Day\n1991\n\n\nTrue Lies\n1994\n\n\nWaterworld\n1995\n\n\nBatman & Robin\n1997\n\n\nDante’s Peak\n1997\n\n\nPrincess Mononoke\n1997\n\n\nSpeed 2: Cruise Control\n1997\n\n\nStarship Troopers\n1997\n\n\nTitanic\n1997\n\n\nTomorrow Never Dies\n1997\n\n\n\n\n\n\nNow restrict the query to only return Spanish language films.\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year BETWEEN 1990 AND 2000\nAND budget &gt; 100000000\nAND language = 'Spanish';\n\n\n1 records\n\n\ntitle\nrelease_year\n\n\n\n\nTango\n1998\n\n\n\n\n\n\nFinally, modify to your previous query to include all Spanish language or French language films with the same criteria as before. Don’t forget your parentheses!\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year BETWEEN 1990 AND 2000\nAND budget &gt; 100000000\nAND (language = 'Spanish' OR language = 'French');\n\n\n2 records\n\n\ntitle\nrelease_year\n\n\n\n\nLes couloirs du temps: Les visiteurs II\n1998\n\n\nTango\n1998\n\n\n\n\n\nWell done! Off to the next filtering operator!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#where-in",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#where-in",
    "title": "Introduction to SQL",
    "section": "2.12 WHERE IN",
    "text": "2.12 WHERE IN\nAs you’ve seen, WHERE is very useful for filtering results. However, if you want to filter based on many conditions, WHERE can get unwieldy. For example:\n\nSELECT name\nFROM kids\nWHERE age = 2\nOR age = 4\nOR age = 6\nOR age = 8\nOR age = 10;\n\nEnter the IN operator! The IN operator allows you to specify multiple values in a WHERE clause, making it easier and quicker to specify multiple OR conditions! Neat, right?\nSo, the above example would become simply:\n\nSELECT name\nFROM kids\nWHERE age IN (2, 4, 6, 8, 10);\n\nTry using the IN operator yourself!\nSteps\n\nGet the title and release year of all films released in 1990 or 2000 that were longer than two hours. Remember, duration is in minutes!\n\n\nSELECT title, release_year\nFROM films\nWHERE release_year IN (1990, 2000)\nAND duration &gt; 120;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nrelease_year\n\n\n\n\nDances with Wolves\n1990\n\n\nDie Hard 2\n1990\n\n\nGhost\n1990\n\n\nGoodfellas\n1990\n\n\nMo’ Better Blues\n1990\n\n\nPretty Woman\n1990\n\n\nThe Godfather: Part III\n1990\n\n\nThe Hunt for Red October\n1990\n\n\nAll the Pretty Horses\n2000\n\n\nAlmost Famous\n2000\n\n\n\n\n\n\nGet the title and language of all films which were in English, Spanish, or French.\n\n\nSELECT title, language\nFROM films\nWHERE language IN ('English', 'Spanish', 'French');\n\n\nDisplaying records 1 - 10\n\n\ntitle\nlanguage\n\n\n\n\nThe Broadway Melody\nEnglish\n\n\nHell’s Angels\nEnglish\n\n\nA Farewell to Arms\nEnglish\n\n\n42nd Street\nEnglish\n\n\nShe Done Him Wrong\nEnglish\n\n\nIt Happened One Night\nEnglish\n\n\nTop Hat\nEnglish\n\n\nModern Times\nEnglish\n\n\nThe Charge of the Light Brigade\nEnglish\n\n\nSnow White and the Seven Dwarfs\nEnglish\n\n\n\n\n\n\nGet the title and certification of all films with an NC-17 or R certification.\n\n\nSELECT title, certification\nFROM films\nWHERE certification IN ('NC-17', 'R');\n\n\nDisplaying records 1 - 10\n\n\ntitle\ncertification\n\n\n\n\nPsycho\nR\n\n\nA Fistful of Dollars\nR\n\n\nRosemary’s Baby\nR\n\n\nMississippi Mermaid\nR\n\n\nThe Wild Bunch\nR\n\n\nCatch-22\nR\n\n\nCotton Comes to Harlem\nR\n\n\nThe Ballad of Cable Hogue\nR\n\n\nThe Conformist\nR\n\n\nWoodstock\nR\n\n\n\n\n\nYour SQL vocabulary is growing by the minute!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#introduction-to-null-and-is-null",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#introduction-to-null-and-is-null",
    "title": "Introduction to SQL",
    "section": "2.13 Introduction to NULL and IS NULL",
    "text": "2.13 Introduction to NULL and IS NULL\nIn SQL, NULL represents a missing or unknown value. You can check for NULL values using the expression IS NULL. For example, to count the number of missing birth dates in the people table:\n\nSELECT COUNT(*)\nFROM people\nWHERE birthdate IS NULL;\n\nAs you can see, IS NULL is useful when combined with WHERE to figure out what data you’re missing.\nSometimes, you’ll want to filter out missing values so you only get results which are not NULL. To do this, you can use the IS NOT NULL operator.\nFor example, this query gives the names of all people whose birth dates are not missing in the people table.\n\nSELECT name\nFROM people\nWHERE birthdate IS NOT NULL;\n\n\n2.14 Question\nWhat does NULL represent?  ⬜ A corrupt entry ✅ A missing value ⬜ An empty string ⬜ An invalid value\n\nCorrect! NULL is used to represent unknown values.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#null-and-is-null",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#null-and-is-null",
    "title": "Introduction to SQL",
    "section": "2.15 NULL and IS NULL",
    "text": "2.15 NULL and IS NULL\nNow that you know what NULL is and what it’s used for, it’s time for some practice!\nSteps\n\nGet the names of people who are still alive, i.e. whose death date is missing.\n\n\nSELECT name\nFROM people\nWHERE deathdate IS NULL;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\n50 Cent\n\n\nA. Michael Baldwin\n\n\nA. Raven Cruz\n\n\nA.J. Buckley\n\n\nA.J. DeLucia\n\n\nA.J. Langer\n\n\nAaron Ashmore\n\n\nAaron Hann\n\n\nAaron Hill\n\n\nAaron Hughes\n\n\n\n\n\n\nGet the title of every film which doesn’t have a budget associated with it.\n\n\nSELECT title\nFROM films\nWHERE budget IS NULL;\n\n\nDisplaying records 1 - 10\n\n\ntitle\n\n\n\n\nPandora’s Box\n\n\nThe Prisoner of Zenda\n\n\nThe Blue Bird\n\n\nBambi\n\n\nState Fair\n\n\nOpen Secret\n\n\nDeadline - U.S.A.\n\n\nOrdet\n\n\nThe Party’s Over\n\n\nThe Torture Chamber of Dr. Sadism\n\n\n\n\n\n\nGet the number of films which don’t have a language associated with them.\n\n\nSELECT COUNT(*)\nFROM films\nWHERE language IS NULL;\n\n\n1 records\n\n\ncount\n\n\n\n\n11\n\n\n\n\n\nAlright! Are you ready for a last type of operator?",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#like-and-not-like",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#like-and-not-like",
    "title": "Introduction to SQL",
    "section": "2.16 LIKE and NOT LIKE",
    "text": "2.16 LIKE and NOT LIKE\nAs you’ve seen, the WHERE clause can be used to filter text data. However, so far you’ve only been able to filter by specifying the exact text you’re interested in. In the real world, often you’ll want to search for a pattern rather than a specific text string.\nIn SQL, the LIKE operator can be used in a WHERE clause to search for a pattern in a column. To accomplish this, you use something called a wildcard as a placeholder for some other values. There are two wildcards you can use with LIKE:\nThe % wildcard will match zero, one, or many characters in text. For example, the following query matches companies like 'Data', 'DataC' 'DataCamp', 'DataMind', and so on:\n\nSELECT name\nFROM companies\nWHERE name LIKE 'Data%';\n\nThe _ wildcard will match a single character. For example, the following query matches companies like 'DataCamp', 'DataComp', and so on:\n\nSELECT name\nFROM companies\nWHERE name LIKE 'DataC_mp';\n\nYou can also use the NOT LIKE operator to find records that don’t match the pattern you specify.\nGot it? Let’s practice!\nSteps\n\nGet the names of all people whose names begin with ‘B’. The pattern you need is 'B%'.\n\n\nSELECT name\nFROM people\nWHERE name LIKE 'B%';\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nB.J. Novak\n\n\nBabak Najafi\n\n\nBabar Ahmed\n\n\nBahare Seddiqi\n\n\nBai Ling\n\n\nBailee Madison\n\n\nBalinese Tari Legong Dancers\n\n\nBálint Péntek\n\n\nBaltasar Kormákur\n\n\nBalthazar Getty\n\n\n\n\n\n\nGet the names of people whose names have ‘r’ as the second letter. The pattern you need is '_r%'.\n\n\nSELECT name\nFROM people\nWHERE name LIKE '_r%';\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nAra Celi\n\n\nAramis Knight\n\n\nArben Bajraktaraj\n\n\nArcelia Ramírez\n\n\nArchie Kao\n\n\nArchie Panjabi\n\n\nAretha Franklin\n\n\nAri Folman\n\n\nAri Gold\n\n\nAri Graynor\n\n\n\n\n\n\nGet the names of people whose names don’t start with A. The pattern you need is 'A%'.\n\n\nSELECT name\nFROM people\nWHERE name NOT LIKE 'A%';\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\n50 Cent\n\n\nÁlex Angulo\n\n\nÁlex de la Iglesia\n\n\nÁngela Molina\n\n\nB.J. Novak\n\n\nBabak Najafi\n\n\nBabar Ahmed\n\n\nBahare Seddiqi\n\n\nBai Ling\n\n\nBailee Madison\n\n\n\n\n\nThis concludes the second chapter of the intro to SQL course. Rush over to chapter 3 if you want to learn more about aggregate functions!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#aggregate-functions-1",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#aggregate-functions-1",
    "title": "Introduction to SQL",
    "section": "3.1 Aggregate functions",
    "text": "3.1 Aggregate functions\nOften, you will want to perform some calculation on the data in a database. SQL provides a few functions, called aggregate functions, to help you out with this.\nFor example,\n\nSELECT AVG(budget)\nFROM films;\n\ngives you the average value from the budget column of the films table. Similarly, the MAX() function returns the highest budget:\n\nSELECT MAX(budget)\nFROM films;\n\nThe SUM() function returns the result of adding up the numeric values in a column:\n\nSELECT SUM(budget)\nFROM films;\n\nYou can probably guess what the MIN() function does! Now it’s your turn to try out some SQL functions.\nSteps\n\nUse the SUM() function to get the total duration of all films.\n\n\nSELECT SUM(duration)\nFROM films;\n\n\n1 records\n\n\nsum\n\n\n\n\n534882\n\n\n\n\n\n\nGet the average duration of all films.\n\n\nSELECT AVG(duration)\nFROM films;\n\n\n1 records\n\n\navg\n\n\n\n\n107.9479\n\n\n\n\n\n\nGet the duration of the shortest film.\n\n\nSELECT MIN(duration)\nFROM films;\n\n\n1 records\n\n\nmin\n\n\n\n\n7\n\n\n\n\n\n\nGet the duration of the longest film.\n\n\nSELECT MAX(duration)\nFROM films;\n\n\n1 records\n\n\nmax\n\n\n\n\n334\n\n\n\n\n\nSequelistic!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#aggregate-functions-practice",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#aggregate-functions-practice",
    "title": "Introduction to SQL",
    "section": "3.2 Aggregate functions practice",
    "text": "3.2 Aggregate functions practice\nGood work. Aggregate functions are important to understand, so let’s get some more practice!\nSteps\n\nUse the SUM() function to get the total amount grossed by all films.\n\n\nSELECT SUM(gross)\nFROM films;\n\n\n1 records\n\n\nsum\n\n\n\n\n202515840134\n\n\n\n\n\n\nGet the average amount grossed by all films.\n\n\nSELECT AVG(gross)\nFROM films;\n\n\n1 records\n\n\navg\n\n\n\n\n48705108\n\n\n\n\n\n\nGet the amount grossed by the worst performing film.\n\n\nSELECT MIN(gross)\nFROM films;\n\n\n1 records\n\n\nmin\n\n\n\n\n162\n\n\n\n\n\n\nGet the amount grossed by the best performing film.\n\n\nSELECT MAX(gross)\nFROM films;\n\n\n1 records\n\n\nmax\n\n\n\n\n936627416\n\n\n\n\n\nWell done! Don’t forget about these functions. You’ll find yourself using them over and over again to get a quick grasp of the data in a SQL database.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#combining-aggregate-functions-with-where",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#combining-aggregate-functions-with-where",
    "title": "Introduction to SQL",
    "section": "3.3 Combining aggregate functions with WHERE",
    "text": "3.3 Combining aggregate functions with WHERE\nAggregate functions can be combined with the WHERE clause to gain further insights from your data.\nFor example, to get the total budget of movies made in the year 2010 or later:\n\nSELECT SUM(budget)\nFROM films\nWHERE release_year &gt;= 2010;\n\nNow it’s your turn!\nSteps\n\nUse the SUM() function to get the total amount grossed by all films made in the year 2000 or later.\n\n\nSELECT SUM(gross)\nFROM films\nWHERE release_year &gt;= 2000;\n\n\n1 records\n\n\nsum\n\n\n\n\n150900926358\n\n\n\n\n\n\nGet the average amount grossed by all films whose titles start with the letter ‘A’.\n\n\nSELECT AVG(gross)\nFROM films\nwhere title LIKE 'A%';\n\n\n1 records\n\n\navg\n\n\n\n\n47893236\n\n\n\n\n\n\nGet the amount grossed by the worst performing film in 1994.\n\n\nSELECT MIN(gross)\nFROM films\nWHERE release_year = 1994;\n\n\n1 records\n\n\nmin\n\n\n\n\n125169\n\n\n\n\n\n\nGet the amount grossed by the best performing film between 2000 and 2012, inclusive.\n\n\nSELECT MAX(gross)\nFROM films\nWHERE release_year BETWEEN 2000 AND 2012;\n\n\n1 records\n\n\nmax\n\n\n\n\n760505847\n\n\n\n\n\nNice. Can you see how SQL basically provides you a bunch of building blocks that you can combine in all kinds of ways? Hence the name: Structured Query Language.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#a-note-on-arithmetic",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#a-note-on-arithmetic",
    "title": "Introduction to SQL",
    "section": "3.4 A note on arithmetic",
    "text": "3.4 A note on arithmetic\nIn addition to using aggregate functions, you can perform basic arithmetic with symbols like +, -, *, and /.\nSo, for example, this gives a result of 12:\n\nSELECT (4 * 3);\n\nHowever, the following gives a result of 1:\n\nSELECT (4 / 3);\n\nWhat’s going on here?\nSQL assumes that if you divide an integer by an integer, you want to get an integer back. So be careful when dividing!\nIf you want more precision when dividing, you can add decimal places to your numbers. For example,\n\nSELECT (4.0 / 3.0) AS result;\n\ngives you the result you would expect: 1.333.\n\n3.5 Question\nWhat is the result of SELECT (10 / 3);?  ⬜ 2.333 ⬜ 3.333 ✅ 3 ⬜ 3.0",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#its-as-simple-as-aliasing",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#its-as-simple-as-aliasing",
    "title": "Introduction to SQL",
    "section": "3.6 It’s AS simple AS aliasing",
    "text": "3.6 It’s AS simple AS aliasing\nYou may have noticed in the first exercise of this chapter that the column name of your result was just the name of the function you used. For example,\n\nSELECT MAX(budget)\nFROM films;\n\ngives you a result with one column, named max. But what if you use two functions like this?\n\nSELECT MAX(budget), MAX(duration)\nFROM films;\n\nWell, then you’d have two columns named max, which isn’t very useful!\nTo avoid situations like this, SQL allows you to do something called aliasing. Aliasing simply means you assign a temporary name to something. To alias, you use the AS keyword, which you’ve already seen earlier in this course.\nFor example, in the above example we could use aliases to make the result clearer:\n\nSELECT MAX(budget) AS max_budget,\n       MAX(duration) AS max_duration\nFROM films;\n\nAliases are helpful for making results more readable!\nSteps\n\nGet the title and net profit (the amount a film grossed, minus its budget) for all films. Alias the net profit as net_profit.\n\n\nSELECT title, gross - budget AS net_profit\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nnet_profit\n\n\n\n\nIntolerance: Love’s Struggle Throughout the Ages\nNA\n\n\nOver the Hill to the Poorhouse\n2900000\n\n\nThe Big Parade\nNA\n\n\nMetropolis\n-5973565\n\n\nPandora’s Box\nNA\n\n\nThe Broadway Melody\n2429000\n\n\nHell’s Angels\nNA\n\n\nA Farewell to Arms\nNA\n\n\n42nd Street\n1861000\n\n\nShe Done Him Wrong\nNA\n\n\n\n\n\n\nGet the title and duration in hours for all films. The duration is in minutes, so you’ll need to divide by 60.0 to get the duration in hours. Alias the duration in hours as duration_hours.\n\n\nSELECT title, duration / 60.0 AS duration_hours\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nduration_hours\n\n\n\n\nIntolerance: Love’s Struggle Throughout the Ages\n2.050000\n\n\nOver the Hill to the Poorhouse\n1.833333\n\n\nThe Big Parade\n2.516667\n\n\nMetropolis\n2.416667\n\n\nPandora’s Box\n1.833333\n\n\nThe Broadway Melody\n1.666667\n\n\nHell’s Angels\n1.600000\n\n\nA Farewell to Arms\n1.316667\n\n\n42nd Street\n1.483333\n\n\nShe Done Him Wrong\n1.100000\n\n\n\n\n\n\nGet the average duration in hours for all films, aliased as avg_duration_hours.\n\n\nSELECT AVG(duration) / 60.0 AS avg_duration_hours  \nFROM films;\n\n\n1 records\n\n\navg_duration_hours\n\n\n\n\n1.799132",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#even-more-aliasing",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#even-more-aliasing",
    "title": "Introduction to SQL",
    "section": "3.7 Even more aliasing",
    "text": "3.7 Even more aliasing\nLet’s practice your newfound aliasing skills some more before moving on!\nRecall: SQL assumes that if you divide an integer by an integer, you want to get an integer back.\nThis means that the following will erroneously result in 400.0:\n\nSELECT 45 / 10 * 100.0;\n\nThis is because 45 / 10 evaluates to an integer (4), and not a decimal number like we would expect.\nSo when you’re dividing make sure at least one of your numbers has a decimal place:\n\nSELECT 45 * 100.0 / 10;\n\nThe above now gives the correct answer of 450.0 since the numerator (45 * 100.0) of the division is now a decimal!\nSteps\n\nGet the percentage of people who are no longer alive. Alias the result as percentage_dead. Remember to use 100.0 and not 100!\n\n\n-- get the count(deathdate) and multiply by 100.0\n-- then divide by count(*) \nSELECT COUNT(deathdate) * 100.0 / COUNT(*) AS percentage_dead\nFROM people;\n\n\n1 records\n\n\npercentage_dead\n\n\n\n\n9.372395\n\n\n\n\n\n\nGet the number of years between the newest film and oldest film. Alias the result as difference.\n\n\nSELECT MAX(release_year) - MIN(release_year)\nAS difference\nFROM films;\n\n\n1 records\n\n\ndifference\n\n\n\n\n100\n\n\n\n\n\n\nGet the number of decades the films table covers. Alias the result as number_of_decades. The top half of your fraction should be enclosed in parentheses.\n\n\nSELECT (MAX(release_year) - MIN(release_year)) / 10.0\nAS number_of_decades\nFROM films;\n\n\n1 records\n\n\nnumber_of_decades\n\n\n\n\n10\n\n\n\n\n\nWe’re at the end of chapter 3! In chapter 4, you will learn about sorting, grouping and joins. Head over there quickly!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#order-by",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#order-by",
    "title": "Introduction to SQL",
    "section": "4.1 ORDER BY",
    "text": "4.1 ORDER BY\nCongratulations on making it this far! You now know how to select and filter your results.\nIn this chapter you’ll learn how to sort and group your results to gain further insight. Let’s go!\nIn SQL, the ORDER BY keyword is used to sort results in ascending or descending order according to the values of one or more columns.\nBy default ORDER BY will sort in ascending order. If you want to sort the results in descending order, you can use the DESC keyword. For example,\n\nSELECT title\nFROM films\nORDER BY release_year DESC;\n\ngives you the titles of films sorted by release year, from newest to oldest.\n\n4.2 Question\nHow do you think ORDER BY sorts a column of text values by default?  ✅ Alphabetically (A-Z) ⬜ Reverse alphabetically (Z-A) ⬜ There’s no natural ordering to text data ⬜ By number of characters (fewest to most)",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-single-columns",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-single-columns",
    "title": "Introduction to SQL",
    "section": "4.3 Sorting single columns",
    "text": "4.3 Sorting single columns\nNow that you understand how ORDER BY works, give these exercises a go!\nSteps\n\nGet the names of people from the people table, sorted alphabetically.\n\n\nSELECT name\nFROM people\nORDER BY name;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\n50 Cent\n\n\nA. Michael Baldwin\n\n\nA. Raven Cruz\n\n\nA.J. Buckley\n\n\nA.J. DeLucia\n\n\nA.J. Langer\n\n\nAJ Michalka\n\n\nAaliyah\n\n\nAaron Ashmore\n\n\nAaron Hann\n\n\n\n\n\n\nGet the names of people, sorted by birth date.\n\n\nSELECT name\nFROM people\nORDER BY birthdate;\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nRobert Shaw\n\n\nLucille La Verne\n\n\nMary Carr\n\n\nD.W. Griffith\n\n\nFinlay Currie\n\n\nLionel Barrymore\n\n\nBilly Gilbert\n\n\nCecil B. DeMille\n\n\nLeopold Stokowski\n\n\nÉric Tessier\n\n\n\n\n\n\nGet the birth date and name for every person, in order of when they were born.\n\n\nSELECT birthdate, name\nFROM people\nORDER BY birthdate;\n\n\nDisplaying records 1 - 10\n\n\nbirthdate\nname\n\n\n\n\n1837-10-10\nRobert Shaw\n\n\n1872-11-07\nLucille La Verne\n\n\n1874-03-14\nMary Carr\n\n\n1875-01-22\nD.W. Griffith\n\n\n1878-01-20\nFinlay Currie\n\n\n1878-04-28\nLionel Barrymore\n\n\n1880-03-21\nBilly Gilbert\n\n\n1881-08-12\nCecil B. DeMille\n\n\n1882-04-18\nLeopold Stokowski\n\n\n1883-05-28\nÉric Tessier",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-single-columns-2",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-single-columns-2",
    "title": "Introduction to SQL",
    "section": "4.4 Sorting single columns (2)",
    "text": "4.4 Sorting single columns (2)\nLet’s get some more practice with ORDER BY!\nSteps\n\nGet the title of films released in 2000 or 2012, in the order they were released.\n\n\nSELECT title\nFROM films\nWHERE release_year IN (2000, 2012)\nORDER BY release_year;\n\n\nDisplaying records 1 - 10\n\n\ntitle\n\n\n\n\n102 Dalmatians\n\n\n28 Days\n\n\n3 Strikes\n\n\nAberdeen\n\n\nAll the Pretty Horses\n\n\nAlmost Famous\n\n\nAmerican Psycho\n\n\nAmores Perros\n\n\nAn Everlasting Piece\n\n\nAnatomy\n\n\n\n\n\n\nGet all details for all films except those released in 2015 and order them by duration.\n\n\nSELECT *\nFROM films\nWHERE release_year &lt;&gt; 2015\nORDER BY duration;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nrelease_year\ncountry\nduration\nlanguage\ncertification\ngross\nbudget\n\n\n\n\n2926\nThe Touch\n2007\nUSA\n7\nEnglish\nNA\nNA\n13000\n\n\n4098\nVessel\n2012\nUSA\n14\nEnglish\nNA\nNA\nNA\n\n\n2501\nWal-Mart: The High Cost of Low Price\n2005\nUSA\n20\nEnglish\nNot Rated\nNA\n1500000\n\n\n566\nMarilyn Hotchkiss’ Ballroom Dancing and Charm School\n1990\nUSA\n34\nEnglish\nNA\n333658\n34000\n\n\n2829\nJesus People\n2007\nUSA\n35\nEnglish\nNA\nNA\nNA\n\n\n462\nEvil Dead II\n1987\nUSA\n37\nEnglish\nX\n5923044\n3600000\n\n\n3579\nSea Rex 3D: Journey to a Prehistoric World\n2010\nUK\n41\nEnglish\nNA\n4074023\n5000000\n\n\n2985\nDolphins and Whales 3D: Tribes of the Ocean\n2008\nUK\n42\nEnglish\nNA\n7518876\n6000000\n\n\n2997\nFlame and Citron\n2008\nDenmark\n45\nDanish\nNot Rated\n145109\n45000000\n\n\n4358\nAlpha and Omega 4: The Legend of the Saw Toothed Cave\n2014\nUSA\n45\nNA\nNA\nNA\n7000000\n\n\n\n\n\n\nGet the title and gross earnings for movies which begin with the letter ‘M’ and order the results alphabetically.\n\n\nSELECT title, gross\nFROM films\nWHERE title LIKE 'M%'\nORDER BY title;\n\n\nDisplaying records 1 - 10\n\n\ntitle\ngross\n\n\n\n\nMacGruber\n8460995\n\n\nMachete\n26589953\n\n\nMachete Kills\n7268659\n\n\nMachine Gun McCain\nNA\n\n\nMachine Gun Preacher\n537580\n\n\nMad City\n10556196\n\n\nMad Hot Ballroom\n8044906\n\n\nMad Max\nNA\n\n\nMad Max 2: The Road Warrior\n9003011\n\n\nMad Max Beyond Thunderdome\n36200000\n\n\n\n\n\nCan you feel the SQL power dawn on you?!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-single-columns-desc",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-single-columns-desc",
    "title": "Introduction to SQL",
    "section": "4.5 Sorting single columns (DESC)",
    "text": "4.5 Sorting single columns (DESC)\nTo order results in descending order, you can put the keyword DESC after your ORDER BY. For example, to get all the names in the people table, in reverse alphabetical order:\n\nSELECT name\nFROM people\nORDER BY name DESC;\n\nNow practice using ORDER BY with DESC to sort single columns in descending order!\nSteps\n\nGet the IMDB score and film ID for every film from the reviews table, sorted from highest to lowest score.\n\n\nSELECT imdb_score, film_id\nFROM reviews\nORDER BY imdb_score DESC;\n\n\nDisplaying records 1 - 10\n\n\nimdb_score\nfilm_id\n\n\n\n\n9.5\n4960\n\n\n9.3\n742\n\n\n9.2\n178\n\n\n9.1\n4866\n\n\n9.0\n3110\n\n\n9.0\n192\n\n\n8.9\n676\n\n\n8.9\n69\n\n\n8.9\n120\n\n\n8.9\n723\n\n\n\n\n\n\nGet the title for every film, in reverse order.\n\n\nSELECT title\nFROM films\nORDER BY title DESC;\n\n\nDisplaying records 1 - 10\n\n\ntitle\n\n\n\n\nÆon Flux\n\n\nxXx: State of the Union\n\n\nxXx\n\n\neXistenZ\n\n\n[Rec] 2\n\n\n[Rec]\n\n\nZulu\n\n\nZoom\n\n\nZoolander 2\n\n\nZoolander\n\n\n\n\n\n\nGet the title and duration for every film, in order of longest duration to shortest.\n\n\nSELECT title, duration\nFROM films\nORDER BY duration DESC;\n\n\nDisplaying records 1 - 10\n\n\ntitle\nduration\n\n\n\n\nDestiny\nNA\n\n\nShould’ve Been Romeo\nNA\n\n\nHum To Mohabbat Karega\nNA\n\n\nHarry Potter and the Deathly Hallows: Part I\nNA\n\n\nBarfi\nNA\n\n\nRomantic Schemer\nNA\n\n\nWolf Creek\nNA\n\n\nDil Jo Bhi Kahey…\nNA\n\n\nThe Naked Ape\nNA\n\n\nBlack Water Transit\nNA\n\n\n\n\n\nNice. Let’s explore how you can sort multiple columns!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-multiple-columns",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#sorting-multiple-columns",
    "title": "Introduction to SQL",
    "section": "4.6 Sorting multiple columns",
    "text": "4.6 Sorting multiple columns\nORDER BY can also be used to sort on multiple columns. It will sort by the first column specified, then sort by the next, then the next, and so on. For example,\n\nSELECT birthdate, name\nFROM people\nORDER BY birthdate, name;\n\nsorts on birth dates first (oldest to newest) and then sorts on the names in alphabetical order. The order of columns is important!\nTry using ORDER BY to sort multiple columns! Remember, to specify multiple columns you separate the column names with a comma.\nSteps\n\nGet the birth date and name of people in the people table, in order of when they were born and alphabetically by name.\n\n\nSELECT birthdate, name\nFROM people\nORDER BY birthdate, name;\n\n\nDisplaying records 1 - 10\n\n\nbirthdate\nname\n\n\n\n\n1837-10-10\nRobert Shaw\n\n\n1872-11-07\nLucille La Verne\n\n\n1874-03-14\nMary Carr\n\n\n1875-01-22\nD.W. Griffith\n\n\n1878-01-20\nFinlay Currie\n\n\n1878-04-28\nLionel Barrymore\n\n\n1880-03-21\nBilly Gilbert\n\n\n1881-08-12\nCecil B. DeMille\n\n\n1882-04-18\nLeopold Stokowski\n\n\n1883-05-28\nÉric Tessier\n\n\n\n\n\n\nGet the release year, duration, and title of films ordered by their release year and duration.\n\n\nSELECT release_year, duration, title\nFROM films\nORDER BY release_year, duration;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\nrelease_year\nduration\ntitle\n\n\n\n\n1916\n123\nIntolerance: Love’s Struggle Throughout the Ages\n\n\n1920\n110\nOver the Hill to the Poorhouse\n\n\n1925\n151\nThe Big Parade\n\n\n1927\n145\nMetropolis\n\n\n1929\n100\nThe Broadway Melody\n\n\n1929\n110\nPandora’s Box\n\n\n1930\n96\nHell’s Angels\n\n\n1932\n79\nA Farewell to Arms\n\n\n1933\n66\nShe Done Him Wrong\n\n\n1933\n89\n42nd Street\n\n\n\n\n\n\nGet certifications, release years, and titles of films ordered by certification (alphabetically) and release year.\n\n\nSELECT certification, release_year, title\nFROM films\nORDER BY certification, release_year;\n\n\nDisplaying records 1 - 10\n\n\ncertification\nrelease_year\ntitle\n\n\n\n\nApproved\n1933\nShe Done Him Wrong\n\n\nApproved\n1935\nTop Hat\n\n\nApproved\n1936\nThe Charge of the Light Brigade\n\n\nApproved\n1937\nSnow White and the Seven Dwarfs\n\n\nApproved\n1937\nThe Prisoner of Zenda\n\n\nApproved\n1938\nYou Can’t Take It with You\n\n\nApproved\n1938\nAlexander’s Ragtime Band\n\n\nApproved\n1940\nPinocchio\n\n\nApproved\n1940\nThe Blue Bird\n\n\nApproved\n1941\nHow Green Was My Valley\n\n\n\n\n\n\nGet the names and birthdates of people ordered by name and birth date.\n\n\nSELECT name, birthdate\nFROM people\nORDER BY name, birthdate;\n\n\nDisplaying records 1 - 10\n\n\nname\nbirthdate\n\n\n\n\n50 Cent\n1975-07-06\n\n\nA. Michael Baldwin\n1963-04-04\n\n\nA. Raven Cruz\nNA\n\n\nA.J. Buckley\n1978-02-09\n\n\nA.J. DeLucia\nNA\n\n\nA.J. Langer\n1974-05-22\n\n\nAJ Michalka\n1991-04-10\n\n\nAaliyah\n1979-01-16\n\n\nAaron Ashmore\n1979-10-07\n\n\nAaron Hann\nNA\n\n\n\n\n\nWell done. Notice how the second column you order on only steps in when the first column is not decisive to tell the order. The second column acts as a tie breaker.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#group-by",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#group-by",
    "title": "Introduction to SQL",
    "section": "4.7 GROUP BY",
    "text": "4.7 GROUP BY\nNow you know how to sort results! Often you’ll need to aggregate results. For example, you might want to count the number of male and female employees in your company. Here, what you want is to group all the males together and count them, and group all the females together and count them. In SQL, GROUP BY allows you to group a result by one or more columns, like so:\n\nSELECT sex, count(*)\nFROM employees\nGROUP BY sex;\n\nThis might give, for example:\n\n\n\nsex\ncount\n\n\n\n\nmale\n15\n\n\nfemale\n19\n\n\n\nCommonly, GROUP BY is used with aggregate functions like COUNT() or MAX(). Note that GROUP BY always goes after the FROM clause!\n\n4.8 Question\nWhat is GROUP BY used for?  ⬜ Performing operations by column ⬜ Performing operations all at once ⬜ Performing operations in a particular order ✅ Performing operations by group\n\nCorrect! GROUP BY is for performing operations within groups.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#group-by-practice",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#group-by-practice",
    "title": "Introduction to SQL",
    "section": "4.9 GROUP BY practice",
    "text": "4.9 GROUP BY practice\nAs you’ve just seen, combining aggregate functions with GROUP BY can yield some powerful results!\nA word of warning: SQL will return an error if you try to SELECT a field that is not in your GROUP BY clause without using it to calculate some kind of value about the entire group.\nNote that you can combine GROUP BY with ORDER BY to group your results, calculate something about them, and then order your results. For example,\n\nSELECT sex, count(*)\nFROM employees\nGROUP BY sex\nORDER BY count DESC;\n\nmight return something like\n\n\n\nsex\ncount\n\n\n\n\nfemale\n19\n\n\nmale\n15\n\n\n\nbecause there are more females at our company than males. Note also that ORDER BY always goes after GROUP BY. Let’s try some exercises!\nSteps\n\nGet the release year and count of films released in each year.\n\n\nSELECT release_year, COUNT(*)\nFROM films\nGROUP BY release_year;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\ncount\n\n\n\n\n1964\n10\n\n\n1969\n10\n\n\nNA\n42\n\n\n1991\n31\n\n\n1989\n33\n\n\n1974\n9\n\n\n1977\n16\n\n\n1971\n11\n\n\n1935\n1\n\n\n1983\n22\n\n\n\n\n\n\nGet the release year and average duration of all films, grouped by release year.\n\n\nSELECT release_year, AVG(duration)\nFROM films\nGROUP BY release_year;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\navg\n\n\n\n\n1964\n119.40000\n\n\n1969\n126.00000\n\n\nNA\n77.43902\n\n\n1991\n113.06452\n\n\n1989\n113.12121\n\n\n1974\n113.77778\n\n\n1977\n112.81250\n\n\n1971\n105.54545\n\n\n1935\n81.00000\n\n\n1983\n114.09091\n\n\n\n\n\n\nGet the release year and largest budget for all films, grouped by release year.\n\n\nSELECT release_year, MAX(budget)\nFROM films\nGROUP BY release_year;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\nmax\n\n\n\n\n1964\n19000000\n\n\n1969\n20000000\n\n\nNA\n15000000\n\n\n1991\n102000000\n\n\n1989\n69500000\n\n\n1974\n13000000\n\n\n1977\n26000000\n\n\n1971\n9000000\n\n\n1935\n609000\n\n\n1983\n39000000\n\n\n\n\n\n\nGet the IMDB score and count of film reviews grouped by IMDB score in the reviews table.\n\n\nSELECT imdb_score, COUNT(*)\nFROM reviews\nGROUP BY imdb_score;\n\n\nDisplaying records 1 - 10\n\n\nimdb_score\ncount\n\n\n\n\n8.7\n11\n\n\n9.3\n1\n\n\n4.4\n25\n\n\n2.1\n3\n\n\n3.0\n5\n\n\n8.4\n22\n\n\n1.6\n1\n\n\n4.3\n31\n\n\n9.5\n1\n\n\n2.8\n9\n\n\n\n\n\nNow that you’ve accustomed yourself with GROUP BY, let’s throw it in the mix with other SQL constructs you already know!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#group-by-practice-2",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#group-by-practice-2",
    "title": "Introduction to SQL",
    "section": "4.10 GROUP BY practice (2)",
    "text": "4.10 GROUP BY practice (2)\nNow practice your new skills by combining GROUP BY and ORDER BY with some more aggregate functions!\nMake sure to always put the ORDER BY clause at the end of your query. You can’t sort values that you haven’t calculated yet!\nSteps\n\nGet the release year and lowest gross earnings per release year.\n\n\nSELECT release_year, MIN(gross)\nFROM films\nGROUP BY release_year;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\nmin\n\n\n\n\n1964\n12438\n\n\n1969\n26893\n\n\nNA\n145118\n\n\n1991\n869325\n\n\n1989\n792966\n\n\n1974\n21000000\n\n\n1977\n1000000\n\n\n1971\n8231\n\n\n1935\n3000000\n\n\n1983\n12200000\n\n\n\n\n\n\nGet the language and total gross amount films in each language made.\n\n\nSELECT language, SUM(gross)\nFROM films\nGROUP BY language;\n\n\nDisplaying records 1 - 10\n\n\nlanguage\nsum\n\n\n\n\nNA\n4319281\n\n\nArabic\n1681831\n\n\nKannada\nNA\n\n\nCzech\n617228\n\n\nKorean\n6603670\n\n\nPersian\n9137632\n\n\nChinese\n50000\n\n\nNorwegian\n1804549\n\n\nEnglish\n201364863894\n\n\nGreek\n110197\n\n\n\n\n\n\nGet the country and total budget spent making movies in each country.\n\n\nSELECT country, SUM(budget)\nFROM films\nGROUP BY country;\n\n\nDisplaying records 1 - 10\n\n\ncountry\nsum\n\n\n\n\nNA\n3500000\n\n\nIndonesia\n1100000\n\n\nCameroon\nNA\n\n\nCzech Republic\n146450000\n\n\nSweden\n50400000\n\n\nDominican Republic\n500000\n\n\nCambodia\nNA\n\n\nIreland\n99190000\n\n\nFinland\n3850000\n\n\nColombia\n3000000\n\n\n\n\n\n\nGet the release year, country, and highest budget spent making a film for each year, for each country. Sort your results by release year and country.\n\n\nSELECT release_year, country, MAX(budget)\nFROM films\nGROUP BY release_year, country\nORDER BY release_year, country;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\ncountry\nmax\n\n\n\n\n1916\nUSA\n385907\n\n\n1920\nUSA\n100000\n\n\n1925\nUSA\n245000\n\n\n1927\nGermany\n6000000\n\n\n1929\nGermany\nNA\n\n\n1929\nUSA\n379000\n\n\n1930\nUSA\n3950000\n\n\n1932\nUSA\n800000\n\n\n1933\nUSA\n439000\n\n\n1934\nUSA\n325000\n\n\n\n\n\n\nGet the country, release year, and lowest amount grossed per release year per country. Order your results by country and release year.\n\n\nSELECT country, release_year, MIN(gross)\nFROM films\nGROUP BY country, release_year\nORDER BY country, release_year;\n\n\nDisplaying records 1 - 10\n\n\ncountry\nrelease_year\nmin\n\n\n\n\nAfghanistan\n2003\n1127331\n\n\nArgentina\n2000\n1221261\n\n\nArgentina\n2004\n304124\n\n\nArgentina\n2009\n20167424\n\n\nAruba\n1998\n10076136\n\n\nAustralia\n1979\nNA\n\n\nAustralia\n1981\n9003011\n\n\nAustralia\n1982\nNA\n\n\nAustralia\n1985\n36200000\n\n\nAustralia\n1986\n174635000\n\n\n\n\n\nOff to the next statement!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#having-a-great-time",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#having-a-great-time",
    "title": "Introduction to SQL",
    "section": "4.11 HAVING a great time",
    "text": "4.11 HAVING a great time\nIn SQL, aggregate functions can’t be used in WHERE clauses. For example, the following query is invalid:\n\nSELECT release_year\nFROM films\nGROUP BY release_year\nWHERE COUNT(title) &gt; 10;\n\nThis means that if you want to filter based on the result of an aggregate function, you need another way! That’s where the HAVING clause comes in. For example,\n\nSELECT release_year\nFROM films\nGROUP BY release_year\nHAVING COUNT(title) &gt; 10;\n\nshows only those years in which more than 10 films were released.\n\n4.12 Question\nIn how many different years were more than 200 movies released?  ⬜ 2 ✅ 13 ⬜ 44 ⬜ 63",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#all-together-now",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#all-together-now",
    "title": "Introduction to SQL",
    "section": "4.13 All together now",
    "text": "4.13 All together now\nTime to practice using ORDER BY, GROUP BY and HAVING together.\nNow you’re going to write a query that returns the average budget and average gross earnings for films in each year after 1990, if the average budget is greater than $60 million.\nThis is going to be a big query, but you can handle it!\nSteps\n\nGet the release year, budget and gross earnings for each film in the films table.\n\n\nSELECT release_year, budget, gross\nFROM films;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\nbudget\ngross\n\n\n\n\n1916\n385907\nNA\n\n\n1920\n100000\n3000000\n\n\n1925\n245000\nNA\n\n\n1927\n6000000\n26435\n\n\n1929\nNA\n9950\n\n\n1929\n379000\n2808000\n\n\n1930\n3950000\nNA\n\n\n1932\n800000\nNA\n\n\n1933\n439000\n2300000\n\n\n1933\n200000\nNA\n\n\n\n\n\n\nModify your query so that only records with a release_year after 1990 are included.\n\n\nSELECT release_year, budget, gross\nFROM films\nWHERE release_year &gt; 1990;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\nbudget\ngross\n\n\n\n\n1991\n6000000\n869325\n\n\n1991\n20000000\n38037513\n\n\n1991\n6000000\n57504069\n\n\n1991\n35000000\n79100000\n\n\n1991\n15000000\n30102717\n\n\n1991\n35000000\n14587732\n\n\n1991\n8500000\n34872293\n\n\n1991\n23000000\n7434726\n\n\n1991\n70000000\n119654900\n\n\n1991\n5000000\n19281235\n\n\n\n\n\n\nRemove the budget and gross columns, and group your results by release year.\n\n\nSELECT release_year\nFROM films\nWHERE release_year &gt; 1990\nGROUP BY release_year;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\n\n\n\n\n1991\n\n\n2009\n\n\n2013\n\n\n2003\n\n\n1997\n\n\n2016\n\n\n1994\n\n\n2014\n\n\n2000\n\n\n2006\n\n\n\n\n\n\nModify your query to include the average budget and average gross earnings for the results you have so far. Alias the average budget as avg_budget; alias the average gross earnings as avg_gross.\n\n\nSELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross\nFROM films\nWHERE release_year &gt; 1990\nGROUP BY release_year;\n\n\nDisplaying records 1 - 10\n\n\nrelease_year\navg_budget\navg_gross\n\n\n\n\n1991\n25176548\n53844502\n\n\n2009\n37073287\n46207440\n\n\n2013\n40519045\n56158358\n\n\n2003\n37208649\n48727747\n\n\n1997\n59424491\n44793772\n\n\n2016\n56642742\n76924036\n\n\n1994\n29013774\n59395666\n\n\n2014\n35325799\n62412137\n\n\n2000\n34931376\n42172628\n\n\n2006\n93968930\n39237856\n\n\n\n\n\n\nModify your query so that only years with an average budget of greater than $60 million are included.\n\n\nSELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross\nFROM films\nWHERE release_year &gt; 1990\nGROUP BY release_year\nHAVING AVG(budget) &gt; 60000000;\n\n\n2 records\n\n\nrelease_year\navg_budget\navg_gross\n\n\n\n\n2006\n93968930\n39237856\n\n\n2005\n70323938\n41159143\n\n\n\n\n\n\nFinally, modify your query to order the results from highest average gross earnings to lowest.\n\n\nSELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross\nFROM films\nWHERE release_year &gt; 1990\nGROUP BY release_year\nHAVING AVG(budget) &gt; 60000000\nORDER BY avg_gross DESC;\n\n\n2 records\n\n\nrelease_year\navg_budget\navg_gross\n\n\n\n\n2005\n70323938\n41159143\n\n\n2006\n93968930\n39237856\n\n\n\n\n\nWooooow! Let’s do another one!",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#all-together-now-2",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#all-together-now-2",
    "title": "Introduction to SQL",
    "section": "4.14 All together now (2)",
    "text": "4.14 All together now (2)\nGreat work! Now try another large query. This time, all in one go!\nRemember, if you only want to return a certain number of results, you can use the LIMIT keyword to limit the number of rows returned\nSteps\n\nGet the country, average budget, and average gross take of countries that have made more than 10 films. Order the result by country name, and limit the number of results displayed to 5. You should alias the averages as avg_budget and avg_gross respectively.\n\n\n-- select country, average budget, \n-- and average gross\nSELECT country, AVG(budget) AS avg_budget, \n       AVG(gross) AS avg_gross\n-- from the films table\nFROM films\n-- group by country \nGROUP BY country\n-- where the country has more than 10 titles\nHAVING COUNT(title) &gt; 10\n-- order by country\nORDER BY country\n-- limit to only show 5 results\nLIMIT 5;\n\n\n5 records\n\n\ncountry\navg_budget\navg_gross\n\n\n\n\nAustralia\n31172110\n40205910\n\n\nCanada\n14798459\n22432067\n\n\nChina\n62219000\n14143041\n\n\nDenmark\n13922222\n1418469\n\n\nFrance\n30672035\n16350594\n\n\n\n\n\nSuperb work on a selection saga! SELECT queries can get rather long, but breaking them down into individual clauses makes them easier to write.",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "content/sql/introduction_to_sql/introduction_to_sql.html#a-taste-of-things-to-come",
    "href": "content/sql/introduction_to_sql/introduction_to_sql.html#a-taste-of-things-to-come",
    "title": "Introduction to SQL",
    "section": "4.15 A taste of things to come",
    "text": "4.15 A taste of things to come\nCongrats on making it to the end of the course! By now you should have a good understanding of the basics of SQL.\nThere’s one more concept we’re going to introduce. You may have noticed that all your results so far have been from just one table, e.g., films or people.\nIn the real world however, you will often want to query multiple tables. For example, what if you want to see the IMDB score for a particular movie?\nIn this case, you’d want to get the ID of the movie from the films table and then use it to get IMDB information from the reviews table. In SQL, this concept is known as a join, and a basic join is shown in the editor to the right.\nThe query in the editor gets the IMDB score for the film To Kill a Mockingbird! Cool right?\nAs you can see, joins are incredibly useful and important to understand for anyone using SQL.\nWe have a whole follow-up course dedicated to them called Joining Data in SQL for you to hone your database skills further!\nSteps\n\nSubmit the code in the editor and inspect the results.\n\n\nSELECT title, imdb_score\nFROM films\nJOIN reviews\nON films.id = reviews.film_id\nWHERE title = 'To Kill a Mockingbird';\n\n\n1 records\n\n\ntitle\nimdb_score\n\n\n\n\nTo Kill a Mockingbird\n8.4\n\n\n\n\n\n\n4.16 Question\nWhat is the IMDB score for the film To Kill a Mockingbird?  ⬜ 8.1 ✅ 8.4 ⬜ 7.7 ⬜ 9.3",
    "crumbs": [
      "SQL",
      "Introduction to SQL"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joschka Schwarz",
    "section": "",
    "text": "Hi There! Thanks for stopping by my little corner of the internet! My name is Joschka Schwarz and I am excited to meet you. Below you will find my current CV.\n\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        \n          \n          LinkedIn\n        \n        \n          \n          Github\n        \n        \n          \n          Email\n        \n        \n          \n          Download CV\n        \n      \n      \n        \n          8 years’ experience working with data. Expertise in quantitative modeling, growing people and making data-driven decisions. Currently working as a Research Associate (Ph.D. Candidate) at the intersection of computer science, statistics and the social sciences as member of both the Entrepreneurship & Data Science groups.\n        \n      \n    \n  \n\n\n  \n\n\n  \n    \n    \n      \n        \n          \n            \n              \n            \n            \n              Industry Experience\n            \n          \n        \n        \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Consultant | Research Associate\n                      \n                        Hamburg University of Technology | Startup Dock | Tutech\n                      \n                      \n                        Jun 2019 - Present\n                        \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Led multifaceted data-driven initiatives, combining expertise in data science, business strategy and project management to deliver innovative solutions across consulting, education and research domains.\n                    \n                      Consulting: Spearheaded strategic and operational consulting projects using agile methodologies, big data analytics and AI-driven insights, fostering continuous improvement in fast-paced environments.\n                      Project Management: Orchestrated the SMILE project, empowering start-ups and scale-ups in smart manufacturing. This initiative fostered cross-border collaboration among multiple European participants.\n                      Teaching: Designed and delivered courses on Business Innovation and Data-Driven Decision Making, incorporating hands-on projects in data analysis, machine learning, reporting and dashboard creation.\n                      Research: Conducted comprehensive analysis of AI trends and Venture Capital investments, providing valuable insights for strategic decision-making in the rapidly evolving tech startup ecosystem.\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Innovation & Technology Project Lead\n                      \n                        Lufthansa Technik AG\n                      \n                      \n                        Mar 2017 - Jun 2019\n                        \n                        Miama, USA & Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Led the design, development and integration of a custom AI-driven solution for the Aviation industry in collaboration with IBM Watson, enhancing cost efficiency and response time for critical Aircraft on Ground (AOG) processes.\n                    \n                      Implemented NLP and text mining capabilities using IBM Watson Studio to streamline sales workflows\n                      Facilitated Design Thinking workshops with the IBM DSE teams to align solution design with business needs\n                      Managed the scale-up from PoC to full deployment, ensuring client satisfaction and successful rollout\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Management Consulting, intern\n                      \n                        Horváth\n                      \n                      Oct 2016 – Feb 2017 \n                        Frankfurt on the Main, Germany\n                      \n                    \n                  \n                  \n                  Managed the PMO for a large-scale project developing new location concepts for Deutsche Bahn’s maintenance depots, aiming to enhance equipment reliability, efficiency and availability.\n                    \n                      Led agile PMO operations, including reporting, budget tracking and managing timelines and resources\n                      Collaborated with stakeholders to align project goals with operational needs and business objectives\n                      Conducted profitability assessments and capacity analyses for each site, informing strategic decisions\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Student assistant\n                      \n                        Fraunhofer-Institut für Lasertechnik ILT\n                      \n                      Oct 2014 – Sep 2016 \n                        Aachen, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Operational Excellence, intern\n                      \n                        Audi AG\n                      \n                      Sep 2013 – Dec 2013 \n                        Neckarsulm, Germany\n                      \n                    \n                  \n                  \n                    As the Lean Manufacturing Team coordinator, I facilitated the adoption of Lean Management principles and operational excellence strategies at AUDI, focusing on the Audi Production System (APS).\n                    \n                      Taught lean methodologies, focusing on problem-solving, logistics and process synchronization\n                      Planned and moderated shopfloor management projects across both direct and indirect operational areas\n                      Enhanced processes through detailed process mapping and value stream analysis\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Student assistant\n                      \n                        Fraunhofer-Institut für Produktionstechnologie IPT\n                      \n                      Nov 2012 – Jul 2013 \n                        Aachen, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n  \n  \n    \n    \n      \n        \n          \n            \n              \n            \n            \n              Teaching Experience\n            \n          \n        \n        \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Data Science\n                      \n                        Hamburg University of Technology\n                      \n                      Jan 2023 – Mar 2023 \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Designed and instructed business-oriented Data Science and Machine Learning courses for diverse organizations, covering data analysis, predictive modeling, statistics and advanced ML algorithms:\n                    \n                      Business Data Science Basics\n                      Business Decisions with Machine Learning\n                      Building Business Data Products\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Data Science certificate program\n                      \n                        Northern Institute of Technology Management\n                      \n                      Jun 2020\n                        Hamburg, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Pytorch Geometric\n                      \n                        MLE-Days\n                      \n                      Jun 2021 \n                        Hamburg, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Entrepreneurship & Innovation\n                      \n                        Hamburg University of Technology\n                      \n                      Sep 2018 – Present \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Instructing students on essential skills for launching tech startups, covering ideation, market research, product development, marketing, financial planning and entrepreneurial mindset cultivation:\n                    \n                      Startup Engineering\n                      Technology Entrepreunrship\n                      Sustainable Entrepreunrship\n                      Creation of Business Opportunities\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n  \n  \n    \n    \n      \n        \n           Education\n        \n        \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Visiting Research Scholar\n                      \n                        RMIT Melbourne\n                      \n                      Jan 2023 – Mar 2023 \n                        Melbourne, Australia\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Ph.D. Thesis in Innovation Management & Entrepreneurship Science\n                      \n                        Hamburg University of Technology\n                      \n                      Jun 2018 – Present \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Quantitative dissertation (using SQL, R, Python) on the influence of social, structural and reputational effects on entrepreuneurial success.\n                    \n                      Creation of various databases by scraping different data providers and collaborative platforms\n                      Natural language processing on linkedin and startup databases to extract features for network analysis\n                      Machine Learning and statistical analysis on large (&gt; 5 billion records) github and stackoverflow datasets\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Visiting Research Scholar\n                      \n                        Maastricht University\n                      \n                      Feb 2016 – Jun 2016 \n                        Maastricht, Netherlands\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      M.Sc. in Business Administration & Engineering\n                      \n                        RWTH Aachen\n                      \n                      Apr 2015 – Mar 2018 \n                        Aachen, Germany\n                      \n                    \n                  \n                  \n                    \n                      Grade: 1.6 (Selected to the Dean’s List)\n                      Finished within the designated period of study\n                      Engineering specialization: Iron Metallurgy\n                      Economics specialization: Supply Chain Management & Controlling\n                      Final Thesis: The impact of an artificial intelligence-based e-mail parsing system on the aircraft spare parts quotation process - a Lufthansa Technik AG case\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Visiting Research Scholar\n                      \n                        Polytechnic University of Valencia\n                      \n                      Feb 2014 – Jun 2014 \n                        Valencia, Spain\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      B.Sc. in Business Administration & Engineering\n                      \n                        RWTH Aachen\n                      \n                      Oct 2011 – Mar 2015 \n                        Aachen, Germany\n                      \n                    \n                  \n                  Final Thesis: Low distortion softening of flanges on b-pillars made of press-hardened steel through local heat treatment with laser radiation (in cooperation with Fraunhofer ILT)\n                \n              \n            \n          \n        \n      \n    \n  \n  \n    \n    \n      \n        \n          Accomplish­ments\n        \n        \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Statistician with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n              \n                Course List. Click to expand!\n                  \n                    \n                      1. Introduction to Statistics in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      2. Foundations of Probability in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      3. Introduction to Regression in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      4. Intermediate Regression in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      5. Generalized Linear Models in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      6. Modeling with Data in the Tidyverse\n                    \n                    See certificate\n                  \n                  \n                    \n                      7. Sampling in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      8. Hypothesis Testing in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      9. Experimental Design in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      10. Introduction to A/B Testing in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      11. Dealing With Missing Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      12. Handling Missing Data with Imputations in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      13. Analyzing Survey Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      14. Survey and Measurement Development in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      15. Hierarchical and Mixed Effects Models in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      16. Survival Analysis in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      17. Mixture Models in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      18. Fundamentals of Bayesian Data Analysis in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      19. Bayesian Regression Modeling with rstanarm\n                    \n                    See certificate\n                  \n                  \n                    \n                      20. Bayesian Modeling with RJAGS\n                    \n                    See certificate\n                  \n                  \n                    \n                      21. Factor Analysis in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      22. Structural Equation Modeling with lavaan in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      23. Foundations of Inference in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      24. Inference for Categorical Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      25. Inference for Numerical Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      26. Inference for Linear Regression in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      27. Data Privacy and Anonymization in R\n                    \n                    See certificate\n                  \n              \n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Machine Learning Scientist with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n              \n                Course List. Click to expand!\n                  \n                    Introduction to Statistics in R\n                    See certificate\n                  \n                  \n                    table_row_1\n                    See certificate\n                  \n                  \n                    table_row_1\n                    See certificate\n                  \n              \n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Data Scientist with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Data Analyst with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  \n                    DS4B 101-R: Business Analysis With R\n                  \n                  \n                    business-science\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              A 7-week curriculum that methodically teaches the foundations of data science using R & tidyverse. \n                  \n                    Data Import: readr & odbc\n                  \n                  \n                    Data Cleaning & Wrangling: dplyr & tidyr\n                  \n                  \n                    Time Series, Text, & Categorical Data: lubridate, stringr, & forcats\n                  \n                  \n                    Visualization: ggplot2\n                  \n                  \n                    Functions & Iteration: purrr\n                  \n                  \n                    Modeling & Machine Learning: parnsip (xgboost, glmnet, kernlab, broom, & more)\n                  \n                  \n                    Business Reporting: rmarkdown\n                  \n                \n              \n              See certificate\n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  \n                    DS4B 201-R: Data Science For Business With R\n                  \n                  \n                    business-science\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              A 10-week curriculum that incorporates R & H2O AutoML to use machine learning within a business problem solving framework called the BSPF\n              See certificate"
  },
  {
    "objectID": "projects/classic/index.html",
    "href": "projects/classic/index.html",
    "title": "Getting started with paged.js",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "projects/spotify/index.html",
    "href": "projects/spotify/index.html",
    "title": "Advanced paged.js",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#table-of-contents",
    "href": "revealjs/slides/2023/cs/cs.html#table-of-contents",
    "title": "CoreSignal Analysis",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#load-init-companies-crunchbase-pitchbook-coresignal",
    "href": "revealjs/slides/2023/cs/cs.html#load-init-companies-crunchbase-pitchbook-coresignal",
    "title": "CoreSignal Analysis",
    "section": "Load & Init Companies (CrunchBase, PitchBook & CoreSignal)",
    "text": "Load & Init Companies (CrunchBase, PitchBook & CoreSignal)\n\nCBPB: SELECTCBPB: WRANGLECBPB: CLEANCS: SELECTCS: WRANGLECS: CLEAN\n\n\nCrunchbase data contains 150,838 startups with a valid funding trajectory.\n\np_load(arrow, dplyr, tidyr)\n\nfunded_companies_prqt &lt;- open_dataset(\"funded_companies_identifiers.parquet\") \nfunded_companies_prqt\n\n\n\n#&gt; # A tibble: 150,838 × 3\n#&gt;   company_id            domain linkedin_url                                     \n#&gt;        &lt;int&gt; &lt;list&lt;character&gt;&gt; &lt;chr&gt;                                            \n#&gt; 1          1               [1] &lt;NA&gt;                                             \n#&gt; 2          2               [1] https://www.linkedin.com/company/luna-pharmaceut…\n#&gt; 3          3               [1] http://www.linkedin.com/company/chainsync        \n#&gt; # ℹ 150,835 more rows\n\n\n\nMultiple domains (Unnesting via Arrow not possible. Options: Spark & sparklyr.nested):\n\nfc_unnested_tbl &lt;- funded_companies_prqt |&gt; collect() |&gt; \n                      # 1. Allow multiple domains per company. No multiple linkedin handles.\n                      unnest(domain) \nfc_unnested_tbl\n\n#&gt; # A tibble: 155,413 × 3\n#&gt;   company_id domain              linkedin_url                                   \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                                          \n#&gt; 1          1 zana.io             &lt;NA&gt;                                           \n#&gt; 2          2 premamawellness.com https://www.linkedin.com/company/luna-pharmace…\n#&gt; 3          3 chainsync.com       http://www.linkedin.com/company/chainsync      \n#&gt; # ℹ 155,410 more rows\n\n\n\n\n\n\nMust have identifier (domain, linkedin)\nClean identifiers\nRemove duplicates\n\n\nlibrary(stringr)\nfc_unnested_tbl |&gt; \n  \n  # 1. At least 1 identifier: 4.518 observations are filtered out\n  filter(if_any(c(domain, linkedin_url), ~!is.na(.))) |&gt;\n  \n  # 2. Extract linkedin handle & clean domains\n  mutate(linkedin_handle = linkedin_url |&gt; str_extract(\"(?&lt;=linkedin\\\\.com/company/).*?(?=(?:\\\\?|$|/))\")) |&gt;\n  mutate(domain          = domain |&gt; clean_domain()) |&gt;\n\n  # 3. Remove 532 duplicates\n  distinct()\n\n\n\n#&gt; # A tibble: 150,363 × 3\n#&gt;   company_id domain              linkedin_handle          \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                    \n#&gt; 1          1 zana.io             &lt;NA&gt;                     \n#&gt; 2          2 premamawellness.com luna-pharmaceuticals-inc-\n#&gt; 3          3 chainsync.com       chainsync                \n#&gt; # ℹ 150,360 more rows\n\n\n–&gt; 145.991 distinct examineable companies.\n\n\nIssue: Some extracted domains are not unique and associated with multiple companies. Manual Cleaning: Domains with a count exceeding two were analyzed and set to NA if they do not correspond to the actual one.\n\n# ANALYZE\n# fc_wrangled_tbl |&gt; \n#   distinct(company_id, domain) |&gt; \n#   count(domain, sort = T) |&gt; \n#   filter(n&gt;2)`\n\nunwanted_domains_cb &lt;- c(\"webflow.io\", \"angel.co\", \"weebly.com\", \"wordpress.com\", \"wixsite.com\", \"squarespace.com\", \n                         \"webflow.io\", \"crypt2esports.com\", \"myshopify.com\", \"business.site\", \"mystrikingly.com\", \n                         \"launchrock.com\", \"square.site\", \"google.com\", \"sites.google.com\", \"t.co\", \"linktr.ee\",\n                         \"netlify.app\", \"itunes.apple.com\", \"apple.com\", \"crunchb.com\", \"tumblr.com\", \"linkedin.com\",\n                         \"godaddysites.com\", \"mit.edu\", \"paloaltonetworks.com\", \" wpengine.com\", \"facebook.com\",\n                         \"intuit.com\", \"medium.com\", \"salesforce.com\", \"strikingly.com\", \"wix.com\", \"cisco.com\",\n                         \"digi.me\", \"apps.apple.com\", \"bit.ly\", \"fleek.co\", \"harvard.edu\", \"ibm.com\", \"jimdo.com\",\n                         \"myftpupload.com\", \"odoo.com\", \"storenvy.com\", \"twitter.com\", \"umd.edu\", \"umich.edu\", \"vmware.com\", \"webs.com\")\n\n# Not all observations with unwanted domains are bad per se:\nwanted_ids_cb &lt;- c(angel = 128006, `catapult-centres-uk` = 115854, digime1 = 140904, digimi2 = 95430, fleek = 50738, \n                   jimdo = 108655, medium = 113415, storenvy = 85742, strikingly = 95831, substack = 34304, \n                   tumblr = 84838, twitter = 53139, weebly = 91365, wpengine = 91720)\n\n# Set misleading domains to NA\nfunded_companies_clnd &lt;- fc_wrangled_tbl |&gt; \n                              \n  mutate(domain = if_else(\n    domain %in% unwanted_domains_cb & !(company_id %in% wanted_ids_cb), \n    NA_character_, domain))\n\n\n\nIt appears that CoreSignal has been able to locate 45.026 companies within our gathered data.\n\n# Selection & Wrangle has been done already\ncs_companies_base_slct &lt;- readRDS(\"cs_companies_base_slct.rds\") \ncs_companies_base_slct\n\n\n\n#&gt; # A tibble: 45,362 × 4\n#&gt;      id name                                domain               linkedin_handle\n#&gt;   &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;                &lt;chr&gt;          \n#&gt; 1   305 Blueprint, a David's Bridal Company blueprintregistry.c… blueprint-regi…\n#&gt; 2   793 BookingLive                         bookinglive.com      bookinglive    \n#&gt; 3  2425 Brandvee                            momentum.ai          brandvee       \n#&gt; # ℹ 45,359 more rows\n\n\n\n\ncs_companies_base_slct$id |&gt; n_distinct()\n\n#&gt; [1] 45026\n\n\n\n\n\ncs_companies_base_slct |&gt; janitor::get_dupes(id)\n\n#&gt; # A tibble: 672 × 5\n#&gt;        id dupe_count name          domain           linkedin_handle\n#&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;          \n#&gt; 1  596494          2 Vi            vi.co            vitrainer      \n#&gt; 2  596494          2 Vi            vi.co            vi             \n#&gt; 3 1324413          2 Patch Lending patchlending.com patch-of-land  \n#&gt; # ℹ 669 more rows\n\n\n\n\n\nNothing to wrangle …\n\ncs_companies_base_wrangled &lt;- cs_companies_base_slct |&gt; select(-name) |&gt; \n  \n                                        # Add suffixes to col names\n                                        rename_with(~ paste(., \"cs\", sep = \"_\"))\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMore cleaning necessary (same as CBPB)! The task was undertaken with a limited degree of enthusiasm.\n\n\n\n\nunwanted_domains_cs    &lt;- c(\"bit.ly\", \"linktr.ee\", \"facebook.com\", \"linkedin.com\", \"twitter.com\", \"crunchbase.com\")\nwanted_ids_cs          &lt;- c(crunchbase = 1634413, linkedin = 8568581, twitter = 24745469)\n\ncs_companies_base_clnd &lt;- cs_companies_base_wrangled |&gt; \n  \n  mutate(domain_cs = if_else(\n    domain_cs %in% unwanted_domains_cs & !(id_cs %in% wanted_ids_cs), \n    NA_character_, \n    domain_cs)\n    )"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#join-companies-member-experiences-and-funding-information",
    "href": "revealjs/slides/2023/cs/cs.html#join-companies-member-experiences-and-funding-information",
    "title": "CoreSignal Analysis",
    "section": "Join companies, member experiences and funding information",
    "text": "Join companies, member experiences and funding information\n\nCompaniesJobs (all)Jobs (dist)Jobs (focal)FundingConversionJoin\n\n\nWe were able to match 37.287 CS & CB/PB companies.\n\ncb_cs_joined &lt;- funded_companies_clnd |&gt; \n\n    # Leftjoins\n    left_join(cs_companies_base_clnd |&gt; select(id_cs, domain_cs),          by = c(domain          = \"domain_cs\"),          na_matches = \"never\") |&gt; \n    left_join(cs_companies_base_clnd |&gt; select(id_cs, linkedin_handle_cs), by = c(linkedin_handle = \"linkedin_handle_cs\"), na_matches = \"never\") |&gt; \n\n    # Remove obs with no cs_id\n    filter(!is.na(id_cs)) |&gt;\n    \n    # Remove matches, that matched different domains, but same company (e.g. company_id: 83060, id_cs: 4507928) block.xyz & squareup.com\n    select(company_id, id_cs) |&gt; \n    distinct()\n    \ncb_cs_joined\n\n\n\n#&gt; # A tibble: 38,118 × 2\n#&gt;   company_id    id_cs\n#&gt;        &lt;int&gt;    &lt;int&gt;\n#&gt; 1          2  8345218\n#&gt; 2          5 28149599\n#&gt; 3          8  4469271\n#&gt; 4         11  5349023\n#&gt; 5         12  9364263\n#&gt; # ℹ 38,113 more rows\n\n\n\n\ncb_cs_joined |&gt; distinct(company_id) |&gt; nrow()\n\n#&gt; [1] 37287\n\n\n\n\n\nWe got over 460 million employment observations from CoreSignal.\n\n# Other data versions\n# 1. Complete: \nmember_experience_dt \n#&gt; {462.711.794}  \n\n# 2. Distinct1: \nmember_experience_dist_dt &lt;- unique(member_experience_dt) \n#&gt; {432.368.479}\n\n# 3. Distinct2: \nunique(member_experience_dist_dt[order(id)], by = setdiff(names(member_experience_dist_dt), \"id\")) \n#&gt; {431.899.547}\n\n\n\nBut only ~50 Mil distinct employments\n\n# Load distinct member experiences\nme_dist8_prqt &lt;- arrow::open_dataset(\"cs_me_dist8_unest_empl_hist.parquet\") \nme_dist8_prqt |&gt; glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 51,621,196 rows x 10 columns\n#&gt; $ id_tie                 &lt;int32&gt; 16615559, 16615560, 16615561, 16615562, 1661556…\n#&gt; $ id                    &lt;double&gt; 2244288231, 254049663, 948937291, 254049667, 25…\n#&gt; $ member_id              &lt;int32&gt; 179313066, 179313066, 179313066, 179313066, 179…\n#&gt; $ company_id             &lt;int32&gt; 865089, 9098713, 9098713, NA, 865089, 9020540, …\n#&gt; $ company_name          &lt;string&gt; \"heritage community bank\", \"aurora bank fsb\", \"…\n#&gt; $ title                 &lt;string&gt; \"AVP Chief Compliance/BSA Officer\", \"AVP Compli…\n#&gt; $ date_from_parsed &lt;date32[day]&gt; 2010-02-01, 2012-07-01, 2011-11-01, 1997-07-01,…\n#&gt; $ date_to_parsed   &lt;date32[day]&gt; 2011-11-01, 2013-06-01, 2012-07-01, 2006-05-01,…\n#&gt; $ date_from_parsed_year  &lt;int32&gt; 2010, 2012, 2011, 1997, 2006, 2019, 2017, 2021,…\n#&gt; $ date_to_parsed_year    &lt;int32&gt; 2011, 2013, 2012, 2006, 2010, 2021, 2018, NA, 1…\n#&gt; Call `print()` for full schema details\n\n\nExample\n\nme_orig &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_orig/\")\nme_dist &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_dist/\")\n\nme_orig |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed) |&gt; print(n=19)\nme_dist |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed)\n\n\n\nOver 10 million (valid: must have starting date) employments at our crunchbase / pitchbook data set companies. 385.100 with a title containing the string founder.\n\n# Distinct company ids\ncb_cs_joined_cs_ids &lt;- cb_cs_joined |&gt; distinct(id_cs) |&gt; pull(id_cs)\nme_wrangled_prqt    &lt;- me_dist8_prqt |&gt; \n  \n                          # Select features\n                          select(member_id, company_id, exp_id = \"id\", date_from_parsed) |&gt; \n                          \n                          # Select observations\n                          filter(company_id %in% cb_cs_joined_cs_ids) |&gt; \n                          # - 967.080 observations (date_to not considered yet)\n                          filter(!is.na(date_from_parsed)) |&gt; \n\n                          # Add suffix to col names\n                          rename_with(~ paste(., \"cs\", sep = \"_\")) |&gt; \n                          compute()\n\nme_wrangled_prqt |&gt; \n  glimpse()\n\n#&gt; Table\n#&gt; 11,050,164 rows x 4 columns\n#&gt; $ member_id_cs              &lt;int32&gt; 9436436, 9436453, 9436478, 9436478, 9436513,…\n#&gt; $ company_id_cs             &lt;int32&gt; 573738, 3073966, 4577566, 4577566, 4577566, …\n#&gt; $ exp_id_cs                &lt;double&gt; 1891262301, 923902432, 1399967039, 525842890…\n#&gt; $ date_from_parsed_cs &lt;date32[day]&gt; 2018-04-01, 2015-04-01, 2006-01-01, 2004-03-…\n#&gt; Call `print()` for full schema details\n\n\n\n\nMultiple Funding Dates –&gt; Take oldest\n\nfc_wrangled_tbl &lt;- funded_companies_tbl |&gt; \n  \n  # Consider multiple founding dates: Take oldest founding date\n  unnest(founded_on) |&gt; \n  arrange(company_id, founded_on) |&gt; \n  group_by(company_id) |&gt; slice(1) |&gt; ungroup()\n\n\n\n\nExample of funding round data:\n\nfc_wrangled_tbl$funding_rounds[[1]] |&gt;  \n  glimpse()\n\n\n\n#&gt; Rows: 15\n#&gt; Columns: 14\n#&gt; $ round_id             &lt;int&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n#&gt; $ round_uuid_pb        &lt;chr&gt; NA, \"47208-70T\", NA, \"58843-18T\", NA, NA, NA, \"78…\n#&gt; $ round_uuid_cb        &lt;chr&gt; \"a6d3bfd9-5afa-47ce-86de-30a3abad6c9b\", NA, \"ea3b…\n#&gt; $ announced_on         &lt;date&gt; 2013-01-01, 2014-04-01, 2015-06-01, 2015-10-07, …\n#&gt; $ round_new            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 12, 13\n#&gt; $ round                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n#&gt; $ exit_cycle           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n#&gt; $ last                 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1…\n#&gt; $ round_type_new       &lt;fct&gt; Seed, Series A, Series B, Series C, Series D, Ser…\n#&gt; $ round_type           &lt;list&gt; \"angel\", \"angel\", \"early_vc\", \"early_vc\", \"conver…\n#&gt; $ round_types          &lt;list&gt; &lt;\"angel\", \"angel_group\", \"investor\", \"company\", \"…\n#&gt; $ raised_amount        &lt;dbl&gt; NA, 520000, NA, 1399999, NA, NA, NA, 3250000, NA,…\n#&gt; $ post_money_valuation &lt;dbl&gt; NA, NA, NA, 3399998, NA, NA, NA, 10249998, NA, N…\n#&gt; $ investors_in_round   &lt;list&gt; [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df…\n\n\n\n\n\nJoining via dplyr due to memory constraint not possible.\nJoining via Arrow due to structure constraints not possible.\n–&gt; Joining via data.table most efficient.\n\nConversion to data.tables necessary:\n\n# 1.  Funding Data\n# 1.1 Level 1\nfc_wrangled_dt |&gt; setDT()\n\n# 1.2 Funding Data Level 2 (funding_rounds)\npurrr::walk(fc_wrangled_dt$funding_rounds, setDT)\n\n# 1.3 Remove unnecessary columns + initialize dummy for before_join\npurrr::walk(fc_wrangled_dt$funding_rounds, ~ .x[, \n          `:=`(round_uuid_pb = NULL, round_uuid_cb        = NULL, round_new          = NULL, round          = NULL,\n               exit_cycle    = NULL, last                 = NULL, round_type         = NULL, round_type_new = NULL, \n               round_types   = NULL, post_money_valuation = NULL, investors_in_round = NULL, before_join    = NA)\n          ]\n        )\n\n# 2. Matching Table\ncb_cs_joined_slct_dt |&gt; setDT()\n\n# 3. Member experiences\nme_wrangled_dt &lt;- me_wrangled_prqt |&gt; collect()\n\n\n\nWorking data.table solution (efficiency increase through join by reference possible).\n\n# 1. Add company_id from funded_companies to member experiences\nme_joined_dt &lt;- cb_cs_joined_slct_dt[me_wrangled_dt, on = .(id_cs = company_id_cs), allow.cartesian = TRUE]\n#&gt; 12.978.226\n\n# 2. Add funding data from funded_companies\nme_joined_dt &lt;- fc_wrangled_dt[me_joined_dt, on = .(company_id)]\n#&gt; 12.270.572\n\n# 3. Remove duplicates (why are there any?)\nme_joined_dt &lt;- unique(me_joined_dt, by = setdiff(names(me_joined_dt), \"funding_rounds\"))\n#&gt; 12.270.572 .... No duplicates anymore. Removed from cb_cs_joined_slct_dt\n\nNot working dplyr solution\n\nme_joined_dt_dplyr &lt;- me_wrangled_dt |&gt;\n\n  # Add company_id from funded_companies\n  left_join(cb_cs_joined_slct_dt,\n            by = c(company_id_cs = \"id_cs\")) |&gt;\n\n  # Add data from funded_companies\n  left_join(funded_companies_wrangled_dt,\n            by = \"company_id\")  |&gt;\n  distinct()\n\nArrow because of nested funding data not possible."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#feature-engineering",
    "href": "revealjs/slides/2023/cs/cs.html#feature-engineering",
    "title": "CoreSignal Analysis",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nF1: Tjoin - TfoundPrep1F2, F3Prep2Titles\n\n\nHow many month have passed since the company was founded and before the person joined the company (in months)?\n\nlibrary(lubridate)\nme_joined_dt[, tjoin_tfound := (interval(founded_on, date_from_parsed_cs) %/% months(1))]\n\n\n\nUnnesting necessary due to memory constraints (takes multiple hours … to be measured).\n\n# Working: data.table\nme_joined_unnested_dt &lt;- me_joined_dt[,rbindlist(funding_rounds), by = setdiff(names(me_joined_dt), \"funding_rounds\")]\n# Not working: dplyr\nme_joined_unnested_tbl &lt;- me_joined_dt |&gt; unnest(funding_rounds)\n\nAdd feature whether or not member joined before Announcement of a funding round:\n\n# Add feature whether or not member joined before Announcement of a funding round\nme_joined_unnested_dt[,before_join := date_from_parsed_cs &gt;= announced_on]\n\n# Inspect\nopen_dataset(\"me_joined_unnested1.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 88,429,236 rows x 15 columns\n#&gt; $ company_id_cbpb           &lt;int32&gt; 85514, 85514, 85514, 85514, 85514, 85514, 85…\n#&gt; $ founded_on_cbpb     &lt;date32[day]&gt; 2007-01-01, 2007-01-01, 2007-01-01, 2007-01-…\n#&gt; $ company_id_cs             &lt;int32&gt; 10830353, 10830353, 10830353, 10830353, 1083…\n#&gt; $ id_tie                    &lt;int32&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 29…\n#&gt; $ exp_id_cs                &lt;double&gt; 606461989, 606461989, 606461989, 606461989, …\n#&gt; $ member_id_cs              &lt;int32&gt; 162, 162, 162, 162, 162, 162, 162, 162, 162,…\n#&gt; $ company_name_cs          &lt;string&gt; \"Kony, Inc.\", \"Kony, Inc.\", \"Kony, Inc.\", \"K…\n#&gt; $ title_cs           &lt;large_string&gt; \"Associate Engineer\", \"Associate Engineer\", …\n#&gt; $ date_from_parsed_cs &lt;date32[day]&gt; 2015-06-01, 2015-06-01, 2015-06-01, 2015-06-…\n#&gt; $ date_to_parsed_cs   &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ tjoin_tfound             &lt;double&gt; 101, 101, 101, 101, 101, 101, 101, 101, 101,…\n#&gt; $ round_id                  &lt;int32&gt; 259195, 259196, 259197, 259198, 259199, 2592…\n#&gt; $ announced_on        &lt;date32[day]&gt; 2011-02-24, 2011-03-01, 2011-06-07, 2012-01-…\n#&gt; $ raised_amount            &lt;double&gt; 13370002, 8280000, 2352002, 2000000, 1500000…\n#&gt; $ before_join                &lt;bool&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n#&gt; Call `print()` for full schema details\n\n\n\n\nF2. How much capital has been acquired by the time the person joins? F3. How many funding rounds have been acquired by the time the person joins?\n\n# Initialize empty columns (not sure yet if that increases performance)\nme_joined_unnested_dt[, `:=` (raised_amount_before_join = NA_real_, \n                              num_rounds_before_join    = NA_real_)]\n\n# Add features\nme_joined_unnested_dt[, `:=` (raised_amount_before_join = sum(raised_amount[before_join == T], na.rm = T),\n                              num_rounds_before_join    = sum(  before_join[before_join == T])), \n                      by = .(company_id, exp_id_cs)]\n\nopen_dataset(\"me_joined_unnested2.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 88,429,236 rows x 17 columns\n#&gt; $ company_id_cbpb            &lt;int32&gt; 85514, 85514, 85514, 85514, 85514, 85514, 8…\n#&gt; $ founded_on_cbpb      &lt;date32[day]&gt; 2007-01-01, 2007-01-01, 2007-01-01, 2007-01…\n#&gt; $ company_id_cs              &lt;int32&gt; 10830353, 10830353, 10830353, 10830353, 108…\n#&gt; $ id_tie                     &lt;int32&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n#&gt; $ exp_id_cs                 &lt;double&gt; 606461989, 606461989, 606461989, 606461989,…\n#&gt; $ member_id_cs               &lt;int32&gt; 162, 162, 162, 162, 162, 162, 162, 162, 162…\n#&gt; $ company_name_cs           &lt;string&gt; \"Kony, Inc.\", \"Kony, Inc.\", \"Kony, Inc.\", \"…\n#&gt; $ title_cs            &lt;large_string&gt; \"Associate Engineer\", \"Associate Engineer\",…\n#&gt; $ date_from_parsed_cs  &lt;date32[day]&gt; 2015-06-01, 2015-06-01, 2015-06-01, 2015-06…\n#&gt; $ date_to_parsed_cs    &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ tjoin_tfound              &lt;double&gt; 101, 101, 101, 101, 101, 101, 101, 101, 101…\n#&gt; $ round_id                   &lt;int32&gt; 259195, 259196, 259197, 259198, 259199, 259…\n#&gt; $ announced_on         &lt;date32[day]&gt; 2011-02-24, 2011-03-01, 2011-06-07, 2012-01…\n#&gt; $ raised_amount             &lt;double&gt; 13370002, 8280000, 2352002, 2000000, 150000…\n#&gt; $ before_join                 &lt;bool&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n#&gt; $ raised_amount_before_join &lt;double&gt; 120528113, 120528113, 120528113, 120528113,…\n#&gt; $ num_rounds_before_join    &lt;double&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5…\n#&gt; Call `print()` for full schema details\n\n\n\n\nNest again\n\n# data.table\nexcluded_cols       &lt;- setdiff(names(me_joined_unnested_dt), c(\"round_id\", \"announced_on\", \"raised_amount\", \"before_join\"))\nme_joined_nested_dt &lt;- me_joined_unnested_dt[, list(funding_rounds=list(.SD)), by=excluded_cols]\n\n# Dplyr (not working)\n# me_joined_nested_dt &lt;- me_joined_unnested_dt |&gt; \n#         nest(funding_rounds = c(\"round_id\", \"announced_on\", \"raised_amount\", \"before_join\"))\n\nopen_dataset(\"me_joined_nested.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 12,654,304 rows x 13 columns\n#&gt; $ company_id                 &lt;int32&gt; 71668, 5070, 117119, 5070, 117119, 7920, 52…\n#&gt; $ founded_on           &lt;date32[day]&gt; 2019-01-01, 2014-07-01, 2015-01-01, 2014-07…\n#&gt; $ id_cs                      &lt;int32&gt; 23865165, 10861408, 10861408, 10861408, 108…\n#&gt; $ exp_id_cs                 &lt;double&gt; 1927546132, 1267852578, 1267852578, 2670635…\n#&gt; $ member_id_cs               &lt;int32&gt; 1874511, 1874513, 1874513, 1874513, 1874513…\n#&gt; $ company_name_cs           &lt;string&gt; \"Three Good\", \"Point\", \"Point\", \"Point\", \"P…\n#&gt; $ title_cs                  &lt;string&gt; \"Founder & CEO\", \"Customer Operations at Po…\n#&gt; $ date_from_parsed_cs  &lt;date32[day]&gt; 2015-04-01, 2018-01-01, 2018-01-01, 2018-11…\n#&gt; $ date_to_parsed_cs    &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, 2019-04-01, 2019-04…\n#&gt; $ tjoin_tfound              &lt;double&gt; -45, 42, 36, 52, 46, 50, 105, -33, 49, 131,…\n#&gt; $ raised_amount_before_join &lt;double&gt; 0, 11399999, 12100000, 11399999, 12100000, …\n#&gt; $ num_rounds_before_join    &lt;double&gt; 0, 2, 3, 2, 3, 6, 9, 0, 6, 9, 12, 7, 4, 9, …\n#&gt; $ funding_rounds         &lt;list&lt;...&gt;&gt; [&lt;tbl_df[5 x 4]&gt;], [&lt;tbl_df[5 x 4]&gt;], [&lt;tbl…\n#&gt; Call `print()` for full schema details\n\n\n\n\nTo differentiate between founder and non-founder CS titles are needed\n\n# Prep data (shrink / remove unnecessary data)\nme_joined_nested_foc_dt[, funding_rounds := NULL]\n\n# Prep titles\nme_wrangled_wt_dt &lt;-  me_dist_prqt |&gt; \n                          filter(company_id %in% cb_cs_joined_cs_ids, !is.na(date_from_parsed)) |&gt;  \n                          select(exp_id_cs, title_cs) |&gt; \n                          collect() |&gt; \n                          setDT()\n\n# Join\nme_joined_nested_foc_dt[me_wrangled_wt_dt, on = .(exp_id_cs), title_cs := i.title_cs]\n\n# Inspect\nopen_dataset(\"me_joined_nested_foc.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 12,654,304 rows x 12 columns\n#&gt; $ company_id                 &lt;int32&gt; 71668, 5070, 117119, 5070, 117119, 7920, 52…\n#&gt; $ founded_on           &lt;date32[day]&gt; 2019-01-01, 2014-07-01, 2015-01-01, 2014-07…\n#&gt; $ id_cs                      &lt;int32&gt; 23865165, 10861408, 10861408, 10861408, 108…\n#&gt; $ exp_id_cs                 &lt;double&gt; 1927546132, 1267852578, 1267852578, 2670635…\n#&gt; $ member_id_cs               &lt;int32&gt; 1874511, 1874513, 1874513, 1874513, 1874513…\n#&gt; $ company_name_cs           &lt;string&gt; \"Three Good\", \"Point\", \"Point\", \"Point\", \"P…\n#&gt; $ title_cs                  &lt;string&gt; \"Founder & CEO\", \"Customer Operations at Po…\n#&gt; $ date_from_parsed_cs  &lt;date32[day]&gt; 2015-04-01, 2018-01-01, 2018-01-01, 2018-11…\n#&gt; $ date_to_parsed_cs    &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, 2019-04-01, 2019-04…\n#&gt; $ tjoin_tfound              &lt;double&gt; -45, 42, 36, 52, 46, 50, 105, -33, 49, 131,…\n#&gt; $ raised_amount_before_join &lt;double&gt; 0, 11399999, 12100000, 11399999, 12100000, …\n#&gt; $ num_rounds_before_join    &lt;double&gt; 0, 2, 3, 2, 3, 6, 9, 0, 6, 9, 12, 7, 4, 9, …\n#&gt; Call `print()` for full schema details"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#plots",
    "href": "revealjs/slides/2023/cs/cs.html#plots",
    "title": "CoreSignal Analysis",
    "section": "Plots",
    "text": "Plots\n\nDataF1F2F3\n\n\nSeparate between “Founder” & “Non-Founder” and calculate summary statistics necessary for plotting.\n\nlibrary(ggplot2)\n\nlookup_term &lt;- \"founder\"\ndata        &lt;- me_joined_nested_foc_prqt |&gt; \n                  filter(!is.na(title_cs)) |&gt; \n                  mutate(Role = title_cs |&gt; tolower() |&gt; str_detect(lookup_term)) |&gt; \n                  collect() |&gt; \n                  mutate(\n                    Role = Role |&gt; factor(levels = c(TRUE, FALSE), \n                                                labels = c('Founder', 'Non-Founder'))\n                    ) \n\n# Summary Statistics (Mean & Median)\ndf_vline_long &lt;- data |&gt; \n  group_by(Role) |&gt; \n  summarise(Mean = mean(tjoin_tfound), Median = median(tjoin_tfound), n = n()) |&gt; \n  pivot_longer(c(Mean, Median,n ), names_to = \"Statistic\", values_to = \"Value\") |&gt; \n  mutate(Value_chr    = format(round(Value, 1), big.mark=\".\", decimal.mark = \",\", drop0trailing = T),\n         gg_pos_y = rep(c(0.07,0.06, 0.05),2),\n         gg_color = rep(c(\"#8F8470\", \"#BF9240\", \"#FEA400\"), 2))\n\n\n\n\n\n\nHow many month have passed since the company was founded and before the person joined the company (binwidth: 3 months)?\n\n\ndata |&gt; \n  \n  # Plot\n  ggplot(aes(x = tjoin_tfound, fill = Role, color = Role)) +\n  geom_histogram(aes(y =..density..), size = .2, binwidth = 3, alpha = 0.5) +\n  facet_wrap(~Role, nrow=2) +\n  \n  # Statistics & Design\n  ggnewscale::new_scale_color() +\n  geom_vline(data = df_vline_long |&gt; filter(Statistic != \"n\"), aes(xintercept = Value, linetype = Statistic, color = Statistic), key_glyph = \"path\") +\n  scale_linetype_manual(values = c(2,3)) +\n  scale_color_manual(values = c(\"#8F8470\", \"#BF9240\", \"#FEA400\")) +\n  geom_label(data = df_vline_long, aes(x = 100, y = gg_pos_y, label = paste0(Statistic, ' = ', Value_chr)), \n             color = df_vline_long$gg_color, fill = \"transparent\", alpha = 0.5, size = 3, hjust = \"left\") +\n  xlim(-250, 250) +\n  labs(x = \"Δ T_join, T_foundation (in month)\", y = \"Density\") + \n  theme(legend.key=element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\nHow much capital has been acquired by the time the person joins?\n\n\ndata |&gt; \n  \n  # Plot\n  ggplot(aes(x = raised_amount_before_join, color = Role, fill = Role)) + \n  geom_histogram(aes(y =..density..), alpha=0.5) +\n  facet_wrap(~Role, nrow=2) +\n  \n  # Design\n  scale_x_continuous(labels = scales::label_number(prefix = \"$\", accuracy = 0.1, scale_cut = scales::cut_short_scale()), limits = c(NA,1e+09)) +\n  labs(x = \"Raised amount before join\", y = \"Density\", fill=\"\", color = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nHow many funding rounds have been acquired by the time the person joins?\n\n\ndata |&gt; \n  \n  ggplot(aes(x = num_rounds_before_join, color = Role, fill = Role)) + \n  geom_histogram(aes(y =..density..), binwidth = 1, alpha=0.5) +\n  facet_wrap(~Role, nrow=2) +\n  \n  # Design\n  xlim(NA, 20) +\n  labs(x = \"# Rounds before join\", y = \"Density\", fill=\"\", color = \"\")"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#fortune500",
    "href": "revealjs/slides/2023/cs/cs.html#fortune500",
    "title": "CoreSignal Analysis",
    "section": "Fortune500",
    "text": "Fortune500\n\nme_matched_members_dt &lt;- me_matched_members_prqt |&gt; collect()\n\n# 46 Batches (Chunks with 5 Million rows)\nslice_ids &lt;- tibble(from = seq(1, 230000000, 5000000),to = c(seq(5000000, 225000000, 5000000), 229065592))\nfor (i in 1:46) {\n  # Build Batch\n  x &lt;- slice_ids$from[i]; y &lt;- slice_ids$to[i]\n  me_matched_members_slice_dt &lt;- me_matched_members_dt[x:y,]\n  # Create Features\n  me_matched_members_slice_dt[, `:=` (f500 = (purrr::pmap_lgl(list(company_name, date_from_parsed, date_to_parsed), check_f500, .progress = TRUE)),\n                                      role = title |&gt; tolower() |&gt; stringr::str_detect(\"founder\"))]\n  # Save\n  me_matched_members_slice_dt |&gt; write_parquet(paste0(\"/media/tie/ssd2/joschka/me_f500/me_f500_\", cur_id, \".parquet\"))\n}\n\n\ncheck_f500 &lt;- function(name,year_from,year_to) {\n  \n  if (is.na(year_to))   {year_to &lt;- 2023}\n  if (is.na(year_from)) {return(NA)}\n  \n  data &lt;- fortune500 |&gt; \n    filter(year |&gt; between(year_from, year_to)) |&gt; \n    pull(company)\n  \n  name |&gt; tolower() %in% data\n}"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#employment-history-worked-for-fortune-500-company",
    "href": "revealjs/slides/2023/cs/cs.html#employment-history-worked-for-fortune-500-company",
    "title": "CoreSignal Analysis",
    "section": "Employment History (Worked for Fortune 500 company?)",
    "text": "Employment History (Worked for Fortune 500 company?)\nContent-related problems\n\nMatching problems:\n\namd &lt;&gt; advanced micro devices\nintel &lt;&gt; intel corporation\n\nGeographical issues:\n\nJust US companies (use Fortune Global data)\n\n\nTechnical issues\nTakes loooooooong time to calculate…"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#code-i-data-function",
    "href": "revealjs/slides/2023/cs/cs.html#code-i-data-function",
    "title": "CoreSignal Analysis",
    "section": "Code I (Data + Function)",
    "text": "Code I (Data + Function)\nFunction\n\n# 1. Function\ncheck_f500 &lt;- function(title, year_from, year_to) {\n  \n  # Handle NA inputs\n  if (is.na(year_to))   {year_to &lt;- 2023}\n  if (is.na(year_from)) {return(NA)}\n  \n  # Filter time frame\n  data &lt;- fortune500 |&gt; \n    \n    filter(year |&gt; between(year_from, year_to)) |&gt; \n    pull(company)\n  \n  # Check match and return bool\n  title |&gt; tolower() %in% data\n}\n\n\n\nCoreSignal data (n = 229.065.592)\n\n# 1. Data\nme_matched_members_prqt &lt;- open_dataset(\"me_matched_members.parquet\") \nme_matched_members_prqt |&gt; \n  \n  head() |&gt; collect()\n\n\n\n#&gt; # A tibble: 6 × 7\n#&gt;       id member_id company_id company_name title date_from_parsed date_to_parsed\n#&gt;    &lt;dbl&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;date&gt;           &lt;date&gt;        \n#&gt; 1 6.07e8   2605953         NA arkansas se… Corp… 2015-02-01       NA            \n#&gt; 2 6.07e8   2605953         NA freelance m… Free… 2016-05-01       NA            \n#&gt; 3 6.30e8   2605953   10415396 101 magazine Cont… 2012-08-01       2014-05-01    \n#&gt; # ℹ 3 more rows\n\n\n\nUS Fortune 500 data\n\nfortune500 &lt;- readRDS(\"fortune_500_1955-2022.rds\") |&gt; \n                \n                select(year, company) |&gt; \n                mutate(company = company |&gt; tolower())\nfortune500\n\n\n\n#&gt; # A tibble: 34,000 × 2\n#&gt;    year company       \n#&gt;   &lt;dbl&gt; &lt;chr&gt;         \n#&gt; 1  1955 general motors\n#&gt; 2  1955 exxon mobil   \n#&gt; 3  1955 u.s. steel    \n#&gt; # ℹ 33,997 more rows"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#code-ii-chunkwise-execution",
    "href": "revealjs/slides/2023/cs/cs.html#code-ii-chunkwise-execution",
    "title": "CoreSignal Analysis",
    "section": "Code II (Chunkwise Execution)",
    "text": "Code II (Chunkwise Execution)\n\n# 1. Collect data\nme_matched_members_dt &lt;- me_matched_members_prqt |&gt; collect()\n# 2.Build batches\nslice_ids &lt;- tibble(\n  from  = seq(1, 230000000, 5000000),\n  to    = c(seq(5000000, 225000000, 5000000), 229065592),\n)\n\nfor (i in 1:46) {\n\n  # Create current batch  \n  x &lt;- slice_ids$from[i]\n  y &lt;- slice_ids$to[i]\n  me_matched_members_slice_dt &lt;- me_matched_members_dt[x:y,]\n\n  # Add features\n  me_matched_members_slice_dt[, `:=` (f500 = (purrr::pmap_lgl(list(company_name, date_from_parsed, date_to_parsed), check_f500, .progress = TRUE)),\n                                      role = title |&gt; tolower() |&gt; stringr::str_detect(\"founder\"))]\n  \n  # Save\n  me_matched_members_slice_dt |&gt; write_parquet(paste0(\"me_f500/me_f500_\", cur_id, \".parquet\"))\n}"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#code-iii-build-feature",
    "href": "revealjs/slides/2023/cs/cs.html#code-iii-build-feature",
    "title": "CoreSignal Analysis",
    "section": "Code III (Build feature)",
    "text": "Code III (Build feature)\n\n# 1. Load data \nme_f500 &lt;- open_dataset(\"me_f500/\") |&gt; \n  collect()\n\n# 2. Add Ids\nme_dist_ids_prqt &lt;- arrow::open_dataset(\"me_dist4.parquet\") |&gt; select(id, member_id) |&gt; collect()\nme_f500_id       &lt;-  me_dist_ids_prqt[me_f500, on = .(id)] \n\n# 3. Add features\n# 3.1 Earliest founding date & earliest f500 date (if founded)\nme_f500_id[,`:=` (founding_min      = (ifelse(any(role == T),                  min(date_from_parsed[role==T]),   NA_real_)),\n                  f500_min_founding = (ifelse(any(role == T) & any(f500 == T), min(date_from_parsed[f500 == T]), NA_real_))),\n           by = .(member_id)]\n\n# 3.2 Compare\nme_f500_id[, f500_before_founding := f500_min_founding &lt;= founding_min]"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#plot",
    "href": "revealjs/slides/2023/cs/cs.html#plot",
    "title": "CoreSignal Analysis",
    "section": "Plot",
    "text": "Plot\nConstraints:\n\nAnalysis takes place on an annual level.\nFirst funding event is being considered\nExact matches only\n\n\n\nme_f500_id &lt;- arrow::open_dataset(\"~/02_diss/01_coresignal/02_data/me_f500_id.parquet\")\ndata &lt;- me_f500_id |&gt; \n  \n          # Filter \"Founder\"\n          filter(role == T) |&gt; collect() \n  \ndata |&gt; \n  ggplot(aes(x = f500_before_founding)) + \n    geom_bar() +\n    scale_x_discrete(labels=c(\"Employment at Fortune500\\nAFTER\\nfounding\", \"Employment at Fortune500\\nBEFORE\\nfounding\", \"Neither case\")) +\n    scale_y_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6, accuracy = 0.1)) + \n    labs(x = \"\", y = \"Count\")"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#cluster-skills",
    "href": "revealjs/slides/2023/cs/cs.html#cluster-skills",
    "title": "CoreSignal Analysis",
    "section": "Cluster skills",
    "text": "Cluster skills\ntbd\n\nmember_skills_prqt &lt;- arrow::open_dataset(\"member_skills.parquet\")\nmember_skills_prqt |&gt; \n  glimpse()\n\n\nskill_names_tbl &lt;- readRDS(\"skill_names_tbl.rds\")\nskill_names_tbl\n\n\n\n#&gt; # A tibble: 2,423,690 × 2\n#&gt;   skill_id skill_name \n#&gt;      &lt;int&gt; &lt;fct&gt;      \n#&gt; 1        1 mathematics\n#&gt; 2        2 swimming   \n#&gt; 3        3 analytics  \n#&gt; # ℹ 2,423,687 more rows"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#built-variables",
    "href": "revealjs/slides/2023/cs/cs.html#built-variables",
    "title": "CoreSignal Analysis",
    "section": "Built variables",
    "text": "Built variables\n\nAllGeneralEDAExp (dummy)Exp (quant)EduFund\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 67\n#&gt; $ company_id_cbpb                      &lt;int&gt; 90591, 152845, 90440, 138208, 116…\n#&gt; $ funding_after_mid                    &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"…\n#&gt; $ funding_after_early                  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\",…\n#&gt; $ member_id                            &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005…\n#&gt; $ id_tie                               &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175…\n#&gt; $ exp_id_cs                            &lt;dbl&gt; 2481733250, 1423977093, 2638, 263…\n#&gt; $ exp_corporate                        &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.000…\n#&gt; $ exp_funded_startup                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0,…\n#&gt; $ exp_founder                          &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0…\n#&gt; $ exp_f500                             &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.0000…\n#&gt; $ exp_research                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ company_id_cs                        &lt;int&gt; 140537, 10644128, 6068905, 606890…\n#&gt; $ company_name_cs                      &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"…\n#&gt; $ company_name_cbpb                    &lt;chr&gt; \"receptos\", \"HERE Technologies Ch…\n#&gt; $ founded_on_cbpb                      &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-…\n#&gt; $ closed_on_cbpb                       &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-…\n#&gt; $ title_cs                             &lt;chr&gt; \"Key Account Manager\", \"GIS Analy…\n#&gt; $ date_from_parsed_cs                  &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_to_parsed_cs                    &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-…\n#&gt; $ tjoin_tfound                         &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, …\n#&gt; $ raised_amount_before_join_company    &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333…\n#&gt; $ num_rounds_before_join               &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, …\n#&gt; $ is_f500                              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, F…\n#&gt; $ is_founder                           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research                          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research_ivy                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ date_1st_founder_exp                 &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_f500_exp                    &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010…\n#&gt; $ date_1st_funded_startup_exp          &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_1st_research_exp                &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_research_ivy_exp            &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_corporate_exp               &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, …\n#&gt; $ time_since_1st_corporate_exp         &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40,…\n#&gt; $ time_since_1st_founder_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_f500_exp              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_funded_startup_exp    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, N…\n#&gt; $ time_since_1st_research_exp          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_research_ivy_exp      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_experience            &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176…\n#&gt; $ raised_amount_before_founder_member  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ raised_amount_before_all_member      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA,…\n#&gt; $ was_corporate_before                 &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, …\n#&gt; $ was_founder_before                   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_f500_before                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_fc_before                        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_uni_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_ivy_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ stage_mid                            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ stage_late                           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, FALSE, NA…\n#&gt; $ date_from_stage                      &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\",…\n#&gt; $ company_start_mid                    &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-…\n#&gt; $ company_start_late                   &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n#&gt; $ num_rounds_cumulated_founder         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ num_rounds_cumulated_all             &lt;int&gt; NA, NA, NA, NA, NA, NA, 1, 1, NA,…\n#&gt; $ announced_on_sB                      &lt;date&gt; 2012-02-03, 2018-01-04, 2011-03-…\n#&gt; $ round_type_new_next                  &lt;fct&gt; Series C, Series C, Series C, Ser…\n#&gt; $ raised_amount_cumsum_sB              &lt;dbl&gt; 46043054, 0, 1905000, 11022796, 2…\n#&gt; $ raised_amount_cumsum_sB_next         &lt;dbl&gt; 76043054, 0, 8712306, 13854868, 4…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(id_tie, member_id, exp_id_cs, company_id_cbpb, company_name_cbpb, company_id_cs, company_name_cs, \n         founded_on_cbpb, closed_on_cbpb,\n         title_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ id_tie            &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175, 209, 243, 321, 37…\n#&gt; $ member_id         &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005, 4224, 4224, 4317,…\n#&gt; $ exp_id_cs         &lt;dbl&gt; 2481733250, 1423977093, 2638, 2638, 1736317868, 3084…\n#&gt; $ company_id_cbpb   &lt;int&gt; 90591, 152845, 90440, 138208, 116099, 97810, 40123, …\n#&gt; $ company_name_cbpb &lt;chr&gt; \"receptos\", \"HERE Technologies Chicago\", \"crowdtwist…\n#&gt; $ company_id_cs     &lt;int&gt; 140537, 10644128, 6068905, 6068905, 11825305, 194148…\n#&gt; $ company_name_cs   &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"Oracle\", \"Oracle\", …\n#&gt; $ founded_on_cbpb   &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-01, 2006-01-01, 201…\n#&gt; $ closed_on_cbpb    &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-09, NA, NA, NA, NA,…\n#&gt; $ title_cs          &lt;chr&gt; \"Key Account Manager\", \"GIS Analyst I\", \"QA\", \"QA\", …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_parsed_cs, date_to_parsed_cs, \n         tjoin_tfound, raised_amount_before_join_company, num_rounds_before_join) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 5\n#&gt; $ date_from_parsed_cs               &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_to_parsed_cs                 &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-01,…\n#&gt; $ tjoin_tfound                      &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, 44,…\n#&gt; $ raised_amount_before_join_company &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333333…\n#&gt; $ num_rounds_before_join            &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, 2, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(starts_with(\"is_\"),\n         starts_with(\"was_\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ is_f500              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FAL…\n#&gt; $ is_founder           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research_ivy      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_corporate_before &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRU…\n#&gt; $ was_founder_before   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_f500_before      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_fc_before        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n#&gt; $ was_uni_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_ivy_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(\n    starts_with(\"date_1st_\"),\n    starts_with(\"time_since_1st_\"),\n    starts_with(\"exp_\"), -exp_id_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 19\n#&gt; $ date_1st_founder_exp              &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_f500_exp                 &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010-01…\n#&gt; $ date_1st_funded_startup_exp       &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_1st_research_exp             &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_research_ivy_exp         &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_corporate_exp            &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, 200…\n#&gt; $ time_since_1st_corporate_exp      &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40, 17…\n#&gt; $ time_since_1st_founder_exp        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_f500_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_funded_startup_exp &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, NA, …\n#&gt; $ time_since_1st_research_exp       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_research_ivy_exp   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_experience         &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176, 4…\n#&gt; $ exp_corporate                     &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.00000,…\n#&gt; $ exp_funded_startup                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0,…\n#&gt; $ exp_founder                       &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.00…\n#&gt; $ exp_f500                          &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.00000, …\n#&gt; $ exp_research                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(score_global_2023_best,\n         starts_with(\"rank\"),\n         starts_with(\"degree\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 8\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_stage, company_start_mid, company_start_late,\n         raised_amount_before_founder_member, raised_amount_before_all_member,\n         funding_after_mid, funding_after_early) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 7\n#&gt; $ date_from_stage                     &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\", …\n#&gt; $ company_start_mid                   &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-0…\n#&gt; $ company_start_late                  &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-0…\n#&gt; $ raised_amount_before_founder_member &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n#&gt; $ raised_amount_before_all_member     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA, …\n#&gt; $ funding_after_mid                   &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"y…\n#&gt; $ funding_after_early                 &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", …"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#further-analysis-is-necessary",
    "href": "revealjs/slides/2023/cs/cs.html#further-analysis-is-necessary",
    "title": "CoreSignal Analysis",
    "section": "Further analysis is necessary",
    "text": "Further analysis is necessary\n\nMatch employment history with Fortune500 (revenue based selection)\nCluster job titles\nCluster skills"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#table-of-contents",
    "href": "revealjs/slides/2023/diss/diss.html#table-of-contents",
    "title": "Diss",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#possible-overarching-topics",
    "href": "revealjs/slides/2023/diss/diss.html#possible-overarching-topics",
    "title": "Diss",
    "section": "Possible Overarching Topics",
    "text": "Possible Overarching Topics\n\nBricolage and effectuation\nTechnical Entrepreneur (prior knowledge)\nContingency\nNovelty and Technological uncertainty\nKnowledge spillover\n\nWe are interested in how actors are influenced by and interact with their social and cultural environments to bring about novelty, e.g. with regard to ideas, teams, products or business practices."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#research-scope",
    "href": "revealjs/slides/2023/diss/diss.html#research-scope",
    "title": "Diss",
    "section": "Research Scope",
    "text": "Research Scope\n\n \n\n  \n  \n  W-11 Research Focus & Fields\n  We are interested in how actors are influenced by and interact with their social and cultural environments to bring about novelty, e.g. with regard to ideas, teams, products or business practices."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#step1-identity-matching",
    "href": "revealjs/slides/2023/diss/diss.html#step1-identity-matching",
    "title": "Diss",
    "section": "Step1: Identity matching",
    "text": "Step1: Identity matching\nLinking Developer with startup data"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#github-1.-ghtorrent-and-2.-github-api",
    "href": "revealjs/slides/2023/diss/diss.html#github-1.-ghtorrent-and-2.-github-api",
    "title": "Diss",
    "section": " GitHub: 1. GHTorrent and 2. Github API",
    "text": "GitHub: 1. GHTorrent and 2. Github API\n\n\n1. Schema2. Conn3. User (GHT)4. User (Website)5. User (API)6. Orgs7. Merged\n\n\n\n\n\n\n\n\np_load(RMariaDB, dplyr)\ncon &lt;- dbConnect(\n  drv      = MariaDB(),\n  dbname   = \"ghtorrent_restore\",\n  username = \"ghtorrentuser\",\n1  password = Sys.getenv(\"GHTORRENTPASSWORD\"),\n2  host     = \"127.0.0.1\",\n  port     = 3307\n)\n\ncon |&gt; dbListTables()\n\n\n1\n\nPassword is located in a .Renviron file that is not stored in version control (GitHub)\n\n2\n\nSSH port forwarding/tunneling for MySQL connection is used\n\n\n\n\n\n\n\n\ntbl(con, \"users\") |&gt; \n  count()\n\n\n\n\n\n\ntbl(con,\"projects\") |&gt; \n  count()\n\n\n\n\n\n\ntbl(con,\"project_commits\") |&gt; \n  count()\n\n\n\n\n\n\n\n\n\nGHTorrent user data does not contain much valuable information:\n\n\n\nusers &lt;- tbl(con, \"users\") \nusers |&gt;   \n  glimpse()\n\n\n\n\nusers |&gt; \n  filter(login == \"christophihl\") |&gt; \n  glimpse()\n\n\n\n\n\n\n\n\n\n\n\n\n\n       \n\n\n       \n\n\n       \n\n\n       \n\n\n\n\n       \n\n\n       \n\n\n       \n\n\n       \n\n\n       \n\n\n\n\nhttps://github.com/jspolsky\n\n\n\n\nMore sensible data via API: Name, Email, Avatar, Blog/URL, Bio\n\n\nSingle API Request\n\nhttr::GET(\"https://api.github.com/users/christophihl\")\nhttr::GET(\"https://api.github.com/user/8004978\")\n\n\n\n\n\n\n#&gt; Rows: 1\n#&gt; Columns: 28\n#&gt; $ login               &lt;chr&gt; \"christophihl\"\n#&gt; $ id                  &lt;int&gt; 8004978\n#&gt; $ node_id             &lt;chr&gt; \"MDQ6VXNlcjgwMDQ5Nzg=\"\n#&gt; $ avatar_url          &lt;chr&gt; \"https://avatars.githu…\n#&gt; $ gravatar_id         &lt;chr&gt; \"\"\n#&gt; $ url                 &lt;chr&gt; \"https://api.github.co…\n#&gt; $ html_url            &lt;chr&gt; \"https://github.com/ch…\n#&gt; $ followers_url       &lt;chr&gt; \"https://api.github.co…\n#&gt; $ following_url       &lt;chr&gt; \"https://api.github.co…\n#&gt; $ gists_url           &lt;chr&gt; \"https://api.github.co…\n#&gt; $ starred_url         &lt;chr&gt; \"https://api.github.co…\n#&gt; $ subscriptions_url   &lt;chr&gt; \"https://api.github.co…\n#&gt; $ organizations_url   &lt;chr&gt; \"https://api.github.co…\n#&gt; $ repos_url           &lt;chr&gt; \"https://api.github.co…\n#&gt; $ events_url          &lt;chr&gt; \"https://api.github.co…\n#&gt; $ received_events_url &lt;chr&gt; \"https://api.github.co…\n#&gt; $ type                &lt;chr&gt; \"User\"\n#&gt; $ site_admin          &lt;lgl&gt; FALSE\n#&gt; $ name                &lt;chr&gt; \"Christoph Ihl\"\n#&gt; $ company             &lt;chr&gt; \"Hamburg University of…\n#&gt; $ blog                &lt;chr&gt; \"www.startupengineer.i…\n#&gt; $ location            &lt;chr&gt; \"Hamburg\"\n#&gt; $ public_repos        &lt;int&gt; 21\n#&gt; $ public_gists        &lt;int&gt; 1\n#&gt; $ followers           &lt;int&gt; 12\n#&gt; $ following           &lt;int&gt; 1\n#&gt; $ created_at          &lt;chr&gt; \"2014-06-27T11:22:22Z\"\n#&gt; $ updated_at          &lt;chr&gt; \"2024-04-23T13:22:01Z\"\n\n\n\n\nFinal API dataset\n\nopen_dataset(\"gh_api_users_wrangled.parquet\") |&gt; \n  glimpse() \n\n\n\n\n\n\nTwo different types: Organizations and User accounts\n\n\n\nhttr::GET(\"https://api.github.com/users/TUHHStartupEngineers\")\nhttr::GET(\"https://api.github.com/user/30825260\")\n\n\n\n#&gt; Rows: 1\n#&gt; Columns: 29\n#&gt; $ login               &lt;chr&gt; \"TUHHStartupEngineers\"\n#&gt; $ id                  &lt;int&gt; 30825260\n#&gt; $ node_id             &lt;chr&gt; \"MDEyOk9yZ2FuaXphdGlvbjMwODI1M…\n#&gt; $ avatar_url          &lt;chr&gt; \"https://avatars.githubusercon…\n#&gt; $ gravatar_id         &lt;chr&gt; \"\"\n#&gt; $ url                 &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ html_url            &lt;chr&gt; \"https://github.com/TUHHStartu…\n#&gt; $ followers_url       &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ following_url       &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ gists_url           &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ starred_url         &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ subscriptions_url   &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ organizations_url   &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ repos_url           &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ events_url          &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ received_events_url &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ type                &lt;chr&gt; \"Organization\"\n#&gt; $ site_admin          &lt;lgl&gt; FALSE\n#&gt; $ name                &lt;chr&gt; \"TUHH Institute of Entrepreneu…\n#&gt; $ blog                &lt;chr&gt; \"www.startupengineer.io\"\n#&gt; $ location            &lt;chr&gt; \"Hamburg University of Technol…\n#&gt; $ email               &lt;chr&gt; \"startup.engineer@tuhh.de\"\n#&gt; $ bio                 &lt;chr&gt; \"Data Science, Research & Prac…\n#&gt; $ public_repos        &lt;int&gt; 8\n#&gt; $ public_gists        &lt;int&gt; 0\n#&gt; $ followers           &lt;int&gt; 9\n#&gt; $ following           &lt;int&gt; 0\n#&gt; $ created_at          &lt;chr&gt; \"2017-08-08T07:40:56Z\"\n#&gt; $ updated_at          &lt;chr&gt; \"2024-05-03T11:35:16Z\"\n\n\n\n\n\norg_members &lt;- tbl(con, \"organization_members\")\norg_members\n\n\norg_members |&gt; \n  left_join(users, by = c(org_id  = \"id\")) |&gt; \n  left_join(users, by = c(user_id = \"id\")) |&gt; \n  filter(login.x == \"TUHHStartupEngineers\") |&gt; \n  select(login.y)\n\n\n\n\n\n\n\n\n\n\n\n\nOrganization affiliation:\n\n\ngh_org_affil &lt;- org_members |&gt; \n  # Logins for users & orgs (via GHT)\n  left_join(gh_ght_users, by = c(org_id  = \"id\")) |&gt; \n  left_join(gh_ght_users, by = c(user_id = \"id\")) |&gt; \n  \n  # Domains for orgs (via API)\n  left_join(gh_api_users, by = c(org_login = \"login\"))\n\n\n\n\n\n\nFinal dataset:\n\n\ngh_api_users |&gt; \n  \n  # Add org memberships (+ domains)\n  left_join(gh_org_affil) |&gt; \n\n  # Add location data\n  left_join(gh_ght_users)"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#stackoverflow",
    "href": "revealjs/slides/2023/diss/diss.html#stackoverflow",
    "title": "Diss",
    "section": " Stackoverflow",
    "text": "Stackoverflow\n\nOverviewDumpsIdentity MatchingWebsiteScraped DataMerged\n\n\n\nTwo among the most widely adopted and studied platforms are GitHub and StackOverflow\nThese two platforms serve different purposes: code sharing and collaborative development vs. information and knowledge exchange.\nAt the same time, they both serve potentially the same community of developers for the same overall goal, i.e., software development.\n\n\n\n\n\nCurrent Dump from 2021\n\n\n\nopen_dataset(\"so_dump_2021_09_users.parquet\") |&gt; \n  \n  glimpse() \n\n\n\n\nopen_dataset(\"so_dump_2021_09_users.parquet\") |&gt; \n  filter(DisplayName == \"Christoph Ihl\")\n  glimpse() \n\n\n\n\n\n\nOld Dump from 2013\n\n\n\nopen_dataset(\"so_dump_2013_09_users.parquet\") |&gt; \n  \n  glimpse() \n\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\n\n\n\n\n\n\n\nName and Location\n\n\n\n\n\n\nProfile images\n\n\n\n\n\nProfile images\n\n\n\n\n\n\n\n\njoin &lt;- open_dataset(\"gh_api_users.parquet\") |&gt; \n  \n  inner_join( \n    open_dataset(\"so_users_joined.parquet\") , \n      by = c(name  = \"DisplayName\", location = \"Location\"), \n      na_matches = \"never\"\n    ) |&gt; \n  \n  compute()\n\njoin|&gt; \n  nrow()\n\n\n\n#&gt; [1] 724961\n\n\n\n\njoin |&gt; \n  filter(location == \"Hamburg\") |&gt; \n  count(name, sort = T)\n\n\n\n#&gt; # A tibble: 43 × 2\n#&gt;   name        n\n#&gt;   &lt;chr&gt;   &lt;int&gt;\n#&gt; 1 Jan        10\n#&gt; 2 Alex        8\n#&gt; 3 Chris       8\n#&gt; 4 Patrick     6\n#&gt; 5 Nils        5\n#&gt; 6 Fabian      4\n#&gt; 7 Dennis      3\n#&gt; # ℹ 36 more rows\n\n\n\n\nOptimization: OpenStreetMap API and only full names\n\n\n\n\n\n# Stackoverflow\nknitr::include_graphics(\n  \"https://graph.facebook.com/920949401423102/picture?type=large\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# GitHub\nknitr::include_graphics(\n  \"https://avatars.githubusercontent.com/u/8004978?v=4\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       \n\n\n\n\nJob title and company\n\n\n\n\n       \n\n\n\n\nUnique Identifier\n\n\n\n\n       \n\n\n\n\nAble to run through a specific ID\n\n\n\n\n\n\n\nopen_dataset(\"so_users_joined.parquet\") |&gt; \n  \n  glimpse()\n\n\n\nopen_dataset(\"so_users_joined.parquet\") |&gt; \n  filter(DisplayName == \"Christoph Ihl\") |&gt; \n  glimpse()\n\n\n\n\n\n\ngh_api_users_orgs_locs_nested_tbl &lt;- gh_api_users_orgs_locs_tbl |&gt; \n                                        nest(organization = c(ght_org_id, org_login, org_domain, member_created_at))\n\ngh_so_joined_tbl &lt;- gh_api_users_orgs_locs_nested_tbl |&gt; \n  left_join(so_joined_tbl, by = c(login      = \"github_handle\"), na_matches = \"never\") |&gt; \n  left_join(so_joined_tbl, by = c(email_hash = \"EmailHash\"),     na_matches = \"never\")\n\ngh_so_joined_tbl |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 42,557,276 rows x 22 columns\n#&gt; $ api_usr_id                     &lt;int32&gt; 1, 2, 3, 4, 5, 6, 7, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30…\n#&gt; $ ght_usr_id                     &lt;int32&gt; 9236, 1570, 13256, 3892, 96349, 17407, 52402, 171316, 41811, 2159, 1300…\n#&gt; $ login                         &lt;string&gt; \"mojombo\", \"defunkt\", \"pjhyett\", \"wycats\", \"ezmobius\", \"ivey\", \"evanphx…\n#&gt; $ type                 &lt;dictionary&lt;...&gt;&gt; User, User, User, User, User, User, User, User, User, User, User, User,…\n#&gt; $ name                          &lt;string&gt; \"Tom Preston-Werner\", \"Chris Wanstrath\", \"PJ Hyett\", \"Yehuda Katz\", \"Ez…\n#&gt; $ company                       &lt;string&gt; NA, \"@github \", \"GitHub, Inc.\", \"Tilde, Inc.\", \"Stuffstr PBC\", \"@RiotGa…\n#&gt; $ blog                          &lt;string&gt; \"http://tom.preston-werner.com\", \"http://chriswanstrath.com/\", \"https:/…\n#&gt; $ email                         &lt;string&gt; \"tom@mojombo.com\", \"chris@github.com\", \"pj@hyett.com\", \"wycats@gmail.co…\n#&gt; $ email_hash                    &lt;string&gt; \"25c7c18223fb42a4c6ae1c8db6f50f9b\", \"74858be1905a8bbdb565109107384bd9\",…\n#&gt; $ bio                           &lt;string&gt; NA, \"🍔 \", NA, NA, NA, NA, NA, NA, NA, \"Co-founder and CEO, Code Climat…\n#&gt; $ location                      &lt;string&gt; \"San Francisco\", \"San Francisco\", \"San Francisco\", \"San Francisco\", \"In…\n#&gt; $ country_code         &lt;dictionary&lt;...&gt;&gt; US, NA, us, US, DE, US, US, US, US, US, NA, US, CA, US, US, US, US, AL,…\n#&gt; $ state                         &lt;string&gt; \"CA\", NA, \"San Francisco County\", \"CA\", \"Nordrhein-Westfalen\", \"AL\", \"C…\n#&gt; $ city                          &lt;string&gt; \"San Francisco\", NA, \"San Francisco\", \"San Francisco\", \"Hennef (Sieg)\",…\n#&gt; $ usr_created_at &lt;timestamp[us, tz=UTC]&gt; 2007-10-20 05:24:19, 2007-10-20 05:24:19, 2008-01-07 17:54:22, 2008-01-…\n#&gt; $ organization               &lt;list&lt;...&gt;&gt; [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[1 x 4]&gt;], [&lt;tbl_df[16 x …\n#&gt; $ so_id                         &lt;double&gt; NA, NA, NA, 122162, NA, 239960, 1335022, NA, 365701, 7392312, NA, 26342…\n#&gt; $ so_login                      &lt;string&gt; NA, NA, NA, \"Yehuda Katz\", NA, \"Michael D. Ivey\", \"Evan Phoenix\", NA, \"…\n#&gt; $ so_current_position           &lt;string&gt; NA, NA, NA, \"Founder at Tilde, Inc.\", NA, NA, NA, NA, NA, \"CEO at Code …\n#&gt; $ so_twitter_handle             &lt;string&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"brynary\", NA, NA, NA, NA, NA, \"top…\n#&gt; $ so_website_url                &lt;string&gt; NA, NA, NA, \"http://www.tilde.io\", NA, \"http://gweezlebur.com\", \"http:/…\n#&gt; $ so_location                   &lt;string&gt; NA, NA, NA, \"Portland, OR\", NA, \"Bay Minette, AL\", NA, NA, \"Buffalo, NY…\n#&gt; Call `print()` for full schema details"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#angellist",
    "href": "revealjs/slides/2023/diss/diss.html#angellist",
    "title": "Diss",
    "section": " Angellist",
    "text": "Angellist\n\n1. User2. OrgsData ISchemaMergedMerged CB\n\n\n\n\n\n\n\n\n\nHTML is missing in source folder\n\n\n\n\n\n\n\n\n\n\n\nIterated over 10,3 million user ids\n\n\nopen_dataset(\"al_profiles.parquet\") |&gt; \n  glimpse() \n\n\n\nIterated over 7,3 million company ids\n\n\nopen_dataset(\"al_orgs.parquet\") |&gt; \n  glimpse() \n\n\n\n\n\n\n\n\nIterated over 10,3 million user ids\n\n\nopen_dataset(\"al_profiles_main_wrangled.parquet\") |&gt; \n  glimpse() \n\n\n\nIterated over 7,3 million company ids\n\n\nopen_dataset(\"al_employees.parquet\") |&gt; \n  glimpse() \n\n\n\n\n\n\n\n\n\n\n\n\n\ngh_so_al_final &lt;- gh_so_joined_tbl |&gt; \n  \n  # 1. Via github_handle\n  left_join(al_profiles_main_wrangled, by = c(\"login\"    = \"github_handle\"),  na_matches = \"never\") |&gt; \n  # 2. Via SO_handle(s) \n  left_join(al_profiles_main_wrangled, by = c(\"so_login\" = \"so_handle\"),      na_matches = \"never\") |&gt; \n  # 3. Via twitter_handle\n  left_join(al_profiles_main_wrangled, by = c(\"so_twitter_handle\" = \"twitter_handle\"), na_matches = \"never\")\n\n\n\n\ngh_so_al_unnested_tbl &lt;- gh_so_al_unnested_tbl |&gt; \n  \n  left_join(cb_people_handles, by = c(facebook_usr_al = \"facebook_handle\"), na_matches = \"never\") |&gt; \n  left_join(cb_people_handles, by = c(twitter_usr     = \"twitter_handle\"),  na_matches = \"never\") |&gt; \n  left_join(cb_people_handles, by = c(linkedin_usr_al = \"linkedin_handle\"), na_matches = \"never\") |&gt;  \n\n  left_join(cb_orgs_handles,   by = c(facebook_org_al = \"facebook_handle\"), na_matches = \"never\") |&gt; \n  left_join(cb_orgs_handles,   by = c(twitter_org_al  = \"twitter_handle\"),  na_matches = \"never\") |&gt; \n  left_join(cb_orgs_handles,   by = c(linkedin_org_al = \"linkedin_handle\"), na_matches = \"never\") |&gt; \n  \n  left_join(cb_jobs,           by = c(uuid_usr_cb     = \"person_uuid\"))"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#coresignal",
    "href": "revealjs/slides/2023/diss/diss.html#coresignal",
    "title": "Diss",
    "section": " Coresignal",
    "text": "Coresignal\n\nOverview12\n\n\nasd\n\n\nasd\n\n\nasd"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-1-menubarclass",
    "href": "revealjs/slides/2023/diss/diss.html#option-1-menubarclass",
    "title": "Diss",
    "section": "Option 1: menubarclass",
    "text": "Option 1: menubarclass\nThe menubarclass option sets the classname of menubars.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      menubarclass: \"menubar\"\n\nSimplemenu will show the menubar(s) on all pages. If you do not want to show the menubar on certain pages, use data-state=“hide-menubar” on that section. This behaviour also works when exporting to PDF using the Reveal.js ?print-pdf option."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-2-menuclass",
    "href": "revealjs/slides/2023/diss/diss.html#option-2-menuclass",
    "title": "Diss",
    "section": "Option 2: menuclass",
    "text": "Option 2: menuclass\nThe menuclass option sets the classname of the menu.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      menuclass: \"menu\"\nSimplemenu looks inside this menu for list items (LI’s)."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-3-activeclass",
    "href": "revealjs/slides/2023/diss/diss.html#option-3-activeclass",
    "title": "Diss",
    "section": "Option 3: activeclass",
    "text": "Option 3: activeclass\nThe activeclass option is the class an active menuitem gets.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      activeclass: \"active\""
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-4-activeelement",
    "href": "revealjs/slides/2023/diss/diss.html#option-4-activeelement",
    "title": "Diss",
    "section": "Option 4: activeelement",
    "text": "Option 4: activeelement\nThe activeelement option sets the item that gets the active class.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      activeelement: \"li\"\nYou may want to change it to the anchor inside the li, like this: activeelement: \"a\"."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-5-barhtml",
    "href": "revealjs/slides/2023/diss/diss.html#option-5-barhtml",
    "title": "Diss",
    "section": "Option 5: barhtml",
    "text": "Option 5: barhtml\nYou can add the HTML for the header (and/or footer) through this option. This way you no longer need to edit the template.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      barhtml:\n        header: \"&lt;div class='menubar'&gt;&lt;ul class='menu'&gt;&lt;/ul&gt;&lt;/div&gt;\"\n        footer: \"\""
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-5-barhtml-continued",
    "href": "revealjs/slides/2023/diss/diss.html#option-5-barhtml-continued",
    "title": "Diss",
    "section": "Option 5: barhtml (Continued)",
    "text": "Option 5: barhtml (Continued)\nYou can also move the slide number or the controls to your header or footer. If they are nested there manually, or through the barhtml option, they will then display inside that header or footer.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      barhtml:\n        header: \"\"\n        footer: \"&lt;div class='menubar'&gt;&lt;ul class='menu'&gt;&lt;/ul&gt;&lt;div class='slide-number'&gt;&lt;/div&gt;&lt;/div&gt;\""
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-6-flat",
    "href": "revealjs/slides/2023/diss/diss.html#option-6-flat",
    "title": "Diss",
    "section": "Option 6: flat",
    "text": "Option 6: flat\nSometimes you’ll want to limit your presentation to horizontal slides only. To still use ‘chapters’, you can use the flat option. By default, it is set to false, but you can set it to true. Then, when a data-name is set for a slide, any following slides will keep that menu name.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      flat: true\nTo stop inheriting the previous slide menu name, start a new named section, or add data-sm=\"false\" to your slide."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-7-scale",
    "href": "revealjs/slides/2023/diss/diss.html#option-7-scale",
    "title": "Diss",
    "section": "Option 7: scale",
    "text": "Option 7: scale\nWhen you have a lot of subjects/chapters in your menubar, they might not all fit in a row. You can then tweak the scale in the options. Simplemenu copies the Reveal.js (slide) scaling and adds a scale option on top of that.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      scale: 0.67\nIt is set to be two-thirds of the main scaling."
  },
  {
    "objectID": "slides/random-numbers.html",
    "href": "slides/random-numbers.html",
    "title": "Generating random numbers",
    "section": "",
    "text": "In your final project, you will generate a synthetic dataset and use it to conduct an evaluation of some social program. Generating fake or simulated data is an incredibly powerful skill, but it takes some practice. Here are a bunch of helpful resources and code examples of how to use different R functions to generate random numbers that follow specific distributions (or probability shapes).\nThis example focuses primarily on distributions. Each of the columns you’ll generate will be completely independent from each other and there will be no correlation between them. The example for generating synthetic data provides code and a bunch of examples of how to build in correlations between columns.\nFirst, make sure you load the libraries we’ll use throughout the example:\nlibrary(tidyverse)\nlibrary(patchwork)",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#seeds",
    "href": "slides/random-numbers.html#seeds",
    "title": "Generating random numbers",
    "section": "1 Seeds",
    "text": "1 Seeds\nWhen R (or any computer program, really) generates random numbers, it uses an algorithm to simulate randomness. This algorithm always starts with an initial number, or seed. Typically it will use something like the current number of milliseconds since some date, so that every time you generate random numbers they’ll be different. Look at this, for instance:\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\n\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\n\nThey’re different both times.\nThat’s ordinarily totally fine, but if you care about reproducibility (like having a synthetic dataset with the same random values, or having jittered points in a plot be in the same position every time you knit), it’s a good idea to set your own seed. This ensures that the random numbers you generate are the same every time you generate them.\nDo this by feeding set.seed() some numbers. It doesn’t matter what number you use—it just has to be a whole number. People have all sorts of favorite seeds:\n\n1\n13\n42\n1234\n12345\n20201101 (i.e. the current date)\n8675309\n\nYou could even go to random.org and use atmospheric noise to generate a seed, and then use that in R.\nHere’s what happens when you generate random numbers after setting a seed:\n\n# Set a seed\nset.seed(1234)\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\n\n# Set a seed\nset.seed(1234)\n\n# Choose another 3 numbers between 1 and 10\nsample(1:10, 3)\n\nThey’re the same!\nOnce you set a seed, it influences any function that does anything random, but it doesn’t reset. For instance, if you set a seed once and then run sample() twice, you’ll get different numbers the second time, but you’ll get the same different numbers every time:\n\n# Set a seed\nset.seed(1234)\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\nsample(1:10, 3)  # This will be different!\n\n# Set a seed again\nset.seed(1234)\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\nsample(1:10, 3)  # This will be different, but the same as before!\n\nTypically it’s easiest to just include set.seed(SOME_NUMBER) at the top of your script after you load all the libraries. Some functions have a seed argument, and it’s a good idea to use it: position_jitter(..., seed = 1234).",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#distributions",
    "href": "slides/random-numbers.html#distributions",
    "title": "Generating random numbers",
    "section": "2 Distributions",
    "text": "2 Distributions\nRemember in elementary school when you’d decide on playground turns by saying “Pick a number between 1 and 10” and whoever was the closest would win? When you generate random numbers in R, you’re essentially doing the same thing, only with some fancier bells and whistles.\nWhen you ask someone to choose a number between 1 and 10, any of those numbers should be equally likely. 1 isn’t really less common than 5 or anything. In some situations, though, there are numbers that are more likely to appear than others (i.e. when you roll two dice, it’s pretty rare to get a 2, but pretty common to get a 7). These different kinds of likelihood change the shape of the distribution of possible values. There are hundreds of different distributions, but for the sake of generating data, there are only a few that you need to know.\n\nUniform distribution\nIn a uniform distribution, every number is equally likely. This is the “pick a number between 1 and 10” scenario, or rolling a single die. There are a couple ways to work with a uniform distribution in R: (1) sample() and (2) runif().\n\nsample()\nThe sample() function chooses an element from a list.\nFor instance, let’s pretend we have six possible numbers (like a die, or like 6 categories on a survey), like this:\n\npossible_answers &lt;- c(1, 2, 3, 4, 5, 6)  # We could also write this as 1:6 instead\n\nIf we want to randomly choose from this list, you’d use sample(). The size argument defines how many numbers to choose.\n\n# Choose 1 random number\nsample(possible_answers, size = 1)\n\n# Choose 3 random numbers\nsample(possible_answers, size = 3)\n\nOne important argument you can use is replace, which essentially puts the number back into the pool of possible numbers. Imagine having a bowl full of ping pong balls with the numbers 1–6 on them. If you take the number “3” out, you can’t draw it again. If you put it back in, you can pull it out again. The replace argument puts the number back after it’s drawn:\n\n# Choose 10 random numbers, with replacement\nsample(possible_answers, size = 10, replace = TRUE)\n\nIf you don’t specify replace = TRUE, and you try to choose more numbers than are in the set, you’ll get an error:\n\n# Choose 8 numbers between 1 and 6, but don't replace them.\n# This won't work!\nsample(possible_answers, size = 8)\n\nIt’s hard to see patterns in the outcomes when generating just a handful of numbers, but easier when you do a lot. Let’s roll a die 1,000 times:\n\nset.seed(1234)\ndie &lt;- tibble(value = sample(possible_answers,\n                             size = 1000,\n                             replace = TRUE))\ndie %&gt;%\n  count(value)\n\nggplot(die, aes(x = value)) +\n  geom_bar() +\n  labs(title = \"1,000 rolls of a single die\")\n\nIn this case, 3 and 6 came up more often than the others, but that’s just because of randomness. If we rolled the die 100,000 times, the bars should basically be the same:\n\nset.seed(1234)\ndie &lt;- tibble(value = sample(possible_answers,\n                             size = 100000,\n                             replace = TRUE))\n\nggplot(die, aes(x = value)) +\n  geom_bar() +\n  labs(title = \"100,000 rolls of a single die\")\n\n\n\nrunif()\nAnother way to generate uniformly distributed numbers is to use the runif() function (which is short for “random uniform”, and which took me years to realize, and for years I wondered why people used a function named “run if” when there’s no if statement anywhere??)\nrunif() will choose numbers between a minimum and a maximum. These numbers will not be whole numbers. By default, the min and max are 0 and 1:\n\nrunif(5)\n\nHere are 5 numbers between 35 and 56:\n\nrunif(5, min = 35, max = 56)\n\nSince these aren’t whole numbers, you can round them to make them look more realistic (like, if you were generating a column for age, you probably don’t want people who are 21.5800283 years old):\n\n# Generate 5 people between the ages of 18 and 35\nround(runif(5, min = 18, max = 35), 0)\n\nYou can confirm that each number has equal probability if you make a histogram. Here are 5,000 random people between 18 and 35:\n\nset.seed(1234)\nlots_of_numbers &lt;- tibble(x = runif(5000, min = 18, max = 35))\n\nggplot(lots_of_numbers, aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 18)\n\n\n\n\nNormal distribution\nThe whole “choose a number between 1 and 10” idea of a uniform distribution is neat and conceptually makes sense, but most numbers that exist in the world tend to have higher probabilities around certain values—almost like gravity around a specific point. For instance, income in the United States is not uniformly distributed—a handful of people are really really rich, lots are very poor, and most are kind of clustered around an average.\nThe idea of having possible values clustered around an average is how the rest of these distributions work (uniform distributions don’t have any sort of central gravity point; all these others do). Each distribution is defined by different things called parameters, or values that determine the shape of the probabilities and locations of the clusters.\nA super common type of distribution is the normal distribution. This is the famous “bell curve” you learn about in earlier statistics classes. A normal distribution has two parameters:\n\nA mean (the center of the cluster)\nA standard deviation (how much spread there is around the mean).\n\nIn R, you can generate random numbers from a normal distribution with the rnorm() function. It takes three arguments: the number of numbers you want to generate, the mean, and the standard deviation. It defaults to a mean of 0 and a standard deviation of 1, which means most numbers will cluster around 0, with a lot between −1 and 1, and some going up to −2 and 2 (technically 67% of numbers will be between −1 and 1, while 95% of numbers will be between −2–2ish)\n\nrnorm(5)\n\n# Cluster around 10, with an SD of 4\nrnorm(5, mean = 10, sd = 4)\n\nWhen working with uniform distributions, it’s easy to know how high or low your random values might go, since you specify a minimum and maximum number. With a normal distribution, you don’t specify starting and ending points—you specify a middle and a spread, so it’s harder to guess the whole range. Plotting random values is thus essential. Here’s 1,000 random numbers clustered around 10 with a standard deviation of 4:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(x = rnorm(1000, mean = 10, sd = 4))\nhead(plot_data)\n\nggplot(plot_data, aes(x = x)) +\n  geom_histogram(binwidth = 1, boundary = 0, color = \"white\")\n\nNeat. Most numbers are around 10; lots are between 5 and 15; some go as high as 25 and as low as −5.\nWatch what happens if you change the standard deviation to 10 to make the spread wider:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(x = rnorm(1000, mean = 10, sd = 10))\nhead(plot_data)\n\nggplot(plot_data, aes(x = x)) +\n  geom_histogram(binwidth = 1, boundary = 0, color = \"white\")\n\nIt’s still centered around 10, but now you get values as high as 40 and as low as −20. The data is more spread out now.\nWhen simulating data, you’ll most often use a normal distribution just because it’s easy and lots of things follow that pattern in the real world. Incomes, ages, education, etc. all have a kind of gravity to them, and a normal distribution is a good way of showing that gravity. For instance, here are 1,000 simulated people with reasonable random incomes, ages, and years of education:\n\nset.seed(1234)\n\nfake_people &lt;- tibble(income = rnorm(1000, mean = 40000, sd = 15000),\n                      age = rnorm(1000, mean = 25, sd = 8),\n                      education = rnorm(1000, mean = 16, sd = 4))\nhead(fake_people)\n\nfake_income &lt;- ggplot(fake_people, aes(x = income)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0) +\n  labs(title = \"Simulated income\")\n\nfake_age &lt;- ggplot(fake_people, aes(x = age)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0) +\n  labs(title = \"Simulated age\")\n\nfake_education &lt;- ggplot(fake_people, aes(x = education)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0) +\n  labs(title = \"Simulated education\")\n\nfake_income + fake_age + fake_education\n\nThese three columns all have different centers and spreads. Income is centered around $45,000, going up to almost $100,000 and as low as −$10,000; age is centered around 25, going as low as 0 and as high as 50; education is centered around 16, going as low as 3 and as high as 28. Cool.\nAgain, when generating these numbers, it’s really hard to know how high or low these ranges will be, so it’s a good idea to plot them constantly. I settled on sd = 4 for education only because I tried things like 1 and 10 and got wild looking values (everyone basically at 16 with little variation, or everyone ranging from −20 to 50, which makes no sense when thinking about years of education). Really it’s just a process of trial and error until the data looks good and reasonable.\n\n\nTruncated normal distribution\nSometimes you’ll end up with negative numbers that make no sense. Look at income in the plot above, for instance. Some people are earning −$10,000 year. The rest of the distribution looks okay, but those negative values are annoying.\nTo fix this, you can use something called a truncated normal distribution, which lets you specify a mean and standard deviation, just like a regular normal distribution, but also lets you specify a minimum and/or maximum so you don’t get values that go too high or too low.\nR doesn’t have a truncated normal function built-in, but you can install the truncnorm package and use the rtruncnorm() function. A truncated normal distribution has four parameters:\n\nA mean (mean)\nA standard deviation (sd)\nA minimum (optional) (a)\nA maximum (optional) (b)\n\nFor instance, let’s pretend you have a youth program designed to target people who are between 12 and 21 years old, with most around 14. You can generate numbers with a mean of 14 and a standard deviation of 5, but you’ll create people who are too old, too young, or even negatively aged!\n\nset.seed(1234)\n\nplot_data &lt;- tibble(fake_age = rnorm(1000, mean = 14, sd = 5))\nhead(plot_data)\n\nggplot(plot_data, aes(x = fake_age)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0)\n\nTo fix this, truncate the range at 12 and 21:\n\nlibrary(truncnorm)  # For rtruncnorm()\n\nset.seed(1234)\n\nplot_data &lt;- tibble(fake_age = rtruncnorm(1000, mean = 14, sd = 5, a = 12, b = 21))\nhead(plot_data)\n\nggplot(plot_data, aes(x = fake_age)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 0)\n\nAnd voila! A bunch of people between 12 and 21, with most around 14, with no invalid values.\n\n\nBeta distribution\nNormal distributions are neat, but they’re symmetrical around the mean (unless you truncate them). What if your program involves a test with a maximum of 100 points where most people score around 85, but a sizable portion score below that. In other words, it’s not centered at 85, but is skewed left.\nTo simulate this kind of distribution, we can use a Beta distribution. Beta distributions are neat because they naturally only range between 0 and 1—they’re perfect for things like percentages or proportions or or 100-based exams.\nUnlike a normal distribution, where you use the mean and standard deviation as parameters, Beta distributions take two non-intuitive parameters:\n\nshape1\nshape2\n\nWhat the heck are these shapes though?! This answer at Cross Validated does an excellent job of explaining the intuition behind Beta distributions and it’d be worth it to read it.\nBasically, Beta distributions are good at modeling probabilities of things, and shape1 and shape2 represent specific parts of a probability formula.\nLet’s say that there’s an exam with 10 points where most people score a 6/10. Another way to think about this is that an exam is a collection of correct answers and incorrect answers, and that the percent correct follows this equation:\n\\[\n\\frac{\\text{Number correct}}{\\text{Number correct} + \\text{Number incorrect}}\n\\]\nIf you scored a 6, you could write that as:\n\\[\n\\frac{6}{6 + 4}\n\\]\nTo make it more general, we can use Greek variable names: \\(\\alpha\\) for the number correct and \\(\\beta\\) for the number incorrect, leaving us with this:\n\\[\n\\frac{\\alpha}{\\alpha + \\beta}\n\\]\nNeat.\nIn a Beta distribution, the \\(\\alpha\\) and \\(\\beta\\) in that equation correspond to shape1 and shape2. If we want to generate random scores for this test where most people get 6/10, we can use rbeta():\n\nset.seed(1234)\n\nplot_data &lt;- tibble(exam_score = rbeta(1000, shape1 = 6, shape2 = 4)) %&gt;%\n  # rbeta() generates numbers between 0 and 1, so multiply everything by 10 to\n  # scale up the exam scores\n  mutate(exam_score = exam_score * 10)\n\nggplot(plot_data, aes(x = exam_score)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:10)\n\nMost people score around 6, with a bunch at 5 and 7, and fewer in the tails. Importantly, it’s not centered at 6—the distribution is asymmetric.\nThe magic of—and most confusing part about—Beta distributions is that you can get all sorts of curves by just changing the shape parameters. To make this easier to see, we can make a bunch of different Beta distributions. Instead of plotting them with histograms, we’ll use density plots (and instead of generating random numbers, we’ll plot the actual full range of the distribution (that’s what dbeta and geom_function() do in all these examples)).\nHere’s what we saw before, with \\(\\alpha\\) (shape1) = 6 and \\(\\beta\\) (shape2) = 4:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 6, shape2 = 4))\n\nAgain, there’s a peak at 0.6 (or 6), which is what we expected.\nWe can make the distribution narrower if we scale the shapes up. Here pretty much everyone scores around 50% and 75%.\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 60, shape2 = 40))\n\nSo far all these curves look like normal distributions, just slightly skewed. But when if most people score 90–100%? Or most fail? A Beta distribution can handle that too:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 9, shape2 = 1), color = \"blue\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 1, shape2 = 9), color = \"red\")\n\nWith shape1 = 9 and shape2 = 1 (or \\(\\frac{9}{9 + 1}\\)) we get most around 90%, while shape1 = 1 and shape2 = 9 (or \\(\\frac{1}{1 + 9}\\)) gets us most around 10%.\nCheck out all these other shapes too:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 5, shape2 = 5), color = \"blue\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 2, shape2 = 5), color = \"red\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 80, shape2 = 23), color = \"orange\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 13, shape2 = 17), color = \"brown\")\n\nIn real life, if I don’t want to figure out the math behind the \\(\\frac{\\alpha}{\\alpha + \\beta}\\) shape values, I end up just choosing different numbers until it looks like the shape I want, and then I use rbeta() with those parameter values. Like, how about we generate some numbers based on the red line above, with shape1 = 2 and shape2 = 5, which looks like it should be centered around 0.2ish (\\(\\frac{2}{2 + 5} = 0.2857\\)):\n\nset.seed(1234)\n\nplot_data &lt;- tibble(thing = rbeta(1000, shape1 = 2, shape2 = 5)) %&gt;%\n  mutate(thing = thing * 100)\nhead(plot_data)\n\nggplot(plot_data, aes(x = thing)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0)\n\nIt worked! Most values are around 20ish, but some go up to 60–80.\n\n\nBinomial distribution\nOften you’ll want to generate a column that only has two values: yes/no, treated/untreated, before/after, big/small, red/blue, etc. You’ll also likely want to control the proportions (25% treated, 62% blue, etc.). You can do this in two different ways: (1) sample() and (2) rbinom().\n\nsample()\nWe already saw sample() when we talked about uniform distributions. To generate a binary variable with sample(), just feed it a list of two possible values:\n\nset.seed(1234)\n\n# Choose 5 random T/F values\npossible_things &lt;- c(TRUE, FALSE)\nsample(possible_things, 5, replace = TRUE)\n\nR will choose these values with equal/uniform probability by default, but you can change that in sample() with the prob argument. For instance, pretend you want to simulate an election. According to the latest polls, one candidate has an 80% chance of winning. You want to randomly choose a winner based on that chance. Here’s how to do that with sample():\n\nset.seed(1234)\ncandidates &lt;- c(\"Person 1\", \"Person 2\")\nsample(candidates, size = 1, prob = c(0.8, 0.2))\n\nPerson 1 wins!\nIt’s hard to see the weighted probabilities when you just choose one, so let’s pretend there are 1,000 elections:\n\nset.seed(1234)\nfake_elections &lt;- tibble(winner = sample(candidates,\n                                         size = 1000,\n                                         prob = c(0.8, 0.2),\n                                         replace = TRUE))\nfake_elections %&gt;%\n  count(winner)\n\nggplot(fake_elections, aes(x = winner)) +\n  geom_bar()\n\nPerson 1 won 792 of the elections. Neat.\n(This is essentially what election forecasting websites like FiveThirtyEight do! They just do it with way more sophisticated simulations.)\n\n\nrbinom()\nInstead of using sample(), you can use a formal distribution called the binomial distribution. This distribution is often used for things that might have “trials” or binary outcomes that are like success/failure or yes/no or true/false\nThe binomial distribution takes two parameters:\n\nsize: The number of “trials”, or times that an event happens\nprob: The probability of success in each trial\n\nIt’s easiest to see some examples of this. Let’s say you have a program that has a 60% success rate and it is tried on groups of 20 people 5 times. The parameters are thus size = 20 (since there are twenty people per group) and prob = 0.6 (since there is a 60% chance of success):\n\nset.seed(1234)\n\nrbinom(5, size = 20, prob = 0.6)\n\nThe results here mean that in group 1, 15/20 (75%) people had success, in group 2, 11/20 (55%) people had success, and so on. Not every group will have exactly 60%, but they’re all kind of clustered around that.\nHOWEVER, I don’t like using rbinom() like this, since this is all group-based, and when you’re generating fake people you generally want to use individuals, or groups of 1. So instead, I assume that size = 1, which means that each “group” is only one person large. This forces the generated numbers to either be 0 or 1:\n\nset.seed(1234)\n\nrbinom(5, size = 1, prob = 0.6)\n\nHere, only 1 of the 5 people were 1/TRUE/yes, which is hardly close to a 60% chance overall, but that’s because we only generated 5 numbers. If we generate lots, we can see the probability of yes emerge:\n\nset.seed(12345)\n\nplot_data &lt;- tibble(thing = rbinom(2000, 1, prob = 0.6)) %&gt;%\n  # Make this a factor since it's basically a yes/no categorical variable\n  mutate(thing = factor(thing))\n\nplot_data %&gt;%\n  count(thing) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(plot_data, aes(x = thing)) +\n  geom_bar()\n\n58% of the 2,000 fake people here were 1/TRUE/yes, which is close to the goal of 60%. Perfect.\n\n\n\nPoisson distribution\nOne last common distribution that you might find helpful when simulating data is the Poisson distribution (in French, “poisson” = fish, but here it’s not actually named after the animal, but after French mathematician Siméon Denis Poisson).\nA Poisson distribution is special because it generates whole numbers (i.e. nothing like 1.432) that follow a skewed pattern (i.e. more smaller values than larger values). There’s all sorts of fancy math behind it that you don’t need to worry about so much—all you need to know is that it’s good at modeling things called Poisson processes.\nFor instance, let’s say you’re sitting at the front door of a coffee shop (in pre-COVID days) and you count how many people are in each arriving group. You’ll see something like this:\n\n1 person\n1 person\n2 people\n1 person\n3 people\n2 people\n1 person\n\nLots of groups of one, some groups of two, fewer groups of three, and so on. That’s a Poisson process: a bunch of independent random events that combine into grouped events.\nThat sounds weird and esoteric (and it is!), but it reflects lots of real world phenomena, and things you’ll potentially want to measure in a program. For instance, the number of kids a family has follows a type of Poisson process. Lots of families have 1, some have 2, fewer have 3, even fewer have 4, and so on. The number of cars in traffic, the number of phone calls received by an office, arrival times in a line, and even the outbreak of wars are all examples of Poisson processes.\nYou can generate numbers from a Poisson distribution with the rpois() function in R. This distribution only takes a single parameter:\n\nlambda (\\(\\lambda\\))\n\nThe \\(\\lambda\\) value controls the rate or speed that a Poisson process increases (i.e. jumps from 1 to 2, from 2 to 3, from 3 to 4, etc.). I have absolutely zero mathematical intuition for how it works. The two shape parameters for a Beta distribution at least fit in a fraction and you can wrap your head around that, but the lambda in a Poisson distribution is just a mystery to me. So whenever I use a Poisson distribution for something, I just play with the lambda until the data looks reasonable.\nLet’s assume that the number of kids a family has follows a Poisson process. Here’s how we can use rpois() to generate that data:\n\nset.seed(123)\n\n# 10 different families\nrpois(10, lambda = 1)\n\nCool. Most families have 0–1 kids; some have 2; one has 3.\nIt’s easier to see these patterns with a plot:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(num_kids = rpois(500, lambda = 1))\nhead(plot_data)\n\nplot_data %&gt;%\n  group_by(num_kids) %&gt;%\n  summarize(count = n()) %&gt;%\n  mutate(proportion = count / sum(count))\n\nggplot(plot_data, aes(x = num_kids)) +\n  geom_bar()\n\nHere 75ish% of families have 0–1 kids (36% + 37.4%), 17% have 2 kids, 6% have 3, 2% have 4, and only 0.6% have 5.\nWe can play with the \\(\\lambda\\) to increase the rate of kids per family:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(num_kids = rpois(500, lambda = 2))\nhead(plot_data)\n\nplot_data %&gt;%\n  group_by(num_kids) %&gt;%\n  summarize(count = n()) %&gt;%\n  mutate(proportion = count / sum(count))\n\nggplot(plot_data, aes(x = num_kids)) +\n  geom_bar()\n\nNow most families have 1–2 kids. Cool.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#rescaling-numbers",
    "href": "slides/random-numbers.html#rescaling-numbers",
    "title": "Generating random numbers",
    "section": "3 Rescaling numbers",
    "text": "3 Rescaling numbers\nAll these different distributions are good at generating general shapes:\n\nUniform: a bunch of random numbers with no central gravity\nNormal: an average ± some variation\nBeta: different shapes and skews and gravities between 0 and 1\nBinomial: yes/no outcomes that follow some probability\n\nThe shapes are great, but you also care about the values of these numbers. This can be tricky. As we saw earlier with a normal distribution, sometimes you’ll get values that go below zero or above some value you care about. We fixed that with a truncated normal distribution, but not all distributions have truncated versions. Additionally, if you’re using a Beta distribution, you’re stuck in a 0–1 scale (or 0–10 or 0–100 if you multiply the value by 10 or 100 or whatever).\nWhat if you want a fun skewed Beta shape for a variable like income or some other value that doesn’t fit within a 0–1 range? You can rescale any set of numbers after-the-fact using the rescale() function from the scales library and rescale things to whatever range you want.\nFor instance, let’s say that income isn’t normally distributed, but is right-skewed with a handful of rich people. This might look like a Beta distribution with shape1 = 2 and shape2 = 5:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 2, shape2 = 5))\n\nIf we generate random numbers from this distribution, they’ll all be stuck between 0 and 1:\n\nset.seed(1234)\n\nfake_people &lt;- tibble(income = rbeta(1000, shape1 = 2, shape2 = 5))\n\nggplot(fake_people, aes(x = income)) +\n  geom_histogram(binwidth = 0.1, color = \"white\", boundary = 0)\n\nWe can take those underling 0–1 values and rescale them to some other range using the rescale() function. We can specify the minimum and maximum values in the to argument. Here we’ll scale it up so that 0 = $10,000 and 1 = $100,000. Our rescaled version follows the same skewed Beta distribution shape, but now we’re using better values!\n\nlibrary(scales)\n\nfake_people_scaled &lt;- fake_people %&gt;%\n  mutate(income_scaled = rescale(income, to = c(10000, 100000)))\nhead(fake_people_scaled)\n\nggplot(fake_people_scaled, aes(x = income_scaled)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0)\n\nThis works for anything, really. For instance, instead of specifying a mean and standard deviation for a normal distribution and hoping that the generated values don’t go too high or too low, you can generate a normal distribution with a mean of 0 and standard deviation of 1 and then rescale it to the range you want:\n\nset.seed(1234)\n\nfake_data &lt;- tibble(age_not_scaled = rnorm(1000, mean = 0, sd = 1)) %&gt;%\n  mutate(age = rescale(age_not_scaled, to = c(18, 65)))\nhead(fake_data)\n\nplot_unscaled &lt;- ggplot(fake_data, aes(x = age_not_scaled)) +\n  geom_histogram(binwidth = 0.5, color = \"white\", boundary = 0)\n\nplot_scaled &lt;- ggplot(fake_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0)\n\nplot_unscaled + plot_scaled\n\nThis gives you less control over the center of the distribution (here it happens to be 40 because that’s in the middle of 18 and 65), but it gives you more control over the edges of the distribution.\nRescaling things is really helpful when building in effects and interacting columns with other columns, since multiplying variables by different coefficients can make the values go way out of the normal range. You’ll see a lot more of that in the synthetic data example.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#summary",
    "href": "slides/random-numbers.html#summary",
    "title": "Generating random numbers",
    "section": "4 Summary",
    "text": "4 Summary\nPhew. We covered a lot here, and we barely scratched the surface of all the distributions that exist. Here’s a helpful summary of the main distributions you should care about:",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#example",
    "href": "slides/random-numbers.html#example",
    "title": "Generating random numbers",
    "section": "5 Example",
    "text": "5 Example\nAnd here’s an example dataset of 1,000 fake people and different characteristics. One shortcoming of this fake data is that each of these columns is completely independent—there’s no relationship between age and education and family size and income. You can see how to make these columns correlated (and make one cause another!) in the example for synthetic data.\n\nset.seed(1234)\n\n# Set the number of people here once so it's easier to change later\nn_people &lt;- 1000\n\nexample_fake_people &lt;- tibble(\n  id = 1:n_people,\n  opinion = sample(1:5, n_people, replace = TRUE),\n  age = runif(n_people, min = 18, max = 80),\n  income = rnorm(n_people, mean = 50000, sd = 10000),\n  education = rtruncnorm(n_people, mean = 16, sd = 6, a = 8, b = 24),\n  happiness = rbeta(n_people, shape1 = 2, shape2 = 1),\n  treatment = sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.3, 0.7)),\n  size = rbinom(n_people, size = 1, prob = 0.5),\n  family_size = rpois(n_people, lambda = 1) + 1  # Add one so there are no 0s\n) %&gt;%\n  # Adjust some of these columns\n  mutate(opinion = recode(opinion, \"1\" = \"Strongly disagree\",\n                          \"2\" = \"Disagree\", \"3\" = \"Neutral\",\n                          \"4\" = \"Agree\", \"5\" = \"Strongly agree\")) %&gt;%\n  mutate(size = recode(size, \"0\" = \"Small\", \"1\" = \"Large\")) %&gt;%\n  mutate(happiness = rescale(happiness, to = c(1, 8)))\n\nhead(example_fake_people)\n\n\nplot_opinion &lt;- ggplot(example_fake_people, aes(x = opinion)) +\n  geom_bar() +\n  guides(fill = \"none\") +\n  labs(title = \"Opinion (uniform with sample())\")\n\nplot_age &lt;- ggplot(example_fake_people, aes(x = age)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0) +\n  labs(title = \"Age (uniform with runif())\")\n\nplot_income &lt;- ggplot(example_fake_people, aes(x = income)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0) +\n  labs(title = \"Income (normal)\")\n\nplot_education &lt;- ggplot(example_fake_people, aes(x = education)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0) +\n  labs(title = \"Education (truncated normal)\")\n\nplot_happiness &lt;- ggplot(example_fake_people, aes(x = happiness)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 1:8) +\n  labs(title = \"Happiness (Beta, rescaled to 1-8)\")\n\nplot_treatment &lt;- ggplot(example_fake_people, aes(x = treatment)) +\n  geom_bar() +\n  labs(title = \"Treatment (binary with sample())\")\n\nplot_size &lt;- ggplot(example_fake_people, aes(x = size)) +\n  geom_bar() +\n  labs(title = \"Size (binary with rbinom())\")\n\nplot_family &lt;- ggplot(example_fake_people, aes(x = family_size)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 1:7) +\n  labs(title = \"Family size (Poisson)\")\n\n(plot_opinion + plot_age) / (plot_income + plot_education)\n\n\n(plot_happiness + plot_treatment) / (plot_size + plot_family)",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/slides_2022.html",
    "href": "slides/slides_2022.html",
    "title": "2022 - February",
    "section": "",
    "text": "First presentation",
    "crumbs": [
      "Presentations",
      "2022",
      "February"
    ]
  },
  {
    "objectID": "slides/slides_2024.html",
    "href": "slides/slides_2024.html",
    "title": "2024",
    "section": "",
    "text": "Druid2024",
    "crumbs": [
      "Presentations",
      "2024",
      "Conferences"
    ]
  }
]