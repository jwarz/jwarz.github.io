{
  "hash": "7ce59245fa58e3d618643b1b22e4944a",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Regression Modeling with rstanarm\"\nauthor: \"Joschka Schwarz\"\ntoc-depth: 2\n---\n\n\n\n\n**Short Description**\n\nLearn how to leverage Bayesian estimation methods to make better inferences about linear regression models.\n\n**Long Description**\n\nBayesian estimation offers a flexible alternative to modeling techniques where the inferences depend on p-values. In this course, you’ll learn how to estimate linear regression models using Bayesian methods and the rstanarm package. You’ll be introduced to prior distributions, posterior predictive model checking, and model comparisons within the Bayesian framework. You’ll also learn how to use your estimated model to make predictions for new data.\n\n# 1. Introduction to Bayesian Linear Models\n\nA review of frequentist regression using lm(), an introduction to Bayesian regression using stan_glm(), and a comparison of the respective outputs.\n\n## Non-Bayesian Linear Regression\n\nTheory. Coming soon ...\n\n**1. Welcome!**\n\nHello! My name is Jake Thompson. I'm a psychometrician at the University of Kansas, and I'll be your instructor in this course. For this course I'm assuming that you are already familiar with linear regression and know the basics of Bayesian analysis.\n\n**2. Overview**\n\nThroughout this course we'll learn how to estimate a Bayesian model, customize a model, evaluate a model and its predictive power, and finally how to present and use a Bayesian regression model.\n\n**3. A review of frequentist regression**\n\nBefore we get into the Bayesian methods, we'll first review linear regression using non-Bayesian, or frequentist, methods. This will provide helpful comparisons between the inferences we can make when using frequentist and Bayesian methods. For examples, I'll be using the kidiq data from the rstanarm package, a package for Bayesian applied regression modeling which you'll be introduced to in the next video and use throughout this course. This dataset includes scores of kids on an IQ test, along with the mother's IQ, age, and whether or not she finished high school.\n\n**4. A review of frequentist regression**\n\nWe can estimate a frequentist linear regression by using the lm function. For example, we can predict a child's score from the mother's IQ. We can then look at a summary of the model. This output should look familiar. We have information about the model's residuals, coefficient estimates, and information about how our model is performing.\n\n**5. Examing model coefficients**\n\nIf we only want information about the coefficients, we can use the tidy function from the broom package. This shows the estimate, standard error, test statistic and p-value for each coefficient in the model. Using a p-value cutoff of 0.05, we see that the mom's IQ is a significant predictor of the child's score on the IQ test.However, recall what the p-value really tells us. This only tells us the probability of observing data that give rise to a test statistic this large if the true value of the parameter were zero. This is the key problem with frequentist regression.\n\n**6. Comparing Frequentist and Bayesian probabilities**\n\nTo illustrate, let's calculate the probability of a woman having cancer, given a positive mammogram. We know that if a woman has cancer, they will have a positive mammogram 90% of the time. This is like the p-value, the probability of our data, given a null hypothesis. We also know that in the United States, 0.4% of women have breast cancer. This what we will later call our prior, or our belief about the parameter before looking at the data. From this we can calculate that the probability of a random woman getting a positive mammogram is 10%. So, given a positive mammogram, what are the chances that the woman has cancer? Only 3.6%! This is very different from the 90%, and illustrates the importance of making inferences about the parameter we are interested in (the probability of cancer), rather than the data (the probability of a positive mammogram).In this course, we'll apply these Bayesian methods to regression to make better inferences about model parameters.\n\n**7. Spotify data**\n\nFor the exercises throughout this course, we'll be using data on Adele, Beyoncé, and Taylor Swift songs from the Spotify API. This data includes the name and artist of each song, the age of the song in days, the valence, or how positive or negative the song sounds, the tempo, or speed, of the song, the popularity according to Spotify, and the length of the song. Throughout this course, we'll predict the popularity from the other variables.\n\n**8. Let's practice!**\n\nLet's start with a frequentist regression.\n\n## Exploring the data\n\nLet's get familiar with the Spotify data, `songs`, which is already loaded for you. Before we start developing models, it's a good idea to take a peek at our data to make sure we know everything that is included.\n\n**Steps**\n\n1. Print the first 6 rows of the dataset.\n2. Print the structure of the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nlibrary(readr)\nsongs <- read_csv(\"data/datacamp-spotify-data.csv\")\n\n# Print the first 6 rows\nhead(songs)\n\n# Print the structure\nstr(songs)\n```\n:::\n\n\nGreat! The data contains information on the name and artist of each song, along with quantitative information related to the tempo, valence, popularity, age, and length. Now let's use the data to estimate a regression model.\n\n## Fitting a frequentist linear regression\n\nPractice creating a linear model using data on songs from Spotify. This will give us base line to compare our Bayesian model to. The `songs` dataset is already loaded for you.\n\n**Steps**\n\n1. Create a linear model, `lm_model`, that predicts song popularity from song age.\n2. Print a summary of the linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the model here\nlm_model <- lm(popularity ~ song_age, data = songs)\n\n# Produce the summary\nsummary(lm_model)\n```\n:::\n\n\n3. Use the **broom** package to view only the coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n# Print a tidy summary of the coefficients\ntidy(lm_model)\n```\n:::\n\n\nNice work! You've created a frequentist linear model and learned how to examine the output. In this model, the coefficient for `song_age` is -0.00585, which means we'd expected the popularity of a song to decrease by -0.00585 for each additional day.\n\n## Bayesian Linear Regression\n\nTheory. Coming soon ...\n\n**1. Bayesian Linear Regression**\n\nNow that we've reviewed the characteristics of a frequentist regression, let's examine how to estimate that same regression using Bayesian methods.\n\n**2. Why use Bayesian methods?**\n\nAs we learned in the last lesson, if we want to make inferences about the actual values of parameters, p-values and frequentist regression fails us. Bayesian estimation is one solution to this problem. With Bayesian methods the likelihood that used for frequentist regression, and what's known as a prior, form a posterior distribution. The details of this process are beyond the scope of this course. The key point is that Bayesian methods sample from this posterior distribution, and we can then create summaries of the distributions to make parameter inferences. Using the summaries allows us to make inferences about what values parameters might take.\n\n**3. The rstanarm package**\n\nTo estimate Bayesian regression models in this course, we'll be using the rstanarm package. rstanarm is an interface to Stan, which is a programming language for Bayesian inference. The rstanarm package offers a high level interface with pre-written Stan scripts for common models, like linear regression.\n\n**4. Using rstanarm**\n\nWe can load rstanarm using the normal library command that we use to load all packages. We can then estimate a linear regression using the stan_glm function. Here, we estimate the same model that we estimated using the lm function. Normally, when using the stan_glm function, we would see a great deal of output that looks like this. This output mainly provides progress updates. However, the models we'll estimate in this course all estimate very quickly. Therefore, this output has been suppressed in the rest of the course.\n\n**5. Examining an rstanarm model**\n\nJust like a regression estimated with lm, we can look at a summary of a regression estimated with stan_glm. Using the summary function, we are presented with some information about the estimated model, the parameter estimates, and some model diagnostics.\n\n**6. rstanarm summary: Estimates**\n\nNotice the the parameter estimates no longer have test statistics and p-values as in the frequentist regression. This is because Bayesian estimation samples from the posterior distribution. This means that instead of a point estimate and a test statistic, we get a distribution of plausible values for the parameters, and the estimates section summarizes those distributions. Specifically we get the mean, standard deviation, and commonly used percentiles.We also see that there are parameters in the estimates section other than the (Intercept) and mom_iq coefficient that we entered into the model. Sigma represents the standard deviation of errors, mean_ppd is the mean of the posterior predictive distribution our our outcome variable, kid_iq. We'll learn more about predictive distributions in Chapter 3. Finally, log-posterior is analogous to the likelihood of a frequentist regression. This represents the log of the combined posterior distributions. This will be used for model comparisons, which we'll also explore in Chapter 3.\n\n**7. rstanarm summary: Diagnostics**\n\nIn the diagnostics section, the most important statistic to pay attention to is the R-hat. Unlike in frequentist regression where there is always a solution using ordinary least squares, in Bayesian models we have to check to make sure the model converged. If a model is converged, then the parameter estimates are stable. Otherwise, our results will be unreliable. In Bayesian estimation, posterior distributions are sampled in groups, known as chains. By comparing the variance within chains to the variance across chains, we can measure the stability of our estimates. This is the R-hat statistic. In general, we want all R-hat values to be less than 1.1 in order to conclude the model has converged, as in this example.\n\n**8. Let's practice!**\n\nNow you try estimating a Bayesian regression model.\n\n## Fitting a Bayesian linear regression\n\nPractice fitting a Bayesian model. This is the same model we already estimated with frequentist methods, so we'll be able to compare the parameter outputs later. The `songs` data is already loaded.\n\n**Steps**\n\n1. Create a Bayesian linear model, `stan_model`, that predicts song popularity from song age\n2. Print a summary of the Bayesian linear model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(rstanarm)\n\n# Create the model here\nstan_model <- stan_glm(popularity ~ song_age, data = songs)\n\n# Produce the summary\nsummary(stan_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#   The supplied model object seems to be outputted from the rstanarm package. Tidiers for mixed model output now live in the broom.mixed package.\n# Load package\nlibrary(broom.mixed)\n\n# Print a tidy summary of the coefficients\ntidy(stan_model)\n```\n:::\n\n\nYes! You now know how to estimate a Bayesian regression model! Notice that the parameter estimates are very similar to those of the frequentist model. Bayesian estimation won't usually have a large impact on your estimates, but will greatly influence the types of estimates you are able to make.\n\n## Convergence criteria\n\n> *Question*\n> ---\n> What is the accepted threshold for Rhat to conclude the model has converged?<br>\n> <br>\n> ⬜ 1.0<br>\n> ✅ 1.1<br>\n> ⬜ 1.5<br>\n> ⬜ 2<br>\n\nThat's right! If all Rhat values are below 1.1, your model converged!\n\n## Assessing model convergence\n\nHas the Bayesian regression model `stan_model` converged?\n\n> *Question*\n> ---\n> ???<br>\n> <br>\n> ✅ Yes!<br>\n> ⬜ No way!<br>\n> ⬜ We don't have enough information.<br>\n\nCorrect! All of the Rhat values are less than 1.1.\n\n## Comparing frequentist and Bayesian methods\n\nTheory. Coming soon ...\n\n## Difference between frequentists and Bayesians\n\n> *Question*\n> ---\n> What is the core difference between frequentists and Bayesians?<br>\n> <br>\n> ✅ Frequentists believe data is random, Bayesians assume parameters are random<br>\n> ⬜ Frequentists believe data is fixed, Bayesians assume parameters are fixed<br>\n> ⬜ There is no difference, just a matter of preference<br>\n> ⬜ Bayesian estimation requires special data<br>\n\nYes! Frequentists assume data is random and parameters are fixed, whereas Bayesian assume the opposite.\n\n## Creating credible intervals\n\nPractice creating credible intervals. Credible intervals allow us to make inferences about the probability of a parameter taking a given value. This is how we determine if a parameter is meaningful when estimated with Bayesian methods. The Bayesian model, `stan_model`, is already created for you.\n\n**Steps**\n\n1. Create 90% credible intervals for the parameters in `stan_model`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the 90% credible intervals\nposterior_interval(stan_model)\n```\n:::\n\n\n2. Create 95% credible intervals for the parameters in `stan_model`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the 95% credible intervals\nposterior_interval(stan_model, prob = 0.95)\n```\n:::\n\n\n3. Create 80% credible intervals for the parameters in `stan_model`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the 80% credible intervals\nposterior_interval(stan_model, prob = 0.80)\n```\n:::\n\n\nGreat job! You're well on your way to becoming a Bayesian! Here, we've learned how to create a credible interval for our parameters, and how to change how big of an interval we want. These intervals allow us to make inferences about the actual values of the parameters, unlike in frequentist regression.\n\n# 2. Modifying a Bayesian Model\n\nLearn how to modify your Bayesian model including changing the number and length of chains, changing prior distributions, and adding predictors.\n\n## What's in a Bayesian Model?\n\nTheory. Coming soon ...\n\n**1. What's in a Bayesian Model?**\n\nIn the previous chapter we learned how to estimate a Bayesian linear regression. In this chapter we're going learn how these models can be modified. Unlike in a frequentist regression, there are several ways we can modify the estimation process. This is important because these factors will ultimately impact your results. We'll start by looking at the sampling of the posterior that creates the distribution summaries that we see in the output.\n\n**2. Posterior distributions**\n\nAs we briefly talked about last chapter, the posterior distribution is sampled in groups, called chains. Each sample from within the chain is called an iteration. The chains begin at a random locations. As it samples, the chain moves toward the area where the combination of likelihood and prior indicates a high probability of the true parameter value residing. The more iterations in a chain, the larger the samples of the posterior distribution will be. This means that the summaries are directly impacted by the length of the chain, as larger samples allow for more robust estimates of those summary statistics.\n\n**3. Sampling the posterior distribution**\n\nHere is an example of this process. This is known as a trace plot. It shows the value of the parameter at each iteration for each chain. We can see that each chain starts in a different location, but they all converge on the same area. This is the convergence we talked about measuring with the R-hat parameter. Convergence is important because it ensures stable estimates. We can see that this model has not converged at the beginning, as the chains are in different places and not horizontal, meaning that the estimates are not stable.\n\n**4. Sampling the Posterior Distribution**\n\nBecause the model has not converged at the beginning, we discard these iterations, leaving only the converged iterations to make up our final posterior distribution. Here, we can see that by only using the final 1,000 iterations, all of the chains are fully mixed. The iterations that are discarded are know as warm-up, or burn-in. By default, the rstanarm package estimates 4 chains. Each chain is 2,000 iterations long, and the first 1,000 are discarded for warm-up. For the exercises in this course, to cut down on estimation time, we've changed the default to 2 chains, each with 1,000 iterations, and the first 500 discarded for warm-up. In your own work, we recommend using the rstanarm defaults.\n\n**5. Changing the number and length of chains**\n\nWe change this behavior in rstanarm by using the chains, iter, and warm-up arguments. Here we've specified that we want three chains, each with 1,000 iterations, and the first 500 should be discarded for warm-up. This means that our posterior distributions will be made of 1,500 total samples (500 from each chain).\n\n**6. Changing the number and length of chains**\n\nIndeed, when we look at the summary of the model, under Model Info, we see that the sample is 1500.\n\n**7. Too short chains**\n\nHowever, we have to be careful about making the number or length of the chains to short. Using our same example from earlier, if we had instead requested only 500 iterations, and discarded the first 250, our posterior distribution wouldn't be converged, because the chains haven't mixed.\n\n**8. How many iterations?**\n\nBecause of this, the number of iterations is a balancing act. Fewer iterations means the model estimates faster, but too few iterations may keep the model from converging. The number of iterations needed is different for each model, so it's important to pay attention to our R-hat values. If you estimate a model and it hasn't converged, increasing the number of iterations and the length of the warm-up is a good place to start.\n\n**9. Let's practice!**\n\nNow let's try some examples.\n\n## Altering chains\n\nLet's practice changing the number and length of chains so that we can get a posterior distribution of different sizes. By changing the size of the posterior, we can change the number of samples used for the posterior summaries, and impact the estimation time. The `songs` data is already loaded.\n\n**Steps**\n\n1. For all models, predict `popularity` from the `song_age`.\n2. Estimate a model with 3 chains, each 1000 iterations long, with the first 500 discarded\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3 chains, 1000 iterations, 500 warmup\nmodel_3chains <- stan_glm(popularity ~ song_age, data = songs,\n    chains = 3, iter = 1000, warmup = 500)\n\n# Print a summary of model_3chains\nsummary(model_3chains)\n```\n:::\n\n\n3. Estimate a model with 2 chains, each  100 iterations long, discarding the first 50\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2 chains, 100 iterations, 50 warmup\nmodel_2chains <- stan_glm(popularity ~ song_age, data = songs,\n    chains = 2, iter = 100, warmup = 50)\n\n# Print a summary of model_2chains\nsummary(model_2chains)\n```\n:::\n\n\nCorrect! Now you can alter the size of the sample of your poterior distribution! Be careful when making your chains shorter though. Notice the warning messages that we received for `model_2chains`. These are an indication that we didn't draw enough samples from the posterior distribution to get good estimates of the parameters.\n\n## Do I have enough iterations?\n\nThe `model_2chains` object from the last exercise is load in the environment.\n\n> *Question*\n> ---\n> Has the model converged?<br>\n> <br>\n> ⬜ Yeah!<br>\n> ✅ Not a chance!<br>\n> ⬜ We can't tell.<br>\n\nCorrect! The Rhat values are above 1.1. (depends on the seed)\n\n## Prior distributions\n\nTheory. Coming soon ...\n\n\n**1. Prior distributions**\n\nNow that we've talked about how to control the size of our posterior distribution samples, we can talk about one of the components of the posterior distribution that we've only mentioned in passing so far: prior distributions.\n\n**2. What's a prior distribution?**\n\nPrior distributions reflect our prior beliefs about the values of parameters. This information gets combined with the likelihood of the data to create the posterior distribution.\n\n**3. Visualizing prior distributions**\n\nHere is an example of how prior distributions can affect the resulting posterior distribution. The likelihood of the of the data, indicated by the dashed purple line, stays the same across all three panels. Notice how, as the teal line showing the priors gets more narrow or informative, the posterior distribution begins to shift closer to the prior, and away from the likelihood. In general, priors get more informative when their distributions are narrow, or we have less data. We can think about the prior like an additional data point. If we have a sample of five, a sixth data point can be really influential. If we have a sample of 5000, one extra data point won't have much of an effect.Because priors have the potential to be influential, it is almost always a good idea to use non-informative or weakly informative priors, unless you have a good reason for believe that your parameters come from the informative distribution specified by the prior.\n\n**4. Prior distributions in rstanarm**\n\nWe can view the prior distributions for each of our parameters in an rstanarm model by using the `prior_summary` function. By default the intercept gets a normally distributed prior with a mean of 0 an standard deviation of 10, and other coefficients get a normally distributed prior with a mean of 0 and standard deviation of 2.5. Auxiliary is the error standard deviation. This uses an exponentially distributed prior with a rate of 1. However, notice that there are also adjusted scales listed. This is because rstanarm recognizes that these defaults may not be appropriate for every dataset. Therefore they adjust the variance based on your data. For example, here, the standard deviation for the prior of the intercept was 204.11.\n\n**5. Calculating adjusted scales**\n\nThe adjusted scale for the intercept is calculated as 10 times the standard deviation of your dependent variable. We use 10, because this is the default scale used by rstanarm for intercept. For predictors, the scale is calculated as 2.5 divided by the standard deviation of your predictor times the standard deviation of the dependent variable. Just like with the intercept, we use 2.5 because this is the default scale used for predictors. Look again at the priors for the intercept and predictor in the children's IQ model. By taking 10 times the standard deviation of kid_score, our dependent variable, we get the adjusted scale of 204.11. Similarly, by taking 2.5 divided by the standard deviation of the mom's IQ times the standard deviation of the the children's IQ, we get that adjusted scale of 3.40.\n\n**6. Unadjusted Priors**\n\nrstanarm uses automatically adjusted the priors in order to ensure the the specified priors are not too informative. However, we can turn off this adjustment if we want to. To do this, we can specify autoscale as false for the intercept prior (which is prior_int), coefficients (which is just prior), the error (which is prior_aux) or any combination. After specifying autoscale equals false, we can see that there are no longer adjusted scales in the prior_summary output.\n\n**7. Let's practice!**\n\nNow it's your turn to explore priors in rstanarm.\n\n## Determine Prior Distributions\n\nNow let's explore the prior distributions for a Bayesian model, so that we can understand how `rstanarm` handles priors. Priors can have a large impact on our model, so it's important to know which prior distributions were used in an estimated model. The `songs` dataset is already loaded.\n\n**Steps**\n\n1. Estimate a model predicting `popularity` from `song_age`\n2. Print a summary of the prior distributions to the screen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model\nstan_model <- stan_glm(popularity ~ song_age, data = songs)\n\n# Print a summary of the prior distributions\nprior_summary(stan_model)\n```\n:::\n\n\nGreat! Now you know how to identify the prior distributions of your model! The intercept uses a normal prior with a mean of 0 and scale of 10. The coefficient for the predictor uses a normal prior with a mean of 0 and a scale of 2.5. Finally, the error variance uses an exponential priors with a rate of 1. However, notice that all priors also have an adjusted scale. In the next exercise we'll examine how these are calculated.\n\n## Calculate Adjusted Scales\n\nIt's important to understand how `rstanarm` calculates adjusted scales for prior distributions, as priors can have a large impact on our estimates if not used in an appropriate manner. Calculate what the adjusted scales should be using the already loaded `songs` data.\n\n**Steps**\n\n1. Calculate the adjusted scale of the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the adjusted scale for the intercept\n10 * sd(songs$popularity)\n```\n:::\n\n\n2. Calculate the adjusted scale of the `song_age`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the adjusted scale for `song_age`\n(2.5 / sd(songs$song_age)) * sd(songs$popularity)\n```\n:::\n\n\n3. What would be the adjusted scale of `valence` if it were in the model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the adjusted scale for `valence`\n(2.5 / sd(songs$valence)) * sd(songs$popularity)\n```\n:::\n\n\nAwesome! Great work calculating those adjusted scales! These scales are exactly the same as the adjusted scales that we saw in the previous exercise.\n\n## Unadjusted Priors\n\nNow let's specify a model that doesn't use adjusted scales for prior distributions, so that we alter `rstanarm` default behavior. This will allow us to have more direct control over the information going into the estimation. The `songs` data is already loaded.\n\n**Steps**\n\n1. Predict `popularity` from `song_age`\n2. Tell `rstanarm` not to autoscale the parameters\n3. Print a prior summary to confirm there was no adjustment\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model with unadjusted scales\nno_scale <- stan_glm(popularity ~ song_age, data = songs,\n    prior_intercept = normal(autoscale = FALSE),\n    prior = normal(autoscale = FALSE),\n    prior_aux = exponential(autoscale = FALSE)\n)\n\n# Print the prior summary\nprior_summary(no_scale)\n```\n:::\n\n\nPerfect! You're well on your way to fully controlling the prior distributions. Notice that now that `autoscale = FALSE` has been specified, the prior summary no longer includes adjusted scales. Now that we've learned how to modify the default priors, we can move onto specifying priors that are entirely our own.\n\n## User Specified Priors\n\nTheory. Coming soon ...\n\n**1. User Specified Priors**\n\nIn the last lesson we talked about the prior distributions that rstanarm uses by default, and we can exert some control by deciding whether or not to adjust the scales of those distributions. But what if we want to use a completely different distribution? We can define a new prior distribution using the same arguments that we learned about in the last lesson.\n\n**2. Why change the default prior?**\n\nThere are a couple of reasons we may want to specify our own prior distributions. First, there may be a lot of research suggesting the parameter should be around a certain value. In this scenario, we should take advantage of our knowledge and include this information through the prior distribution. Alternatively, we may have a parameter that we know is constrained in some way. For example, maybe a parameter has to be positive, like a variance. Here we could specify a prior distribution that is also always positive to ensure that the parameter is correctly constrained.\n\n**3. Specify a prior**\n\nWe can be explicit about the prior distributions to be used by specifying location and scale values in the prior functions. For example, here we've specified the intercept prior should have a mean of zero and a standard deviation of 10.\n\n**4. Specify a prior**\n\nHowever, just like when these values are unspecified, rstanarm will adjust the prior scales using the same rules we learned about. So if you want to make sure you are using the exact distribution you specified, make sure that autoscale is set to FALSE.\n\n**5. Specify a prior**\n\nUsing the prior arguments, we can specify different priors for different parameters. For example, here we've specified that the intercept should have a normally distributed prior with a mean of three and a standard deviation of two, and the predictors should have a Cauchy prior with a mean of zero and a standard deviation of 1.There are many different prior distributions that can be used. We've already seen the normal and exponential distributions, but there are t distributions and the Cauchy distribution as we've used here. We can see a full list of available distributions by looking at the priors help page. The process of how to choose a good prior distribution is beyond the scope of this course. However, a good practice is to choose a prior distribution that is consistent the expected distribution of your parameters. For example, we know from the central limit theorem that predictor coefficients are normally distributed. Therefore, it makes sense to use a normal prior for these parameters.\n\n**6. Flat priors**\n\nOne situation we haven't discussed is how to specify completely uninformative, or flat priors. What if we're in a situation where we want the prior to provide no information? We can accomplish this by setting the priors to NULL. When we look at the prior summary, we can now see that a flat prior has been used. In practice this is not usually a good idea, because we are rarely in a situation where we have no prior information. In a linear regression for example, we know that the coefficients should be normally distributed. Even if we have no idea what the parameter values should be, it's generally better practice to specify a weakly informative prior using an adjusted scale than to use a completely flat prior.\n\n**7. Let's practice!**\n\nNow let's change the priors in our Spotify model.\n\n## Changing Priors\n\nNow let's change the prior distributions for our Spotify model. Changing the priors allows us to specify our own beliefs about the expected values of the parameters. The `songs` dataset is already loaded.\n\n**Steps**\n\n1. Predict `popularity` from `song_age`\n2. Create a model, `flat_prior` that uses flat priors for all parameters\n3. Print a summary of the prior distributions to the screen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate a model with flat priors\nflat_prior <- stan_glm(popularity ~ song_age, data = songs,\n    prior_intercept = NULL, prior = NULL, prior_aux = NULL)\n\n# Print a prior summary\nprior_summary(flat_prior)\n```\n:::\n\n\nAwesome job! You've learned how to specify your own flat prior distributions! Flat priors provide no additional information to the model. This is often not the best choice, but specifying priors that provide too much information can also be problematic. We'll explore this in the next exercise.\n\n## Specifying informative priors\n\nNow let's specify a custom prior so that we can have more control over our model. The `songs` dataset is already loaded.\n\n**Steps**\n\n1. Predict `popularity` from `song_age`\n2. Specify a normal prior distribution for the predictor with a mean of 20 and standard deviation of 0.1\n3. Print the prior summary to the screen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model with an informative prior\ninform_prior <- stan_glm(popularity ~ song_age, data = songs,\n    prior = normal(location = 20, scale = 0.1, autoscale = FALSE))\n\n# Print the prior summary\nprior_summary(inform_prior)\n```\n:::\n\n\nGreat! Now you know how to specify a custom prior distribution! Notice in the prior summary that we've said the coefficient for `song_age` has a location of 20 with a very small variance. Therefore, we would expect the parameter estimate to also be very close to 20.\n\n## Consequences of informative priors\n\nThe `inform_prior` model you estimated is loaded in the environment. As a reminder, that model was specified with a normal prior on the predictor variable with a mean of 20 and standard deviation of 0.1.\n\n> *Question*\n> ---\n> How did the specified prior affect the parameter estimates?<br>\n> <br>\n> ✅ The estimate was unaffected by the prior<br>\n> ⬜ The estimate was slightly affected by the prior<br>\n> ⬜ The estimate was almost the same as the prior<br>\n\nCorrect! The estimate is almost the same as the mean of the specified prior.\n\n## Altering the estimation process\n\nTheory. Coming soon ...\n\n**1. Altering the estimation process**\n\nSo far we've talked about how to edit characteristics of the model such as the number and length of chains, and prior distributions. In addition to these characteristics, there are some internal options that affect how the sampling of the posterior distribution occurs. In most cases, the default options are sufficient. However, the need to alter these options comes up often enough that it's worth discussing them. The technical details of these options are beyond the scope of this course, but we will talk about how they can be changed to resolve error message we sometimes get.\n\n**2. Divergent transitions**\n\nThe first error message we'll talk about concerns divergent transitions. This happens when the size betweens the steps of the estimator are too big. The Stan documentation describes this like walking down a steep hill. Taking too big of a step may make you fall. But you can take smaller steps and make it down safely, even if it takes longer. We  can control the step size in Stan by sending a list to the `control` argument that specifies an adapt_delta. The adapt_delta argument can range from 0 to 1, and is set to 0-point-9-5 by default in rstanarm. By increasing the adapt_delta, we can decrease the step size, and in many cases resolve the divergent transitions error.\n\n**3. Exceeding the Maximum Treedepth**\n\nThe second error message you may sometimes encounter is one saying that the maximum tree depth has been reached. In the sampling algorithm used by Stan and therefore rstanarm, the sampler looks for a place to \"U-Turn\" in a series of possible branches. Again, the specifics of this process are beyond the scope of this course. What's important to know here is that if the sampler reachers the max tree depth before finding a good place to \"U-Turn\", then the sample terminated the iteration before finding a good stopping place. Therefore the posterior distribution is not being sampled as efficiently as it should be.As with adapt_delta, we  can control the maximum tree depth in Stan by sending a list to the control argument that specifies a max_treedepth. The max_treedepth is set to 10 by default in rstanarm. By increasing the max_treedepth, we allow the to look further for a good place to \"U-Turn\" .\n\n**4. Tuning the estimation**\n\nDivergent transitions and hitting the maximum tree depth are important issues to pay attention to during the estimation of a model. These errors may represent threats to the validity of your inferences about parameter values. Although these errors are uncommon for the models we're estimating in this course, these are important concepts to understand and know how to address in practice. Luckily, rstanarm allows for these errors to be easily addressed, so that we can be sure that our parameter estimates are coming from a stable estimation.\n\n**5. Let's practice!**\n\nNow let's practice tuning the estimation process.\n\n## Altering the Estimation\n\nNow let's alter the estimation options so that we can be prepared to resolve errors that may arise. It's important for these errors to be resolved if they come up so that we can be sure we are making valid inferences. The `songs` data is already loaded.\n\n**Steps**\n\n1. Estimate two models predicting `popularity` from `song_age`\n2. In the first model, set `adapt_delta` to 0.99\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model with a new `adapt_delta`\nadapt_model <- stan_glm(popularity ~ song_age, data = songs,\n                        control = list(adapt_delta = 0.99))\n\n# View summary\nsummary(adapt_model)\n```\n:::\n\n\n3. In the second model, set the `max_treedepth` to 15\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model with a new `max_treedepth`\ntree_model <- stan_glm(popularity ~ song_age, data = songs,\n                       control = list(max_treedepth = 15))\n\n# View summary\nsummary(tree_model)\n```\n:::\n\n\nWonderful! Now you're prepared to handle rstanarms most common estimation errors. Notice in these summaries that nothing looks different. These options don't alter the model itself, but instead modify the underlying estimation algorithm.\n\n# 3. Assessing Model Fit\n\nIn this chapter, we'll learn how to determine if our estimated model fits our data and how to compare competing models.\n\n## Using the R Squared statistic\n\nTheory. Coming soon ...\n\n**1. Using the R Squared statistic**\n\nSo far in the course, we've talked about how to estimate and modify a model. In the final two chapters, we'll be looking at how the models we estimate can be evaluated and used to make predictions. In this chapter, we'll focus on the first part: evaluating models. This is a critically important part of modeling building because if our model doesn't fit, then we won't be able to make very good predictions. We'll start by using one of the most common measures of model fit in linear regression: the r-squared statistic.\n\n**2. What is R squared?**\n\nThe R squared statistic is a measure of how well the independent variables in the model are able to predict the dependent variable. Specifically, the R squared statistic measures the proportion of variance in the dependent variable that can be explained by the independent variables. As with all proportions, the R squared ranges from 0 to 1, with 0 representing no variance explained, and 1 representing all the variance explained, or a deterministic model. Because of this, the R squared is also known as the coefficient of determination.The R squared is calculated as 1 minus the sum of squared residuals (called the residual sum of squares) divided by the sum of squared deviations of the data from the mean (called the total sum of squares).\n\n**3. What is R squared?**\n\nIn other words, we take the observed value for an observation and subtract the predicted value, square it, and then sum that over all observations. That is the numerator.\n\n**4. What is R squared?**\n\nIn the denominator, we take the observed value for an observation, subtract the mean of the observed value, square that, and sum over all observations. This variance of residuals to total variance ratio is what drives the R squared.\n\n**5. Calculating R squared statistic**\n\nIn a frequentist regression, we can get the R squared by estimating a model with the lm() function, and saving a summary of the object. That object contains an `r.squared`, that we can pull out to view.However, we can also calculate this by hand. We can define the residual sum of squares as the variance of the residuals of lm_model and the total sum of squares as the sum of the variance of the residuals of lm_model and the variance of the predicted values of lm_model. Taking 1 minus the residual sum of square divided by the total sum of squares, gives us the same value that was saved in the lm_summary.\n\n**6. The R squared statistic of a Bayesian Model**\n\nIn rstanarm, the R squared is not saved in the summary object as in the lm() summary function. However, we can still calculate the R squared by hand using the exact same formulas as we did for the frequentist regression. We define the residual sum of squares and the total sum of squares, and then use those to calculate the R squared value. Comparing this value to the value we got in the frequentist regression, we see that they are almost identical.\n\n**7. Let's practice!**\n\nNow it's your turn to calculate the R squared for our model using Spotify data.\n\n## Calculating Frequentist R-squared\n\nLet's practice calculating the R-squared. By starting with the frequentist R-squared, we can check our formulas for calculating the R-squared by hand, by checking against the value in the frequentist summary. The `lm_model` and `lm_summary` objects are already in your environment.\n\n**Steps**\n\n1. Print the R-squared value from the `lm_summary` object\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_summary <- summary(lm_model)\n\n# Print the R-squared from the linear model\nlm_summary$r.squared\n```\n:::\n\n\n2. Calculate the variance of the residuals of the `lm_model`\n3. Calculate the variance of the predicted values of the `lm_model`\n4. Calculate the R-squared value\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calulate sums of squares\nss_res <- var(residuals(lm_model))\nss_fit <- var(fitted(lm_model))\n\n# Calculate the R-squared\n1 - (ss_res / (ss_res + ss_fit))\n```\n:::\n\n\nAwesome! Notice we get the same value of 0.23 with both methods. This means that the `song_age` explains 23% of the variance in `popularity`. Now that we've learned how to manually calculate the R-squared, we can apply these formulas to the Bayesian model.\n\n## R-squared for a Bayesian Model\n\nNow let's calculate the R-squared for a Bayesian model so we can assess the predictions of models estimated with `stan_glm`. The `stan_model` object is already loaded.\n\n**Steps**\n\n1. Calculate the variance of the residuals of `stan_model`\n2. Calculate the variance of the fitted values of `stan_model`\n3. Calculate the R-squared\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the variance of residulas\nss_res <- var(residuals(stan_model))\n\n# Save the variance of fitted values\nss_fit <- var(fitted(stan_model))\n\n# Calculate the R-squared\n1 - (ss_res / (ss_res + ss_fit))\n```\n:::\n\n\nGood job! For the Bayesian model, we get an R-squared of 0.23, which is very close to the R-squared for the frequentist model, as would be expected given how similar the parameter estimates are. Now let's look at some new methods for assessing model fit that are only available for Bayesian models.\n\n## Posterior predictive model checks\n\nTheory. Coming soon ...\n\n**1. Posterior predictive model checks**\n\nThe R-squared statistic tells us how much variance in the dependent variable can be explained by the predictors. But this isn't the only way to assess model fit. In fact there are several methods that are specific to Bayesian estimation. The most important among these is a class of analyses called posterior predictive model checks, which can only be calculated when a Bayesian estimation process is used.\n\n**2. Using posterior distributions**\n\nAs the name might suggest, posterior predictive model checks make use of the posterior distributions. Recall our model predicting a child's IQ score from their mom's IQ score. By  default, this model is estimated with 4 chains, and 2,000 iterations in each chain, with the first 1,000 iterations of each discarded for warmup. This means that we have a total of 4,000 iterations that make up our posterior distribution (1,000 from each chain). Here we can see the values of the intercept and coefficient for the `mom_iq` variable for the first 10 iterations. As we can see, the values change with each iteration, which is expected as we are randomly sampling from the posterior distribution.\n\n**3. Posterior predictions**\n\nWe can use these posterior draws to calculate predicted score for our data. For example, we can calculate predicted scores using the parameter values at iteration 1, another set of predicted scores using the parameter values at iteration 2, and so on. Ultimately, because we have 4,000 iteration, we can calculate 4,000 predicted scores for each observation. We can get all the predicted values for all observations using the posterior_linpred function. This returns a matrix that has a row for each iteration, and a column for each observation. In this example, we can see the predicted IQ scores for the first 10 iterations for children 1 through 5. Because these scores are generated using the model parameters, these predicted scores form a distribution of what our observed scores should look like - if the specified model was correct. Thus, deviations from these predictive distributions are an indication of poor fit.\n\n**4. Comparing score distributions**\n\nOne way we can look for deviations is to compare the distribution of predicted scores in an iteration to the distribution of observed scores. For example we can get the predicted scores for the first and second iterations by pulling the first and second rows of the matrix return by posterior_linpred. We can then look at a summary of those scores, compared the summary of the observed scores. Here we can see that the mean of the iterations is similar to the mean of the observed data, but min and max values are less extreme than in the observed data.\n\n**5. Comparing single scores**\n\nSimilarly, we can compare individual scores to their expected distribution. For example, child 24 had an observed score of 87, which falls in line with the distribution of predicted scores for that child, as shown in column 24 of the matrix returned by posterior_linpred. In contrast, child 185 had an observed score of 111, which is quite different from the distribution of predicted scores for this student. This means that the model does not do a very good job of predicting the IQ score for this student.\n\n**6. Let's practice**\n\nNow it's your turn to use posterior predictive model checks!\n\n## Predicted score distributions\n\nNow let's practicing using posterior predictive scores to look at how our Spotify model fits to the real data.\n\n**Steps**\n\n1. Calculate the posterior predicted scores for `stan_model`\n2. Print a summary of the observed popularity scores in the `songs` data\n3. Compare this to a summary of the 1st and 10th replications\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate posterior predictive scores\npredictions <- posterior_linpred(stan_model)\n\n# Print a summary of the observed data\nsummary(songs$popularity)\n\n# Print a summary of the 1st replication\nsummary(predictions[1,])\n\n# Print a summary of the 10th replication\nsummary(predictions[10,])\n```\n:::\n\n\nGreat work! Now you know the basics of comparing score distributions. Notice that the observed data goes from 0 to 85, but in the simualted datasets, the predicted scores only range from about 37 to 66. However the means of all three summaries are very similar. This suggests that the model is better at predicting songs with an average level of popularity than the edge cases.\n\n## Distributions for a single observation\n\nThe `songs` dataset and the posterior predictions `predictions` are already loaded. Look at the observed popularity and posterior predictive sample for the popularity of song number 12.\n\n> *Question*\n> ---\n> Does the observed popularity fit within the expected distribution?<br>\n> <br>\n> ⬜ You bet<br>\n> ✅ No way<br>\n> ⬜ We need more information<br>\n\nCorrect! The observed data falls outside the range of predicted scores.\n\n## Model fit with posterior predictive model checks\n\nTheory. Coming soon ...\n\n**1. Model fit with posterior predictive model checks**\n\nIn the last lesson, we talked about how we can get predicted scores from our posterior, and compare those predictions to our observed data. However, it would be incredibly tedious to compare the observed data to every replication one at a time, or look at the results for each observation independently. In this lesson we'll learn how we can summarize results across all replications to make global evaluations of how well the model fits the data.\n\n**2. R squared posterior distribution**\n\nThe first measure of fit using posterior predictive model checks is one we've looked at before: the R-squared. With our estimated `stan_model`, we can get a posterior distribution of the R-squared using the `bayes_R2` function. We can then look at a summary to get an idea of the distribution, or create a 95% credible interval using the quantile function.\n\n**3. R squared histogram**\n\nWe can also make a histogram of the R-squared values. From this, we can see that the true R-squared of our model is mostly likely somewhere between .1 and .3.\n\n**4. Density overlay**\n\nWe can also compare the distributions from all replications to the observed data at one time using the `pp_check` function, and specifying a density overlay. This will generate a plot like this. Here, each light blue line represents the distribution of predicted scores from a single replications, and the dark blue line represents the observed data. If the model fits, the dark blue line should align closely with the light blue lines.\n\n**5. Posterior predictive tests**\n\nWe can also test certain characteristics of the dependent variable. By specifying \"stat\" in the `pp_check` function, we  can get a distribution of the mean of the dependent variable from all replications. These are the light blue bars: the means from each replication plotted as a histogram. The mean from the observed data is then plotted on top as a dark blue bar. In our example, the observed mean of kids' IQ scores falls within the expected range of means from the posterior predictions. Therefore, we have evidence that our model fits.\n\n**6. Posterior predictive tests**\n\nHowever, the mean is just one aspect of the observed data. By changing \"stat\" to \"stat_2d\", we can look at multiple aspects of our dependent variable: the mean and standard deviation. In this plot, each light blue dot represents the mean and standard deviation of the predicted IQ scores in one replication. The dark blue dot shows our observed data. Because the dark blue dot is inside the mass of light blue dots, we have more evidence that our model fits our data.\n\n**7. Let's practice!**\n\nNow you try using posterior predictive model checks to assess the model fit of our Spotify model.\n\n## R-squared Posterior\n\nFirst, let's get a posterior distribution of the R-squared statistic so we can make inferences about how predictive our model is likely to be. The `stan_model` using the Spotify data is already loaded.\n\n**Steps**\n\n1. Calculate the posterior distribution of the R-squared statistic\n2. Create a histogram of the R-squared distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the posterior distribution of the R-squared\nr2_posterior <- bayes_R2(stan_model)\n\n# Make a histogram of the distribution\nhist(r2_posterior)\n```\n:::\n\n\nAwesome! Now you can see a range for how predictive your model may be. Notice that the distribution is centered around 0.23, which is the point estimate we calculated earlier. But now, we can see that the true value of the R-squared could plausibly range from about 0.10 to 0.35\n\n## Posterior Predictive Testing\n\nNow let's practice comparing our replicated predictions to our observed data more than one at a time. By comparing our data to all replications, we can assess how well the model fits the data.\n\n**Steps**\n\n1. Plot the density of predicted scores from replication compared to the observed density\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create density comparison\npp_check(stan_model, \"dens_overlay\")\n```\n:::\n\n\n2. Create a scatter plot of the mean and standard deviations of the replicated predicted scores compared to the observed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create scatter plot of means and standard deviations\npp_check(stan_model, \"stat_2d\")\n```\n:::\n\n\nGreat work! Now you can evaluate if your model fits the data! In the first plot, we can see there is a second mode of popularity scores around 10 that is not captured by the model, even though the peaks of the observed data and model are in similar places. In the second plot, the mean and standard deviation of the observed data is right in the middle of the expected distribution of points, indicating that these two characteristics are recovered well.\n\n## Bayesian model comparisons\n\nTheory. Coming soon ...\n\n**1. Bayesian model comparisons**\n\nSo far we've looked at how to determine if a single model fits our observed data. But what do we do if we have multiple models, and we want to figure out which one fits the best? This is known as model comparison. In this lesson, we'll talk about how we can compare two, or more, regression models that were estimated using rstanarm.\n\n**2. The loo package**\n\nWhen we estimate a model with rstanarm, we can use the loo package for model comparisons. LOO stands for \"leave-one-out\" which is one variety of a class of model comparison tools more widely known as cross validation. The loo package doesn't use true cross validation. Instead, the algorithm approximates the leave-one-out cross validation. The details of how exactly this is done are beyond the scope of this course, but if you want to learn more, the loo package documentation provides many resources for exploration. In this lesson, we'll instead focus on how the loo package can be used to compare models, and interpret the output.\n\n**3. Using loo on a single model**\n\nTo view the LOO estimates for a model, we simply send an estimated model as the argument to the loo() function. This gives us several pieces of information. First, we see that the estimates were computed from a log-likelihood matrix that has 4000 rows (the total number of iterations in our posterior) and 434 columns (the total number of observations in the kidiq dataset). We get the LOO estimate, called elpd_loo, the effective number of parameters in the model, p_loo, and the LOO estimate converted to a deviance scale, looic. The deviance scale is simply minus two times the LOO estimate, but is more common in some fields. Finally, we're provided with some diagnostics from the approximation algorithm. However, these numbers aren't very useful in isolation. For example, what does an epld_loo values of minus 1878-point-5 mean? This value really only has meaning relative to the values of competing models.\n\n**4. Model comparisons with loo**\n\nLet's say that we have two possible models. In the first model, we predict a kid's IQ scores from only their mother's IQ. In the second model, we predict the kid's IQ score from not only their mom's IQ, but also whether or not their mom graduated high school, and the interaction between mom's IQ and mom's high school completion. We want to know which model does a better job of predicting the kid's IQ score. To do this, we can save the loo estimates from the 1 predictor and 2 predictor models, and then use the `compare` function.\n\n**5. Model comparisons with loo**\n\nThe compare function provides use with the difference in loo estimates, along with a standard error of the difference. A positive difference score like in this example, means that the second model is favored (the model with both predictors), whereas a negative score would indicate a preference for the first model. The standard error helps us decide if the difference in meaningful. As a rule of thumb, if absolute value of the difference is less than the standard error, the models don't perform differently, and we should choose the simpler model (the one with fewer parameters) because it is more parsimonious. If the absolute value of the difference is greater than the standard error, then we will prefer the model indicated by the test. In this example, because the difference of 6.1 is positive and greater than the standard error of 3.9, we would choose the second model with both predictors.\n\n**6. Let's practice!**\n\nNow it's your turn to compare models.\n\n## Calculating the LOO estimate\n\nNow let's practice using the `loo` package on our Spotify model so that we can determine which model provides the best fit to our data. The `songs` dataset is already loaded.\n\n**Steps**\n\n1. Estimate a model predicting `popularity` from `song_age`\n2. Print the LOO approximation for this model with 1 predictor\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model with 1 predictor\nmodel_1pred <- stan_glm(popularity ~ song_age, data = songs)\n\n# Print the LOO estimate for the 1 predictor model\nloo::loo(model_1pred)\n```\n:::\n\n\n3. Estimate a model predicting `popularity` from `song_age`, `artist_name`, and their interaction\n4. Print the LOO approximation for this model with 2 independent variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the model with both predictors\nmodel_2pred <- stan_glm(popularity ~ song_age * artist_name, data = songs)\n\n# Print the LOO estimates for the 2 predictor model\nloo::loo(model_2pred)\n```\n:::\n\n\nAwesome! In the summary, we see that the model with two predictors has LOO approximation of -865.0, and the model with one predictor has a LOO approximation of -888.1. Because the two predictor model is higher (less negative), we would expect that model to have better predictions. However, in order to know if this increase is meaningful, we need to direclty compare the model and compare the difference to the standard error.\n\n## Comparing models\n\nThe models from the last exercise, `model_1pred` and `model_2pred` are available in the environment. \n\n> *Question*\n> ---\n> According to the LOO approximation, which model should be preferred?<br>\n> <br>\n> ⬜ They fit equally well, prefer `model_1pred` because it has fewer parameters<br>\n> ⬜ `model_1pred`<br>\n> ✅ `model_2pred`<br>\n> ⬜ LOO can't give us this information<br>\n\nCorrect! The model with both predictors has a significantly better LOO approximation.\n\n# 4. Presenting and Using a Bayesian Regression\n\nIn this chapter, we'll learn how to use the estimated model to create visualizations of your model and make predictions for new data.\n\n## Visualizing a Bayesian model\n\nTheory. Coming soon ...\n\n**1. Visualizing a Bayesian model**\n\nSo far we've learned how to estimate, customize, and evaluate a Bayesian model. The final step of the analysis pipeline is to present the results of your model. In this chapter, we'll learn how to make predictions using new data and how to create visualizations of the model using ggplot2. With these tools, you'll be ready to complete a fully Bayesian analysis from model creation to presentation.\n\n**2. Saving model coefficients**\n\nWe'll start by making a plot of our model predicting a kid's IQ score from their mom's IQ score. To do this, we need to save the values of the estimated parameters. Recall that we can view the parameter estimates by using the tidy() function from the broom package. To save the parameter values, we can first save the tidy summary, and then pull values from the estimate column of the data frame.\n\n**3. Creating a plot**\n\nWe can then create a plot using ggplot2 syntax that you are already likely already familiar with. We define our dataset, kidiq, and then specify that we want mom_iq on the x-axis, and the kid's score on the y-axis. We can then add a geom_point layer to create the scatter plot. Finally, we use geom_abline to add a straight line to the plot. In geom_abline, we specify the intercept and slope of the line to be the model parameters that we just saved. And just like that, we have a plot showing our data and the estimated regression line!\n\n**4. Plotting uncertainty**\n\nWe can do more than a simple plot though! Because we used a Bayesian estimation, we can use the posterior distributions to also plot the uncertainty of our regression line. Just like when we learned about posterior predictive checks, we can save the posterior draws for our intercept and mom_iq predictor using the spread_draws function from the tidybayes package. This gives us the value of the intercept and slope at each draw from the posterior. Using these draws, we can plot a regression line for each iteration.\n\n**5. Plotting uncertainty**\n\nWe start in the same way we did on the previous plot. We define our data, put mom_iq on the x-axis and kid_score on the y-axis, and then add a geom_point layer to create the scatter plot.\n\n**6. Plotting uncertainty**\n\nWe can then plot the lines for each iteration using geom_abline, just like in the last plot. The difference now is that we define a new dataset, `draws`, which we just created and contains the values of the parameters at each iteration. We then specify that the intercept should be the \"(Intercept)\" column, and the slope should be the mom_iq column. Finally, because there are 4,000 lines being drawn, we can make them small and add transparency so we can tell where there is more density from our posterior predictions. This results in plot with a line for each iteration, showing the range of plausible regression lines.\n\n**7. Plotting uncertainty**\n\nThe final step is to add our mean regression line. This can be done with the same geom_abline code that we used in our first graphic. We define the intercept and slope to be the saved model_intercept and model_slope objects respectively. Now we have a complete graphic showing our data, the estimated regression line, and the uncertainty around our estimated line.\n\n**8. Let's practice**\n\nNow it's your turn! Let make some visualizations of our Spotify model, predicting the popularity of a song from it's age.\n\n## Plotting a Bayesian model\n\nIn previous exercises we have estimated a Bayesian model predicting a song's popularity (`popularity`) from its age (`song_age`). Now let's visualize the model. Using the `songs` dataset and `stan_model` object that are already loaded, create a visualization showing the data the estimated regression line using ggplot2.\n\n**Steps**\n\n1. Save a tidy summary of the model parameters to `tidy_coef`\n2. Pull out the estimated intercept and slope from `tidy_coef`\n3. Create a plot showing the data and estimate regression line with `song_age` on the x-axis and `popularity` on the y-axis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(ggplot2)\n\n# Save the model parameters\ntidy_coef <- tidy(stan_model)\n\n# Extract intercept and slope\nmodel_intercept <- tidy_coef$estimate[1]\nmodel_slope     <- tidy_coef$estimate[2]\n\n# Create the plot\nggplot(songs, aes(x = song_age, y = popularity)) +\n  geom_point() +\n  geom_abline(intercept = model_intercept, slope = model_slope)\n```\n:::\n\n\nGreat work! In this plot, we can see that older songs are less popular than new songs, as we might expect. This is also reflected by the negative slope coefficient for `song_age` that we have seen in previous exercises. Unfortunately, this plot doesn't show us any measure of uncertainty or confidence in our line. We'll add this in the next exercise.\n\n## Plotting Model Uncertainty\n\nBecause we used a Bayesian estimation, we can use the posterior distributions to create a predicted regression line from each draw in our posterior samples. These lines will show the uncertainty around our overall line. The `songs` and `stan_model` objects are already loaded, along with the `model_intercept` and `model_slope` that you used in the last exercise.\n\n**Steps**\n\n1. Save the values of the `(Intercept)` and `song_age` from each draw from the posterior distributions of `stan_model`\n2. Print the values to the screen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(tidybayes)\n\n# Save the values from each draw of the posterior distribution\ndraws <- spread_draws(stan_model, `(Intercept)`, `song_age`)\n\n# Print the `draws` data frame to the console\ndraws\n```\n:::\n\n\n3. Start the plot by creating a scatter plot with `song_age` on the x-axis and `popularity` on the y-axis\n4. Add points to the plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the values from each draw of the posterior distribution\ndraws <- spread_draws(stan_model, `(Intercept)`, `song_age`)\n\n# Create the plot\nggplot(songs, aes(x = song_age, y = popularity)) +\n  geom_point()\n```\n:::\n\n\n5. Plot the uncertainty by creating a regression line for each draw from the posterior distributions\n6. Plot these lines in `\"skyblue\"` to make them stand out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the values from each draw of the posterior distribution\ndraws <- spread_draws(stan_model, `(Intercept)`, `song_age`)\n\n# Create the plot\nggplot(songs, aes(x = song_age, y = popularity)) +\n    geom_point() +\n    geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age),\n                size = 0.1, alpha = 0.2, color = \"skyblue\")\n```\n:::\n\n\n7. Add the final regression line by a plotting a line with final estimated intercept and slope\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the values from each draw of the posterior distribution\ndraws <- spread_draws(stan_model, `(Intercept)`, `song_age`)\n\n# Create the plot\nggplot(songs, aes(x = song_age, y = popularity)) +\n    geom_point() +\n    geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age),\n                size = 0.1, alpha = 0.2, color = \"skyblue\") +\n    geom_abline(intercept = model_intercept, slope = model_slope)\n```\n:::\n\n\nAwesome work! In our final plot we can see the data, the estimated regression line, and the uncertainty around the line. This is a ton of information conveyed in easily understandable plot that took only a few lines of code to create!\n\n## Making predictions\n\nTheory. Coming soon ...\n\n**1. Making predictions**\n\nIn the last lesson we talked about how to visualize your model. For the rest of this chapter, we'll be looking at how we can use our estimated model to make predictions for observations in our dataset, and for new data.\n\n**2. Making predictions for observed data**\n\nWe'll start by getting predictions for the observations in our kidiq dataset. We first estimate a model predicting the child's IQ from the mom's IQ and whether or not their mom completed high school. We can then get a predicted score at each iteration of the model for each kid by using the posterior_predict function. This returns a matrix that has a row for every iteration and a column for each observation. This means that we can get a posterior distribution of the predicted score for each student, just like when we were doing posterior predictive model checks in chapter three. In chapter three, we got these predictions using the posterior_linpred function. The benefit of using the `posterior_predict` function now is that we can get predictions for new data that weren't used to estimated the model.\n\n**3. Making predictions for new data**\n\nTo make predictions for new data, we first have to create the data we want to predict. For this example, we will predict the IQ for two children whose mothers both had an IQ of 110, one who completed high school, and one who didn't. For these predictions, we create a new data frame with the same variable names as our observed data. Our data frame then has two columns, one for each predictor in our model, and two rows, one for each prediction that we want to make.\n\n**4. Making predictions for new data**\n\nAfter creating the new data for predictions, we can supply this data frame to the newdata argument of the posterior_predict function. This creates predictions for the new data at all 4,000 draws from the posterior distributions. Here we can see the predicted scores for these observations at the first 10 iterations. We can also look at a summary for each column. Looking at the summaries, we see that the predicted scores for the observations with a mother who completed high school are consistently higher.\n\n**5. Let's practice**\n\nNow it's your turn to make some predictions about the popularity of songs in the Spotify data!\n\n## Popularity for Observed Songs\n\nLet's practice making predictions about song popularity from the Spotify `songs` data. This will get us used to the syntax we will use for making predictions for new data that was not observed.\n\n**Steps**\n\n1. Estimate a model predicting `popularity` from `song_age` and `artist_name`\n2. Print a summary of the estimated model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the regression model\nstan_model <- stan_glm(popularity ~ song_age + artist_name, data = songs)\n\n# Print the model summary\nsummary(stan_model)\n```\n:::\n\n\n3. Create posterior distributions of the predicted scores for each song\n4. Print the first 10 predicted scores for the first 5 songs\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get posteriors of predicted scores for each observation\nposteriors <- posterior_predict(stan_model)\n\n# Print 10 predicted scores for 5 songs\nposteriors[1:10, 1:5]\n```\n:::\n\n\nGreat work! We now have a posterior distribution for the predicted popularity of every song in our dataset. But what if we want to predict the popularity for songs that aren't in our dataset? We'll do this in the next exercise.\n\n## Popularity for New Songs\n\nBeyoncé's most recent album, *Lemonade*, is not in our `songs` dataset. Let's predict how popular a song on that album would be. The *Lemonade* album was released 663 days before this dataset was created. The `stan_model` object you created in the last exercise is loaded in your environment.\n\n**Steps**\n\n1. Create a data frame of new data to be predicted including the `song_age` and `artist_name` variables\n2. Create a posterior distribution for predicted popularity of a song on *Lemonade*\n3. Print the predicted popularity for the first 10 draws from the posterior distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data frame of new data\npredict_data    <- data.frame(song_age = 663, artist_name = \"Beyoncé\")\n\n# Create posterior predictions for Lemonade album\nnew_predictions <- posterior_predict(stan_model, newdata = predict_data)\n\n# Print first 10 predictions for the new data\nnew_predictions[1:10,]\n```\n:::\n\n\n4. Print a summary of the posterior predictions for the 1 new observation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print a summary of the posterior distribution of predicted popularity\nsummary(new_predictions[, 1])\n```\n:::\n\n\nAwesome! We now have a posterior distribution for the predicted popularity of a song from the the *Lemonade* album. From the posterior summary we can see there is large range of plausible values, ranging from 10.85 to 98.45!\n\n## Visualizing predictions\n\nTheory. Coming soon ...\n\n**1. Visualizing predictions**\n\nIn the last lesson we learned how to create posterior distributions for predictions of both observed and unobserved data. Earlier in this chapter we reviewed how to create visualizations for our observed data. In the final lesson of this course, we'll learn how to create visualizations for the predictions of new data. Thus, we'll be able to effectively communicate the output of our model in the context of out-of-sample data.\n\n**2. Plotting new predictions**\n\nLet's start with the same model we were working with in the last lesson. We're predicting a child's IQ score from their mother's IQ and whether or not their mother completed high school. We previously created a new data frame to predict the IQ scores of 2 kids whose mothers both had an IQ of 110, but one completed high school and the other did not. Using the `posterior_predict` function, we were able to get the posterior distributions for the prediction of each kid's IQ score. Now we want to visualize these distributions.\n\n**3. Formatting the data**\n\nThe first step is to format the data so that we can plot it with ggplot2. To do this, we will first convert our posterior predictions to a data frame using the `as.data.frame` function. We then set the column names to be \"No HS\" and \"Completed HS\" so that we can identify which prediction is in each column. Finally, we use the `gather` function from the tidyr package to get the data into the structure needed for ggplot2. This leaves us with 2 columns: one indicating whether HS was completed, and one with the draws from the posterior distribution.\n\n**4. Creating the plot**\n\nTo create the plot, we call the `ggplot` function, specify that we are using the plot_posterior data frame and that we want the `predict` column to be on the x-axis. We then use `facet_wrap` to put each level of high school completion in its own plot. Using `ncol = 1`, we make the plots be stacked vertically. Finally, we use `geom_density to create a density plot. With this visualization, we can see how the mother's completion of high school affects the predicted IQ score for the kids. The distribution for the kid with a mother who completed high school is shifted a little further to the right, indicating higher scores. However, for the most part, these distributions are very similar. This is also consistent with what we saw in the previous lesson when looking at the numerical summaries of the distributions. In those summaries, the average was slightly higher when the mother had completed high school. Thus these visualizations give us another tool to help communicate the predictions made by our model.\n\n**5. Let's practice**\n\nNow let's make some visualizations for some new predictions about song popularity using the Spotify data!\n\n## Format prediction posteriors\n\nNow let's plot some new predictions. In this exercise, we'll predict how popular a song would be that was newly released and has a `song_age` of 0. We're still predicting `popularity` from `song_age` and `artist_name`. The `new_predictions` object has already been created and contains the distributions for the predicted scores for a new song from Adele, Taylor Swift, and Beyoncé.\n\n**Steps**\n\n1. Print the predicted scores from the first 10 iterations of `new_predictions`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View new data predictions\nnew_predictions[1:10, ]\n```\n:::\n\n\n2. Convert `new_predictions` to a data frame and name the columns of the data frame \"Adele\", \"Taylor Swift\", and \"Beyoncé\".\n3. Structure the data in long format, with only two columns: `artist_name` and `predict`.\n4. Print the first six rows of the newly structured `plot_posterior` data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nnew_predictions <- readRDS(\"data/new_predictions.rds\")\n\n# Convert to data frame and rename variables\nnew_predictions <- as.data.frame(new_predictions)\ncolnames(new_predictions) <- c(\"Adele\", \"Taylor Swift\", \"Beyoncé\")\n\n# Create tidy data structure\n# plot_posterior <- tidyr::gather(new_predictions, key = \"artist_name\", value = \"predict\")\nplot_posterior <- tidyr::pivot_longer(new_predictions, cols =  everything(), names_to = \"artist_name\", values_to = \"predict\")\n\n# Print formated data\nhead(plot_posterior)\n```\n:::\n\n\nGreat work! We now have a data frame with two columns: `artist_name` and `predict`. We can use this dataset to create of plot of the predicted popularity of songs for each artist that are brand new! We'll make this plot in the next exercise.\n\n## Visualize New Predictions\n\nNow that you've formatted the data, it's time to create the plot of predicted popularity distributions. The `plot_posterior` data frame that you just created is already available in the environment. We will use that data frame to create a visualization to communicate the results of our analysis.\n\n**Steps**\n\n1. Create a ggplot graphic with `predict` on the x-axis\n2. Wrap the graphic so that each `artist_name` gets its own plot\n3. Keep all facets of the plot in one column\n4. Draw a density curve for the predicted popularity\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create plot of \nggplot(plot_posterior, aes(x = predict)) +\n    facet_wrap(~ artist_name, ncol = 1) +\n    geom_density()\n```\n:::\n\n\nGreat work! You've create a great looking plot to visualize the predicted popularity of a new song from each of these artists. We can see that the distribution for Adele has the highest predicted popularity, followed by Taylor Swift, and then Beyoncé.\n\n## Conclusion\n\nTheory. Coming soon ...\n\n**1. Conclusion**\n\nBayesian estimation of regressions models with rstanarm offers one solution to problem presented by inferences made with frequentist regression. I hope you have enjoyed this course and learning about how to implement a Bayesian model on your own.\n\n**2. What we've learned**\n\nIn this course you've learned how to complete a Bayesian regression analysis from beginning to end. We started by learning how to estimate a Bayesian regression model, including the differences from a frequentist regression, and how Bayesian methods allow us to make inferences about the actual parameter values. We then explored how we can modify a Bayesian model by altering the size of our posterior distribution, changing priors, and altering the estimation algorithm.\n\n**3. What we've learned**\n\nWe then learned about how to evaluate the fit of our model using the R-squared statistic, posterior predictive model checks, and model comparison. Finally, we looked how we can use our estimated model to make predictions and visualizations to communicate our results.\n\n**4. What we've missed**\n\nWe've covered a lot in this course, but there is much more to learn. There are many topics that are important for Bayesian inference that we touched on, but were beyond the scope of this course. For example, the mathematics of posterior distribution calculations and LOO approximations, how to choose the best prior distribution, and the causes of estimation errors in the sampling algorithm. Andrew Gelman's *Bayesian Data Analysis* gives a good overview of most of these topics. Information on the LOO approximation can be found in the documentation for the LOO package, along with accompanying resources and research articles. Finally, the Stan documentation and reference manual gives more details about the sampling algorithm, and how errors can arise.\n\n**5. What comes next?**\n\nThis was just the beginning of Bayesian modeling. If you want to learn more, I encourage you to check out the other Bayesian data analysis courses here on DataCamp. In addition, the rstanarm website has many resources for estimating all different types of regression model including Poisson and logistic regression, and multi-level models. Finally, if you are interested in the more technical aspects of Bayesian estimation, I would highly recommend the Bayesian Data Analysis book by Andrew Gelman and his colleagues that we mentioned previously.\n\n**6. Thank you!**\n\nThank you for following along with this course. I hope you continue to learn about Bayesian modeling and use it in your own work!",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}