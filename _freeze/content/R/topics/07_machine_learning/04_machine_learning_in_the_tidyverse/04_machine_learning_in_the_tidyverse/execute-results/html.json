{
  "hash": "51c80efda99b7a8ad74ff4de6493620b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Machine Learning in the Tidyverse\"\nauthor: \"Joschka Schwarz\"\n---\n\n\n\n\nThis course will teach you to leverage the tools in the \"tidyverse\" to generate, explore, and evaluate machine learning models. Using a combination of tidyr and purrr packages, you will build a foundation for how to work with complex model objects in a \"tidy\" way. You will also learn how to leverage the broom package to explore your resulting models. You will then be introduced to the tools in the test-train-validate workflow, which will empower you evaluate the performance of both classification and regression models as well as provide the necessary information to optimize model performance via hyperparameter tuning.\n\n# Foundations of \"tidy\" Machine learning\n\nThis chapter will introduce you to the backbone of machine learning in the tidyverse, the List Column Workflow (LCW). The LCW will empower you to work with many models in one dataframe. <br> This chapter will also introduce you to the fundamentals of the broom package for exploring your models.\n\n## Foundations of \"tidy\" machine learning\n\nTheory. Coming soon ...\n\n\n**1. Foundations of Tidy Machine Learning**\n\nHi, my name is Dima, and I am excited to welcome you to the Machine Learning in the Tidyverse course.If you're here then you must already know how easy it is to explore, manipulate and analyze your data with tools from the tidyverse.The good news is that the tidyverse tools also work exceptionally well for building machine learning models.\n\n**2. The Core of Tidy Machine Learning**\n\nThe reason for this is that the tidyverse tools center around the data frame structure known as a tibble. What makes a tibble special for machine learning is that it can natively store arbitrarily complex objects using a special column known as the list column. This is particularly helpful for storing models since the outputs of these models are always complex objects. With tibbles you can store models in these list columns and, as a result, explore and evaluate them with the rest of the suite of tidy tools.\n\n**3. The Core of Tidy Machine Learning**\n\nAlong with the tibble, the functions in the tidyr and purrr packages form the foundational tools for working with list columns. You will use these tools as part of a framework called the List Column Workflow.\n\n**4. List Column Workflow**\n\nAt its core, this workflow can be summed up in three basic steps.The first step is to make a list column. The second step involves using appropriate tools to work with the list column. And the third and final step is to simplify the list columns into a format that allows further exploration using the familiar tidyverse tools.These three steps rely on the map family of functions from purrr and the nest and unnest functions from tidyr.To learn how to use the list column workflow you willwork with the gapminder dataset.\n\n**5. The Gapminder Dataset**\n\nUnlike previous courses that have used the gapminder package, this course will use a more granular collection of gapminder data adapted from the dslabs package. This version contains observations for 77 countries across a time period of 52 years. Each observation has six informational elements associated with it, we will refer to these elements as the features of these observations.\n\n**6. List Column Workflow**\n\nIn this video and the exercises that follow it you will learn how to use the nest and unnest functions to manipulate the gapminder data.\n\n**7. Step 1: Make a List Column - Nest Your Data**\n\nHere is an excerpt of the gapminder data colored by country.\n\n**8. Step 1: Make a List Column - Nest Your Data**\n\nThe process of nesting compacts the chunk of data for each country into a corresponding entry in the new nested data frame. This is accomplished by the nest function.\n\n**9. Nesting By Country**\n\nTo nest the gapminder data by country you first need to use group_by() to group the data by country then use nest() to create a series of nested data frames for each country.This process creates a new list column named data. Each element in this column contains the corresponding subsetted data frames.\n\n**10. Viewing a Nested Tibble**\n\nBecause the data column in the nested data frame is a list column you can access it directly. This can be very helpful for exploring the data and prototyping your approach.\n\n**11. Viewing a Nested Tibble**\n\nFor example, you can view the fourth list entry, the data for Austria, by specifying the data column and extracting the list with the double brackets.\n\n**12. Step 3: Simplify List Columns - unnest()**\n\nFor the third step of the list column workflow, you need to simplify list columns. If the list column contains data frames, like in this example, you can simplify it using the unnest() function.\n\n**13. Step 3: Simplify List Columns - unnest()**\n\nIn this example, you can see how the nested data frames were simplified into a data frame with regular columns.Here the column to unnest is specified as an argument in the unnest() function. If no arguments are provided to unnest() it will, by default, attempt to unnest all list columns.\n\n**14. Let's Get Started!**\n\nNow it's your turn to practice using these tools.\n\n## Nesting your data\n\nIn this course, you will work with a collection of economic and social indicators for 77 countries over a period of 52 years. This data is stored in the `gapminder` data frame.\n\nIn this exercise, you will transform your `gapminder` data into a **nested** data frame by using the first tool needed to build the foundation of **tidy** machine learning skills: `nest()`. \n\n*Note: This is a more granular version than the dataset available from the `gapminder` package. This version is available in the `dslabs` package.*\n\n**Steps**\n\n1. Take a look at the first six rows of `gapminder`.  \n2. Now leverage `group_by()` and `nest()` to nest your data frames by `country`, save this as `gap_nested`. \n3. Explore the first six rows of the newly created data frame `gap_nested`, note the new complex column **data** containing tibbles.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load data\ngapminder <- readRDS(\"data/gapminder.rds\")\n\n# Explore gapminder\nhead(gapminder)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"year\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"infant_mortality\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"life_expectancy\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fertility\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"population\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"gdpPercap\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Algeria\",\"2\":\"1960\",\"3\":\"148.2\",\"4\":\"47.50\",\"5\":\"7.65\",\"6\":\"11124892\",\"7\":\"1242\"},{\"1\":\"Algeria\",\"2\":\"1961\",\"3\":\"148.1\",\"4\":\"48.02\",\"5\":\"7.65\",\"6\":\"11404859\",\"7\":\"1047\"},{\"1\":\"Algeria\",\"2\":\"1962\",\"3\":\"148.2\",\"4\":\"48.55\",\"5\":\"7.65\",\"6\":\"11690152\",\"7\":\"820\"},{\"1\":\"Algeria\",\"2\":\"1963\",\"3\":\"148.4\",\"4\":\"49.07\",\"5\":\"7.65\",\"6\":\"11985130\",\"7\":\"1075\"},{\"1\":\"Algeria\",\"2\":\"1964\",\"3\":\"148.7\",\"4\":\"49.58\",\"5\":\"7.65\",\"6\":\"12295973\",\"7\":\"1109\"},{\"1\":\"Algeria\",\"2\":\"1965\",\"3\":\"149.1\",\"4\":\"50.09\",\"5\":\"7.66\",\"6\":\"12626953\",\"7\":\"1147\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Prepare the nested data frame gap_nested\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#> ✔ purrr     1.0.2     ✔ tidyr     1.3.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\ngap_nested <- gapminder %>% \n                nest(!country)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Supplying `...` without names was deprecated in tidyr 1.0.0.\n#> ℹ Please specify a name for each selection.\n#> ℹ Did you want `data = !country`?\n```\n\n\n:::\n\n```{.r .cell-code}\n  # group_by(country) %>% \n  # nest()\n\n# Explore gap_nested\nhead(gap_nested)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"data\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Algeria\",\"2\":\"<tibble[,6]>\"},{\"1\":\"Argentina\",\"2\":\"<tibble[,6]>\"},{\"1\":\"Australia\",\"2\":\"<tibble[,6]>\"},{\"1\":\"Austria\",\"2\":\"<tibble[,6]>\"},{\"1\":\"Bangladesh\",\"2\":\"<tibble[,6]>\"},{\"1\":\"Belgium\",\"2\":\"<tibble[,6]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nYou're off to a great start! Notice that each row in `gap_nested` contains a tibble.\n\n## Unnesting your data\n\nAs you've seen in the previous exercise, a nested data frame is simply a way to shape your data. Essentially taking the `group_by()` windows and packaging them in corresponding rows. \n\nIn the same way you can use the `nest()` function to break your data into nested chunks, you can use the `unnest()` function to expand the data frames that are nested in these chunks.\n\n**Steps**\n\n1. Use `unnest()` on the `gap_nested` data frame to take a nested column and expand it into a new data frame and save it as `gap_unnested`. \n2. Make sure that `gapminder` and `gap_unnested` are identical by using the `identical()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create the unnested data frame called gap_unnnested\ngap_unnested <- gap_nested %>% \n                  unnest(cols = c(data))\n  \n# Confirm that your data was not modified  \nidentical(gapminder, gap_unnested)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] TRUE\n```\n\n\n:::\n:::\n\n\nGreat work!! Notice that this transformation only reshaped your data, it did not modify it.\n\n## Explore a nested cell\n\nIn the first exercise, you successfully created a nested data frame `gap_nested`. The `data` column contains tibbles for each country. In this exercise, you will explore one of these nested chunks.\n\n**Steps**\n\n1. Extract the nested data for Algeria and store this as `algeria_df`. \n2. Calculate the following summary stats for Algeria's population: `min()`, `max()` and `mean()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract the data of Algeria\nalgeria_df <- gap_nested$data[[1]]\n\n# Calculate the minimum of the population vector\nmin(algeria_df$population)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 11124892\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the maximum of the population vector\nmax(algeria_df$population)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 36717132\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the mean of the population vector\nmean(algeria_df$population)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 23129438\n```\n\n\n:::\n:::\n\n\nWell done! You can see that working with a single chunk in a nested data frame is identical to working with regular data frames. In the next section you will learn how to scale this approach to work on a vector of nested data frames using the `map` family of functions.\n\n## The map family of functions\n\nTheory. Coming soon ...\n\n**1. The map family of functions**\n\n**2. List Column Workflow**\n\nIn the last video and exercise series, you learned how to use the nest() and unnest() functions for steps one and three of the list column workflow.\n\n**3. List Column Workflow**\n\nIn this lesson, I will introduce you to the map_*() family of functions. These functions fulfill the roles of steps two and three of this workflow.\n\n**4. The map Function**\n\nThe map() function applies a desired function to every element in a vector or a list and always returns a list as its result. This function requires two parameters, dot x and dot f.\n\n**5. The map Function**\n\nDot x is the vector or list that you want to iterate over while dot f is the function. The function can either be a predefined function or it can be an anonymous function using the formula syntax.\n\n**6. The map Function**\n\nFor example, if you wanted to use the mean() function you can refer to it directly or you can build an anonymous function using the tilda to indicate that you are using a formula and the dot x to indicate the value placeholder.\n\n**7. Population Mean by Country**\n\nIn the previous exercise, you calculated the mean population of the country of Algeria by extracting the first element of the nested data frame then calculating the mean of the population column. The structure of this is very similar when using map(). You will use map() to calculate the population mean for each country using the corresponding nested data frame of that country.\n\n**8. Population Mean by Country**\n\nHere the dot x parameter is the data column in the nested data frame. Remember that this column is a list of data frames corresponding to each country. Since these are data frames you need to use an anonymous function to explicitly calculate the mean for the population column of each data frame. Remember that the dot x here acts as the placeholder for each element of the list. Since you know that this list contains data frames and you want to calculate the mean of the population column from each data frame, you can refer to this placeholder the same way you would for working with a single element. The result of this function is a list of population means for the 77 countries.\n\n**9. 2: Work with List Columns - map() and mutate()**\n\nRemember that tibbles are special data frames that allow us to store arbitrarily complex list columns. Because of this you can append the resulting list of population means using the mutate() function. Of course, storing a list of doubles isn't very practical for exploration\n\n**10. 3: Simplify List Columns - unnest()**\n\nso you need to simplify these columns using unnest(). Let's revisit these steps in the context of the list column workflow.\n\n**11. List Column Workflow**\n\nFirst we made a list column of data frames for each country using nest(). Then we worked with the list columns by calculating the population mean of each data frame using map(). Finally, we simplified the resulting nested column with the unnest() function. In certain situations, you can combine the last two steps using another function from the map_*() family.\n\n**12. Work With + Simplify List Columns With map_*()**\n\nIf you know that the output of the mapped function is a vector of a specific type, you can use a map function corresponding to that type to calculate the result and explicitly return a vector of the expected type.\n\n**13. Work With + Simplify List Columns With map_dbl()**\n\nFor example, the mean() function returns a vector of type double, as such you can use map_double() to return a vector of doubles instead of a list of doubles. This can be done like so, and as a result, mutate() appends a vector of type double to the data frame instead of a list.\n\n**14. Build Models with map()**\n\nYou can also use map() to build models for each country. Here the lm() function is used to build linear models to predict the population using the fertility feature. You can define the model using the formula parameter and provide the data for each model using the dot x approach to refer to each country's data frame when mapping.\n\n**15. Let's map something!**\n\nSo let's map some data. \n\n## Mapping your data\n\nIn combination with `mutate(),` you can use `map()` to append the results of your calculation to a data frame. Since the `map()` function always returns a vector of lists you must use `unnest()` to extract this information into a numeric vector.\n\nHere you will explore this functionality by calculating the mean population of each country in the `gapminder` dataset.\n\n**Steps**\n\n1. Use `map()` to apply the `mean()` function to calculate the population mean for each country and append this new list column called `mean_pop` using `mutate()`. \n2. Explore the first 6 rows of `pop_nested`. \n3. Use `unnest()` to convert the `mean_pop` list into a numeric column and save this as the `pop_mean` data frame. \n4. Explore `pop_mean` using `head()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the mean population for each country\npop_nested <- gap_nested %>%\n  mutate(mean_pop = map(data, ~mean(.x$population)))\n\n# Take a look at pop_nested\nhead(pop_nested)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"data\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"mean_pop\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Algeria\",\"2\":\"<tibble[,6]>\",\"3\":\"<dbl [1]>\"},{\"1\":\"Argentina\",\"2\":\"<tibble[,6]>\",\"3\":\"<dbl [1]>\"},{\"1\":\"Australia\",\"2\":\"<tibble[,6]>\",\"3\":\"<dbl [1]>\"},{\"1\":\"Austria\",\"2\":\"<tibble[,6]>\",\"3\":\"<dbl [1]>\"},{\"1\":\"Bangladesh\",\"2\":\"<tibble[,6]>\",\"3\":\"<dbl [1]>\"},{\"1\":\"Belgium\",\"2\":\"<tibble[,6]>\",\"3\":\"<dbl [1]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Extract the mean_pop value by using unnest\npop_mean <- pop_nested %>% \n  unnest(mean_pop)\n\n# Take a look at pop_mean\nhead(pop_mean)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"data\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"mean_pop\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Algeria\",\"2\":\"<tibble[,6]>\",\"3\":\"23129438\"},{\"1\":\"Argentina\",\"2\":\"<tibble[,6]>\",\"3\":\"30783053\"},{\"1\":\"Australia\",\"2\":\"<tibble[,6]>\",\"3\":\"16074837\"},{\"1\":\"Austria\",\"2\":\"<tibble[,6]>\",\"3\":\"7746272\"},{\"1\":\"Bangladesh\",\"2\":\"<tibble[,6]>\",\"3\":\"97649407\"},{\"1\":\"Belgium\",\"2\":\"<tibble[,6]>\",\"3\":\"9983596\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nExcellent job! Here you can see how to leverage the `map()` function to apply a desired function and store it. In the next exercise you will see how this can be done more concisely using `map_dbl()`.\n\n## Expecting mapped output\n\nWhen you know that the output of your mapped function is an expected type (here it is a numeric vector) you can leverage the `map_*()` family of functions to explicitly try to return that object type instead of a list. \n\nHere you will again calculate the mean population of each country, but instead, you will use `map_dbl()` to explicitly append the numeric vector returned by `mean()` to your data frame.\n\n**Steps**\n\n1. Generate the `pop_mean` data frame using the `map_dbl()` function to calculate the population mean for each nested data frame. \n2. Explore the `pop_mean` data frame using `head()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate mean population and store result as a double\npop_mean <- gap_nested %>%\n  mutate(mean_pop = map_dbl(data, ~mean(.x$population)))\n\n# Take a look at pop_mean\nhead(pop_mean)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"data\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"mean_pop\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Algeria\",\"2\":\"<tibble[,6]>\",\"3\":\"23129438\"},{\"1\":\"Argentina\",\"2\":\"<tibble[,6]>\",\"3\":\"30783053\"},{\"1\":\"Australia\",\"2\":\"<tibble[,6]>\",\"3\":\"16074837\"},{\"1\":\"Austria\",\"2\":\"<tibble[,6]>\",\"3\":\"7746272\"},{\"1\":\"Bangladesh\",\"2\":\"<tibble[,6]>\",\"3\":\"97649407\"},{\"1\":\"Belgium\",\"2\":\"<tibble[,6]>\",\"3\":\"9983596\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nYou're doing great! With the `nest()` and `map_*()` functions in hand you now have the foundation for building multiple models.\n\n## Mapping many models\n\nThe `gap_nested` data frame available in your workspace contains the gapminder dataset nested by country. \n\nYou will use this data to build a linear model for each country to predict **life expectancy** using the **year** feature.\n\n**Note:** The term *feature* is synonymous with the terms *variable* or *predictor*. It refers to an attribute of your data that can be used to build a machine learning model.\n\n**Steps**\n\n1. Build a linear model for each country predicting `life_expectancy` using the `year` feature. Use the `lm()` function for this and save this new data frame containing models as `gap_models`. \n2. Extract the first model from this data frame and save this as `algeria_model`. \n3. View the information about the model using `summary()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build a linear model for each country\ngap_models <- gap_nested %>%\n    mutate(model = map(data, ~lm(formula = life_expectancy~year, data = .x)))\n    \n# Extract the model for Algeria    \nalgeria_model <- gap_models$model[[1]]\n\n# View the summary for the Algeria model\nsummary(algeria_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = life_expectancy ~ year, data = .x)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -4.044 -1.577 -0.543  1.700  3.843 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -1.197e+03  3.994e+01  -29.96   <2e-16 ***\n#> year         6.349e-01  2.011e-02   31.56   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.177 on 50 degrees of freedom\n#> Multiple R-squared:  0.9522,\tAdjusted R-squared:  0.9513 \n#> F-statistic: 996.2 on 1 and 50 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nFantastic work! You've just built **77** models for **77** countries with just a few lines of code. <br> In the next series of exercises, you will learn how you can extract information from `summary()` in a tidy fashion.\n\n## Tidy your models with broom\n\nTheory. Coming soon ...\n\n\n**1. Tidy your models with broom**\n\nNow that you know how to work with list columns in a tidy manner you can begin to work with the tools you need to explore and evaluate machine learning models.\n\n**2. List Column Workflow**\n\nAs you can probably imagine, the bulk of the work of machine learning resides in step two of this workflow. Since you can store complex model objects in your data frame you can also work with these objects using the tools available in various R packages.\n\n**3. List Column Workflow**\n\nIn this video, we will focus on the broom package. A package designed to convert useful model outputs into tidy data frames.\n\n**4. Broom Toolkit**\n\nThe core of broom is encapsulated by three functions which aim to extract conceptually different information from any model. - tidy() is used to extract the statistical findings of a model.- glance() provides a one row summary of a model, and - augmment() appends the predicted values of a model to the data being modeled.Let's explore each of these in greater detail by reviewing the results of the linear model that you created for Algeria.\n\n**5. Summary of algeria_model**\n\nIf you look at the summary() of the Algeria model you can see that there is a lot of useful information here. However, this information is not particularly easy to extract directly from the object as it is to simply print it. But using tidy() and glance() you can easily extract this information into data frames.\n\n**6. tidy()**\n\nThe tidy() function collects the statistical findings of a model into a data frame.When used with a linear model, tidy() returns the coefficients and their corresponding statistics for that model.\n\n**7. tidy()**\n\nTo extract these statistics you simply apply the tidy() function to the model object as shown here.\n\n**8. glance()**\n\nThe next broom function, glance(), is used to return a one row summary of a model. For a linear model, this summary contains various statistics about the fit of the model such as the r squared.\n\n**9. glance()**\n\nExtracting this information into a data frame is as simple as calling the function on the model object.\n\n**10. augment()**\n\nFinally, the augment() function builds an observation-level data frame containing the original data used to build the model as well as the predicted value for each observation as the column dot fitted. Furthermore, augment() appends model-specific statistics of fit for each observation.By constructing a data frame containing both the original values and those predicted by our model you can explore the fit of the model.\n\n**11. Plotting Augmented Data**\n\nFor instance, you can visualize how well your model fits the data by plotting the predicted and actual values of life expectancy with respect to year.In this plot the actual values are the black points and the fit of the model, or predicted values, is shown as the red line.By examining this plot you can learn that a simple linear model may not be the best approach for this example and would consider either including more features or using a non-linear approach to better capture this relationship.\n\n**12. Let's use broom!**\n\nUsing these three tools makes it easy to extract model coefficients, fit statistics and observation-level performance for many different machine learning models.In chapter two we will use broom as a part of the list column workflow to do this for all 77 of our country-level models with just a few lines of code. But first, let's review what you have learned with a few exercises.\n\n## The three ways to tidy your model\n\n> *Question*\n> ---\nBelow are the descriptions of the three functions in the **broom** package. Which ones are correct?<br>\n> <br>\n> A) **tidy()** returns the statistical findings of the model (such as coefficients)<br>\n> B) **glance()** returns a concise one-row summary of the model<br>\n> C) **augment()** adds prediction columns to the data being modeled<br>\n> <br>\n> ⬜ Only A<br>\n> ⬜ A and C<br>\n> ⬜ None are correct<br>\n> ✅ All are correct<br>\n\nGreat job! These are the three main functions that `broom` provides for tidying the output of models.\n\n## Extracting model statistics tidily\n\nIn this exercise, you will use the `tidy()` and `glance()` functions to extract information from `algeria_model` in a tidy manner.\n\nFor a linear model, `tidy()` extracts the model coefficients while `glance()` returns the model statistics such as the \\\\(R^2\\\\).\n\n**Steps**\n\n1. Extract the coefficient information as a tidy data frame of the `algeria_model` using `tidy()`. \n2. Extract the model statistics of `algeria_model` using `glance()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\n\n# Extract the coefficients of the algeria_model as a data frame\ntidy(algeria_model)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-1196.5647772\",\"3\":\"39.93891866\",\"4\":\"-29.95987\",\"5\":\"1.319126e-33\"},{\"1\":\"year\",\"2\":\"0.6348625\",\"3\":\"0.02011472\",\"4\":\"31.56209\",\"5\":\"1.108517e-34\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Extract the statistics of the algeria_model as a data frame\nglance(algeria_model)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"r.squared\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"adj.r.squared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logLik\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AIC\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BIC\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"deviance\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df.residual\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"nobs\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.9522064\",\"2\":\"0.9512505\",\"3\":\"2.176948\",\"4\":\"996.1653\",\"5\":\"1.108517e-34\",\"6\":\"1\",\"7\":\"-113.2171\",\"8\":\"232.4342\",\"9\":\"238.288\",\"10\":\"236.9552\",\"11\":\"50\",\"12\":\"52\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nGreat job! As you can see `tidy()` and `glance()` both return data frames, this feature can be very useful for managing the results of many models in one data frame.\n\n## Augmenting your data\n\nFrom the results of `glance()`, you learned that using the available features the linear model fits well with an adjusted \\\\(R^2\\\\) of *0.99*. The `augment()` function can help you explore this fit by appending the predictions to the original data. \n\nHere you will leverage this to compare the predicted values of `life_expectancy` with the original ones based on the `year` feature.\n\n**Steps**\n\n1. Build the augmented data frame `algeria_fitted` using `augment()`. \n2. Visualize the fit of the model with respect to `year` by plotting both `life_expectancy` as points and `.fitted` as a line.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build the augmented data frame\nalgeria_fitted <- augment(algeria_model)\n\n# Compare the predicted values with the actual values of life expectancy\nalgeria_fitted %>% \n  ggplot(aes(x = year)) +\n  geom_point(aes(y = life_expectancy)) + \n  geom_line(aes(y = .fitted), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](04_machine_learning_in_the_tidyverse_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nCongratulations! \n\nYou've successfully completed Chapter 1. In the next chapter you will see how you can leverage the tools you learned to build, evaluate and explore the models you created for each country.\n\n# 2. Multiple Models with broom\n\nThis chapter leverages the List Column Workflow to build and explore the attributes of 77 models. You will use the tools from the broom package to gain a multidimensional understanding of all of these models.\n\n## Exploring coefficients across models\n\nTheory. Coming soon ...\n\n\n**1. Exploring coefficients across models**\n\nIn the last chapter you learned about the list column workflow to build multiple models, and you learned about the three functions from the broom package that allow you to explore these models. In this chapter, you will combine these techniques to learn more about your models and your data.\n\n**2. 77 models**\n\nRecall that the gap_models data frame contains information about each country from 1960 to 2011, and that the features are nested as a tibble for each country. Using these tibbles, you built simple linear models predicting life_expectancy by year for each country. In this video and exercises that follow, you will learn how to use the coefficients of these models to gain new insights into the gapminder data.\n\n**3. Regression coefficients**\n\nSo let's briefly review how to interpret the coefficients for a simple linear regression model.Remember that this involves calculating two coefficient terms that relate the dependent variable y to the independent variable x.\n\n**4. Regression coefficients**\n\nFor our models, the y variable is life expectancy as it relates to the year, our x variable.The coefficient of the intercept tells us the expected life expectancy at year zero. This isn't meaningful for our data so we won't focus on this term.Instead, we are interested in the estimate of the year coefficient which, for a simple linear model, directly corresponds to the slope.Using the tidy() function on the first model we learn that with each passing year the average life expectancy of the population of this country increases by approximately 0.63 years. This approach can provide you with information about the growth or lack of growth in life expectancy over time for the countries that you are modeling.\n\n**5. Coefficients of multiple models**\n\nYou can generate these coefficients by mapping the tidy() function for each of your models and then simplifying the new data frame by using the unnest() function. This results in a tibble containing the estimate for each coefficient of every country model.\n\n**6. Let's practice!**\n\nNow let's explore these values to see what you can learn from this data.\n\n## Tidy up the coefficients of your models\n\nIn this exercise you will leverage the list column workflow along with the `tidy()` function from `broom` to extract and explore the coefficients for the 77 models you built.\n\nRemember the `gap_models` data frame contains a model predicting **life expectancy** by **year** for 77 countries.\n\n**Steps**\n\n1. Use `tidy()` to append a column (`coef`) containing coefficient statistics for each model to the `gap_models` data frame and save it as `model_coef_nested`. \n2. Simplify this data frame using `unnest()` to extract these coefficients in your data frame. \n3. Explore the coefficient estimates for the year feature across your 77 models by plotting a histogram of their values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract the coefficient statistics of each model into nested data frames\nmodel_coef_nested <- gap_models %>% \n    mutate(coef = map(model, ~tidy(.x)))\n    \n# Simplify the coef data frames for each model    \nmodel_coef <- model_coef_nested %>%\n    unnest(coef)\n\n# Plot a histogram of the coefficient estimates for year         \nmodel_coef %>% \n  filter(term == \"year\") %>% \n  ggplot(aes(x = estimate)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04_machine_learning_in_the_tidyverse_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nGreat job! Now that you have the slope for each model let's explore their distribution.\n\n## What can we learn about these 77 countries?\n\nExplore the `model_coef` data frame you just created to answer the following question:\n\n> *Question*\n> ---\n> Which of the following conclusions can we make from the coefficients of our models?<br>\n> <br>\n> ⬜ About **10%** of the 77 countries had a decrease of life expectancy between 1960 and 2011.<br>\n> ⬜ The **United States** experienced the fastest growth in life expectancy.<br>\n> ✅ The majority of the 77 countries experienced a growth in average life expectancy between 1960 and 2011.<br>\n> ⬜ All of these conclusions are correct.<br>\n> ⬜ None of these conclusions are correct.<br>\n\nYou got it! Based on thse models we can conclude that 73 of the 77 countries experienced a growth in life expectancy during this time period.\n\n## Evaluating the fit of many models\n\nTheory. Coming soon ...\n\n\n**1. Evaluating the fit of many models**\n\nIn the last series of exercises you leveraged the tidy() function from broom to explore the coefficients of your models. By doing so you gained insight into how life expectancy changed with time for each of the 77 countries in your dataset.Now you will learn how to use the glance() function to measure how well each of the 77 models fit their underlying data.\n\n**2. The fit of our models**\n\nOne way you can measure the fit of a model is to calculate its rsquared metric.The R-squared metric measures the relationship between the variation explained by the regression model and the total variation in the data. It takes on values between 0 and 1.\n\n**3. The fit of our models**\n\nHere are two example datasets with a low and a high Rsquared value.On the left, is a dataset with an Rsquared value close to 0 indicating that a linear model is capturing a proportionally small amount of the variation in the data and hence is not a good fit. In contrast, the model on the right has an Rsquared value closer to one indicating that this linear model fits the data well.You can evaluate the fit of all 77 of your models by measuring the Rsquared value for each model.\n\n**4. Glance across your models**\n\nTo do this you use map() and glance() to create a data frame of summary statistics for each model stored as the coef column. You can then simplify these data frames by using the unnest() function.This results in a tibble containing the model statistics for every country model.Looking at the rsquared values of the first 6 models you can see that all 6 of these models have a high rsquared suggesting that they have fit the data for that country well.\n\n**5. Best &amp; worst fitting models**\n\nYou can now explore the fit of all 77 models. For instance, you can use the top_n() function to find the best fitting models like so. Likewise, you can find the models with the worst fit by negating the weight vector like so.\n\n**6. Let's practice!**\n\nIn the next series of exercises you will build this data frame and explore it to learn more about the fit of each of your 77 models.\n\n## Glance at the fit of your models\n\nIn this exercise you will use `glance()` to calculate how well the linear models fit the data for each country.\n\n**Steps**\n\n1. Append a column (`fit`) containing the fit statistics for each model to the `gap_models` data frame and save it as `model_perf_nested`. \n2. Simplify this data frame using `unnest()` to extract these fit statistics of each model and save it as `model_perf`. \n3. Finally, use `head()` to take a peek at `model_perf`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract the fit statistics of each model into data frames\nmodel_perf_nested <- gap_models %>% \n    mutate(fit = map(model, ~glance(.x)))\n\n# Simplify the fit data frames for each model    \nmodel_perf <- model_perf_nested %>% \n    unnest(fit)\n\n# Look at the first six rows of model_perf\nhead(model_perf)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"data\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"model\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"r.squared\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"adj.r.squared\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logLik\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AIC\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BIC\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"deviance\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df.residual\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"nobs\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Algeria\",\"2\":\"<tibble[,6]>\",\"3\":\"<S3: lm>\",\"4\":\"0.9522064\",\"5\":\"0.9512505\",\"6\":\"2.1769481\",\"7\":\"996.1653\",\"8\":\"1.108517e-34\",\"9\":\"1\",\"10\":\"-113.21711\",\"11\":\"232.43422\",\"12\":\"238.28795\",\"13\":\"236.955152\",\"14\":\"50\",\"15\":\"52\"},{\"1\":\"Argentina\",\"2\":\"<tibble[,6]>\",\"3\":\"<S3: lm>\",\"4\":\"0.9843108\",\"5\":\"0.9839970\",\"6\":\"0.4309295\",\"7\":\"3136.8994\",\"8\":\"8.782656e-47\",\"9\":\"1\",\"10\":\"-28.99091\",\"11\":\"63.98182\",\"12\":\"69.83555\",\"13\":\"9.285013\",\"14\":\"50\",\"15\":\"52\"},{\"1\":\"Australia\",\"2\":\"<tibble[,6]>\",\"3\":\"<S3: lm>\",\"4\":\"0.9830777\",\"5\":\"0.9827393\",\"6\":\"0.5108679\",\"7\":\"2904.6857\",\"8\":\"5.825245e-46\",\"9\":\"1\",\"10\":\"-37.83956\",\"11\":\"81.67912\",\"12\":\"87.53285\",\"13\":\"13.049298\",\"14\":\"50\",\"15\":\"52\"},{\"1\":\"Austria\",\"2\":\"<tibble[,6]>\",\"3\":\"<S3: lm>\",\"4\":\"0.9866741\",\"5\":\"0.9864076\",\"6\":\"0.4380888\",\"7\":\"3702.0867\",\"8\":\"1.480855e-48\",\"9\":\"1\",\"10\":\"-29.84771\",\"11\":\"65.69542\",\"12\":\"71.54915\",\"13\":\"9.596088\",\"14\":\"50\",\"15\":\"52\"},{\"1\":\"Bangladesh\",\"2\":\"<tibble[,6]>\",\"3\":\"<S3: lm>\",\"4\":\"0.9485248\",\"5\":\"0.9474953\",\"6\":\"1.8325143\",\"7\":\"921.3416\",\"8\":\"7.099983e-34\",\"9\":\"1\",\"10\":\"-104.26089\",\"11\":\"214.52178\",\"12\":\"220.37552\",\"13\":\"167.905441\",\"14\":\"50\",\"15\":\"52\"},{\"1\":\"Belgium\",\"2\":\"<tibble[,6]>\",\"3\":\"<S3: lm>\",\"4\":\"0.9902805\",\"5\":\"0.9900861\",\"6\":\"0.3313223\",\"7\":\"5094.3033\",\"8\":\"5.538711e-52\",\"9\":\"1\",\"10\":\"-15.32255\",\"11\":\"36.64511\",\"12\":\"42.49884\",\"13\":\"5.488723\",\"14\":\"50\",\"15\":\"52\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nGreat job! You have successfully calculated the fit statistics for all 77 of your models. Next, we'll explore these results.\n\n## Best and worst fitting models\n\nIn this exercise you will answer the following questions:\n\n* Overall, how well do your models fit your data?\n* Which are the best fitting models?\n* Which models do not fit the data well?\n\n**Steps**\n\n1. Plot a histogram of the \\\\(R^2\\\\) values of the 77 models\n2. Extract the 4 best fitting models (based on \\\\(R^2\\\\)) and store this data frame as `best_fit`\n3. Extract the 4 worst fitting models (based on \\\\(R^2\\\\)) and store this data frame as `worst_fit`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Plot a histogram of rsquared for the 77 models    \nmodel_perf %>% \n  ggplot(aes(x = r.squared)) + \n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04_machine_learning_in_the_tidyverse_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# Extract the 4 best fitting models\nbest_fit <- model_perf %>% \n              slice_max(r.squared, n = 4)\n\n# Extract the 4 models with the worst fit\nworst_fit <- model_perf %>% \n              slice_min(r.squared, n = 4)\n```\n:::\n\n\nExcellent work! You have now prepared two data frames, one containing the four best fitting models and another the four worst fitting models. In the next section we will use the `augment()` function to explore these fits visually.\n\n## Visually inspect the fit of many models\n\nTheory. Coming soon ...\n\n\n**1. Visually inspect the fit of your models**\n\nUsing glance() you learned which of your 77 models fit the underlying data well and which do not. You can get more insight into the fit of a model by comparing the original values of life expectancy to the ones predicted by the model for each observation.\n\n**2. Building augmented datframes**\n\nTo do this you first need to build a data frame that contains both the predicted and the original values. This requires first using map() and augment() to work on the list column containing the models to create nested data frames containing both the original and the predicted values. Then you can use unnest() on this new column to simplify these data frames allowing further exploration.Let's visualize some of these models.\n\n**3. Model for Italy $R^2: 0.99$**\n\nFirst, let's look at the Italy model, where, based on the rsquared of 0.99, we can expect that a linear model will fit the data well. You can compare the fit of the model with the original data by plotting both on the same plot. In this example I used ggplot2 to plot the original values of life expectancy as a scatterplot using the geom_point() layer and I added the linear model fit as a red line using the geom_line() layer.Using this plot it is clear that the model was able to fit the data well.\n\n**4. Model for Fiji $R^2: 0.82$**\n\nNext let's look at Fiji model which has an r-squared value lower than the model for Italy.By plotting this data you can see that a linear model does a decent job, but there is clearly room for improvement since it looks like after 1990 the growth of life expectancy levels off.\n\n**5. Model for Kenya $R^2: 0.42$**\n\nFinally, let's look at an example where the Rsquared is much lower. From this plot you can see that a linear model does not adequately capture the relationship of life expectancy with year. As you can see from these three examples, augment() and ggplot make it easy to visually explore the fit of a model.\n\n**6. Let's practice!**\n\nNow it's your turn to use these tools to visualize the fit of your best and worst fitting models.\n\n## Augment the fitted values of each model\n\nIn this exercise you will prepare your four best and worst fitting models for further exploration by augmenting your model data with `augment()`.\n\n**Steps**\n\n1. Build the `best_augmented` data frame by building augmented data frames and simplifying them with `unnest()` using the `best_fit` data frame.\n2. Build the `worst_augmented` data frame by building augmented data frames and simplifying them with `unnest()` using the `worst_fit` data frame.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbest_augmented <- best_fit %>% \n  # Build the augmented data frame for each country model\n  mutate(augmented = map(model, ~augment(.x))) %>% \n  # Expand the augmented data frames\n  unnest(augmented)\n\nworst_augmented <- worst_fit %>% \n  # Build the augmented data frame for each country model\n  mutate(augmented = map(model, ~augment(.x))) %>% \n  # Expand the augmented data frames\n  unnest(augmented)\n```\n:::\n\n\nYou're doing great! You now have the pieces necessary to visually explore the fits of these 8 models.\n\n## Explore your best and worst fitting models\n\nLet's explore your four best and worst fitting models by comparing the fitted lines with the actual values.\n\n**Steps**\n\n1. Visualize the fit of your four best fitting models with respect to `year` by plotting both `life_expectancy` as points and `.fitted` as a line.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compare the predicted values with the actual values of life expectancy \n# for the top 4 best fitting models\nbest_augmented %>% \n  ggplot(aes(x = year)) +\n  geom_point(aes(y = life_expectancy)) + \n  geom_line(aes(y = .fitted), color = \"red\") +\n  facet_wrap(~country, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](04_machine_learning_in_the_tidyverse_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n2. Visualize the fit of your four worst fitting models with respect to `year` by plotting both `life_expectancy` as points and `.fitted` as a line.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compare the predicted values with the actual values of life expectancy \n# for the top 4 worst fitting models\nworst_augmented %>% \n  ggplot(aes(x = year)) +\n  geom_point(aes(y = life_expectancy)) + \n  geom_line(aes(y = .fitted), color = \"red\") +\n  facet_wrap(~country, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](04_machine_learning_in_the_tidyverse_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nCool plots! You can see that a linear model does a great job for the best 4 fitting models but the worst 4 fitting models do not seem to have a linear relationship. You will work to improve this fit in the next series of exercises by incorporating additional features.\n\n## Improve the fit of your models\n\nTheory. Coming soon ...\n\n\n**1. Improve the fit of your models**\n\nUsing the information we gathered with augment() and glance(), we learned that some of the simple linear regression models do not adequately fit the underlying trends in our data. To overcome this we will now employ a multiple regression model.\n\n**2. Multiple Linear Regression model**\n\nThis model is a natural extension of the simple linear regression model. The key difference is that more than one explanatory variable is used to explain the outcome, meaning that rather than fitting a best fit line we are instead fitting a multi-dimensional plane.In the gapminder dataset we can use additional characteristics or features of our observations to model life expectancy. So, let's use them all.\n\n**3. Using all features**\n\nThe choice of which features to use can be controlled in the formula field of the lm() function. Remember that for a simple model you used the formula of life expectancy as explained by year.Similarly, for a multiple linear regression model you can explicitly define the formula by including the name of each feature separated by a plus sign or if you know you want to include all features you can capture them by using a period, as shown here.\n\n**4. Using broom with Multiple Linear Regression models**\n\nThe behavior of the broom functions remains the same. tidy() returns the coefficient estimates of the models, this now includes estimates for the four additional features. Same goes for augment(), in addition to the fitted values for each observation, the values of four new features are returned.And although the expected output of glance() remains the same we have to shift our focus from the r squared value to the adjusted r squared value when evaluating the fit of our models or comparing simple and multiple linear regression models.\n\n**5. Adjusted $R^2$**\n\nRemember that R-squared measures the variation explained by the model. Adding any new feature to a model, regardless of its relationship with the dependent variable, will always increase the model's r squared value. This becomes problematic when comparing the fit of models with different number of explanatory features used. To compensate for this you will instead use the Adjusted R-squared value, this is a modified rsquared metric whose calculation takes into account the number of features used in the model.The interpretation of the adjusted R-squared value is very similar to the R-squared and you will use this to evaluate the fit of your new models and compare them to the previously built simple linear models.\n\n**6. Let's practice!**\n\nSo, let's get started.\n\n## Build better models\n\nEarlier you built a collection of simple models to fit **life expectancy** using the **year** feature. Your previous analysis showed that some of these models didn't fit very well. \n\nIn this exercise you will build multiple regression models for each country using all available features. You may be interested in comparing the performance of the four worst fitting models so their adjusted \\\\(R^2\\\\) are provided below:\n\n|Country  | Adjusted R^2      |\n|:--------|------------------:|\n|Botswana |         -0.0060772|\n|Lesotho  |         -0.0169851|\n|Zambia   |          0.1668999|\n|Zimbabwe |          0.2083979|\n\n**Steps**\n\n1. Build a linear model for each country predicting `life_expectancy` using every feature in the dataset. \n2. Append a column (`fit`) containing fit statistics for each model and simplify this data frame. \n3. Print the adjusted \\\\(R^2\\\\) in `fullmodel_perf` of the four countries from `worst_fit` data frame.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build a linear model for each country using all features\ngap_fullmodel <- gap_nested %>% \n  mutate(model = map(data, ~lm(life_expectancy~., data = .x)))\n\nfullmodel_perf <- gap_fullmodel %>% \n  # Extract the fit statistics of each model into data frames\n  mutate(fit = map(model, ~glance(.x))) %>% \n  # Simplify the fit data frames for each model\n  unnest(fit)\n  \n# View the performance for the four countries with the worst fitting \n# four simple models you looked at before\nfullmodel_perf %>% \n  filter(country %in% worst_fit$country) %>% \n  select(country, adj.r.squared)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"country\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"adj.r.squared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Botswana\",\"2\":\"0.8438327\"},{\"1\":\"Lesotho\",\"2\":\"0.9077870\"},{\"1\":\"Zambia\",\"2\":\"0.7060580\"},{\"1\":\"Zimbabwe\",\"2\":\"0.9775599\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThat was a tough one and you did great! You can see that the performance of each of the four worst performing models based on their adjusted $R^2$ drastically improved once other features were added to the model.\n\n## Predicting the future\n\n|Country | Adjusted R^2      |\n|:-------|------------------:|\n|Brazil  |          0.9994261|\n|Greece  |          0.9994407|\n|Mexico  |          0.9995427|\n|Morocco |          0.9997960|\n\n> *Question*\n> ---\n> Which of these four models do you expect to perform the best for future years?<br>\n> <br>\n> ⬜ Brazil<br>\n> ⬜ Greece<br>\n> ⬜ Mexico<br>\n> ⬜ Morocco<br>\n> ✅ Can not be determined using this information<br>\n\nBingo! While the adjusted \\\\(R^2\\\\) does tell us how well the model fit our data, it does not give any indication on how it would perform on new data. In the upcoming chapter you will learn how to estimate model performance using data withheld from building the model.\n\n# 3. Build, Tune & Evaluate Regression Models\n\nIn this chapter you will learn how to use the List Column Workflow to build, tune and evaluate regression models. You will have the chance to work with two types of models: linear models and random forest models.\n\n## Training, test and validation splits\n\nTheory. Coming soon ...\n\n\n**1. Training, test and validation splits**\n\nTwo of the most important questions that a data scientist must answer when building machine learning models are:How well would my model perform on new data?andDid I select the best performing model? Throughout this chapter you will learn the techniques necessary to answer these questions.\n\n**2. Train-Test Split**\n\nTo answer the first question,\"how well would my model perform on new data?\"Start with all of your data, this contains both the features and the outcome you want to predict\n\n**3. Train-Test Split**\n\nand split it into two portions.\n\n**4. Train-Test Split**\n\nThe first portion is used to train a model and the second portion is used to test how well it performs on new data.This is known as the train-test split. In a disciplined machine learning workflow this is a critical first step. So long as the test data is a fair representation of the data you can expect to see in the future you can use it to estimate the expected performance for future observations.\n\n**5. initial_split()**\n\nTo make the train-test split you will use the initial_split() function from the rsample package.The prop parameter is used to specify the proportion of data that will be selected for the train set, in this case it is 75%. This means that 25% of the data will be randomly withheld as the test set.To prepare the training and the testing data frames you use the functions training() and testing(), respectively. Of the 4004 observations in the gapminder dataset, 3001 or approximately 75% is partitioned into the training data and the remainder 25% is reserved as testing data.\n\n**6. Train-Validate Split**\n\nBecause you are interested in keeping the test data independent you must not use it to make any decisions about your models. So, to answer the second question:\"Did I select the best performing model?\"You must rely exclusively on the train data.\n\n**7. Train-Validate Split**\n\nThe train data can be further split into two partitions of train and validate. Now you can use the new train data to build your models and use validate to calculate their performance.\n\n**8. Cross Validation**\n\nYou can take this one step further by repeating this train-validate split several times. Each time reserving a different portion of the data for evaluation. This is known as cross validation and it provides two key advantages:First, by iteratively withholding different portions of the training data you can essentially use all of it to evaluate the overall performance of a model.Second, you are able to calculate multiple measurements of performance. This helps account for the natural variability that would exist when measuring the performance of your models.\n\n**9. vfold_cv()**\n\nYou can use the function vfold_cv() from the rsample package to build these cross validated pairs of train and validate data. The parameter v is used to indicate how many times the data should be split.This new data frame now brings you back to the list column workflow. In order to build a model for each fold you will need to first extract the training and validation data frames into their own list columns.\n\n**10. Mapping train &amp; validate**\n\nTo do this you will use map() to apply the training() and testing() functions. This creates the desired train and validate data frames for each fold. Notice that this is similar to what you've done with the initial split except now you're doing it for many splits.\n\n**11. Cross Validated Models**\n\nAnd you're back to building many models!Just like in the last chapter you can use each of the 3 train data frames to build corresponding models.\n\n**12. Let's practice!**\n\nNow, let's progress to the exercises and apply what you've learned.\n\n## The test-train split\n\nIn a disciplined machine learning workflow it is crucial to withhold a portion of your data (**testing data**) from any decision-making process. This allows you to independently assess the performance of your model when it is finalized. The remaining data, the **training data**, is used to build and select the best model.\n\nIn this exercise, you will use the `rsample` package to split your data to perform the initial train-test split of your `gapminder` data.\n\n**Note:** *Since this is a random split of the data it is good practice to set a seed before splitting it.*\n\n**Steps**\n\n1. Split your data into 75% training and 25% testing using the `initial_split()` function and assign it to `gap_split`.  \n2. Extract the training data frame from `gap_split` using the `training()` function. \n3. Extract the testing data frame from `gap_split` using the `testing()` function. \n4. Ensure that the dimensions of your new data frames are what you expected by using the `dim()` function on `training_data` and `testing_data`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Load package\nlibrary(rsample)\n\n# Prepare the initial split object\ngap_split <- initial_split(gapminder, prop = 0.75)\n\n# Extract the training data frame\ntraining_data <- training(gap_split)\n\n# Extract the testing data frame\ntesting_data <- testing(gap_split)\n\n# Calculate teh dimensions of both training_data and testing_data\ndim(training_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 3003    7\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(testing_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1001    7\n```\n\n\n:::\n:::\n\n\nGreat work! You have withheld a portion of your data for a final, unbiased, evaluation of your model. Throughout the rest of this chapter you will take the steps necessary to identify the best performing model using only the **training data**. At the end of the chapter you will select the best performing model and measure its performance using the **testing data** that you created here.\n\n## Cross-validation data frames\n\nNow that you have withheld a portion of your data as **testing data**, you can use the remaining portion to find the best performing model. \n\nIn this exercise, you will split the training data into a series of **5** train-validate sets using the `vfold_cv()` function from the `rsample` package.\n\n**Steps**\n\n1. Build a data frame for 5-fold cross validation from the `training_data` using `vfold_cv()` and assign it to `cv_split`.  \n2. Prepare `cv_data` by appending two new columns to `cv_split`: \\n`train`: containing the train data frames by mapping `training()` across the `splits` column. \\n`validate`: containing the validate data frames by using mapping `testing()` across the `splits` column.\n    \n    * `train`: containing the train data frames by mapping `training()` across the `splits` column. \n    * `validate`: containing the validate data frames by using mapping `testing()` across the `splits` column.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Prepare the data frame containing the cross validation partitions\ncv_split <- vfold_cv(training_data, v = 5)\n\ncv_data <- cv_split %>% \n  mutate(\n    # Extract the train data frame for each split\n    train = map(splits, ~training(.x)), \n    # Extract the validate data frame for each split\n    validate = map(splits, ~testing(.x)),\n    # Extract the recorded life expectancy for the records in the validate data frames\n    validate_actual = map(validate, ~.x$life_expectancy),\n  )\n\n# Use head() to preview cv_data\nhead(cv_data)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"splits\"],\"name\":[1],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"id\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"train\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"validate\"],\"name\":[4],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"validate_actual\"],\"name\":[5],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"<S3: vfold_split>\",\"2\":\"Fold1\",\"3\":\"<tibble[,7]>\",\"4\":\"<tibble[,7]>\",\"5\":\"<dbl [601]>\"},{\"1\":\"<S3: vfold_split>\",\"2\":\"Fold2\",\"3\":\"<tibble[,7]>\",\"4\":\"<tibble[,7]>\",\"5\":\"<dbl [601]>\"},{\"1\":\"<S3: vfold_split>\",\"2\":\"Fold3\",\"3\":\"<tibble[,7]>\",\"4\":\"<tibble[,7]>\",\"5\":\"<dbl [601]>\"},{\"1\":\"<S3: vfold_split>\",\"2\":\"Fold4\",\"3\":\"<tibble[,7]>\",\"4\":\"<tibble[,7]>\",\"5\":\"<dbl [600]>\"},{\"1\":\"<S3: vfold_split>\",\"2\":\"Fold5\",\"3\":\"<tibble[,7]>\",\"4\":\"<tibble[,7]>\",\"5\":\"<dbl [600]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nExcellent job! Now you're back to the same list column workflow you've used in the previous chapters. You will use this data frame throughout this chapter to measure and compare the performance of the models you create.\n\n## Measuring cross-validation performance\n\nTheory. Coming soon ...\n\n\n**1. Measuring cross-validation performance**\n\nNow that you've generated your cross-validated data frames and models, let's learn how to use the validation data to measure the performance of each model.\n\n**2. Measuring Performance**\n\nIn order to measure validate performance of your models you need to compare the actual values of life expectancy in the validate data frames to the ones generated using the prediction model. To do this you need to first prepare both sets of values.\n\n**3. Measuring Performance - Truth**\n\nFirst you need to isolate the actual values.\n\n**4. Measuring Performance - Truth**\n\n\n\n**5. Measuring Performance - Truth**\n\nI will refer to this vector of values as actual values.\n\n**6. Measuring Performance - Prediction**\n\nNext, you need to use the features of these observations\n\n**7. Measuring Performance - Prediction**\n\nalong with the model\n\n**8. Measuring Performance - Prediction**\n\nto generate a series of predictions for the validation data.\n\n**9. Measuring Performance**\n\nNow that you have both the predicted and actual values of life expectancy you can compare them directly. By measuring the differences between them you can assess the overall performance using your preferred metric.\n\n**10. Mean Absolute Error**\n\nThe metric I prefer is called the Mean Absolute Error or MAE. This metric captures the average magnitude by which the predictions differ from the actual values. The most appealing trait of this metric is that it has an intuitive interpretation. Using the MAE, you have an idea of how much, on average, your model's prediction will differ from reality.\n\n**11. Ingredients for Performance Measurement**\n\nTo summarize, you need three ingredients to measure performance:The actual life expectancy values, the predicted life expectancy values and a metric to compare the two.Now let's learn how to do this in R.\n\n**12. 1) Extract the actual values**\n\nTo extract the actual values of life expectancy from the validate data frames you can use the map() function. Here the dot x refers to each validate data frame so you can use the dollar operator to access the life expectancy column vector.\n\n**13. The predict() &amp; map2() functions**\n\nIn order to generate the predicted values you need to use the predict() function. This function requires two inputs, the model and the data to predict on. You now need to expand your collection of map tools to include the map2() function. This is very similar to the map() function you've learned in chapter 1 except that you can use two input columns. The syntax is very similar except you now use dot x and dot y as your first two parameters and you refer to these placeholders in the formula in the same way.\n\n**14. 2) Prepare the predicted values**\n\nAs before you can use this map2() function inside mutate() to append a column of predictions for each cross validation fold.\n\n**15. 3) Calculate MAE**\n\nNow that you have the actual and predicted values for each cross validation fold you can compare them by using the mae() function from the Metrics package. Again, you can use a map2() variant. Since you know that the result will be a double vector you can directly use the map2_double() function to ensure that the value is returned as a vector instead of a list. And this is how you can measure the performance for each cross validation fold.\n\n**16. Let's practice!**\n\nNow it's your turn to calculate the performance of your cross-validated linear regression models.\n\n## Build cross-validated models\n\nIn this exercise, you will build a linear model predicting `life_expectancy` using all available features. You will do this for the train data of each cross-validation fold.\n\n**Steps**\n\n1. Build models predicting `life_expectancy` using all available features with the `train` data for each fold of the cross validation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build a model using the train data for each fold of the cross validation\ncv_models_lm <- cv_data %>% \n  mutate(model = map(train, ~lm(formula = life_expectancy~., data = .x)))\n```\n:::\n\n\nYou're doing great! Now that you have the models built, let's prepare the parts we need to evaluate their performance.\n\n## Preparing for evaluation\n\nIn order to measure the **validate** performance of your models you need compare the predicted values of `life_expectancy` for the observations from validate set to the actual values recorded. Here you will prepare both of these vectors for each partition.\n\n**Steps**\n\n<!-- 1. Extract the actual `life_expectancy` from the validate data frames and store these in the column `validate_actual`.  -->\n2. Predict the `life_expectancy` for each validate partition using the `map2()` and `predict()` functions in the column `validate_predicted`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_prep_lm <- cv_models_lm %>% \n  mutate(\n    # Extract the recorded life expectancy for the records in the validate data frames\n    # validate_actual = map(validate, ~.x$life_expectancy),\n    # Predict life expectancy for each validate set using its corresponding model\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))\n  )\n```\n:::\n\n\nGreat work! In the next exercise you will compare `validate_actual` to `validate_predicted` to measure the performance of all 5 models.\n\n## Evaluate model performance\n\nNow that you have both the **actual** and **predicted** values of each fold you can compare them to measure performance. \n\nFor this regression model, you will measure the **Mean Absolute Error (MAE)** between these two vectors. This value tells you the average difference between the **actual** and **predicted** values.\n\n**Steps**\n\n1. Calculate the MAE by comparing the actual with the predicted values for the validate data and assign it to the `validate_mae` column.  \n2. Print the `validate_mae` column (note how they vary). \n3. Calculate the mean of this column.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Metrics)\n# Calculate the mean absolute error for each validate fold       \ncv_eval_lm <- cv_prep_lm %>% \n  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))\n\n# Print the validate_mae column\ncv_eval_lm$validate_mae\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.564897 1.481837 1.395386 1.589818 1.460490\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the mean of validate_mae column\nmean(cv_eval_lm$validate_mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.498485\n```\n\n\n:::\n:::\n\n\nExcellent! You now know that based on 5 train-validate splits, the predictions of the models are on average off by 1.5 years. Can we improve this performance by using a more complex model? Let's find out!\n\n## Building and tuning a random forest model\n\nTheory. Coming soon ...\n\n\n**1. Building and tuning a random forest model**\n\nLet's briefly review what you've done so far to evaluate the cross validation performance of the regression model.\n\n**2. Cross Validation Performance**\n\nUsing cross validation, you split the training data into multiple train-validate pairs.\n\n**3. Cross Validation Performance**\n\nThe train section for each of these cross validation folds was used to build a corresponding model.\n\n**4. Cross Validation Performance**\n\nWhich was then used alongside with the held out validate sets.\n\n**5. Cross Validation Performance**\n\nTo calculate the mean absolute error for each cross validation fold.\n\n**6. Linear Regression Model**\n\nOnce you've taken the average mae across the cross validation folds you've measured the performance of the model on held out data. For your linear regression model, the mean absolute error is 1.5 years, meaning that you can expect the model predictions will be off, on average, by 1.5 years.Is this the best model that we can build?\n\n**7. Another Model**\n\nYou can determine this by repeating these steps with a different model. Because the same data will be used across the models you can directly compare their validation performance between them, allowing you to select the best performing model.You can use this machine learning workflow to compare virtually any model. So let's try this out with a random forest model to see if it achieves a higher performance.\n\n**8. Random Forest Benefits**\n\nThe random forest is a very popular model in the machine learning community. The details of how this algorithm works are outside the scope of this course but can be found in other great datacamp courses on machine learning. In chapter 2, we've learned that there might be a non-linear relationship between the gapminder features and life expectancy. Also we know that the country feature had a direct relationship with other features.The random forest models natively handle both non-linear relationships and feature interactions so we can be optimistic about trying this model.\n\n**9. Basic Random Forest Tools**\n\nYou will use the random forest implementation from the ranger package. To build the random forest model with default hyperparameters you use the following syntax. You need to provide the formula and data just like the regression model. Because a random forest has a random element I recommend using the seed argument to ensure that your results are reproducible.The syntax for preparing the prediction values for new data is also similar to that of a linear model. The only difference is that you need to use the dollar sign to explicitly extract the prediction vector from the ranger prediction object.\n\n**10. Build Basic Random Forest Models**\n\nYou can apply this as before by mapping the train data to build the models for each fold.Then use map2() to generate the predictions for each fold.\n\n**11. ranger Hyper-Parameters**\n\nYou can further improve a model by fine tuning its hyper parameters. Ranger has two parameters that can be tuned, mtry and num.trees.We will focus on tuning the mtry parameter which can range from one to the total number of features available.\n\n**12. Tune The Hyper-Parameters**\n\nTo tune the parameters in a tidyverse fashion you can leverage the crossing() function to expand the cross validation data frame for each value of the hyper parameter you're interesting in trying.\n\n**13. Tune The Hyper-Parameters**\n\nThen you can use map2() to iterate over all the folds and the mtry parameter to build the new ranger models for each fold-mtry combintation. You can then proceed as usual to calculate the mean absolute error for each combination to determine which parameterized model has the best validation performance.\n\n**14. Let's practice!**\n\nNow let's use what you've learned up until now to see if the random forest model will provide better validation performance than the linear regression model.\n\n## Build a random forest model\n\nHere you will use the same cross-validation data to build (using `train`) and evaluate (using `validate`) random forests for each partition. Since you are using the same cross-validation partitions as your regression models, you are able to directly compare the performance of the two models.\n\n**Note:** *We will limit our random forests to contain 100 trees to ensure they finish fitting in a reasonable time. The default number of trees for `ranger()` is 500.*\n\n**Steps**\n\n1. Use `ranger()` to build a random forest predicting `life_expectancy` using all features in `train` for each cross validation partition. \n<!-- 2. Extract the actual `life_expectancy` from the validate data frames and store these in the column `validate_actual`.  -->\n3. Add a new column `validate_predicted` predicting the `life_expectancy` for the observations in `validate` using the random forest models you just created.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ranger)\n\n# Build a random forest model for each fold\ncv_models_rf <- cv_data %>% \n  mutate(model = map(train, ~ranger(formula = life_expectancy~., data = .x,\n                                    num.trees = 100, seed = 42)))\n                                    \n# Generate predictions using the random forest model\ncv_prep_rf <- cv_models_rf %>% \n  mutate(\n    # Extract the recorded life expectancy for the records in the validate data frames\n    # validate_actual    = map(validate, ~.x$life_expectancy),\n    # Predict life expectancy for each validate set using its corresponding model\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions)\n  )\n```\n:::\n\n\nGreat job! In the next exercise, you will evaluate the predictions from this model.\n\n## Evaluate a random forest model\n\nSimilar to the linear regression model, you will use the **MAE** metric to evaluate the performance of the random forest model.\n\n**Steps**\n\n1. Calculate the MAE by comparing the actual with the predicted values for the validate data and assign it to the `validate_mae` column.  \n2. Print the `validate_mae` column (note how they vary). \n3. Calculate the mean of this column. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ranger)\n\n# Calculate validate MAE for each fold\ncv_eval_rf <- cv_prep_rf %>% \n  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))\n\n# Print the validate_mae column\ncv_eval_rf$validate_mae\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.8694553 0.7990644 0.7832452 0.8853153 0.7828388\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the mean of validate_mae column\nmean(cv_eval_rf$validate_mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.8239838\n```\n\n\n:::\n:::\n\n\nImpressive! You've dropped the average error of your predictions from 1.5 to 0.83. That's quite an improvement! In the next exercise you'll see if you can squeeze a bit more performance out by tuning a parameter of the random forest model.\n\n## Fine tune your model\n\nWow! That was a significant improvement over a regression model. Now let's see if you can further improve this performance by fine tuning your random forest models. To do this you will vary the `mtry` parameter when building your random forest models on your `train` data.\n\nThe default value of `mtry` for ranger is the rounded down square root of the total number of features (6). This results in a value of **2**.\n\n**Steps**\n\n1. Use `crossing()` to expand the cross validation data for values of `mtry` ranging from **2** through **5**.\n2. Build random forest models for each fold/mtry combination.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prepare for tuning your cross validation folds by varying mtry\ncv_tune <- cv_data %>% \n  crossing(mtry = 2:5) \n\n# Build a model for each fold & mtry combination\ncv_model_tunerf <- cv_tune %>% \n  mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = life_expectancy~., \n                                              data = .x, mtry = .y, \n                                              num.trees = 100, seed = 42)))\n```\n:::\n\n\nGreat work! You've built a model for each fold/mtry combination. Next, you'll measure the performance of each to find the best performing value of `mtry`.\n\n## The best performing parameter\n\nYou've now built models where you've varied the random forest-specific hyperparameter `mtry` in the hopes of improving your model further. Now you will measure the performance of each `mtry` value across the 5 cross validation partitions to see if you can improve the model.\n\nRemember that the validate MAE you calculated two exercises ago of `0.795` was for the default `mtry` value of **2**.\n\n**Steps**\n\n1. Generate predictions for each mtry/fold combination. \n2. Calculate the **MAE** for each mtry/fold combination. \n3. Calculate the mean **MAE** for each value of `mtry`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generate validate predictions for each model\ncv_prep_tunerf <- cv_model_tunerf %>% \n  mutate(validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions))\n\n# Calculate validate MAE for each fold and mtry combination\ncv_eval_tunerf <- cv_prep_tunerf %>% \n  mutate(validate_mae = map2_dbl(.x = validate_actual, .y = validate_predicted, ~mae(actual = .x, predicted = .y)))\n\n# Calculate the mean validate_mae for each mtry used  \ncv_eval_tunerf %>% \n  group_by(mtry) %>% \n  summarise(mean_mae = mean(validate_mae))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"mean_mae\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"0.8239838\"},{\"1\":\"3\",\"2\":\"0.8186528\"},{\"1\":\"4\",\"2\":\"0.8123850\"},{\"1\":\"5\",\"2\":\"0.8203733\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nExcellent job! Looks like parameter tuning was able to eke out another slight boost in performance, dropping the mae from 0.831 (mtry = 2) to 0.816 (mtry = 4). Assuming that you've finished your model selection you can conclude that your final (best performing) model will be the random forest model built using `ranger` with an `mtry = 4` and `num.trees = 100`. In the next exercise you will build this model using all training data and evaluate its expected future performance using the testing data.\n\n## Measuring the test performance\n\nTheory. Coming soon ...\n\n\n**1. Measuring the Test Performance**\n\nThroughout this chapter you've worked with a classic machine learning workflow.\n\n**2. Machine Learning Workflow**\n\nThe first step of this workflow was to split your data into two sections, train and test.\n\n**3. Machine Learning Workflow**\n\nThe test portion was intentionally held out in order to evaluate the final model with an independent set of data.\n\n**4. Machine Learning Workflow**\n\nThe train portion of the data was further split into iterative sections of train and validate using cross validation for the purpose of model selection.\n\n**5. Machine Learning Workflow**\n\nEach train portion was used to build a model and the held out validate portion was used to evaluate it.\n\n**6. Machine Learning Workflow**\n\nResulting in measures of validation performance for each cross validation fold for each model and hyperparameter. Aggregating the validation performance for each model allowed us to compare multiple models as well as their respective hyper parameters\n\n**7. Machine Learning Workflow**\n\nin order to select the model-hyperparameter combination with the best overall performance. For the gapminder dataset, the best performing model was the random forest model with a hyperparameter mtry of 4.\n\n**8. Machine Learning Workflow**\n\nThis brings us to the final section of this workflow, building and evaluating our final model.Now, you will use all of the train data prepared during the initial split to build the random forest model. This is the final model and is the one you would expect to use in a production environment.\n\n**9. Machine Learning Workflow**\n\nAs such you would like to know how well you can expect this model will perform on new data. To do this you bring back the test data that was intentionally ignored thus far and treat it as the desired new data for evaluation. By comparing the actual values of life expectancy for the test set with the values predicted using the final model you can estimate the model's performance on new data. This is known as the model's test performance.\n\n**10. Measuring the Test Performance**\n\nTo perform these steps in R you first build the best performing model, which in this case was the random forest model built using ranger with an mtry value of 2 and 100 trees.Next, you prepare the actual and predicted values for comparison.Finally, you need to compare the actual and predicted values using a desired metric, in this case the mean absolute error.\n\n**11. Let's practice!**\n\nLet's see how well the final model performed.\n\n## Build & evaluate the best model\n\nUsing cross-validation you were able to identify the best model for predicting `life_expectancy` using all the features in `gapminder`. Now that you've selected your model, you can use the independent set of data (`testing_data`) that you've held out to estimate the performance of this model on new data.\n\nYou will build this model using all `training_data` and evaluate using `testing_data`.\n\n**Steps**\n\n1. Use `ranger()` to build the best performing model (mtry = 4) using all of the training data. Assign this to `best_model`. \n2. Extract the `life_expectancy` column from `testing_data` and assign it to `test_actual`. \n3. Predict `life_expectancy` using the `best_model` on the `testing` data and assign it to `test_predicted`. \n4. Calculate the MAE using `test_actual` and `test_predicted` vectors.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build the model using all training data and the best performing parameter\nbest_model <- ranger(formula = life_expectancy~., data = training_data,\n                     mtry = 4, num.trees = 100, seed = 42)\n\n# Prepare the test_actual vector\ntest_actual <- testing_data$life_expectancy\n\n# Predict life_expectancy for the testing_data\ntest_predicted <- predict(best_model, testing_data)$predictions\n\n# Calculate the test MAE\nmae(test_actual, test_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.6679158\n```\n\n\n:::\n:::\n\n\nFantastic work! You have successfully leveraged the list column workflow to identify and build a model to predict life expectancy. You can claim that based on the test holdout you can expect that your predictions on new data will only be off by a magnitude of 0.663 years.\")\\n\\n\\nex() %>% {\\n  check_function(., \"ranger\") %>% {\\n    check_arg(., \"formula\") %>% check_equal()\\n    check_arg(., \"data\") %>% check_equal()\\n    check_arg(., \"mtry\") %>% check_equal()\\n    check_arg(., \"num.trees\") %>% check_equal()\\n    check_arg(., \"seed\") %>% check_equal()\\n  }\\n  check_object(., \"best_model\")\\n  check_object(., \"test_actual\") %>% check_equal()\\n  check_object(., \"test_predicted\") %>% check_equal(incorrect_msg = \"Did you correctly predict `life_expectancy` using the `best_model` on `testing` data and assign it to `test_predicted`?\", append = FALSE)\\n}\\n\\nex() %>% check_correct({\\n  check_output_expr(., \"mae(test_actual, test_predicted)\", missing_msg = \"Did you correctly calculate the test MAE?\")\\n}, {\\n  check_function(., \"mae\") %>% {\\n    check_arg(., \"actual\") %>% check_equal()\\n    check_arg(., \"predicted\n\n# 4. Build, Tune & Evaluate Classification Models\n\nIn this chapter you will shift gears to build, tune and evaluate classification models.\n\n## Logistic regression models\n\nTheory. Coming soon ...\n\n\n**1. Logistic Regression Models**\n\nWelcome to the final chapter of machine learning in the tidyverse. Throughout this course you have learned a variety of tidyverse tools aimed at building regression models.In this chapter you will shift gears to work with another group of models called binary classification models.\n\n**2. Binary Classification**\n\nBinary classification models are among the most common class of models used by data scientists. These models are trained to assign an observation to one of two possible classes using the available set of features.\n\n**3. The attrition Dataset**\n\nTo learn the tools and skills associated with these models you will explore the attrition dataset.This dataset contains over 1400 observations of employees at a fictional company. Each observation provides a variety of features about the employee such as education, income, work-life balance, and job satisfaction. The outcome variable that you are interested in this data is called Attrition, this indicates whether the employee has left the company or not. Throughout this chapter you will work on building a model that will use the available features to predict if an employee has quit.In a real world scenario a model like this can be used by a company to identify employees that are at risk and potentially intervene.\n\n**4. Logistic Regression**\n\nThe first model that you will work with is the logistic regression model. This is very similar to a linear model except that for a given observation it returns the probability of that observation belonging to the positive class. Here, this would be the probability of attrition.In order to build a logistic regression model in R you will use the generalized linear model function, glm(). Similar to the lm() function you need to provide the formula and the data but now you have a new parameter called family which must be set to binomial for a logistic regression model.\n\n**5. glm()**\n\nWorking with the cross-validated data frame, cv_data, you will build a logistic regression model for each fold.As before, you can leverage the mutate() and map() combination to map the glm() function for each train data frame.\n\n**6. Time to Practice**\n\nIn the next set of exercises you will apply what you've learned in order to prepare the attrition dataset for the train-test-validate splits and build a logistic regression model for each fold.\n\n## Prepare train-test-validate parts\n\nIn this exercise, you will leverage the tools that you have learned thus far to build a classification model to predict employee attrition.\n\nYou will work with the `attrition` dataset, which contains 30 features about employees which you will use to predict if they have left the company.\n\nYou will first prepare the training &amp; testing data sets, then you will further split the training data using cross-validation so that you can search for the best performing model for this task.\n\n**Steps**\n\n1. Split your data into 75% training and 25% testing using the `initial_split()` function.\n2. Extract the training and testing data frames from `data_split` using `training()` and `testing()`, respectively.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Load data\nattrition <- readRDS(\"data/attrition.rds\")\n\n# Prepare the initial split object\ndata_split    <- initial_split(attrition, prop = 0.75)\n\n# Extract the training data frame\ntraining_data <- training(data_split)\n\n# Extract the testing data frame\ntesting_data  <- testing(data_split)\n```\n:::\n\n\n3. Build a data frame for 5-fold cross validation from the `training_data` using `vfold_cv()`.\n4. Prepare the `cv_data` data frame by extracting the train and validate data frames for each fold.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\ncv_split <- vfold_cv(training_data, v = 5)\n\ncv_data <- cv_split %>% \n  mutate(\n    # Extract the train data frame for each split\n    train = map(splits, ~training(.x)),\n    # Extract the validate data frame for each split\n    validate = map(splits, ~testing(.x))\n  )\n```\n:::\n\n\nGreat work!! Now you have the parts necessary to build & tune your classification models.\n\n## Build cross-validated models\n\nIn this exercise, you will build logistic regression models for each fold in your cross-validation. \n\nYou will build this using the `glm()` function and by setting the family argument to `\"binomial\"`.\n\n**Steps**\n\n1. Build models predicting `Attrition` using all available features with the `train` data for each fold of the cross validation.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build a model using the train data for each fold of the cross validation\ncv_models_lr <- cv_data %>% \n  mutate(model = map(train, ~glm(formula = Attrition~., \n                                 data = .x, family = \"binomial\")))\n```\n:::\n\n\nExcellent work! Now let's learn how to evaluate these models.\n\n## Evaluating classification models\n\nTheory. Coming soon ...\n\n**1. Evaluating Classification Models**\n\nNow that you've prepared the train-test-validate splits and built your logistic regression models you need to learn how to evaluate their performance.\n\n**2. Ingredients for Performance Measurement**\n\nThe ingredients needed to measure performance are the same as before. First you need the actual classes of your observations. Second you will need the predicted classes of these observations. Finally you will need a metric relevant to your problem to compare the two and measure performance.\n\n**3. 1) Prepare Actual Classes**\n\nTo prepare the vector of actual classes you need to convert the attrition vector from character to a binary. If you look at one validate data frame from the cross validation folds you can see that the values are all either Yes or No. To convert these to a binary vector you simply need to use the equal to operator to convert all Yes values to TRUE and No values to FALSE.\n\n**4. 2) Prepare Predicted Classes**\n\nTo prepare the predicted classes you first need to prepare the probability vector. To do this for a logistic regression model you will use the predict() function with the argument type equal to response. This generates predicted probability of attrition for each observation. Next, you will need to convert these probability values into a binary vector. Here you can assume that any probability greater than 0.5 will correspond to TRUE and any less than or equal to 0.5 will correspond to FALSE.\n\n**5. 3) A metric to compare 1) & 2)**\n\nNow that you have the actual and predicted binary vectors you can think about what metric is appropriate for the problem you are trying to solve. Here I will introduce you to three popular metrics, accuracy, precision and recall, all three of which are available in the Metrics package you've previously used. To understand these metrics let's start with the contigency table that compares the actual and predicted values. In R we can generate this using the table() function.\n\n**6. 3) Metric: Accuracy**\n\nThe first metric we will consider is Accuracy. Accuracy measures how well your model predicted both the TRUE and FALSE classes. This metric can be useful if it is equally important for you to predict employees that quit and those that don't. You can calculate accuracy by using the function of the same name from the Metrics package. Here you have an accuracy of 90% which, when looking at the contingency table, you can see is primarily driven by the model's ability to correctly classify cases where attrition is FALSE.\n\n**7. 3) Metric: Precision**\n\nThe next metric we will consider is precision. This metric calculates how often the model is correct at predicting the TRUE class. You calculate it using the precision() function. The resulting value tells us that of the employees the model classified as having quit, 78% of them did indeed leave the company. This metric can be appropriate when you want to minimize how often the model incorrectly predicts an observation to be in the positive class.\n\n**8. 3) Metric: Recall**\n\nFinally, there is recall. This metric compares the number of observations the model has correctly identified as TRUE to the total number of TRUE observations. In other words, it measures the rate at which the model can capture the TRUE class. If you are interested in building a model that would capture as many risky employees as possible you should consider this metric. You can calculate it using the recall() function. The resulting value tells us that of the employees that quit, the model was able to capture 51% of them correctly. For the attrition model let's assume that you need to identify as many employees that are at risk of leaving, as such the best performing model will be selected using the recall metric.\n\n**9. Let's practice!**\n\nNow you're ready to evaluate your classification models. \n\n## Predictions of a single model\n\nTo calculate the performance of a classification model you need to compare the actual values of `Attrition` to those predicted by the model. \nWhen calculating metrics for binary classification tasks (such as precision and recall), the actual and predicted vectors must be converted to **binary** values.\n\nIn this exercise, you will learn how to prepare these vectors using the model and validate data frames from the first cross-validation fold as an example.\n\n**Steps**\n\n1. Extract the `model` and the `validate` data frame from the first fold of the cross-validation. \n2. Extract the `Attrition` column from the `validate` data frame and convert the values to binary (TRUE/FALSE). \n3. Use `model` to predict the probabilities of attrition for the `validate` data frame. \n4. Convert the predicted probabilities to a binary vector, assume all probabilities greater than `0.5` are TRUE.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract the first model and validate \nmodel <- cv_models_lr$model[[1]]\nvalidate <- cv_models_lr$validate[[1]]\n\n# Prepare binary vector of actual Attrition values in validate\nvalidate_actual <- validate$Attrition == \"Yes\"\n\n# Predict the probabilities for the observations in validate\nvalidate_prob <- predict(model, validate, type = \"response\")\n\n# Prepare binary vector of predicted Attrition values for validate\nvalidate_predicted <- validate_prob > 0.5\n```\n:::\n\n\nFantastic! Now you have the actual and predicted vectors. In the next exercise you'll use these vectors to calculate some metrics to check the performance of the model.\n\n## Performance of a single model\n\nNow that you have the binary vectors for the actual and predicted values of the model, you can calculate many commonly used binary classification metrics. In this exercise you will focus on:\n\n* **accuracy:** rate of correctly predicted values relative to all predictions.\n* **precision:** portion of predictions that the model correctly predicted as TRUE.\n* **recall:** portion of actual TRUE values that the model correctly recovered.\n\n**Steps**\n\n1. Use `table()` to compare the `validate_actual` and `validate_predicted` values for the example model and validate data frame. \n2. Calculate the accuracy.  \n3. Calculate the precision.  \n4. Calculate the recall.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(Metrics)\n\n# Compare the actual & predicted performance visually using a table\ntable(validate_actual, validate_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                validate_predicted\n#> validate_actual FALSE TRUE\n#>           FALSE   187    5\n#>           TRUE     14   15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the accuracy\naccuracy(validate_actual, validate_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.9140271\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the precision\nprecision(validate_actual, validate_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.75\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the recall\nrecall(validate_actual, validate_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.5172414\n```\n\n\n:::\n:::\n\n\nGreat work! The type of metric you use should be informed by the application of your model. In the next exercise you will expand on this example to calculate the recall metric for each of your cross validation folds.\n\n## Prepare for cross-validated performance\n\nNow that you know how to calculate the performance metrics for a single model, you are now ready to expand this for all the folds in the cross-validation data frame.\n\n**Steps**\n\n1. Add the `validate_actual` binary column for each cross-validation fold by converting all `\"Yes\"` values to `TRUE`. \n2. Use `model` to predict the probabilities of attrition for each cross-validation fold of `validate`. Convert the predicted probabilities to a binary vector, treating all probabilities greater than **0.5** as TRUE. Name this column `validate_predicted`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_prep_lr <- cv_models_lr %>% \n  mutate(\n    # Prepare binary vector of actual Attrition values in validate\n    validate_actual = map(validate, ~.x$Attrition == \"Yes\"),\n    # Prepare binary vector of predicted Attrition values for validate\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = \"response\") > 0.5)\n  )\n```\n:::\n\n\nGreat work! Next, you'll calculate the recall of these cross validated models.\n\n## Calculate cross-validated performance\n\nIt is crucial to optimize models using a carefully selected metric aimed at achieving the goal of the model. \n\nImagine that in this case you want to use this model to identify employees that are predicted to leave the company. Ideally, you want a model that can capture as many of the ready-to-leave employees as possible so that you can intervene. The corresponding metric that captures this is the **recall** metric. As such, you will exclusively use **recall** to optimize and select your models.\n\n**Steps**\n\n1. Calculate the recall by comparing the actual with the predicted responses for each fold and assign it to the `validate_recall` column. \n2. Print the `validate_recall` column.  \n3. Print the mean of this column.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the validate recall for each cross validation fold\ncv_perf_recall <- cv_prep_lr %>% \n  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, \n                                    ~recall(actual = .x, predicted = .y)))\n\n# Print the validate_recall column\ncv_perf_recall$validate_recall\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.5172414 0.4705882 0.4242424 0.4722222 0.4222222\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the average of the validate_recall column\nmean(cv_perf_recall$validate_recall)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.4613033\n```\n\n\n:::\n:::\n\n\nExcellent! As you can see the validate recall of the model is 0.46, can you beat this using a more complex model. In the next series of exercises you will find out.\n\n## Random forest for classification\n\nTheory. Coming soon ...\n\n\n**1. Classification With Random Forests**\n\nYou've successfully calculated the average cross validation performance for logistic regression. Now let's try the random forest model to see if it improves the prediction performance.\n\n**2. ranger() for Classification**\n\nTuning and building the random forest models is the same as before. The only changes you need to think about is the values of mtry to tune. Since there are 30 features in the attrition dataset, this value can  go as high as 30. For now we will try out a few mtry values.\n\n**3. 1) Prepare Actual Classes**\n\nTo evaluate the random forest model, you use the same framework of comparing the actual and predicted classes. Preparing the actual values is the same as before. You simply convert the Yes and No to TRUE and FALSE, respectively.\n\n**4. 2) Prepare Predicted Classes**\n\nTo generate the predicted values for a ranger model you need to first use the predict() function as shown here. By default, ranger outputs the character class, in this case Yes or No. To calculate the performance you simply need to convert this to a binary vector like so.\n\n**5. Build the Best Attrition Model**\n\nNow you have all of the tools that you need to calculate the validation recall of your random forest models. After building and evaluating these models you can compare their performance to the logistic regression model in order to select the best performing model.This will allow you to prepare your final model and calculate its test performance metrics.\n\n## Tune random forest models\n\nNow that you have a working logistic regression model you will prepare a random forest model to compare it with.\n\n**Steps**\n\n1. Use `crossing()` to expand the cross-validation data for values of `mtry` using the values of 2, 4, 8, and 16.\n2. Build random forest models for each fold/mtry combination.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prepare for tuning your cross validation folds by varying mtry\ncv_tune <- cv_data %>%\n  crossing(mtry = c(2, 4, 8, 16)) \n\n# Build a cross validation model for each fold & mtry combination\ncv_models_rf <- cv_tune %>% \n  mutate(model = map2(train, mtry, ~ranger(formula = Attrition~., \n                                           data = .x, mtry = .y,\n                                           num.trees = 100, seed = 42)))\n```\n:::\n\n\nFantastic work! Next you will evaluate the validation performance of these random forest models.\n\n## Random forest performance\n\nIt is now time to see whether the random forests models you built in the previous exercise are able to outperform the logistic regression model.\n\nRemember that the validate **recall** for the logistic regression model was 0.43.\n\n**Steps**\n\n1. Prepare the `validate_actual` and `validate_predicted` columns for each mtry/fold combination.\n2. Calculate the **recall** for each mtry/fold combination.\n3. Calculate the mean **recall** for each value of `mtry`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_prep_rf <- cv_models_rf %>% \n  mutate(\n    # Prepare binary vector of actual Attrition values in validate\n    validate_actual = map(validate, ~.x$Attrition == \"Yes\"),\n    # Prepare binary vector of predicted Attrition values for validate\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = \"response\")$predictions == \"Yes\")\n  )\n\n# Calculate the validate recall for each cross validation fold\ncv_perf_recall <- cv_prep_rf %>% \n  mutate(recall = map2_dbl(.x = validate_actual, .y = validate_predicted, ~recall(actual = .x, predicted = .y)))\n\n# Calculate the mean recall for each mtry used  \ncv_perf_recall %>% \n  group_by(mtry) %>% \n  summarise(mean_recall = mean(recall))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean_recall\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"0.08879013\"},{\"1\":\"4\",\"2\":\"0.12138669\"},{\"1\":\"8\",\"2\":\"0.18494806\"},{\"1\":\"16\",\"2\":\"0.20757432\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nGreat work! This time you can see that none of the random forest models were able to outperform the logistic regression model with respect to recall.\n\n## Build final classification model\n\nComparing the **recall** performance between the logistic regression model (0.4) and the best performing random forest model (0.2), you've learned that the model with the best performance is the logistic regression model. In this exercise, you will build the logistic regression model using all of the **train** data and you will prepare the necessary vectors for evaluating this model's **test** performance.\n\n**Steps**\n\n1. Build a logistic regression model predicting `Attrition` using all available features in the `training_data`. \n2. Prepare the binary vector of actual test values, `test_actual`.  \n3. Prepare the binary vector of predicted values where a probability greater than 0.5 indicates `TRUE` and store this as `test_predicted`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Build the logistic regression model using all training data\nbest_model <- glm(formula = Attrition~., \n                  data = training_data, family = \"binomial\")\n\n# Prepare binary vector of actual Attrition values for testing_data\ntest_actual <- testing_data$Attrition == \"Yes\"\n\n# Prepare binary vector of predicted Attrition values for testing_data\ntest_predicted <- predict(best_model, testing_data, type = \"response\") > 0.5\n```\n:::\n\n\nAlmost at the finish line. You've now selected & built your best performing model and have prepared the necessary parts to evaluate its performance.\n\n## Measure final model performance\n\nNow its time to calculate the **test performance** of your final model (logistic regression). Here you will use the held out **testing** data to characterize the performance you would expect from this model when it is applied to new data.\n\n**Steps**\n\n1. Use `table()` to compare the `test_actual` and `test_predicted` vectors.  \n2. Calculate the test accuracy.\n3. Calculate the test precision.\n4. Calculate the test recall.\n5. After this exercise, you are done with the course! If you enjoyed the material, feel free to send Dmitriy a thank you via Twitter. He'll appreciate it. Tweet to Dmitriy\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compare the actual & predicted performance visually using a table\ntable(test_actual, test_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            test_predicted\n#> test_actual FALSE TRUE\n#>       FALSE   301    7\n#>       TRUE     33   27\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the test accuracy\naccuracy(test_actual, test_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.8913043\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the test precision\nprecision(test_actual, test_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.7941176\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate the test recall\nrecall(test_actual, test_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.45\n```\n\n\n:::\n:::\n\n\nWell done! You now have a model that you can expect to identify 45% of employees that are at risk to leave the organization.\n\n## Wrap-up\n\nTheory. Coming soon ...\n\n\n**1. Recap: Machine Learning in the Tidyverse**\n\nWell done! You've reached the final video of this course. Let's briefly review what you've learned.\n\n**2. Chapter 1 - The List Column Workflow**\n\nIn chapter one you learned how to use the list column workflow. This workflow is the backbone of working with models in the tidyverse.\n\n**3. Chapter 2 - Explore Multiple Models With broom**\n\nIn chapter 2 you leveraged this workflow to build models for each country in the gapminder dataset. You then learned about the various attributes of these models using the tidy(), glance() and augment() functions from the broom package.\n\n**4. Chapter 3 - Build, Tune &amp; Evaluate Regression Models**\n\nIn chapter 3 you learned about the train-validate-test approach and how it can be used to select and evaluate models. This introduced you to the functions from the rsample, Metrics and ranger packages.\n\n**5. Chapter 4 - Build, Tune &amp; Evaluate Classification Models**\n\nFinally, in chapter 4 you learned how to apply the list column workflow to build, tune and evaluate classification models.\n\n**6. Congratulations!**\n\nThank you for the time that you have dedicated to this course. I find these methods and tools to be indispensable for my work as a data scientist and I hope that you will gain the same value for your work.It has been a pleasure to work with you and I wish you the best of luck on your journey of learning.",
    "supporting": [
      "04_machine_learning_in_the_tidyverse_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}