{
  "hash": "d7436f12a8689e4cb622906d997080e1",
  "result": {
    "markdown": "---\ntitle: \"Factor Analysis in R\"\nauthor: \"Joschka Schwarz\"\n---\n\n\n\n\n**Short Description**\n\nExplore latent variables, such as personality, using exploratory and confirmatory factor analyses.\n\n**SEO Description**\n\nStart this four-hour course today to discover exploratory factor analysis and confirmatory factor analysis in R to explore latent variables such as personality.\n\n**Long Description**\n\nThe world is full of unobservable variables that can't be directly measured. You might be interested in a construct such as math ability, personality traits, or workplace climate. When investigating constructs like these, it's critically important to have a model that matches your theories and data. This course will help you understand dimensionality and show you how to conduct exploratory and confirmatory factor analyses. With these statistical techniques in your toolkit, you'll be able to develop, refine, and share your measures. These analyses are foundational for diverse fields including psychology, education, political science, economics, and linguistics.\n\n# 1. Evaluating your measure with factor analysis\n\nIn Chapter 1, you will learn how to conduct an EFA to examine the statistical properties of a measure designed around one construct.\n\n## Introduction to Exploratory Factor Analysis (EFA)\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Introduction to EFA**\n\nHi, I’m Jennifer Brussow, and I'm a psychometrician - which is a fancy way to say I work in the field of measurement.\n\n**2. Psycho + metrics**\n\nPsychometrics is the study of unobservable variables - \"psycho\" meaning \"of the mind\" and \"metrics\" meaning \"related to measurement.\" Researchers develop measures to capture unobservable variables, such as personality or IQ, and factor analysis is a valuable tool for use both during and after the development process. In Chapter 1, you'll learn how to examine the statistical properties of a measure designed around one construct.\n\n**3. Learning objectives**\n\nIn our first lesson, you'll start by running a unidimensional exploratory factor analysis, or EFA, on examinees' responses to questions, which we'll refer to as items. Next, you'll look at the results of the EFA to examine two key pieces of output. Items' factor loadings quantify their relationship to the underlying factor, which tells you how well each item is performing. Individuals' factor scores provide an estimate of the amount of the underlying factor each examinee possesses, which helps assign scores to examinees.\n\n**4. Factor Analysis' relationship to other analyses**\n\nYou may be familiar with some related ways of analyzing data. Factor analysis can be thought of as midway between classical test theory and structural equation modeling. Whereas classical test theory reports scores as the unweighted sum of item scores, factor analysis assigns item weights according to the correlation matrix. These correlations allow us to infer the presence of a latent variable or variables. Structural equation modeling extends this approach to model the relationships between latent variables.\n\n**5. Types of Factor Analysis**\n\nIt's important to note that there are two different types of factor analysis: exploratory and confirmatory. Exploratory factor analysis is used during measure development to explore factor structure and determine which items do a good job of measuring the construct. Confirmatory factor analysis is used to validate a measure after development.\n\n**6. Package**\n\nThis course primarily uses the psych package, which was developed by William Revelle. You can load the package using the library() function.\n\n**7. Dataset**\n\nYou'll use the gcbs dataset in the first chapter. This dataset contains 2,495 responses to 15 multiple choice questions, or items, which are designed to test respondents' level of belief in conspiracies.\n\n**8. Item types**\n\nItems in the gcbs dataset are categorized into five conspiracy facets.For example, Item 2, \"The government permits or perpetrates acts of terrorism on its own soil, disguising its involvement,\" is a government malfeasance item. Item 8, \"Evidence of alien contact is being concealed from the public,\" is an extraterrestrial coverup item.\n\n**9. Factor structure**\n\nThe 15 items are hypothesized to reflect five lower-order factors corresponding to their five types. These five factors share a single higher-order factor: conspiracist belief. Hierarchical factor structures like this require structural equation modeling to estimate, but exploratory and confirmatory factor analysis can estimate either a single-factor or five-factor structure.\n\n**10. Factor structure**\n\nIn Chapter 1, you'll ignore the five lower-order factors and use a single-factor EFA to estimate the items' relationship to conspiracist belief. This analysis will give you information about how well each item measures a single underlying factor and information about how much of the factor each examinee possesses. You'll learn how to deal with multiple factors in later chapters.\n\n**11. Using the fa() function**\n\nThe fa() function is your ticket to running EFAs in the psych package. The object created from this function contains lots of valuable information such as items' factor loadings, individuals' factor scores, and fit statistics. In the first lesson of this chapter, you'll learn how to use the fa() function to run a single-factor EFA, access and interpret its output, and diagram the results.\n\n**12. Let's practice!**\n\nNow that we've covered the basic theory behind factor analysis let's get to some actual code!\n\n## Starting out with a unidimensional EFA\n\nLet's begin by using the `psych` package and conducting a single-factor explanatory factor analysis (EFA). The `fa()` function conducts an EFA on your data. When you're using this in the real world, be sure to use a dataset that only contains item responses - other types of data will cause errors and/or incorrect results. In the `gcbs` dataset, these are examinees' responses to 15 items from the Generic Conspiracist Beliefs Scale, which is designed to measure conspiracist beliefs. \n\nAn EFA provides information on each item's relationship to a single factor hypothesized to be represented by each of the items. EFA results give you basic information about how well items relate to that hypothesized construct.\n\nBe sure to save the analysis result object so you can return to it later.\n\n**Steps**\n\n1. Load the `psych` package to gain access to the necessary functions for your exploratory factor analysis. \n2. Then, run a single-factor EFA on the `gcbs` dataset and save the result to an object named `EFA_model`.\n3. Finally, call the `EFA_model` object to see how the items in the dataset relate to the extracted factor.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the psych package\nlibrary(psych)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> \n#> Attaching package: 'psych'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> The following objects are masked from 'package:ggplot2':\n#> \n#>     %+%, alpha\n```\n:::\n\n```{.r .cell-code}\n# Data\ngcbs <- readRDS(\"data/GCBS_data.rds\")\n \n# Conduct a single-factor EFA\nEFA_model <- fa(gcbs)\n\n# View the results\nEFA_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Factor Analysis using method =  minres\n#> Call: fa(r = gcbs)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>      MR1   h2   u2 com\n#> Q1  0.70 0.49 0.51   1\n#> Q2  0.72 0.52 0.48   1\n#> Q3  0.64 0.41 0.59   1\n#> Q4  0.77 0.59 0.41   1\n#> Q5  0.67 0.45 0.55   1\n#> Q6  0.75 0.56 0.44   1\n#> Q7  0.73 0.54 0.46   1\n#> Q8  0.65 0.43 0.57   1\n#> Q9  0.70 0.48 0.52   1\n#> Q10 0.56 0.32 0.68   1\n#> Q11 0.72 0.52 0.48   1\n#> Q12 0.79 0.62 0.38   1\n#> Q13 0.68 0.46 0.54   1\n#> Q14 0.74 0.55 0.45   1\n#> Q15 0.57 0.33 0.67   1\n#> \n#>                 MR1\n#> SS loadings    7.27\n#> Proportion Var 0.48\n#> \n#> Mean item complexity =  1\n#> Test of the hypothesis that 1 factor is sufficient.\n#> \n#> The degrees of freedom for the null model are  105  and the objective function was  9.31 with Chi Square of  23173.8\n#> The degrees of freedom for the model are 90  and the objective function was  1.93 \n#> \n#> The root mean square of the residuals (RMSR) is  0.08 \n#> The df corrected root mean square of the residuals is  0.09 \n#> \n#> The harmonic number of observations is  2495 with the empirical chi square  3398.99  with prob <  0 \n#> The total number of observations was  2495  with Likelihood Chi Square =  4809.34  with prob <  0 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.761\n#> RMSEA index =  0.145  and the 90 % confidence intervals are  0.142 0.149\n#> BIC =  4105.36\n#> Fit based upon off diagonal values = 0.97\n#> Measures of factor score adequacy             \n#>                                                    MR1\n#> Correlation of (regression) scores with factors   0.97\n#> Multiple R square of scores with factors          0.94\n#> Minimum correlation of possible factor scores     0.87\n```\n:::\n:::\n\n\nNice job! You now know how to conduct a single-factor EFA, which tells you each variable's relationship to the factor of interest. You can see in the results that the function has named the factor MR1. This name is due to it being the first factor extracted using minimum residual estimation.\n\n## Viewing and visualizing the factor loadings\n\nEach `fa()` results object is actually a list, and each element of the list contains specific information about the analysis, including factor loadings. Factor loadings represent the strength and directionality of the relationship between each item and the underlying factor, and they can range from -1 to 1. \n\nYou can also create a diagram of loadings. The `fa.diagram()` function takes a result object from `fa()` and creates a path diagram showing the items’ loadings ordered from strongest to weakest. Path diagrams are more common for structural equation modeling than for factor analysis, but this type of visualization can be a helpful way to represent your results.\n\n**Steps**\n\n1. View the items' factor loadings by accessing the `loadings` element of the results object. These values show the strength and direction of their relationships.\n2. Then, visualize the EFA results in a path diagram.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Set up the single-factor EFA\nEFA_model <- fa(gcbs)\n\n# View the factor loadings\nEFA_model$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Loadings:\n#>     MR1  \n#> Q1  0.703\n#> Q2  0.719\n#> Q3  0.638\n#> Q4  0.770\n#> Q5  0.672\n#> Q6  0.746\n#> Q7  0.734\n#> Q8  0.654\n#> Q9  0.695\n#> Q10 0.565\n#> Q11 0.719\n#> Q12 0.786\n#> Q13 0.679\n#> Q14 0.743\n#> Q15 0.574\n#> \n#>                  MR1\n#> SS loadings    7.267\n#> Proportion Var 0.484\n```\n:::\n\n```{.r .cell-code}\n# Create a path diagram of the items' factor loadings\nfa.diagram(EFA_model)\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nGreat work! Now you can view factor loadings as a matrix or visualized as a path diagram.\n\n## Interpreting individuals' factor scores\n\nThe `EFA_model` object also contains a named list element, `scores`, which contains factor scores for each person. These factor scores are an indication of how much or how little of the factor each person is thought to possess. Factor scores are not computed for examinees with missing data.\n\n**Steps**\n\n1. Use `rowSums()` to see the total scores for the first six respondents. These values tell you how much of the construct they possess.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at the first few lines of the response data and their corresponding sum scores\nhead(gcbs)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Q1\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q2\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q4\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q5\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q6\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q7\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q8\"],\"name\":[8],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q9\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q10\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q11\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q12\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q13\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q14\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q15\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"5\",\"3\":\"3\",\"4\":\"5\",\"5\":\"5\",\"6\":\"5\",\"7\":\"5\",\"8\":\"3\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"5\",\"3\":\"5\",\"4\":\"5\",\"5\":\"5\",\"6\":\"3\",\"7\":\"5\",\"8\":\"5\",\"9\":\"1\",\"10\":\"4\",\"11\":\"4\",\"12\":\"5\",\"13\":\"4\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"2\"},{\"1\":\"2\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"2\",\"6\":\"2\",\"7\":\"4\",\"8\":\"2\",\"9\":\"2\",\"10\":\"4\",\"11\":\"2\",\"12\":\"4\",\"13\":\"0\",\"14\":\"2\",\"15\":\"4\",\"_rn_\":\"3\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"1\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"1\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"4\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"3\",\"9\":\"1\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"5\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"1\",\"5\":\"1\",\"6\":\"1\",\"7\":\"1\",\"8\":\"1\",\"9\":\"1\",\"10\":\"1\",\"11\":\"1\",\"12\":\"1\",\"13\":\"1\",\"14\":\"1\",\"15\":\"1\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nrowSums(head(gcbs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  1  2  3  4  5  6 \n#> 68 65 37 55 59 15\n```\n:::\n:::\n\n\n2. Use `head()` to look at the first few lines of the response data and their sum scores. Comparing these helps illustrate the relationship between responses and factor scores.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at the first few lines of the response data and their corresponding sum scores\nhead(gcbs)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Q1\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q2\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q4\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q5\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q6\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q7\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q8\"],\"name\":[8],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q9\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q10\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q11\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q12\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q13\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q14\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q15\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"5\",\"3\":\"3\",\"4\":\"5\",\"5\":\"5\",\"6\":\"5\",\"7\":\"5\",\"8\":\"3\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"5\",\"3\":\"5\",\"4\":\"5\",\"5\":\"5\",\"6\":\"3\",\"7\":\"5\",\"8\":\"5\",\"9\":\"1\",\"10\":\"4\",\"11\":\"4\",\"12\":\"5\",\"13\":\"4\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"2\"},{\"1\":\"2\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"2\",\"6\":\"2\",\"7\":\"4\",\"8\":\"2\",\"9\":\"2\",\"10\":\"4\",\"11\":\"2\",\"12\":\"4\",\"13\":\"0\",\"14\":\"2\",\"15\":\"4\",\"_rn_\":\"3\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"1\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"1\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"4\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"3\",\"9\":\"1\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"5\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"1\",\"5\":\"1\",\"6\":\"1\",\"7\":\"1\",\"8\":\"1\",\"9\":\"1\",\"10\":\"1\",\"11\":\"1\",\"12\":\"1\",\"13\":\"1\",\"14\":\"1\",\"15\":\"1\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nrowSums(head(gcbs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  1  2  3  4  5  6 \n#> 68 65 37 55 59 15\n```\n:::\n\n```{.r .cell-code}\n# Then look at the first few lines of individuals' factor scores\nhead(EFA_model$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             MR1\n#> [1,]  1.5614675\n#> [2,]  1.3432026\n#> [3,] -0.3960355\n#> [4,]  0.7478868\n#> [5,]  1.0435203\n#> [6,] -1.7290812\n```\n:::\n:::\n\n\n3. To get a feel for how the factor scores are distributed, use `summary()` to look at summary statistics.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at the first few lines of the response data and their corresponding sum scores\nhead(gcbs)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Q1\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q2\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q4\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q5\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q6\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q7\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q8\"],\"name\":[8],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q9\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q10\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q11\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q12\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q13\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q14\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q15\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"5\",\"3\":\"3\",\"4\":\"5\",\"5\":\"5\",\"6\":\"5\",\"7\":\"5\",\"8\":\"3\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"5\",\"3\":\"5\",\"4\":\"5\",\"5\":\"5\",\"6\":\"3\",\"7\":\"5\",\"8\":\"5\",\"9\":\"1\",\"10\":\"4\",\"11\":\"4\",\"12\":\"5\",\"13\":\"4\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"2\"},{\"1\":\"2\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"2\",\"6\":\"2\",\"7\":\"4\",\"8\":\"2\",\"9\":\"2\",\"10\":\"4\",\"11\":\"2\",\"12\":\"4\",\"13\":\"0\",\"14\":\"2\",\"15\":\"4\",\"_rn_\":\"3\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"1\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"1\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"4\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"3\",\"9\":\"1\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"5\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"1\",\"5\":\"1\",\"6\":\"1\",\"7\":\"1\",\"8\":\"1\",\"9\":\"1\",\"10\":\"1\",\"11\":\"1\",\"12\":\"1\",\"13\":\"1\",\"14\":\"1\",\"15\":\"1\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nrowSums(head(gcbs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  1  2  3  4  5  6 \n#> 68 65 37 55 59 15\n```\n:::\n\n```{.r .cell-code}\n# Then look at the first few lines of individuals' factor scores\nhead(EFA_model$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             MR1\n#> [1,]  1.5614675\n#> [2,]  1.3432026\n#> [3,] -0.3960355\n#> [4,]  0.7478868\n#> [5,]  1.0435203\n#> [6,] -1.7290812\n```\n:::\n\n```{.r .cell-code}\n# To get a feel for how the factor scores are distributed, look at their summary statistics and density plot.\nsummary(EFA_model$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>       MR1           \n#>  Min.   :-1.854703  \n#>  1st Qu.:-0.783260  \n#>  Median :-0.001971  \n#>  Mean   : 0.000000  \n#>  3rd Qu.: 0.728568  \n#>  Max.   : 1.949580\n```\n:::\n:::\n\n\n4. Use `plot()` and `density()` to create a density plot of the estimated factor scores for a visual representation. Density plots show the distribution of data over a continuous interval and can give you a sense of what your data look like.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at the first few lines of the response data and their corresponding sum scores\nhead(gcbs)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Q1\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q2\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q4\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q5\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q6\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q7\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q8\"],\"name\":[8],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q9\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q10\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q11\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q12\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q13\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q14\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Q15\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"5\",\"3\":\"3\",\"4\":\"5\",\"5\":\"5\",\"6\":\"5\",\"7\":\"5\",\"8\":\"3\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"5\",\"3\":\"5\",\"4\":\"5\",\"5\":\"5\",\"6\":\"3\",\"7\":\"5\",\"8\":\"5\",\"9\":\"1\",\"10\":\"4\",\"11\":\"4\",\"12\":\"5\",\"13\":\"4\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"2\"},{\"1\":\"2\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"2\",\"6\":\"2\",\"7\":\"4\",\"8\":\"2\",\"9\":\"2\",\"10\":\"4\",\"11\":\"2\",\"12\":\"4\",\"13\":\"0\",\"14\":\"2\",\"15\":\"4\",\"_rn_\":\"3\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"2\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"1\",\"9\":\"4\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"1\",\"14\":\"4\",\"15\":\"5\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"4\",\"3\":\"1\",\"4\":\"4\",\"5\":\"4\",\"6\":\"5\",\"7\":\"4\",\"8\":\"3\",\"9\":\"1\",\"10\":\"5\",\"11\":\"5\",\"12\":\"5\",\"13\":\"3\",\"14\":\"5\",\"15\":\"5\",\"_rn_\":\"5\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"1\",\"5\":\"1\",\"6\":\"1\",\"7\":\"1\",\"8\":\"1\",\"9\":\"1\",\"10\":\"1\",\"11\":\"1\",\"12\":\"1\",\"13\":\"1\",\"14\":\"1\",\"15\":\"1\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nrowSums(head(gcbs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  1  2  3  4  5  6 \n#> 68 65 37 55 59 15\n```\n:::\n\n```{.r .cell-code}\n# Then look at the first few lines of individuals' factor scores\nhead(EFA_model$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             MR1\n#> [1,]  1.5614675\n#> [2,]  1.3432026\n#> [3,] -0.3960355\n#> [4,]  0.7478868\n#> [5,]  1.0435203\n#> [6,] -1.7290812\n```\n:::\n\n```{.r .cell-code}\n# To get a feel for how the factor scores are distributed, look at their summary statistics and density plot.\nsummary(EFA_model$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>       MR1           \n#>  Min.   :-1.854703  \n#>  1st Qu.:-0.783260  \n#>  Median :-0.001971  \n#>  Mean   : 0.000000  \n#>  3rd Qu.: 0.728568  \n#>  Max.   : 1.949580\n```\n:::\n\n```{.r .cell-code}\nplot(density(EFA_model$scores, na.rm = TRUE), \n     main = \"Factor Scores\")\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nCongrats! Now you know how to view individuals' factor scores.\n\n## Overview of the measure development process\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Overview of the measure development process**\n\nNow that you know how to conduct a single-factor EFA, you're ready to start thinking about how it fits into the process of developing a measure.\n\n**2. Development process**\n\nWhen you're creating a measure of an unobservable variable or variables, you typically want to follow this process. Steps 1, 2, and 4 are theoretical and don't involve any coding so we won't spend much time on those.\n\n**3. Development process**\n\nWhen you have determined what construct you want to measure, the first step is to write questions, or items, for your measure. Always write more items than you think you'll need since they probably won't all perform as well as you'd like.After you've got enough items developed, the next step is to collect pilot data from a representative sample to test the measure. These data will be used to examine your measure and see how it is functioning before you use it for real. You can think of the gcbs dataset as pilot data for these 15 items. Once you've got pilot data, the first step is to check out what the dataset looks like.\n\n**4. Inspecting your dataset**\n\nTo get a sense for your data, use the describe() function to see basic information about each of the items in the dataset.If you look at the first row of output, you can see in the column called n that item Q1 has 2,495 responses, a mean of 3-point-47, and a standard deviation of 1-point-46.You'll notice that the minimum and maximum are 0 and 5 for these items because they've been scored on a Likert scale.\n\n**5. Development process**\n\nNow that you know some basic features of your dataset, you have to consider which analyses you want to run. If you want to both develop and confirm a theory of how items are related to underlying factors, you'll want to use both EFA and CFA on your dataset. In that case, you'll need to split the dataset - step 5 here.\n\n**6. Splitting the dataset**\n\nTo do that, create a set of random indices, then use them to divide the dataset.First, you'll determine the number of rows, N, in the dataset and set up a sequence from 1 to N. Next, you'll use the sample() function to select half of those numbers at random, then assign them to an object called indices_EFA. The other half of the numbers in the sequence are assigned to another object called indices_CFA. Once those sets of indices are established, you'll use them to create two datasets: one for your EFA, and one for your CFA. By creating a theory with half of the data, then testing it on the other half, you'll avoid overfitting your model or falsely confirming your theory.\n\n**7. Development process**\n\nNow that your sample is split into random halves, you'll want to make sure the two halves are similar - step 6 in this process. If the halves aren't similar, they aren't good representations of the population and aren't appropriate to evaluate your measure.\n\n**8. Inspecting the halves**\n\nThe psych package provides some convenience functions to examine a dataset according to a grouping variable. To accomplish this, create a grouping variable from the indices you created to split the data.\n\n**9. Inspecting the halves**\n\nThe grouping variable can then be bound onto the gcbs dataset as a new column using the cbind() function.This grouping variable provides the information needed by describeBy() and statsBy(), which you can use to view some key summary statistics. Watch out though - while the group argument of describeBy() has to be a vector, the group argument of statsBy() has to be the name of a column in your dataframe.\n\n**10. Let's practice!**\n\nLet's see these functions in action.\n\n## Descriptive statistics of your dataset\n\nThe `psych` package provides several functions to help you visualize your data. The `describe()` function tells you basic statistics about each variable in your dataset, including the number of complete cases, the mean, standard deviation, minimum, maximum, range, skew, and kurtosis. \n\nYou can also view graphical representations of the error bars for different variables using `error.dots()` and `error.bars()`. Both of these graphical representations are created from the summary statistics available from calling `describe()`.\n\n**Steps**\n\n1. Check out the basic descriptive statistics provided by the `describe()` function.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Basic descriptive statistics\ndescribe(gcbs)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"vars\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"trimmed\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mad\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"min\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"range\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"skew\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"kurtosis\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"se\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"2495\",\"3\":\"3.472545\",\"4\":\"1.455552\",\"5\":\"4\",\"6\":\"3.591387\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-0.546510460\",\"12\":\"-1.1036577\",\"13\":\"0.02914020\",\"_rn_\":\"Q1\"},{\"1\":\"2\",\"2\":\"2495\",\"3\":\"2.963527\",\"4\":\"1.494669\",\"5\":\"3\",\"6\":\"2.960941\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-0.008310023\",\"12\":\"-1.4002635\",\"13\":\"0.02992331\",\"_rn_\":\"Q2\"},{\"1\":\"3\",\"2\":\"2495\",\"3\":\"2.046894\",\"4\":\"1.387236\",\"5\":\"1\",\"6\":\"1.818728\",\"7\":\"0.0000\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.981805700\",\"12\":\"-0.4442639\",\"13\":\"0.02777251\",\"_rn_\":\"Q3\"},{\"1\":\"4\",\"2\":\"2495\",\"3\":\"2.636072\",\"4\":\"1.451371\",\"5\":\"2\",\"6\":\"2.548322\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.262457451\",\"12\":\"-1.3376479\",\"13\":\"0.02905649\",\"_rn_\":\"Q4\"},{\"1\":\"5\",\"2\":\"2495\",\"3\":\"3.254108\",\"4\":\"1.471855\",\"5\":\"4\",\"6\":\"3.322484\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-0.347396087\",\"12\":\"-1.2685987\",\"13\":\"0.02946658\",\"_rn_\":\"Q5\"},{\"1\":\"6\",\"2\":\"2495\",\"3\":\"3.108617\",\"4\":\"1.506676\",\"5\":\"3\",\"6\":\"3.138207\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-0.165115915\",\"12\":\"-1.4172391\",\"13\":\"0.03016371\",\"_rn_\":\"Q6\"},{\"1\":\"7\",\"2\":\"2495\",\"3\":\"2.666934\",\"4\":\"1.509954\",\"5\":\"2\",\"6\":\"2.587381\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.283472211\",\"12\":\"-1.3907382\",\"13\":\"0.03022932\",\"_rn_\":\"Q7\"},{\"1\":\"8\",\"2\":\"2495\",\"3\":\"2.450501\",\"4\":\"1.569256\",\"5\":\"2\",\"6\":\"2.318478\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.513338904\",\"12\":\"-1.2964184\",\"13\":\"0.03141656\",\"_rn_\":\"Q8\"},{\"1\":\"9\",\"2\":\"2495\",\"3\":\"2.232866\",\"4\":\"1.419266\",\"5\":\"2\",\"6\":\"2.046570\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.756774766\",\"12\":\"-0.8199821\",\"13\":\"0.02841375\",\"_rn_\":\"Q9\"},{\"1\":\"10\",\"2\":\"2495\",\"3\":\"3.502204\",\"4\":\"1.388713\",\"5\":\"4\",\"6\":\"3.627441\",\"7\":\"1.4826\",\"8\":\"1\",\"9\":\"5\",\"10\":\"4\",\"11\":\"-0.586863096\",\"12\":\"-0.9439026\",\"13\":\"0.02780207\",\"_rn_\":\"Q10\"},{\"1\":\"11\",\"2\":\"2495\",\"3\":\"3.265331\",\"4\":\"1.400302\",\"5\":\"4\",\"6\":\"3.336004\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-0.351391806\",\"12\":\"-1.1125082\",\"13\":\"0.02803409\",\"_rn_\":\"Q11\"},{\"1\":\"12\",\"2\":\"2495\",\"3\":\"2.644890\",\"4\":\"1.504787\",\"5\":\"2\",\"6\":\"2.561843\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.287013666\",\"12\":\"-1.3666501\",\"13\":\"0.03012588\",\"_rn_\":\"Q12\"},{\"1\":\"13\",\"2\":\"2495\",\"3\":\"2.103006\",\"4\":\"1.382461\",\"5\":\"1\",\"6\":\"1.891838\",\"7\":\"0.0000\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"0.886403383\",\"12\":\"-0.5634110\",\"13\":\"0.02767691\",\"_rn_\":\"Q13\"},{\"1\":\"14\",\"2\":\"2495\",\"3\":\"2.955912\",\"4\":\"1.489222\",\"5\":\"3\",\"6\":\"2.946420\",\"7\":\"1.4826\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-0.016286947\",\"12\":\"-1.4295864\",\"13\":\"0.02981427\",\"_rn_\":\"Q14\"},{\"1\":\"15\",\"2\":\"2495\",\"3\":\"4.227655\",\"4\":\"1.104538\",\"5\":\"5\",\"6\":\"4.468703\",\"7\":\"0.0000\",\"8\":\"0\",\"9\":\"5\",\"10\":\"5\",\"11\":\"-1.562554955\",\"12\":\"1.7067918\",\"13\":\"0.02211289\",\"_rn_\":\"Q15\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n2. Create a **dot** chart of the mean scores and confidence intervals for each variable in the dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Graphical representation of error\nerror.dots(gcbs)\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n3. Create a **bar** chart of the mean scores and confidence intervals for each variable in the dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Graphical representation of error\nerror.bars(gcbs)\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nGreat job! Now you know how to get a feel for your dataset with basic descriptive statistics and visualizations of error.\n\n## Splitting your dataset\n\nDuring the measure development process, it's important to conduct EFA and CFA on separate datasets because using the same dataset can lead to inflated model fit statistics. Instead, you can split your dataset in half, then use one half for the EFA and the other half for the CFA.\n\n**Steps**\n\n1. Split the dataset in half using two sets of indices to determine which rows belong to each dataset. \n2. Use the first set of indices to create a dataset for your EFA, then use the second set for your CFA dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Establish two sets of indices to split the dataset\nN <- nrow(gcbs)\nindices <- seq(1, N)\nindices_EFA <- sample(indices, floor((.5*N)))\nindices_CFA <- indices[!(indices %in% indices_EFA)]\n\n# Use those indices to split the dataset into halves for your EFA and CFA\ngcbs_EFA <- gcbs[indices_EFA, ]\ngcbs_CFA <- gcbs[indices_CFA, ]\n```\n:::\n\n\nNice job splitting the dataset! Splitting your dataset is useful for creating train/test datasets for machine learning as well as for testing many other types of models.\n\n## Comparing the halves of your dataset\n\nJust as you inspected the features of your full dataset, it's important to examine the halves after you've split the data. You can always use `describe()` on each dataset, but the `psych` package also provides some functions to help compare a dataset according to a grouping variable. \n\nIn this exercise, you'll use the indices created when splitting the dataset to create a grouping variable and attach it to the `gcbs` dataset. Once that grouping variable is set up, you can use `describeBy()` and `statsBy()` to view basic descriptive statistics as well as between-group statistics.\n\nA word of warning: while the `group` argument of `describeBy()` has to be a vector, the `group` argument of `statsBy()` has to be the name of a column in your dataframe. Plan accordingly!\n\n**Steps**\n\n1. To better understand the halves, first create a dichotomous grouping variable from the indices you created in the last exercise (`indices_EFA` and `indices_CFA`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use the indices from the previous exercise to create a grouping variable\ngroup_var <- vector(\"numeric\", nrow(gcbs))\ngroup_var[indices_EFA] <- 1\ngroup_var[indices_CFA] <- 2\n```\n:::\n\n\n2. Bind that variable onto the dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use the indices from the previous exercise to create a grouping variable\ngroup_var <- vector(\"numeric\", nrow(gcbs))\ngroup_var[indices_EFA] <- 1\ngroup_var[indices_CFA] <- 2\n\n# Bind that grouping variable onto the gcbs dataset\ngcbs_grouped <- cbind(gcbs, group_var)\n```\n:::\n\n\n3. Use `describeBy()` and `statsBy()` to compare stats across groups.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use the indices from the previous exercise to create a grouping variable\ngroup_var <- vector(\"numeric\", nrow(gcbs))\ngroup_var[indices_EFA] <- 1\ngroup_var[indices_CFA] <- 2\n\n# Bind that grouping variable onto the gcbs dataset\ngcbs_grouped <- cbind(gcbs, group_var)\n\n# Compare stats across groups\ndescribeBy(gcbs_grouped, group = group_var)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#>  Descriptive statistics by group \n#> group: 1\n#>           vars    n mean   sd median trimmed  mad min max range  skew kurtosis\n#> Q1           1 1247 3.47 1.46      4    3.59 1.48   0   5     5 -0.55    -1.12\n#> Q2           2 1247 3.03 1.51      3    3.05 1.48   0   5     5 -0.07    -1.40\n#> Q3           3 1247 2.07 1.41      1    1.84 0.00   0   5     5  0.95    -0.54\n#> Q4           4 1247 2.63 1.44      2    2.54 1.48   0   5     5  0.28    -1.32\n#> Q5           5 1247 3.25 1.47      4    3.32 1.48   0   5     5 -0.35    -1.27\n#> Q6           6 1247 3.12 1.50      3    3.15 1.48   0   5     5 -0.14    -1.43\n#> Q7           7 1247 2.67 1.51      2    2.59 1.48   0   5     5  0.27    -1.39\n#> Q8           8 1247 2.47 1.57      2    2.35 1.48   0   5     5  0.50    -1.31\n#> Q9           9 1247 2.24 1.42      2    2.05 1.48   0   5     5  0.75    -0.83\n#> Q10         10 1247 3.48 1.39      4    3.60 1.48   1   5     4 -0.54    -1.01\n#> Q11         11 1247 3.25 1.40      3    3.31 1.48   0   5     5 -0.33    -1.12\n#> Q12         12 1247 2.67 1.51      2    2.59 1.48   0   5     5  0.29    -1.38\n#> Q13         13 1247 2.14 1.39      1    1.92 0.00   0   5     5  0.86    -0.60\n#> Q14         14 1247 2.97 1.47      3    2.96 1.48   0   5     5 -0.01    -1.39\n#> Q15         15 1247 4.21 1.13      5    4.46 0.00   0   5     5 -1.54     1.59\n#> group_var   16 1247 1.00 0.00      1    1.00 0.00   1   1     0   NaN      NaN\n#>             se\n#> Q1        0.04\n#> Q2        0.04\n#> Q3        0.04\n#> Q4        0.04\n#> Q5        0.04\n#> Q6        0.04\n#> Q7        0.04\n#> Q8        0.04\n#> Q9        0.04\n#> Q10       0.04\n#> Q11       0.04\n#> Q12       0.04\n#> Q13       0.04\n#> Q14       0.04\n#> Q15       0.03\n#> group_var 0.00\n#> ------------------------------------------------------------ \n#> group: 2\n#>           vars    n mean   sd median trimmed  mad min max range  skew kurtosis\n#> Q1           1 1248 3.48 1.45      4    3.60 1.48   0   5     5 -0.55    -1.09\n#> Q2           2 1248 2.90 1.48      3    2.88 1.48   0   5     5  0.05    -1.39\n#> Q3           3 1248 2.02 1.37      1    1.80 0.00   0   5     5  1.01    -0.35\n#> Q4           4 1248 2.64 1.46      2    2.56 1.48   0   5     5  0.25    -1.36\n#> Q5           5 1248 3.26 1.47      4    3.33 1.48   0   5     5 -0.34    -1.27\n#> Q6           6 1248 3.10 1.52      3    3.13 1.48   0   5     5 -0.19    -1.41\n#> Q7           7 1248 2.66 1.51      2    2.58 1.48   0   5     5  0.30    -1.39\n#> Q8           8 1248 2.43 1.56      2    2.29 1.48   0   5     5  0.53    -1.29\n#> Q9           9 1248 2.23 1.42      2    2.04 1.48   0   5     5  0.76    -0.82\n#> Q10         10 1248 3.52 1.38      4    3.65 1.48   1   5     4 -0.63    -0.88\n#> Q11         11 1248 3.28 1.40      4    3.36 1.48   0   5     5 -0.37    -1.11\n#> Q12         12 1248 2.62 1.50      2    2.53 1.48   0   5     5  0.28    -1.36\n#> Q13         13 1248 2.07 1.38      1    1.86 0.00   0   5     5  0.91    -0.53\n#> Q14         14 1248 2.94 1.51      3    2.93 1.48   0   5     5 -0.02    -1.47\n#> Q15         15 1248 4.24 1.08      5    4.47 0.00   1   5     4 -1.58     1.81\n#> group_var   16 1248 2.00 0.00      2    2.00 0.00   2   2     0   NaN      NaN\n#>             se\n#> Q1        0.04\n#> Q2        0.04\n#> Q3        0.04\n#> Q4        0.04\n#> Q5        0.04\n#> Q6        0.04\n#> Q7        0.04\n#> Q8        0.04\n#> Q9        0.04\n#> Q10       0.04\n#> Q11       0.04\n#> Q12       0.04\n#> Q13       0.04\n#> Q14       0.04\n#> Q15       0.03\n#> group_var 0.00\n```\n:::\n\n```{.r .cell-code}\nstatsBy(gcbs_grouped, group = \"group_var\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Statistics within and between groups  \n#> Call: statsBy(data = gcbs_grouped, group = \"group_var\")\n#> Intraclass Correlation 1 (Percentage of variance due to groups) \n#>        Q1        Q2        Q3        Q4        Q5        Q6        Q7        Q8 \n#>         0         0         0         0         0         0         0         0 \n#>        Q9       Q10       Q11       Q12       Q13       Q14       Q15 group_var \n#>         0         0         0         0         0         0         0         1 \n#> Intraclass Correlation 2 (Reliability of group differences) \n#>        Q1        Q2        Q3        Q4        Q5        Q6        Q7        Q8 \n#>    -46.71      0.79     -0.29    -14.59    -38.19     -6.71    -34.47     -0.80 \n#>        Q9       Q10       Q11       Q12       Q13       Q14       Q15 group_var \n#>    -38.85     -0.89     -1.81     -0.31      0.28     -6.08     -1.67      1.00 \n#> eta^2 between groups  \n#>  Q1.bg  Q2.bg  Q3.bg  Q4.bg  Q5.bg  Q6.bg  Q7.bg  Q8.bg  Q9.bg Q10.bg Q11.bg \n#>      0      0      0      0      0      0      0      0      0      0      0 \n#> Q12.bg Q13.bg Q14.bg Q15.bg \n#>      0      0      0      0 \n#> \n#> To see the correlations between and within groups, use the short=FALSE option in your print statement.\n#> Many results are not shown directly. To see specific objects select from the following list:\n#>  mean sd n F ICC1 ICC2 ci1 ci2 raw rbg pbg rwg nw ci.wg pwg etabg etawg nwg nG Call\n```\n:::\n:::\n\n\nGreat work navigating `describeBy()` and `statsBy()`! These functions are a quick and easy way to compare stats across groups.\n\n## Measure features: correlations and reliability\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Measure features: correlations and reliability**\n\nBy this point, you've looked at basic descriptive statistics of your dataset and learned how to split the data into random halves. When writing a description of your measure, you'll also want to include some more detailed information.\n\n**2. Correlations**\n\nCorrelations are the standard way of reporting relationships between variables. The lowerCor() function provides this data in a more reader-friendly format than base R's cor() function. The diagonal of ones represents the perfect correlation between each item and itself, and the other values are the correlations between each pair of items. lowerCor() displays only the lower triangle of the correlation matrix, so each pair's correlation is only displayed once. This correlation matrix is your first clue about factor structure. Groups of items that are more strongly correlated typically load onto the same factor.\n\n**3. Testing correlations' significance: p-values**\n\nOnce you've used lowerCor() to find the correlations between items, you will likely also want to report their significance and confidence intervals. corr.test() can be used to generate both of these metrics for inter-item correlations. corr.test() generates a lot of output when you run it, and results are given as a full matrix instead of just the lower half like lowerCor(). Its result object is a list, so you can specify named list elements to get only the information you want to view. In this example, we are accessing the 'p' list element to get the p-values for each of the correlations. This slide displays the p-values for the correlations of the items. All those zeroes indicate statistically significant correlations. This is unsurprising given that the gcbs dataset has over 2,000 cases since statistical significance is affected by sample size.\n\n**4. Testing correlations' significance: confidence intervals**\n\nYou can also use corr-dot-test() to view confidence intervals for each of the correlations. By default, corr-dot-test() calculates 95% confidence intervals around the correlation value, r. This means that if we repeated the experiment many times with datasets drawn from the same population, the calculated confidence intervals would contain the true value 95% of the time. These confidence intervals are important to report for many types of publications.The output above shows the results for the first item, Q1.\n\n**5. Coefficient alpha**\n\nCoefficient alpha, also called Cronbach's alpha, is another important statistic to report during measure development. This statistic is a measure of the internal consistency of your measure, which is also called reliability. Most fields of research prefer measures whose alpha is greater than 0-point-8. Using the alpha() function, you can see that the gcbs items have a coefficient alpha of 0-point-93, which suggests excellent reliability.\n\n**6. Coefficient alpha**\n\nThe output from the alpha() function will also tell you some basic stats for each item as well as how the overall alpha value would be affected if an item were dropped. If dropping an item would cause alpha to increase, that's an indicator that that item isn't performing as well.\n\n**7. Split-Half reliability**\n\nSplit-half reliability is another common statistic showing internal consistency. It reflects how well two halves of the test relate to each other. The splitHalf() function displays several common split-half statistics. You will likely want to report the average split-half reliability, which happens to be 0-point-93: the same value as coefficient alpha! This is coincidental, but the reliability metrics are conceptually similar, so it's not surprising.\n\n**8. Let's practice!**\n\nNow, let's put these functions into practice.\n\n## Viewing and testing correlations\n\nOne of the easiest ways to get a feel for your dataset is to examine the relationships of your variables with each other. While base R has the `cor()` function, the `psych` package has the `lowerCor()` function, which only displays the lower triangle of the correlation matrix for easier viewing and interpretation.\n\nYou can also use the `corr.test()` function to view several probability values, including p-values from t-tests of each correlation and confidence intervals for the correlations. P-values are corrected using the Holm correction by default.\n\n**Steps**\n\n1. View the lower triangle of the correlation matrix.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at some correlation data\nlowerCor(gcbs, use = \"pairwise.complete.obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>     Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9   Q10  Q11 \n#> Q1  1.00                                                  \n#> Q2  0.53 1.00                                             \n#> Q3  0.36 0.40 1.00                                        \n#> Q4  0.52 0.53 0.50 1.00                                   \n#> Q5  0.48 0.46 0.40 0.57 1.00                              \n#> Q6  0.63 0.55 0.40 0.61 0.50 1.00                         \n#> Q7  0.47 0.67 0.42 0.57 0.45 0.54 1.00                    \n#> Q8  0.39 0.38 0.78 0.49 0.41 0.41 0.41 1.00               \n#> Q9  0.42 0.49 0.49 0.56 0.46 0.48 0.53 0.48 1.00          \n#> Q10 0.44 0.38 0.32 0.40 0.43 0.41 0.39 0.36 0.37 1.00     \n#> Q11 0.64 0.52 0.34 0.52 0.49 0.62 0.49 0.37 0.46 0.45 1.00\n#> Q12 0.52 0.72 0.44 0.60 0.49 0.59 0.75 0.42 0.57 0.40 0.55\n#> Q13 0.38 0.40 0.71 0.51 0.43 0.42 0.45 0.76 0.54 0.37 0.40\n#> Q14 0.53 0.50 0.43 0.60 0.54 0.55 0.52 0.45 0.55 0.41 0.56\n#> Q15 0.51 0.40 0.27 0.39 0.45 0.47 0.39 0.31 0.32 0.45 0.54\n#>     Q12  Q13  Q14  Q15 \n#> Q12 1.00               \n#> Q13 0.49 1.00          \n#> Q14 0.56 0.50 1.00     \n#> Q15 0.41 0.30 0.46 1.00\n```\n:::\n:::\n\n\n2. Check out the p-values created when calculating correlations. Significant values mean items are meaningfully correlated.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at some correlation data\ncorr.test(gcbs, use = \"pairwise.complete.obs\")$p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                Q1            Q2            Q3            Q4            Q5\n#> Q1   0.000000e+00 1.038105e-175  2.525793e-74 1.746323e-174 5.801103e-143\n#> Q2  1.384140e-177  0.000000e+00  6.236650e-93 8.388030e-183 1.237758e-127\n#> Q3   3.608276e-75  3.282447e-94  0.000000e+00 1.087371e-155  1.282718e-94\n#> Q4  2.359896e-176 1.075388e-184 1.647531e-157  0.000000e+00 7.309882e-214\n#> Q5  1.054746e-144 2.578663e-129  6.108182e-96 8.032837e-216  0.000000e+00\n#> Q6  1.477903e-277 1.757495e-198  1.248831e-96 3.037754e-253 1.470071e-154\n#> Q7  8.142449e-139  0.000000e+00 5.318107e-109 5.260693e-212 2.336455e-122\n#> Q8   1.549786e-91  5.579984e-85  0.000000e+00 1.898345e-150 1.224898e-101\n#> Q9  4.344797e-106 3.376084e-148 3.104324e-151 3.108940e-210 1.462606e-133\n#> Q10 3.550942e-116  9.131376e-87  6.760094e-61  1.298818e-98 1.476449e-111\n#> Q11 9.499179e-292 1.822105e-173  2.580298e-69 9.037626e-175 2.540009e-153\n#> Q12 9.097129e-175  0.000000e+00 1.005698e-117 1.550592e-244 2.612164e-152\n#> Q13  9.052542e-87  1.297778e-96  0.000000e+00 1.798859e-167 1.168044e-111\n#> Q14 1.912163e-184 3.969981e-158 1.103607e-113 8.219433e-248 4.008639e-188\n#> Q15 9.793389e-162  3.129498e-96  1.676489e-41  1.255385e-93 3.849304e-125\n#>                Q6            Q7            Q8            Q9           Q10\n#> Q1  1.448345e-275 4.315498e-137  2.634636e-90 1.433783e-104 1.420377e-114\n#> Q2  1.493871e-196  0.000000e+00  6.695981e-84 1.958129e-146  1.267356e-85\n#> Q3   2.997195e-95 1.861338e-107  0.000000e+00 1.862595e-149  2.704037e-60\n#> Q4  2.916244e-251 4.734624e-210 1.120024e-148 2.766957e-208  3.376926e-97\n#> Q5  9.408454e-153 9.813111e-121 3.429715e-100 7.459290e-132 5.462862e-110\n#> Q6   0.000000e+00 2.582592e-187 1.111480e-101 4.903141e-145  2.966233e-99\n#> Q7  3.149502e-189  0.000000e+00 1.110564e-101 4.177923e-178  1.437786e-89\n#> Q8  3.704932e-103 3.582463e-103  0.000000e+00 3.981155e-142  1.778530e-78\n#> Q9  8.755609e-147 5.497267e-180 7.372509e-144  0.000000e+00  2.784487e-79\n#> Q10 1.098605e-100  9.585238e-91  2.223163e-79  3.093874e-80  0.000000e+00\n#> Q11 4.981130e-268 8.601767e-153  2.643708e-82 1.655959e-131 4.633595e-125\n#> Q12 2.403960e-231  0.000000e+00 5.764588e-109 7.071640e-216  1.561964e-98\n#> Q13 1.353745e-109 7.858024e-124  0.000000e+00 2.581509e-186  4.747674e-81\n#> Q14 7.044729e-195 6.545724e-170 2.485870e-124 2.858794e-201 3.168647e-104\n#> Q15 1.340087e-134  7.843219e-91  4.328489e-55  4.516727e-61 2.157366e-124\n#>               Q11           Q12           Q13           Q14           Q15\n#> Q1  9.404187e-290 6.597467e-173  1.267356e-85 1.472366e-182 6.659505e-160\n#> Q2  1.293694e-171  0.000000e+00  2.997195e-95 2.659887e-156  6.884895e-95\n#> Q3   1.548179e-68 4.123361e-116  0.000000e+00 4.304066e-112  1.676489e-41\n#> Q4  6.597467e-173 1.457557e-242 1.241212e-165 7.808462e-246  2.259693e-92\n#> Q5  1.600206e-151 1.593420e-150 4.438568e-110 3.206912e-186 1.809173e-123\n#> Q6  4.831696e-266 2.235683e-229 4.873482e-108 5.847125e-193 6.968453e-133\n#> Q7  5.333096e-151  0.000000e+00 3.378951e-122 4.582007e-168  1.254915e-89\n#> Q8   2.908079e-81 1.959960e-107  0.000000e+00 1.093783e-122  1.298547e-54\n#> Q9  8.114201e-130 6.505909e-214 2.039392e-184 2.458563e-199  2.258363e-60\n#> Q10 2.131454e-123  3.904910e-97  4.747674e-80 1.013967e-102 9.708145e-123\n#> Q11  0.000000e+00 1.571210e-193  1.584049e-94 1.610933e-205 1.319712e-186\n#> Q12 1.870488e-195  0.000000e+00 9.395071e-146 1.350081e-206 3.398356e-100\n#> Q13  7.920246e-96 1.648258e-147  0.000000e+00 6.585550e-154  2.751475e-52\n#> Q14 1.851647e-207 1.534183e-208 1.013162e-155  0.000000e+00 2.883420e-130\n#> Q15 1.629274e-188 1.171847e-101  1.375737e-52 5.766839e-132  0.000000e+00\n```\n:::\n:::\n\n\n3. View the confidence intervals created when calculating correlations.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Take a look at some correlation data\ncorr.test(gcbs, use = \"pairwise.complete.obs\")$ci\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"lower\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"r\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.4970162\",\"2\":\"0.5259992\",\"3\":\"0.5538098\",\"4\":\"1.384140e-177\",\"_rn_\":\"Q1-Q2\"},{\"1\":\"0.3206223\",\"2\":\"0.3553928\",\"3\":\"0.3892067\",\"4\":\"3.608276e-75\",\"_rn_\":\"Q1-Q3\"},{\"1\":\"0.4953852\",\"2\":\"0.5244323\",\"3\":\"0.5523079\",\"4\":\"2.359896e-176\",\"_rn_\":\"Q1-Q4\"},{\"1\":\"0.4503342\",\"2\":\"0.4810747\",\"3\":\"0.5106759\",\"4\":\"1.054746e-144\",\"_rn_\":\"Q1-Q5\"},{\"1\":\"0.6071117\",\"2\":\"0.6313131\",\"3\":\"0.6543444\",\"4\":\"1.477903e-277\",\"_rn_\":\"Q1-Q6\"},{\"1\":\"0.4412058\",\"2\":\"0.4722710\",\"3\":\"0.5022057\",\"4\":\"8.142449e-139\",\"_rn_\":\"Q1-Q7\"},{\"1\":\"0.3564216\",\"2\":\"0.3902059\",\"3\":\"0.4229712\",\"4\":\"1.549786e-91\",\"_rn_\":\"Q1-Q8\"},{\"1\":\"0.3850453\",\"2\":\"0.4179718\",\"3\":\"0.4498355\",\"4\":\"4.344797e-106\",\"_rn_\":\"Q1-Q9\"},{\"1\":\"0.4034438\",\"2\":\"0.4357865\",\"3\":\"0.4670415\",\"4\":\"3.550942e-116\",\"_rn_\":\"Q1-Q10\"},{\"1\":\"0.6199265\",\"2\":\"0.6435136\",\"3\":\"0.6659388\",\"4\":\"9.499179e-292\",\"_rn_\":\"Q1-Q11\"},{\"1\":\"0.4932727\",\"2\":\"0.5224025\",\"3\":\"0.5503620\",\"4\":\"9.097129e-175\",\"_rn_\":\"Q1-Q12\"},{\"1\":\"0.3464313\",\"2\":\"0.3805006\",\"3\":\"0.4135673\",\"4\":\"9.052542e-87\",\"_rn_\":\"Q1-Q13\"},{\"1\":\"0.5059498\",\"2\":\"0.5345780\",\"3\":\"0.5620298\",\"4\":\"1.912163e-184\",\"_rn_\":\"Q1-Q14\"},{\"1\":\"0.4753633\",\"2\":\"0.5051815\",\"3\":\"0.5338405\",\"4\":\"9.793389e-162\",\"_rn_\":\"Q1-Q15\"},{\"1\":\"0.3618855\",\"2\":\"0.3955108\",\"3\":\"0.4281083\",\"4\":\"3.282447e-94\",\"_rn_\":\"Q2-Q3\"},{\"1\":\"0.5062706\",\"2\":\"0.5348860\",\"3\":\"0.5623248\",\"4\":\"1.075388e-184\",\"_rn_\":\"Q2-Q4\"},{\"1\":\"0.4259018\",\"2\":\"0.4574975\",\"3\":\"0.4879788\",\"4\":\"2.578663e-129\",\"_rn_\":\"Q2-Q5\"},{\"1\":\"0.5234810\",\"2\":\"0.5513960\",\"3\":\"0.5781285\",\"4\":\"1.757495e-198\",\"_rn_\":\"Q2-Q6\"},{\"1\":\"0.6501266\",\"2\":\"0.6722188\",\"3\":\"0.6931753\",\"4\":\"0.000000e+00\",\"_rn_\":\"Q2-Q7\"},{\"1\":\"0.3425926\",\"2\":\"0.3767693\",\"3\":\"0.4099501\",\"4\":\"5.579984e-85\",\"_rn_\":\"Q2-Q8\"},{\"1\":\"0.4556319\",\"2\":\"0.4861810\",\"3\":\"0.5155863\",\"4\":\"3.376084e-148\",\"_rn_\":\"Q2-Q9\"},{\"1\":\"0.3464233\",\"2\":\"0.3804928\",\"3\":\"0.4135598\",\"4\":\"9.131376e-87\",\"_rn_\":\"Q2-Q10\"},{\"1\":\"0.4915283\",\"2\":\"0.5207263\",\"3\":\"0.5487548\",\"4\":\"1.822105e-173\",\"_rn_\":\"Q2-Q11\"},{\"1\":\"0.6962013\",\"2\":\"0.7158851\",\"3\":\"0.7344931\",\"4\":\"0.000000e+00\",\"_rn_\":\"Q2-Q12\"},{\"1\":\"0.3667134\",\"2\":\"0.4001964\",\"3\":\"0.4326439\",\"4\":\"1.297778e-96\",\"_rn_\":\"Q2-Q13\"},{\"1\":\"0.4702225\",\"2\":\"0.5002339\",\"3\":\"0.5290898\",\"4\":\"3.969981e-158\",\"_rn_\":\"Q2-Q14\"},{\"1\":\"0.3659505\",\"2\":\"0.3994560\",\"3\":\"0.4319274\",\"4\":\"3.129498e-96\",\"_rn_\":\"Q2-Q15\"},{\"1\":\"0.4693335\",\"2\":\"0.4993781\",\"3\":\"0.5282679\",\"4\":\"1.647531e-157\",\"_rn_\":\"Q3-Q4\"},{\"1\":\"0.3653695\",\"2\":\"0.3988923\",\"3\":\"0.4313817\",\"4\":\"6.108182e-96\",\"_rn_\":\"Q3-Q5\"},{\"1\":\"0.3667467\",\"2\":\"0.4002287\",\"3\":\"0.4326752\",\"4\":\"1.248831e-96\",\"_rn_\":\"Q3-Q6\"},{\"1\":\"0.3904688\",\"2\":\"0.4232258\",\"3\":\"0.4549125\",\"4\":\"5.318107e-109\",\"_rn_\":\"Q3-Q7\"},{\"1\":\"0.7683496\",\"2\":\"0.7839542\",\"3\":\"0.7986273\",\"4\":\"0.000000e+00\",\"_rn_\":\"Q3-Q8\"},{\"1\":\"0.4601647\",\"2\":\"0.4905484\",\"3\":\"0.5197845\",\"4\":\"3.104324e-151\",\"_rn_\":\"Q3-Q9\"},{\"1\":\"0.2853436\",\"2\":\"0.3209913\",\"3\":\"0.3557521\",\"4\":\"6.760094e-61\",\"_rn_\":\"Q3-Q10\"},{\"1\":\"0.3066780\",\"2\":\"0.3418064\",\"3\":\"0.3760050\",\"4\":\"2.580298e-69\",\"_rn_\":\"Q3-Q11\"},{\"1\":\"0.4061739\",\"2\":\"0.4384278\",\"3\":\"0.4695906\",\"4\":\"1.005698e-117\",\"_rn_\":\"Q3-Q12\"},{\"1\":\"0.6919756\",\"2\":\"0.7118867\",\"3\":\"0.7307155\",\"4\":\"0.000000e+00\",\"_rn_\":\"Q3-Q13\"},{\"1\":\"0.3989973\",\"2\":\"0.4314834\",\"3\":\"0.4628876\",\"4\":\"1.103607e-113\",\"_rn_\":\"Q3-Q14\"},{\"1\":\"0.2285790\",\"2\":\"0.2654400\",\"3\":\"0.3015410\",\"4\":\"1.676489e-41\",\"_rn_\":\"Q3-Q15\"},{\"1\":\"0.5438704\",\"2\":\"0.5709273\",\"3\":\"0.5967985\",\"4\":\"8.032837e-216\",\"_rn_\":\"Q4-Q5\"},{\"1\":\"0.5837641\",\"2\":\"0.6090539\",\"3\":\"0.6331630\",\"4\":\"3.037754e-253\",\"_rn_\":\"Q4-Q6\"},{\"1\":\"0.5394959\",\"2\":\"0.5667395\",\"3\":\"0.5927977\",\"4\":\"5.260693e-212\",\"_rn_\":\"Q4-Q7\"},{\"1\":\"0.4589969\",\"2\":\"0.4894234\",\"3\":\"0.5187032\",\"4\":\"1.898345e-150\",\"_rn_\":\"Q4-Q8\"},{\"1\":\"0.5374441\",\"2\":\"0.5647747\",\"3\":\"0.5909202\",\"4\":\"3.108940e-210\",\"_rn_\":\"Q4-Q9\"},{\"1\":\"0.3706739\",\"2\":\"0.4040387\",\"3\":\"0.4363621\",\"4\":\"1.298818e-98\",\"_rn_\":\"Q4-Q10\"},{\"1\":\"0.4932765\",\"2\":\"0.5224062\",\"3\":\"0.5503655\",\"4\":\"9.037626e-175\",\"_rn_\":\"Q4-Q11\"},{\"1\":\"0.5749365\",\"2\":\"0.6006273\",\"3\":\"0.6251350\",\"4\":\"1.550592e-244\",\"_rn_\":\"Q4-Q12\"},{\"1\":\"0.4833700\",\"2\":\"0.5128834\",\"3\":\"0.5412322\",\"4\":\"1.798859e-167\",\"_rn_\":\"Q4-Q13\"},{\"1\":\"0.5782876\",\"2\":\"0.6038268\",\"3\":\"0.6281838\",\"4\":\"8.219433e-248\",\"_rn_\":\"Q4-Q14\"},{\"1\":\"0.3607035\",\"2\":\"0.3943633\",\"3\":\"0.4269973\",\"4\":\"1.255385e-93\",\"_rn_\":\"Q4-Q15\"},{\"1\":\"0.4650551\",\"2\":\"0.4952588\",\"3\":\"0.5243108\",\"4\":\"1.470071e-154\",\"_rn_\":\"Q5-Q6\"},{\"1\":\"0.4142088\",\"2\":\"0.4461981\",\"3\":\"0.4770864\",\"4\":\"2.336455e-122\",\"_rn_\":\"Q5-Q7\"},{\"1\":\"0.3765708\",\"2\":\"0.4097576\",\"3\":\"0.4418941\",\"4\":\"1.224898e-101\",\"_rn_\":\"Q5-Q8\"},{\"1\":\"0.4328328\",\"2\":\"0.4641904\",\"3\":\"0.4944261\",\"4\":\"1.462606e-133\",\"_rn_\":\"Q5-Q9\"},{\"1\":\"0.3951535\",\"2\":\"0.4277623\",\"3\":\"0.4592945\",\"4\":\"1.476449e-111\",\"_rn_\":\"Q5-Q10\"},{\"1\":\"0.4632435\",\"2\":\"0.4935141\",\"3\":\"0.5226345\",\"4\":\"2.540009e-153\",\"_rn_\":\"Q5-Q11\"},{\"1\":\"0.4617541\",\"2\":\"0.4920795\",\"3\":\"0.5212560\",\"4\":\"2.612164e-152\",\"_rn_\":\"Q5-Q12\"},{\"1\":\"0.3953385\",\"2\":\"0.4279414\",\"3\":\"0.4594675\",\"4\":\"1.168044e-111\",\"_rn_\":\"Q5-Q13\"},{\"1\":\"0.5106389\",\"2\":\"0.5390785\",\"3\":\"0.5663399\",\"4\":\"4.008639e-188\",\"_rn_\":\"Q5-Q14\"},{\"1\":\"0.4189383\",\"2\":\"0.4507697\",\"3\":\"0.4814945\",\"4\":\"3.849304e-125\",\"_rn_\":\"Q5-Q15\"},{\"1\":\"0.5120337\",\"2\":\"0.5404170\",\"3\":\"0.5676214\",\"4\":\"3.149502e-189\",\"_rn_\":\"Q6-Q7\"},{\"1\":\"0.3794902\",\"2\":\"0.4125879\",\"3\":\"0.4446310\",\"4\":\"3.704932e-103\",\"_rn_\":\"Q6-Q8\"},{\"1\":\"0.4534992\",\"2\":\"0.4841255\",\"3\":\"0.5136099\",\"4\":\"8.755609e-147\",\"_rn_\":\"Q6-Q9\"},{\"1\":\"0.3747259\",\"2\":\"0.4079687\",\"3\":\"0.4401639\",\"4\":\"1.098605e-100\",\"_rn_\":\"Q6-Q10\"},{\"1\":\"0.5981808\",\"2\":\"0.6228032\",\"3\":\"0.6462508\",\"4\":\"4.981130e-268\",\"_rn_\":\"Q6-Q11\"},{\"1\":\"0.5610551\",\"2\":\"0.5873651\",\"3\":\"0.6124896\",\"4\":\"2.403960e-231\",\"_rn_\":\"Q6-Q12\"},{\"1\":\"0.3915639\",\"2\":\"0.4242864\",\"3\":\"0.4559371\",\"4\":\"1.353745e-109\",\"_rn_\":\"Q6-Q13\"},{\"1\":\"0.5190730\",\"2\":\"0.5471694\",\"3\":\"0.5740847\",\"4\":\"7.044729e-195\",\"_rn_\":\"Q6-Q14\"},{\"1\":\"0.4345044\",\"2\":\"0.4658040\",\"3\":\"0.4959800\",\"4\":\"1.340087e-134\",\"_rn_\":\"Q6-Q15\"},{\"1\":\"0.3795181\",\"2\":\"0.4126150\",\"3\":\"0.4446571\",\"4\":\"3.582463e-103\",\"_rn_\":\"Q7-Q8\"},{\"1\":\"0.5001718\",\"2\":\"0.5290301\",\"3\":\"0.5567146\",\"4\":\"5.497267e-180\",\"_rn_\":\"Q7-Q9\"},{\"1\":\"0.3547857\",\"2\":\"0.3886172\",\"3\":\"0.4214323\",\"4\":\"9.585238e-91\",\"_rn_\":\"Q7-Q10\"},{\"1\":\"0.4624648\",\"2\":\"0.4927641\",\"3\":\"0.5219138\",\"4\":\"8.601767e-153\",\"_rn_\":\"Q7-Q11\"},{\"1\":\"0.7365820\",\"2\":\"0.7540288\",\"3\":\"0.7704729\",\"4\":\"0.000000e+00\",\"_rn_\":\"Q7-Q12\"},{\"1\":\"0.4167211\",\"2\":\"0.4486267\",\"3\":\"0.4794284\",\"4\":\"7.858024e-124\",\"_rn_\":\"Q7-Q13\"},{\"1\":\"0.4867147\",\"2\":\"0.5160994\",\"3\":\"0.5443174\",\"4\":\"6.545724e-170\",\"_rn_\":\"Q7-Q14\"},{\"1\":\"0.3549662\",\"2\":\"0.3887925\",\"3\":\"0.4216021\",\"4\":\"7.843219e-91\",\"_rn_\":\"Q7-Q15\"},{\"1\":\"0.4490408\",\"2\":\"0.4798277\",\"3\":\"0.5094765\",\"4\":\"7.372509e-144\",\"_rn_\":\"Q8-Q9\"},{\"1\":\"0.3302521\",\"2\":\"0.3647668\",\"3\":\"0.3983073\",\"4\":\"2.223163e-79\",\"_rn_\":\"Q8-Q10\"},{\"1\":\"0.3367608\",\"2\":\"0.3710987\",\"3\":\"0.4044508\",\"4\":\"2.643708e-82\",\"_rn_\":\"Q8-Q11\"},{\"1\":\"0.3904041\",\"2\":\"0.4231631\",\"3\":\"0.4548520\",\"4\":\"5.764588e-109\",\"_rn_\":\"Q8-Q12\"},{\"1\":\"0.7398147\",\"2\":\"0.7570774\",\"3\":\"0.7733440\",\"4\":\"0.000000e+00\",\"_rn_\":\"Q8-Q13\"},{\"1\":\"0.4175690\",\"2\":\"0.4494462\",\"3\":\"0.4802185\",\"4\":\"2.485870e-124\",\"_rn_\":\"Q8-Q14\"},{\"1\":\"0.2696028\",\"2\":\"0.3056115\",\"3\":\"0.3407668\",\"4\":\"4.328489e-55\",\"_rn_\":\"Q8-Q15\"},{\"1\":\"0.3321729\",\"2\":\"0.3666358\",\"3\":\"0.4001210\",\"4\":\"3.093874e-80\",\"_rn_\":\"Q9-Q10\"},{\"1\":\"0.4294991\",\"2\":\"0.4609717\",\"3\":\"0.4913259\",\"4\":\"1.655959e-131\",\"_rn_\":\"Q9-Q11\"},{\"1\":\"0.5439334\",\"2\":\"0.5709876\",\"3\":\"0.5968561\",\"4\":\"7.071640e-216\",\"_rn_\":\"Q9-Q12\"},{\"1\":\"0.5083417\",\"2\":\"0.5368740\",\"3\":\"0.5642288\",\"4\":\"2.581509e-186\",\"_rn_\":\"Q9-Q13\"},{\"1\":\"0.5268510\",\"2\":\"0.5546263\",\"3\":\"0.5812182\",\"4\":\"2.858794e-201\",\"_rn_\":\"Q9-Q14\"},{\"1\":\"0.2858045\",\"2\":\"0.3214413\",\"3\":\"0.3561903\",\"4\":\"4.516727e-61\",\"_rn_\":\"Q9-Q15\"},{\"1\":\"0.4188025\",\"2\":\"0.4506384\",\"3\":\"0.4813679\",\"4\":\"4.633595e-125\",\"_rn_\":\"Q10-Q11\"},{\"1\":\"0.3705162\",\"2\":\"0.4038857\",\"3\":\"0.4362140\",\"4\":\"1.561964e-98\",\"_rn_\":\"Q10-Q12\"},{\"1\":\"0.3339871\",\"2\":\"0.3684007\",\"3\":\"0.4018335\",\"4\":\"4.747674e-81\",\"_rn_\":\"Q10-Q13\"},{\"1\":\"0.3815258\",\"2\":\"0.4145611\",\"3\":\"0.4465387\",\"4\":\"3.168647e-104\",\"_rn_\":\"Q10-Q14\"},{\"1\":\"0.4176732\",\"2\":\"0.4495470\",\"3\":\"0.4803157\",\"4\":\"2.157366e-124\",\"_rn_\":\"Q10-Q15\"},{\"1\":\"0.5197817\",\"2\":\"0.5478491\",\"3\":\"0.5747350\",\"4\":\"1.870488e-195\",\"_rn_\":\"Q11-Q12\"},{\"1\":\"0.3651436\",\"2\":\"0.3986730\",\"3\":\"0.4311694\",\"4\":\"7.920246e-96\",\"_rn_\":\"Q11-Q13\"},{\"1\":\"0.5342028\",\"2\":\"0.5616704\",\"3\":\"0.5879532\",\"4\":\"1.851647e-207\",\"_rn_\":\"Q11-Q14\"},{\"1\":\"0.5111332\",\"2\":\"0.5395529\",\"3\":\"0.5667941\",\"4\":\"1.629274e-188\",\"_rn_\":\"Q11-Q15\"},{\"1\":\"0.4545949\",\"2\":\"0.4851817\",\"3\":\"0.5146255\",\"4\":\"1.648258e-147\",\"_rn_\":\"Q12-Q13\"},{\"1\":\"0.5354702\",\"2\":\"0.5628844\",\"3\":\"0.5891136\",\"4\":\"1.534183e-208\",\"_rn_\":\"Q12-Q14\"},{\"1\":\"0.3766079\",\"2\":\"0.4097936\",\"3\":\"0.4419289\",\"4\":\"1.171847e-101\",\"_rn_\":\"Q12-Q15\"},{\"1\":\"0.4667464\",\"2\":\"0.4968874\",\"3\":\"0.5258754\",\"4\":\"1.013162e-155\",\"_rn_\":\"Q13-Q14\"},{\"1\":\"0.2625236\",\"2\":\"0.2986885\",\"3\":\"0.3340155\",\"4\":\"1.375737e-52\",\"_rn_\":\"Q13-Q15\"},{\"1\":\"0.4302457\",\"2\":\"0.4616926\",\"3\":\"0.4920203\",\"4\":\"5.766839e-132\",\"_rn_\":\"Q14-Q15\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nNice job! Now you can view variables' correlations as well as p-values and confidence intervals for those correlations.\n\n## Internal reliability\n\nYou know how to examine how individual items perform in your measure, but what about how well those items relate to each other - the overall internal reliability of a measure? Coefficient alpha (also called Cronbach's alpha) and split-half reliability are two common ways of assessing reliability. These statistics are a function of the measure length and items' interrelatedness, which you just investigated by looking at the correlation matrix.\n\nIn reliability values greater than 0.8 are desired, though some fields of study have higher or lower guidelines.\n\n**Do the results of `alpha()` and `splitHalf()` indicate the `gcbs` dataset has acceptable reliability?**\n\n**Steps**\n\n1. Use `alpha()` to get estimates of coefficient alpha and inspect the results. Be sure to check out the information about how reliability would change if items were dropped.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Estimate coefficient alpha\nalpha(gcbs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Reliability analysis   \n#> Call: alpha(x = gcbs)\n#> \n#>   raw_alpha std.alpha G6(smc) average_r S/N   ase mean sd median_r\n#>       0.93      0.93    0.94      0.48  14 0.002  2.9  1     0.47\n#> \n#>     95% confidence boundaries \n#>          lower alpha upper\n#> Feldt     0.93  0.93  0.94\n#> Duhachek  0.93  0.93  0.94\n#> \n#>  Reliability if an item is dropped:\n#>     raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\n#> Q1       0.93      0.93    0.94      0.48  13   0.0021 0.0105  0.46\n#> Q2       0.93      0.93    0.94      0.48  13   0.0021 0.0099  0.47\n#> Q3       0.93      0.93    0.94      0.49  13   0.0020 0.0084  0.48\n#> Q4       0.93      0.93    0.94      0.47  13   0.0022 0.0105  0.46\n#> Q5       0.93      0.93    0.94      0.48  13   0.0021 0.0112  0.48\n#> Q6       0.93      0.93    0.94      0.48  13   0.0021 0.0104  0.46\n#> Q7       0.93      0.93    0.94      0.48  13   0.0021 0.0098  0.47\n#> Q8       0.93      0.93    0.94      0.48  13   0.0020 0.0086  0.49\n#> Q9       0.93      0.93    0.94      0.48  13   0.0021 0.0108  0.46\n#> Q10      0.93      0.93    0.94      0.49  14   0.0020 0.0102  0.49\n#> Q11      0.93      0.93    0.94      0.48  13   0.0021 0.0104  0.46\n#> Q12      0.93      0.93    0.94      0.47  13   0.0022 0.0093  0.46\n#> Q13      0.93      0.93    0.94      0.48  13   0.0021 0.0092  0.48\n#> Q14      0.93      0.93    0.94      0.48  13   0.0021 0.0109  0.46\n#> Q15      0.93      0.93    0.94      0.49  14   0.0020 0.0095  0.49\n#> \n#>  Item statistics \n#>        n raw.r std.r r.cor r.drop mean  sd\n#> Q1  2495  0.73  0.73  0.70   0.68  3.5 1.5\n#> Q2  2495  0.74  0.74  0.72   0.69  3.0 1.5\n#> Q3  2495  0.68  0.67  0.66   0.62  2.0 1.4\n#> Q4  2495  0.78  0.78  0.76   0.74  2.6 1.5\n#> Q5  2495  0.70  0.70  0.67   0.65  3.3 1.5\n#> Q6  2495  0.76  0.76  0.74   0.72  3.1 1.5\n#> Q7  2495  0.75  0.75  0.73   0.70  2.7 1.5\n#> Q8  2495  0.69  0.69  0.68   0.63  2.5 1.6\n#> Q9  2495  0.72  0.72  0.69   0.67  2.2 1.4\n#> Q10 2495  0.61  0.61  0.57   0.55  3.5 1.4\n#> Q11 2495  0.74  0.74  0.72   0.69  3.3 1.4\n#> Q12 2495  0.79  0.79  0.79   0.75  2.6 1.5\n#> Q13 2495  0.71  0.71  0.70   0.66  2.1 1.4\n#> Q14 2495  0.76  0.76  0.74   0.71  3.0 1.5\n#> Q15 2495  0.60  0.62  0.58   0.56  4.2 1.1\n#> \n#> Non missing response frequency for each item\n#>        0    1    2    3    4    5 miss\n#> Q1  0.00 0.16 0.12 0.12 0.27 0.32    0\n#> Q2  0.01 0.23 0.19 0.16 0.20 0.22    0\n#> Q3  0.00 0.55 0.13 0.12 0.10 0.10    0\n#> Q4  0.00 0.32 0.18 0.15 0.20 0.14    0\n#> Q5  0.00 0.19 0.14 0.13 0.28 0.26    0\n#> Q6  0.00 0.23 0.15 0.15 0.23 0.24    0\n#> Q7  0.00 0.33 0.19 0.13 0.18 0.17    0\n#> Q8  0.00 0.44 0.12 0.14 0.12 0.18    0\n#> Q9  0.00 0.45 0.19 0.12 0.12 0.11    0\n#> Q10 0.00 0.14 0.12 0.14 0.30 0.30    0\n#> Q11 0.00 0.16 0.14 0.19 0.27 0.24    0\n#> Q12 0.00 0.34 0.18 0.15 0.17 0.17    0\n#> Q13 0.01 0.51 0.15 0.15 0.10 0.09    0\n#> Q14 0.00 0.25 0.17 0.15 0.22 0.20    0\n#> Q15 0.00 0.05 0.05 0.08 0.27 0.55    0\n```\n:::\n:::\n\n\n2. Use `splitHalf()` to estimate split-half reliability.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Estimate coefficient alpha\nalpha(gcbs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Reliability analysis   \n#> Call: alpha(x = gcbs)\n#> \n#>   raw_alpha std.alpha G6(smc) average_r S/N   ase mean sd median_r\n#>       0.93      0.93    0.94      0.48  14 0.002  2.9  1     0.47\n#> \n#>     95% confidence boundaries \n#>          lower alpha upper\n#> Feldt     0.93  0.93  0.94\n#> Duhachek  0.93  0.93  0.94\n#> \n#>  Reliability if an item is dropped:\n#>     raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\n#> Q1       0.93      0.93    0.94      0.48  13   0.0021 0.0105  0.46\n#> Q2       0.93      0.93    0.94      0.48  13   0.0021 0.0099  0.47\n#> Q3       0.93      0.93    0.94      0.49  13   0.0020 0.0084  0.48\n#> Q4       0.93      0.93    0.94      0.47  13   0.0022 0.0105  0.46\n#> Q5       0.93      0.93    0.94      0.48  13   0.0021 0.0112  0.48\n#> Q6       0.93      0.93    0.94      0.48  13   0.0021 0.0104  0.46\n#> Q7       0.93      0.93    0.94      0.48  13   0.0021 0.0098  0.47\n#> Q8       0.93      0.93    0.94      0.48  13   0.0020 0.0086  0.49\n#> Q9       0.93      0.93    0.94      0.48  13   0.0021 0.0108  0.46\n#> Q10      0.93      0.93    0.94      0.49  14   0.0020 0.0102  0.49\n#> Q11      0.93      0.93    0.94      0.48  13   0.0021 0.0104  0.46\n#> Q12      0.93      0.93    0.94      0.47  13   0.0022 0.0093  0.46\n#> Q13      0.93      0.93    0.94      0.48  13   0.0021 0.0092  0.48\n#> Q14      0.93      0.93    0.94      0.48  13   0.0021 0.0109  0.46\n#> Q15      0.93      0.93    0.94      0.49  14   0.0020 0.0095  0.49\n#> \n#>  Item statistics \n#>        n raw.r std.r r.cor r.drop mean  sd\n#> Q1  2495  0.73  0.73  0.70   0.68  3.5 1.5\n#> Q2  2495  0.74  0.74  0.72   0.69  3.0 1.5\n#> Q3  2495  0.68  0.67  0.66   0.62  2.0 1.4\n#> Q4  2495  0.78  0.78  0.76   0.74  2.6 1.5\n#> Q5  2495  0.70  0.70  0.67   0.65  3.3 1.5\n#> Q6  2495  0.76  0.76  0.74   0.72  3.1 1.5\n#> Q7  2495  0.75  0.75  0.73   0.70  2.7 1.5\n#> Q8  2495  0.69  0.69  0.68   0.63  2.5 1.6\n#> Q9  2495  0.72  0.72  0.69   0.67  2.2 1.4\n#> Q10 2495  0.61  0.61  0.57   0.55  3.5 1.4\n#> Q11 2495  0.74  0.74  0.72   0.69  3.3 1.4\n#> Q12 2495  0.79  0.79  0.79   0.75  2.6 1.5\n#> Q13 2495  0.71  0.71  0.70   0.66  2.1 1.4\n#> Q14 2495  0.76  0.76  0.74   0.71  3.0 1.5\n#> Q15 2495  0.60  0.62  0.58   0.56  4.2 1.1\n#> \n#> Non missing response frequency for each item\n#>        0    1    2    3    4    5 miss\n#> Q1  0.00 0.16 0.12 0.12 0.27 0.32    0\n#> Q2  0.01 0.23 0.19 0.16 0.20 0.22    0\n#> Q3  0.00 0.55 0.13 0.12 0.10 0.10    0\n#> Q4  0.00 0.32 0.18 0.15 0.20 0.14    0\n#> Q5  0.00 0.19 0.14 0.13 0.28 0.26    0\n#> Q6  0.00 0.23 0.15 0.15 0.23 0.24    0\n#> Q7  0.00 0.33 0.19 0.13 0.18 0.17    0\n#> Q8  0.00 0.44 0.12 0.14 0.12 0.18    0\n#> Q9  0.00 0.45 0.19 0.12 0.12 0.11    0\n#> Q10 0.00 0.14 0.12 0.14 0.30 0.30    0\n#> Q11 0.00 0.16 0.14 0.19 0.27 0.24    0\n#> Q12 0.00 0.34 0.18 0.15 0.17 0.17    0\n#> Q13 0.01 0.51 0.15 0.15 0.10 0.09    0\n#> Q14 0.00 0.25 0.17 0.15 0.22 0.20    0\n#> Q15 0.00 0.05 0.05 0.08 0.27 0.55    0\n```\n:::\n\n```{.r .cell-code}\n# Calculate split-half reliability\nsplitHalf(gcbs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Split half reliabilities  \n#> Call: splitHalf(r = gcbs)\n#> \n#> Maximum split half reliability (lambda 4) =  0.95\n#> Guttman lambda 6                          =  0.94\n#> Average split half reliability            =  0.93\n#> Guttman lambda 3 (alpha)                  =  0.93\n#> Guttman lambda 2                          =  0.93\n#> Minimum split half reliability  (beta)    =  0.86\n#> Average interitem r =  0.48  with median =  0.47\n```\n:::\n:::\n\n\n> *Question*\n> ---\n> <br>\n> <br>\n> ✅ Yes, both indices are above 0.8.<br>\n> ⬜ No, the coefficient alpha is below 0.8.<br>\n> ⬜ Yes, both indices are below 0.8.<br>\n> ⬜ Yes, the split-half reliability is below0.8.<br>\n\nThat's it! Both indices are well over 0.8. These are great results that would be a good candidate for publication.\n\n## When to use EFA\n\nFor which of these measure development situations would an exploratory factor analysis be appropriate?\n\n> *Question*\n> ---\n> ???<br>\n> <br>\n> ⬜ You are piloting a measure, and the items are clearly assigned to factors based on theory.<br>\n> ✅ You are piloting a measure, but you're not sure which factor(s) each item measures.<br>\n> ⬜ You are finalizing a measure that has been previously piloted.<br>\n> ⬜ You are drafting items for a new measure.<br>\n\nThat's right! Exploratory factor analysis is great when you're not sure about which factor(s) each item should map to.\n\n# 2. Multidimensional EFA\n\nThis chapter will show you how to extend the single-factor EFA you learned in Chapter 1 to multidimensional data.\n\n## Determining dimensionality\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Determining dimensionality**\n\nYou now know how to do a single-factor EFA, which is useful for seeing how each item relates to a single hypothesized factor. However, you may be wondering, \"Haven't I also heard about EFA as a method for dimensionality reduction?\" If so, you're right! In the context of measure development, we think of this as figuring out how many unobservable factors are represented by the items on the measure.\n\n**2. How many dimensions does your data have?**\n\nIn a truly exploratory situation, you may not know which factors the items on your measure are related to - or even how many factors are represented by the items on your measure.\n\n**3. The bfi dataset**\n\nTo switch things up a bit, you'll be using the Big Five Inventory, or bfi, dataset, which contains 2,800 subjects' responses to 25 questions.\n\n**4. Big five personality taits**\n\nFive questions measure each of the big five personality traits: extraversion, agreeableness, openness, conscientiousness, and neuroticism. These \"big 5\" traits have been extensively studied and are agreed upon by many personality researchers.The bfi dataset in the psych package includes three demographic variables. Since your analyses only work with item response data, I've removed those for you. Though we clearly already know the scientific theory behind this measure and dataset, we'll pretend we don't have this information for the purpose of learning how to run EFAs.\n\n**5. The bfi dataset**\n\nIf you use head() to look at the first six rows of the data, you can see that item responses range from 1 to 6, representing respondents' ratings on a six-point scale ranging from Very Inaccurate to Very Accurate. You'll also notice that the column names each consist of a letter and a number. These indicate the personality trait that the item is hypothesized to measure and the question number. For example, A1 is the first question for the agreeableness trait.\n\n**6. Setup: split your dataset**\n\nYou'll remember from Chapter 1 that when you are going to use the same dataset for both exploratory and confirmatory analyses, it's important to split the data. Using the same dataset for both can result in overfitting. We'll use the same splitting strategy as in Chapter 1. Here, you'll use one-half of the data for the EFA and the other for the CFA.\n\n**7. Setup: split your dataset**\n\nChecking the output from head() verifies you've created two distinct halves.\n\n**8. An empirical approach to dimensionality**\n\nThough we know the theorized factors for the bfi dataset, you may not always have a theory to guide analysis. In the absence of theory, you can use an empirical approach to quantify dimensionality.To figure out the number of factors the items represent, you can look at eigenvalues, which are a way of quantifying the unique factors within a correlation matrix. We'll work through an applied example of using eigenvalues next.\n\n**9. Calculate the correlation matrix**\n\nEigenvalues are calculated from matrices, so our first step is to calculate the correlation matrix. Note that we are using the half of the dataset we separated out for the EFA. Also, the correlation matrix is calculated using observations that are pairwise complete.\n\n**10. Eigenvalues**\n\nOnce you've got the correlation matrix, you can use it with eigen() to calculate the eigenvalues. The result is a list object containing several pieces of information. Check out the values element to view the eigenvalues.A general rule is that eigenvalues greater than 1 represent meaningful factors. You can count these values in the results, but there's also a quick way to visualize this information.\n\n**11. Scree plots**\n\nYou can visualize eigenvalues with the scree plot created by using the scree() function on a correlation matrix.\n\n**12. Scree plots**\n\nYou'll get output like this, complete with a horizontal line to help you count the values greater than 1.\n\n**13. Let's practice!**\n\nLet's go try this out!\n\n## Splitting the BFI dataset\n\nFor this chapter, you'll be using the `bfi` dataset, which consists of responses to 25 items measuring the Big Five personality traits. I've trimmed the dataset down to only the item responses for your use. Since you'll be doing both exploratory and confirmatory factor analyses on this data, you'll want to start out by splitting the dataset. You'll use the same process that you learned in Chapter 1 when you split the `gcbs` dataset.\n\n**Steps**\n\n1. Split `bfi` in half using two sets of indices (`indices_EFA` and `indices_CFA`) to determine which rows belong to each dataset. \n2. Use the first set of indices to create a dataset for your EFA, then use the second set for your CFA dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Establish two sets of indices to split the dataset\ndata(\"bfi\")\nbfi <- bfi |> dplyr::select(-c(gender, education, age))\n\nN <- nrow(bfi)\nindices <- seq(1, N)\nindices_EFA <- sample(indices, floor((.5*N)))\nindices_CFA <- indices[!(indices %in% indices_EFA)]\n\n# Use those indices to split the dataset into halves for your EFA and CFA\nbfi_EFA <- bfi[indices_EFA, ]\nbfi_CFA <- bfi[indices_CFA, ]\n```\n:::\n\n\nNice job remembering how to split your dataset! This is important for ensuring you have different data for your exploratory and confirmatory analyses.\n\n## Calculating eigenvalues\n\nTo empirically determine the dimensionality of your data, a common strategy is to examine the eigenvalues. Eigenvalues are numeric representations of the amount of variance explained by each factor or component. Eigenvalues are calculated from a correlation matrix, so you'll need to use `cor()` to calculate and store the dataset's correlation matrix before calculating eigenvalues. You'll need to specify that you want to use pairwise complete observations. The default is to use everything, but if your dataset has any missing values, this will leave you with a matrix full of NAs.\n\nYou'll do these calculations on the `bfi_EFA` dataset you just created - remember, you're saving the data in `bfi_CFA` for your confirmatory analysis!\n\n**Steps**\n\n1. Use `cor()` to calculate the correlation matrix for your EFA dataset. Set the value of the `use` argument to use pairwise-complete observations.\n2. Next, use that correlation matrix with the `eigen()` function to get eigenvalues.\n3. The eigenvalues are stored in the `values` element of the `eigenvals` list object. Take a look!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the correlation matrix first\nbfi_EFA_cor <- cor(bfi_EFA, use = \"pairwise.complete.obs\")\n\n# Then use that correlation matrix to calculate eigenvalues\neigenvals <- eigen(bfi_EFA_cor)\n\n# Look at the eigenvalues returned\neigenvals$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  [1] 4.8527269 2.7697191 2.1130667 1.8949025 1.5919481 1.0891653 0.8441259\n#>  [8] 0.8247894 0.7315975 0.7067286 0.6902688 0.6669863 0.6436505 0.6168723\n#> [15] 0.5734700 0.5545144 0.5364503 0.4943370 0.4873432 0.4596331 0.4331922\n#> [22] 0.3985455 0.3906137 0.3648371 0.2705157\n```\n:::\n:::\n\n\nGreat work! Now you know how to use the correlation matrix to calculate eigenvalues from a dataset.\n\n## Creating a scree plot\n\nThe scree plot is a visual representation of eigenvalues. Visual inspection of the scree plot is a quick and easy way to get a feel for the dimensionality of your dataset. Like the `eigen()` function in the previous exercise, the `scree()` function takes a correlation matrix as its argument. Eigenvalues can be generated from a principal component analysis or a factor analysis, and the `scree()` function calculates and plots both by default. Since `eigen()` finds eigenvalues via principal components analysis, we will use `factors = FALSE` so our scree plot will only display the values corresponding to those results.\n\n**Steps**\n\n1. Once again, calculate the correlation matrix.\n2. Next, use that correlation matrix function to create a scree plot.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the correlation matrix first\nbfi_EFA_cor <- cor(bfi_EFA, use = \"pairwise.complete.obs\")\n\n# Then use that correlation matrix to create the scree plot\nscree(bfi_EFA_cor, factors = FALSE)\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNice job! Now you can create a scree plot from the eigenvalues.\n\n## Interpreting the scree plot\n\nA commonly used criterion for selecting the optimal number of factors is to only consider factors with eigenvalues greater than 1. `scree()` includes a solid horizontal line at 1 on the y-axis to help you quickly interpret your results. Run the code below to recreate the scree plot from the `bfi_EFA` data you created in the previous exercise. Based on the results, how many factors are recommended?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the correlation matrix\nbfi_EFA_cor <- cor(bfi_EFA, use = \"pairwise.complete.obs\")\n\n# Use the correlation matrix to create the scree plot\nscree(bfi_EFA_cor, factors = FALSE)\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n> *Question*\n> ---\n> ???<br>\n> <br>\n> ⬜ 3 factors<br>\n> ⬜ 4 factors<br>\n> ⬜ 5 factors<br>\n> ✅ 6 factors<br>\n\nRight! The scree plot recommends six factors for this data. \n\n## Understanding multidimensional data\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Multidimensionality: What does it mean?**\n\nSo, now you're working with multidimensional data - but what does that mean? What does the number of factors tell you, and why are you getting conflicting information from theory and empirical analyses?\n\n**2. Factors = constructs**\n\nThe answers are in the relationship of theory to statistics. A construct is a hypothesized attribute that can't be directly observed or measured. Examples of commonly studied constructs include self-determination, reasoning ability, and extraversion.\n\n**3. Factors = constructs**\n\nTypically, measures are designed to measure specific constructs. While constructs are theory-driven, factors are their mathematical counterparts. Each factor corresponds to a construct.\n\n**4. Interpreting confirmatory analyses**\n\nWhen you conduct a confirmatory analysis, you evaluate the strength of the hypothesized relationships between items and the constructs they were designed to measure.Model fit statistics provide information about how well the hypothesis fits the data, and factor loadings quantify the relationships between items and constructs for reporting and interpretability. We'll talk about this more in the following chapters.\n\n**5. Interpreting exploratory analyses**\n\nWithout theory, you conduct exploratory analyses guided by mathematical values like eigenvalues. You'll continue the exploration by running a multidimensional EFA to see how the items relate to each of those factors.While all of this information can be very helpful in a situation where you don't know much about your data, a lack of theoretical grounding can make these results very difficult to interpret. You can make an educated guess from the factor loadings resulting from your EFA, but this can be challenging.\n\n**6. Running a multidimensional EFA**\n\nRunning a multidimensional EFA works just like a unidimensional EFA. All that you have to change is the nfactors argument. In the exercises, you'll go with the number of factors recommended from the scree plot, which was six. As you'll remember, the theory behind the dataset recommended five factors. We'll discuss this disagreement in more detail later and offer some empirical ways to compare different models. Remember, to view a summary of results from the model object, all you have to do is enter the model object on its own line. No need to use the summary argument!\n\n**7. Factor loadings**\n\nAs before, you'll be interested in the factor loadings, which represent each item's relationship to each underlying construct. However, the factor loadings look pretty different for a multidimensional EFA! Here are the loadings for the first 15 items. As you can see, the six factors are represented by the six columns. You'll notice the factors have been assigned arbitrary names, and that they're not in the order you'd expect. This is due to the rotation that happens during the mathematical process behind a multidimensional EFA.You'll also notice that some item/factor pairings don't have loadings. The results automatically exclude negligible factor loadings for ease of interpretation.Interpreting these loadings can be challenging. Unlike a CFA, the factors don't have assigned meanings, so the user is left to infer these meanings based on the patterns of item loadings. For example, we can guess that factor MR5 is closely related to agreeableness since the A items all have strong relationships. We can also guess that item A1 should be inversely scored due to its negative relationship with the factor.\n\n**8. Factor scores**\n\nIndividuals' factor scores also look different. Rather than a single factor score, each person now has a factor score estimated for each of the six factors. As before, any missing data means that factor scores will not be estimated, as we can see with person 65237.These factor scores shouldn't be interpreted until you have a good working hypothesis for what each of the factors in your EFA represents. Once you have that hypothesis, you can make inferences about the constructs.\n\n**9. Let's practice!**\n\nOkay! Now it's your turn to run a multidimensional EFA.\n\n## Conducting a multidimensional EFA\n\nNow that you've examined the eigenvalues and scree plot to find the data-driven recommended number of factors, you can get down to actually running the multidimensional EFA. In Chapter 1, you ran a unidimensional EFA by using the `fa()` function. To run a multidimensional EFA, you'll want to use the `nfactors` argument to specify the number of factors desired.\n\nAs in the previous exercises, you'll want to be sure to run this analysis on the `bfi_EFA` dataset you created earlier.\n\n**Steps**\n\n1. Run a multidimensional EFA using the `fa()` function and the number of factor recommended by the scree plot.  \n2. View your model object in the console.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run the EFA with six factors (as indicated by your scree plot)\nEFA_model <- fa(bfi_EFA, nfactors = 6)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required namespace: GPArotation\n```\n:::\n\n```{.r .cell-code}\n# View results from the model object\nEFA_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Factor Analysis using method =  minres\n#> Call: fa(r = bfi_EFA, nfactors = 6)\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>      MR2   MR1   MR3   MR5   MR4   MR6   h2   u2 com\n#> A1  0.11 -0.12  0.10 -0.57  0.00  0.25 0.33 0.67 1.6\n#> A2  0.06 -0.05  0.07  0.70 -0.01 -0.07 0.50 0.50 1.1\n#> A3 -0.04 -0.13  0.02  0.60  0.06  0.16 0.52 0.48 1.3\n#> A4 -0.07 -0.07  0.19  0.39 -0.15  0.09 0.27 0.73 2.1\n#> A5 -0.16 -0.25  0.00  0.39  0.08  0.28 0.48 0.52 3.1\n#> C1  0.02  0.04  0.55 -0.07  0.15  0.09 0.34 0.66 1.2\n#> C2  0.07  0.14  0.68  0.01  0.07  0.16 0.48 0.52 1.2\n#> C3  0.01  0.05  0.55  0.09 -0.09  0.00 0.31 0.69 1.1\n#> C4  0.09  0.05 -0.58 -0.07 -0.03  0.32 0.51 0.49 1.7\n#> C5  0.16  0.16 -0.56  0.00  0.10  0.09 0.44 0.56 1.5\n#> E1 -0.12  0.60  0.11 -0.10 -0.08  0.05 0.38 0.62 1.3\n#> E2  0.01  0.72  0.01 -0.06 -0.07  0.05 0.56 0.44 1.0\n#> E3  0.01 -0.36  0.05  0.12  0.31  0.32 0.50 0.50 3.3\n#> E4 -0.05 -0.58  0.05  0.15 -0.04  0.28 0.56 0.44 1.6\n#> E5  0.15 -0.42  0.28  0.07  0.22 -0.02 0.42 0.58 2.7\n#> N1  0.83 -0.10  0.00 -0.06 -0.05 -0.02 0.68 0.32 1.0\n#> N2  0.84 -0.04  0.01 -0.02  0.00 -0.10 0.67 0.33 1.0\n#> N3  0.66  0.12 -0.03  0.04  0.04  0.14 0.53 0.47 1.2\n#> N4  0.41  0.40 -0.16  0.06  0.12  0.12 0.47 0.53 2.7\n#> N5  0.41  0.24  0.02  0.15 -0.16  0.21 0.35 0.65 2.9\n#> O1 -0.02 -0.08  0.08 -0.05  0.52  0.07 0.32 0.68 1.2\n#> O2  0.12  0.01 -0.04  0.11 -0.44  0.27 0.30 0.70 2.0\n#> O3 -0.01 -0.12  0.04  0.02  0.62  0.12 0.47 0.53 1.2\n#> O4  0.09  0.36 -0.01  0.16  0.38  0.07 0.26 0.74 2.5\n#> O5  0.04 -0.07  0.02 -0.07 -0.53  0.30 0.36 0.64 1.7\n#> \n#>                        MR2  MR1  MR3  MR5  MR4  MR6\n#> SS loadings           2.42 2.30 2.01 1.78 1.63 0.87\n#> Proportion Var        0.10 0.09 0.08 0.07 0.07 0.03\n#> Cumulative Var        0.10 0.19 0.27 0.34 0.41 0.44\n#> Proportion Explained  0.22 0.21 0.18 0.16 0.15 0.08\n#> Cumulative Proportion 0.22 0.43 0.61 0.77 0.92 1.00\n#> \n#>  With factor correlations of \n#>       MR2   MR1   MR3   MR5   MR4   MR6\n#> MR2  1.00  0.21 -0.18 -0.10  0.01  0.19\n#> MR1  0.21  1.00 -0.22 -0.30 -0.18 -0.11\n#> MR3 -0.18 -0.22  1.00  0.18  0.17  0.01\n#> MR5 -0.10 -0.30  0.18  1.00  0.17  0.19\n#> MR4  0.01 -0.18  0.17  0.17  1.00  0.04\n#> MR6  0.19 -0.11  0.01  0.19  0.04  1.00\n#> \n#> Mean item complexity =  1.7\n#> Test of the hypothesis that 6 factors are sufficient.\n#> \n#> The degrees of freedom for the null model are  300  and the objective function was  7.17 with Chi Square of  9960.27\n#> The degrees of freedom for the model are 165  and the objective function was  0.43 \n#> \n#> The root mean square of the residuals (RMSR) is  0.02 \n#> The df corrected root mean square of the residuals is  0.03 \n#> \n#> The harmonic number of observations is  1383 with the empirical chi square  363.67  with prob <  5.4e-17 \n#> The total number of observations was  1400  with Likelihood Chi Square =  601.33  with prob <  6.4e-51 \n#> \n#> Tucker Lewis Index of factoring reliability =  0.918\n#> RMSEA index =  0.043  and the 90 % confidence intervals are  0.04 0.047\n#> BIC =  -593.96\n#> Fit based upon off diagonal values = 0.99\n#> Measures of factor score adequacy             \n#>                                                    MR2  MR1  MR3  MR5  MR4  MR6\n#> Correlation of (regression) scores with factors   0.93 0.90 0.88 0.87 0.85 0.78\n#> Multiple R square of scores with factors          0.86 0.81 0.77 0.75 0.72 0.61\n#> Minimum correlation of possible factor scores     0.72 0.61 0.55 0.51 0.45 0.21\n```\n:::\n:::\n\n\nNice work! Now you can say you've run a multidimensional EFA!\n\n## Interpreting the results\n\nAs before, you'll be interested in items' factor loadings and individuals' factor scores. These will be interpreted in the same way, but since your EFA is multidimensional, you’ll get results for each factor.\n\nRemember, an item's loadings represent the amount of information it provides for each factor. Items’ meaningful loadings will be displayed in the output. You’ll notice that many items load onto more than one factor, which means they provide information about multiple factors. This may not be desirable for measure development, so some researchers consider **only** the strongest loading for each item. \n\nEach examinee will have a factor score for each factor, so that the matrix won't include blanks. However, examinees with missing data will receive NA scores on all factors.\n\n**Steps**\n\n1. Reinforce your learning from the previous exercise: run that multidimensional EFA again using `fa()`!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run the EFA with six factors (as indicated by your scree plot)\nEFA_model <- fa(bfi_EFA, nfactors = 6)\n```\n:::\n\n\n2. Look at the items' factor loadings, which are stored in the `loadings` list element of the result object.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run the EFA with six factors (as indicated by your scree plot)\nEFA_model <- fa(bfi_EFA, nfactors = 6)\n\n# View items' factor loadings\nEFA_model$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Loadings:\n#>    MR2    MR1    MR3    MR5    MR4    MR6   \n#> A1  0.109 -0.121  0.102 -0.566         0.249\n#> A2                       0.696              \n#> A3        -0.126         0.598         0.159\n#> A4                0.191  0.389 -0.155       \n#> A5 -0.160 -0.251         0.392         0.280\n#> C1                0.555         0.147       \n#> C2         0.139  0.677                0.159\n#> C3                0.552                     \n#> C4               -0.579                0.318\n#> C5  0.157  0.162 -0.558         0.104       \n#> E1 -0.123  0.595  0.114                     \n#> E2         0.723                            \n#> E3        -0.362         0.123  0.315  0.323\n#> E4        -0.581         0.150         0.283\n#> E5  0.150 -0.419  0.279         0.219       \n#> N1  0.835                                   \n#> N2  0.839                             -0.104\n#> N3  0.656  0.118                       0.140\n#> N4  0.408  0.402 -0.158         0.117  0.124\n#> N5  0.408  0.239         0.154 -0.155  0.210\n#> O1                              0.524       \n#> O2  0.120                0.113 -0.444  0.269\n#> O3        -0.117                0.624  0.120\n#> O4         0.361         0.156  0.378       \n#> O5                             -0.525  0.302\n#> \n#>                  MR2   MR1   MR3   MR5   MR4   MR6\n#> SS loadings    2.313 2.074 1.904 1.619 1.561 0.788\n#> Proportion Var 0.093 0.083 0.076 0.065 0.062 0.032\n#> Cumulative Var 0.093 0.175 0.252 0.316 0.379 0.410\n```\n:::\n:::\n\n\n3. Then, use the `head()` function to take a look at the first few rows of examinees' factor scores, which are stored in the `scores` list element.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run the EFA with six factors (as indicated by your scree plot)\nEFA_model <- fa(bfi_EFA, nfactors = 6)\n\n# View items' factor loadings\nEFA_model$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Loadings:\n#>    MR2    MR1    MR3    MR5    MR4    MR6   \n#> A1  0.109 -0.121  0.102 -0.566         0.249\n#> A2                       0.696              \n#> A3        -0.126         0.598         0.159\n#> A4                0.191  0.389 -0.155       \n#> A5 -0.160 -0.251         0.392         0.280\n#> C1                0.555         0.147       \n#> C2         0.139  0.677                0.159\n#> C3                0.552                     \n#> C4               -0.579                0.318\n#> C5  0.157  0.162 -0.558         0.104       \n#> E1 -0.123  0.595  0.114                     \n#> E2         0.723                            \n#> E3        -0.362         0.123  0.315  0.323\n#> E4        -0.581         0.150         0.283\n#> E5  0.150 -0.419  0.279         0.219       \n#> N1  0.835                                   \n#> N2  0.839                             -0.104\n#> N3  0.656  0.118                       0.140\n#> N4  0.408  0.402 -0.158         0.117  0.124\n#> N5  0.408  0.239         0.154 -0.155  0.210\n#> O1                              0.524       \n#> O2  0.120                0.113 -0.444  0.269\n#> O3        -0.117                0.624  0.120\n#> O4         0.361         0.156  0.378       \n#> O5                             -0.525  0.302\n#> \n#>                  MR2   MR1   MR3   MR5   MR4   MR6\n#> SS loadings    2.313 2.074 1.904 1.619 1.561 0.788\n#> Proportion Var 0.093 0.083 0.076 0.065 0.062 0.032\n#> Cumulative Var 0.093 0.175 0.252 0.316 0.379 0.410\n```\n:::\n\n```{.r .cell-code}\n# View the first few lines of examinees' factor scores\nhead(EFA_model$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              MR2        MR1        MR3         MR5        MR4        MR6\n#> 62313  0.8406865  0.8852674 -0.5887522  0.96880250  1.2650167  0.6059067\n#> 65959 -0.1562959 -1.8591286  1.1136915 -0.22193046  0.8253958 -0.5465908\n#> 63937 -1.1359272  1.4233318 -1.0156421 -0.59640201  0.4938164 -1.1275671\n#> 61812         NA         NA         NA          NA         NA         NA\n#> 64622  0.3848288  1.3386226 -0.3472678  0.01100322 -0.8555800 -0.8109960\n#> 62837  0.3961697 -0.4323916  0.1002810  0.49907010  1.1330806 -0.7213974\n```\n:::\n:::\n\n\nYou got the most important pieces of information from your EFA! Nicely done!\n\n## Investigating model fit\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Model fit**\n\nNow that you can run multidimensional EFAs, it's time to learn how to interpret various model fit statistics.\n\n**2. Absolute vs. relative model fit**\n\nWhen thinking about model fit, there are two different types of fit statistics you can consider. Absolute fit statistics are useful for making a judgment about whether or not a model fits adequately. These fit statistics have set ranges and meanings, and they have established cutoff values that are commonly used to determine whether a model has good fit. Examples include the chi-square test, TLI, and RMSEA.Relative fit statistics don't have set ranges or meanings, and they are only useful when comparing nested models estimated from the same dataset. We'll focus on the BIC in this course.\n\n**3. Absolute fit statistics**\n\nWhen it comes down to it, all of the absolute fit statistics attempt to quantify the discrepancy between the observed data and the data that would be expected given the model. These three common fit statistics represent this difference on different scales and are calculated differently.Ideally, the chi-square test would have a non-significant result, meaning the observed and expected data aren't significantly different. However, since the value of this test is affected by sample size, this rarely occurs for large datasets. Both TLI and RMSEA range from 0 to 1. The TLI, or Tucker-Lewis Index, is also known as the penalized non-normal fit index, meaning that more complex models are penalized for adding additional parameters. This can be roughly understood as how well the observed data match the expected data. Larger TLI values are better: a model can be considered to have good fit if the value is greater than 0-point-90. RMSEA, or the Root Mean Square Error of Approximation, quantifies the differences between the observed and expected data. RMSEA values are interpreted in the opposite direction as TLI values: smaller values are preferred. Values less than 0-point-05 can be said to indicate good fit.\n\n**4. Finding the fit statistics**\n\nAll the fit statistics discussed here are displayed in the model output. As you can see, when we run the multidimensional EFA on the bfi_EFA dataset with six factors, the chi-square value of 618-point-43 is significantly different from zero. This isn't ideal, but it's unsurprising given the sample size.You can also see the TLI value is point-916, which is above the cutoff, and the RMSEA is 0-point-045, which is below the cutoff. These are good values that indicate that the model adequately fits the data.You'll notice these values also contain the BIC value for this model, which is -576-point-87. However, since relative fit statistics don't have any value on their own, this doesn't mean much to us yet.\n\n**5. Relative model fit**\n\nTo use relative fit statistics, we need two different models to compare. If you remember, the theory behind the BFI dataset recommended five factors, while the eigenvalues recommended six factors. Let's set up those two models by changing up the nfactors argument.When looking at BICs, the lowest BIC is always preferred. You can see that for these two models, the BIC is lower for the bfi_eigen model, which was estimated with six factors.\n\n**6. In sum: evaluating fit**\n\nTo sum up, when you are in the process of model development, the first step is to make sure your model or models have adequate fit according to the absolute fit statistics.If you are comparing multiple models that all have good fit, you can use relative fit statistics to make an empirical determination about which model is mathematically preferred.\n\n**7. Let's practice!**\n\nNow you know how to evaluate models' absolute and relative fit statistics! Let's apply these principles to your data.\n\n## Interpret absolute model fit statistics\n\nNow that you know the basics of model fit, let's take a look at the absolute fit statistics for your six-factor EFA on the `bfi_EFA` dataset. \n\nThose results show the following absolute fit statistics. Which of these indices meet the criteria for good fit?\n\n\n|Chi-square test |   TLI| RMSEA|\n|:---------------|-----:|-----:|\n|p-value         | 0.916| 0.045|\n\n> *Question*\n> ---\n> ???<br>\n> <br>\n> ⬜ Chi-square test and TLI<br>\n> ✅ TLI and RMSEA<br>\n> ⬜ Only RMSEA<br>\n> ⬜ Only chi-square test<br>\n\nThat's right! The TLI is above the 0.90 cutoff for good fit, and the RMSEA is below the 0.05 cutoff for good fit.\n\n## Selecting the best model\n\nNow use your knowledge of finding and interpreting absolute and relative model fit statistics to select the best model for your data. When I introduced this dataset I said that the items were theorized to load onto five factors, but you may have noticed that your scree plot indicated six factors. You might be wondering which you should trust. Not to worry - you can use fit statistics to make am empirical decision about how many factors to use.\n\nFirst, you'll use the `bfi_EFA` dataset to run EFAs with each of the hypothesized number of factors. Then, you can look at the BIC, which is a relative fit statistic, to compare models. Remember, the lowest BIC is preferred!\n\n**Steps**\n\n1. Run both EFAs on the `bfi_EFA` dataset - one with five factors according to the theory, and one with six factors according to the eigenvalues.\n2. Take a look at the BIC value for each of the models. The BIC is stored in the `BIC` list element of the results object.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run each theorized EFA on your dataset\nbfi_theory <- fa(bfi_EFA, nfactors = 5)\nbfi_eigen <- fa(bfi_EFA, nfactors = 6)\n\n# Compare the BIC values\nbfi_theory$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] -336.1329\n```\n:::\n\n```{.r .cell-code}\nbfi_eigen$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] -593.9643\n```\n:::\n:::\n\n\nNice work! You can see that the eigenvalue-driven model has a much lower BIC, so it is  empirically preferred.\n\n# 3. Confirmatory Factor Analysis\n\nThis chapter will cover conducting CFAs with the sem package. Both theory-driven and EFA-driven CFA structures will be covered.\n\n## Setting up a CFA\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Setting up a CFA**\n\nAt this point in the course, you've successfully conducted both single- and multi-factor exploratory factor analyses. Now let's talk about how to conduct a confirmatory factor analysis.\n\n**2. Why a confirmatory analysis?**\n\nIt's all in the name: whereas exploratory analyses estimate all possible variable/factor relationships to focus on exploration, confirmatory analyses explicitly specify variable/factor relationships. Confirmatory analyses are used when you want to test a theory you have already developed.Also, CFA results are generally what you want to publish when you are presenting a newly developed measure to the world. While some journals will publish exploratory results, many peer-reviewed sources want confirmatory results from a separate dataset.\n\n**3. Creating a CFA from EFA results**\n\nOne way to set up a CFA is to use the results of your EFA. This image shows the significant loadings from the six-factor EFA you ran at the end of Chapter 2. The solid black lines denote positive loadings and, the dotted red lines denote negative loadings. The psych package features a wrapper function that automatically creates CFA syntax from the significant loadings from the EFA.\n\n**4. Using the wrapper function to set up a CFA**\n\nThe structure.sem() wrapper function turns an estimated EFA model into CFA syntax. You can see that the results are expressed with arrows showing the factors' relationships to the variables, the names of the parameters associated with those estimated relationships, and the starting values for the parameters.\n\n**5. Syntax created from the wrapper function**\n\nAn illustration, let's take a look at the first entry, which is for parameter F4A1. This represents the path between item A1 and the inferred factor MR5.  In the parameter, this is factor 4, while in the path, it is factor 5. The fa() function automatically names the extracted factors from an EFA before rotation, which is why the numbers don't always match up. Be sure to keep track of these relationships during your analyses.Notice that in the Path column, the directional arrow goes from the factor to the item. This is because, in the underlying theory of statistical modeling, the examinee's level of the latent trait predicts their item responses. In the Parameter column, you'll see that the wrapper function has named the parameter quantifying the relationship between item A1 and the inferred MR5 factor as F4A1. Finally, the NA in the Value column indicates that the starting value for this parameter will be chosen at random during estimation.\n\n**6. Creating CFA syntax from your theory**\n\nThe structure.sem() wrapper function will do most of the work for you if you're creating the CFA syntax from EFA results. However, much of the time, you'll want to set up your CFA syntax from the item/factor loadings specified by a theory. The code here sets up the syntax based on the theory underlying the BFI dataset. Remember that there are five hypothesized factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness.To set up the syntax, create short and memorable names for your factors, then assign items to each factor. The factor name is followed by a colon, and the items are listed in a comma-separated list.This string variable is sufficient to set up the item/factor relationships, but you'll also need to add in the variances and covariances. The cfa() function from the sem package will automatically do this for you. We're going to set the reference-dot-indicators argument of this function to FALSE. This sets the factor variances to 1 rather than estimating them freely. While not ideal for all situations, this speeds up estimation of the model. In a real application, you should make this decision based on your theory.\n\n**7. Let's create some syntax!**\n\nAll right, now you know how to create syntax for a CFA from EFA results or from your theory. Let's go practice these new skills with some exercises.\n\n## Creating CFA syntax from EFA results\n\nThe `psych` package provides the wrapper function `structure.sem()` to help you easily turn your EFA results into syntax that can be used to conduct a CFA using the `sem()` function in the `sem` package. This convenience function can save you a lot of time in writing that syntax, so feel free to use it if you agree with the results of your EFA.\n\n**Steps**\n\n1. First, conduct a five-factor EFA on the EFA half of the `bfi` dataset.\n2. Then, use the resulting model object as an argument to the `structure.sem()` function to set up the syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Conduct a five-factor EFA on the EFA half of the dataset\nEFA_model <- fa(bfi_EFA, nfactors = 5)\n\n# Use the wrapper function to create syntax for use with the sem() function\nEFA_syn <- structure.sem(EFA_model)\n```\n:::\n\n\nNice! The EFA_syn object now contains syntax you can plug into the sem() function.\n\n## Creating CFA syntax from theory\n\nWhile it's easy to create CFA syntax from EFA results, it's a bit trickier to get the syntax set up when you're working directly from a theory. One way to create syntax that can be used with the `sem()` function is to create a series of equations showing which item-factor loadings should be estimated. When creating this matrix, the factor names come first, and a colon separates them from a comma-separated list of items. Once you've set up the initial equations, you can use the `cfa()` function to have the `sem` package add variances and covariances to the model.\n\n**Steps**\n\n1. Write a set of equations showing the five items that load onto each of the five factors: AGE, CON, EXT, NEU, OPE.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Set up syntax specifying which items load onto each factor\ntheory_syn_eq <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5\nNEU: N1, N2, N3, N4, N5\nOPE: O1, O2, O3, O4, O5\n\"\n```\n:::\n\n\n2. Use the `cfa()` function to have variances and covariances automatically added to the model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Package\nlibrary(sem)\n\n# Set up syntax specifying which items load onto each factor\ntheory_syn_eq <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5\nNEU: N1, N2, N3, N4, N5\nOPE: O1, O2, O3, O4, O5\n\"\n\n# Feed the syntax in to have variances and covariances automatically added\ntheory_syn <- cfa(text = theory_syn_eq, \n                  reference.indicators = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> NOTE: adding 25 variances to the model\n```\n:::\n:::\n\n\nNow you have theory-driven syntax that can be used to conduct a CFA!\n\n## Understanding the sem() syntax\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Understanding the sem() syntax**\n\nIn the previous exercises, you set up syntax for running a CFA using the sem() function. Let's take a look at some of the parts that go into that syntax.\n\n**2. Relationships between variables and factors**\n\nIf you take a look at what the cfa() function created from your initial input of factors, one of the first things you'll see is syntax expressing the relationships between the factors and the variables that map to them. You'll see three columns of information for each estimated parameter. First, you see the relationship as expressed by a directional path arrow. Notice the direction of the arrows in the relationships - the relationships go from the factors to the items. This is because in the theory underlying factor analyses, the examinees' levels of the latent traits predict their responses on individual items. This is a little counterintuitive at first, but it's important to understand how the analyses work on a conceptual level.Next, the Parameter column contains a name that is automatically assigned to each parameter - lam stands for lambda, which is common notation for these relationships. Finally, there's a starting value to be used in estimation. The StartValue column is blank here because the starting values weren't specified. If you have hypothesized or previously estimated values, you could specify them here to speed up estimation. It's fine not to specify them, though - the sem() function will randomly choose starting values.\n\n**3. Factor variances**\n\nAnother type of relationship you'll see in the syntax is factor variances. You'll notice that these are expressed with bidirectional arrows. In order to make the model estimable, these values are fixed, so you'll see they don't get parameter names. The StartValue is 1 because these parameter values are fixed at 1.\n\n**4. Factor covariances**\n\nThe relationships between the latent factors are also included in the model as factor covariances, which are also assigned bidirectional arrows. Their parameter names feature a C for covariance followed by brackets enclosing the two factors that covary.When factors covary, it means that respondents' levels of those factors are related, so the factors are probably also theoretically linked as well.\n\n**5. Item variances**\n\nFinally, you'll see the item-level variances expressed in the syntax. These are also bidirectional arrows, and parameter names are assigned with a V for variance followed by the item name within brackets.\n\n**6. Running the CFA**\n\nHopefully, you now have a bit better understanding of the syntax that goes into a CFA. Good news: actually running the CFA is way easier than understanding the syntax! All you have to do is plug in your syntax object and your dataset into the sem() function.\n\n**7. Interpreting the output**\n\nWhen you use the summary() function on the CFA results object, you get similar output to what you got from an EFA. You can see the fit statistics at the top,followed by information about residuals. Below that, there are r-square values for the observed variables. These give you information about how much of the variance in the latent factor each item is capable of explaining.Finally, you get the parameter estimates. These include unstandardized regression coefficients from each item/factor relationship, standard errors, Z-values and their corresponding p-values, and the path representation of the relationship.\n\n**8. Let's practice!**\n\nNow you've got an understanding of the syntax underlying a CFA, you've seen how easy it is to run a CFA once you've got the syntax set up, and you've taken a look at the resulting output. Let's go practice all these new skills!\n\n## Components of sem() syntax\n\nWhich of the following path entries defines the relationship between Agreeableness and the second item in the `sem()` syntax?\n\n> *Question*\n> ---\n> ???<br>\n> <br>\n> ⬜ AGE->A1<br>\n> ✅ AGE->A2<br>\n> ⬜ A1<->A1</-><br>\n> ⬜ AGE<->AGE</-><br>\n\nThat's it, nice!\n\n## Run a CFA and interpret loadings\n\nIt's finally time to actually run a CFA! You've got the syntax all set up, and now all that's left is to plug it into the `sem()` function along with the appropriate dataset.\n\nNow that you've created syntax and run the model, it's time to look at the results. Output can be viewed using the `summary()` function. After the fit statistics, you'll see the r-squared values for each of the items, which show the proportion of variance in the factor explained by that item. Below those, you'll see parameter estimates for the item/factor relationships (denoted with `lam[]`), the covariances between factors (denoted with `C[]`), and variances of each individual item (denoted with `V[]`).\n\n**Steps**\n\n1. Use the theory-based syntax you created earlier in this chapter to run a confirmatory factor analysis using the `sem()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use the sem() function to run a CFA\ntheory_CFA <- sem(theory_syn, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(theory_syn, data = bfi_CFA): -198 observations removed due\n#> to missingness\n```\n:::\n:::\n\n\n2. Just like with the EFAs, you can use the `summary()` function to view the results of your CFA.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use the sem() function to run a CFA\ntheory_CFA <- sem(theory_syn, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(theory_syn, data = bfi_CFA): -198 observations removed due\n#> to missingness\n```\n:::\n\n```{.r .cell-code}\n# Use the summary function to view fit information and parameter estimates\nsummary(theory_CFA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#>  Model Chisquare =  2125.012   Df =  265 Pr(>Chisq) = 3.904482e-287\n#>  AIC =  2245.012\n#>  BIC =  245.7003\n#> \n#>  Normalized Residuals\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -7.1364 -0.4162  0.8205  1.0165  2.4105  8.9602 \n#> \n#>  R-square for Endogenous Variables\n#>     A1     A2     A3     A4     A5     C1     C2     C3     C4     C5     E1 \n#> 0.1309 0.4453 0.5728 0.2792 0.4939 0.3194 0.3779 0.3062 0.5162 0.3498 0.3271 \n#>     E2     E3     E4     E5     N1     N2     N3     N4     N5     O1     O2 \n#> 0.5026 0.3935 0.4926 0.2704 0.6737 0.6436 0.5293 0.3535 0.2730 0.3192 0.1806 \n#>     O3     O4     O5 \n#> 0.5462 0.0605 0.2373 \n#> \n#>  Parameter Estimates\n#>             Estimate   Std Error  z value    Pr(>|z|)                  \n#> lam[A1:AGE] -0.5003102 0.04273721 -11.706663  1.178268e-31 A1 <--- AGE \n#> lam[A2:AGE]  0.7735145 0.03277037  23.604083 3.499425e-123 A2 <--- AGE \n#> lam[A3:AGE]  0.9871081 0.03569264  27.655789 2.377456e-168 A3 <--- AGE \n#> lam[A4:AGE]  0.7839078 0.04397367  17.826755  4.381271e-71 A4 <--- AGE \n#> lam[A5:AGE]  0.8763640 0.03480203  25.181406 6.403528e-140 A5 <--- AGE \n#> lam[C1:CON] -0.7179917 0.03852030 -18.639306  1.542280e-77 C1 <--- CON \n#> lam[C2:CON] -0.8288968 0.04032918 -20.553277  7.193968e-94 C2 <--- CON \n#> lam[C3:CON] -0.6993354 0.03844849 -18.188890  6.320748e-74 C3 <--- CON \n#> lam[C4:CON]  0.9965166 0.04039549  24.669005 2.301419e-134 C4 <--- CON \n#> lam[C5:CON]  0.9606090 0.04889508  19.646333  6.214296e-86 C5 <--- CON \n#> lam[E1:EXT]  0.9318520 0.04758241  19.583959  2.118954e-85 E1 <--- EXT \n#> lam[E2:EXT]  1.1378676 0.04458053  25.523870 1.071181e-143 E2 <--- EXT \n#> lam[E3:EXT] -0.8368783 0.03821732 -21.897880 2.721522e-106 E3 <--- EXT \n#> lam[E4:EXT] -1.0362915 0.04112249 -25.200114 3.994187e-140 E4 <--- EXT \n#> lam[E5:EXT] -0.6856466 0.03914460 -17.515740  1.086637e-68 E5 <--- EXT \n#> lam[N1:NEU]  1.2797952 0.03970836  32.229866 6.737040e-228 N1 <--- NEU \n#> lam[N2:NEU]  1.2214331 0.03911436  31.227232 4.549791e-214 N2 <--- NEU \n#> lam[N3:NEU]  1.1589884 0.04234850  27.367873 6.618175e-165 N3 <--- NEU \n#> lam[N4:NEU]  0.9345088 0.04420387  21.140882  3.347664e-99 N4 <--- NEU \n#> lam[N5:NEU]  0.8419380 0.04645599  18.123346  2.085181e-73 N5 <--- NEU \n#> lam[O1:OPE]  0.6519278 0.03736728  17.446487  3.660654e-68 O1 <--- OPE \n#> lam[O2:OPE] -0.6671608 0.05165164 -12.916548  3.630703e-38 O2 <--- OPE \n#> lam[O3:OPE]  0.8875609 0.03925883  22.607929 3.621353e-113 O3 <--- OPE \n#> lam[O4:OPE]  0.2952155 0.04039263   7.308647  2.698447e-13 O4 <--- OPE \n#> lam[O5:OPE] -0.6551728 0.04386574 -14.935864  1.925616e-50 O5 <--- OPE \n#> C[AGE,CON]  -0.3607261 0.03385340 -10.655536  1.642960e-26 CON <--> AGE\n#> C[AGE,EXT]  -0.6946870 0.02392544 -29.035492 2.346349e-185 EXT <--> AGE\n#> C[AGE,NEU]  -0.2365861 0.03363982  -7.032920  2.022556e-12 NEU <--> AGE\n#> C[AGE,OPE]   0.3454547 0.03598314   9.600459  7.958900e-22 OPE <--> AGE\n#> C[CON,EXT]   0.3623520 0.03401881  10.651519  1.715435e-26 EXT <--> CON\n#> C[CON,NEU]   0.2536479 0.03405696   7.447757  9.494061e-14 NEU <--> CON\n#> C[CON,OPE]  -0.3267677 0.03703495  -8.823226  1.112085e-18 OPE <--> CON\n#> C[EXT,NEU]   0.3069664 0.03275712   9.370982  7.186087e-21 NEU <--> EXT\n#> C[EXT,OPE]  -0.4604294 0.03382689 -13.611342  3.428870e-42 OPE <--> EXT\n#> C[NEU,OPE]  -0.1358940 0.03695809  -3.676976  2.360150e-04 OPE <--> NEU\n#> V[A1]        1.6617194 0.07026679  23.648718 1.216715e-123 A1 <--> A1  \n#> V[A2]        0.7452676 0.03760052  19.820675  1.974421e-87 A2 <--> A2  \n#> V[A3]        0.7266219 0.04365130  16.646054  3.232265e-62 A3 <--> A3  \n#> V[A4]        1.5862359 0.07118863  22.282153 5.504705e-110 A4 <--> A4  \n#> V[A5]        0.7871077 0.04191550  18.778439  1.133635e-78 A5 <--> A5  \n#> V[C1]        1.0983347 0.05234746  20.981625  9.653832e-98 C1 <--> C1  \n#> V[C2]        1.1310071 0.05679677  19.913231  3.124988e-88 C2 <--> C2  \n#> V[C3]        1.1083511 0.05228726  21.197345  1.010372e-99 C3 <--> C3  \n#> V[C4]        0.9307071 0.05660271  16.442800  9.445175e-61 C4 <--> C4  \n#> V[C5]        1.7154550 0.08387349  20.452887  5.662315e-93 C5 <--> C5  \n#> V[E1]        1.7861750 0.08219225  21.731673 1.029820e-104 E1 <--> E1  \n#> V[E2]        1.2814174 0.06862867  18.671752  8.404993e-78 E2 <--> E2  \n#> V[E3]        1.0796099 0.05193791  20.786548  5.728637e-96 E3 <--> E3  \n#> V[E4]        1.1061819 0.05852220  18.901920  1.099765e-79 E4 <--> E4  \n#> V[E5]        1.2681975 0.05662174  22.397712 4.143130e-111 E5 <--> E5  \n#> V[N1]        0.7931706 0.05082964  15.604489  6.785132e-55 N1 <--> N1  \n#> V[N2]        0.8261871 0.04954791  16.674508  2.008578e-62 N2 <--> N2  \n#> V[N3]        1.1943430 0.06052636  19.732609  1.131749e-86 N3 <--> N3  \n#> V[N4]        1.5971318 0.07181979  22.238046 1.472290e-109 N4 <--> N4  \n#> V[N5]        1.8874384 0.08221274  22.957980 1.226417e-116 N5 <--> N5  \n#> V[O1]        0.9062908 0.04645331  19.509715  9.078659e-85 O1 <--> O1  \n#> V[O2]        2.0196781 0.09065767  22.278073 6.029636e-110 O2 <--> O2  \n#> V[O3]        0.6544146 0.05309750  12.324771  6.663243e-35 O3 <--> O3  \n#> V[O4]        1.3521928 0.05664338  23.872035 5.980116e-126 O4 <--> O4  \n#> V[O5]        1.3798579 0.06476901  21.304293 1.035785e-100 O5 <--> O5  \n#> \n#>  Iterations =  25\n```\n:::\n:::\n\n\nCongrats, now you know how to run a CFA and access the results!\n\n## Examine item loadings\n\nTake a look at the item/factor loadings in `theory_CFA`. Which of the `A` items loads most strongly onto the Agreeableness, or AGE, factor?\n\n> *Question*\n> ---\n> ???<br>\n> <br>\n> ⬜ Item A2<br>\n> ✅ [Item A3]<br>\n> ⬜ Item A4<br>\n> ⬜ Item A5<br>\n\nThat's right! With a loading of 1.03, Item A3 is most strongly related to the AGE factor.\n\n## Investigating model fit\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Investigating model fit**\n\nJust like with an EFA, you'll also need to investigate and report on model fit statistics for a CFA.\n\n**2. Default fit statistics**\n\nBy default, the only absolute fit statistic the summary() function prints is the log likelihood test. You can take a look at that, but as you'll recall, the p-value from this test is often significant due to sample size even though the desired result is a lack of significance.\n\n**3. Changing the options**\n\nIn order to get some more meaningful fit statistics, you can set the fit.indices option to include some common fit statistics. The CFI, GFI, and RMSEA are all absolute fit statistics. By using the options() function, you are setting the fit.indices option in the global environment, so the specified indices will always be displayed.The RMSEA should be familiar to you from the EFA chapters. We'll add the GFI, or Goodness of Fit Index, and the CFI, or Comparative Fit Index, which are other fit statistics available from the fit.indices option. The CFI and GFI are conceptually similar to the TLI we discussed in the previous chapter, and the guidelines for \"good\" values are correspondingly the same. You'll remember from the previous chapter that an RMSEA less than 0.05 is generally considered good model fit.\n\n**4. Absolute model fit**\n\nAs promised, you can see that the summary output starts with the likelihood ratio test, which is significant. In theory, we want this to be a non-significant result, but in practice, that rarely happens. Below the likelihood ratio test, you'll see the fit statistics we added. The Goodness of Fit Index here is approximately 0-point-853. A common guideline for this index is that you want it to be greater than 0-point-9. Below that, you'll see the RMSEA. Remember that ideally, you want this value to be less than 0-point-05. For the last absolute fit index, you'll see the CFI. The calculation for this index is similar to the TLI, and it should be interpreted similarly. Ideally, this value would also be greater than 0-point-9.\n\n**5. Relative fit**\n\nThe relative fit statistic in the output is the BIC, which is also included in the summary output by default. This isn't much good when you're looking at a single model, but it can be used to compare models.In addition to looking at the summary output, you can also access it by viewing the BIC list element of the result object from the summary() function. This can come in handy if you want to store the BICs from multiple models in a dataframe.\n\n**6. Relative fit: comparing models**\n\nFor example, let's look at the BICs for the theory-based CFA and the EFA-based CFA you created from the EFA results. Remember that the lower BIC is preferred, so these results indicate that the theory-based model fits better.Relative fit indices such as the BIC are only useful when comparing nested models that are fit to the same dataset. You shouldn't ever try to compare relative fit indices from non-nested models or from results calculated from different datasets.\n\n**7. Let's practice!**\n\nThat's it for your brief overview of how to access and interpret fit statistics from a CFA. Now that you have an understanding of fit statistics let's take these skills and put them into practice.\n\n## Absolute fit statistics\n\nThe absolute fit statistic reported in the CFA output is the likelihood ratio Chi-square test, which tests the hypothesis that the estimated model works as well as the null model. You've seen this fit statistic in the previous chapter, so you probably remember that the criteria for this test are very difficult to meet. Most models have a statistically significant result, which is not desirable here, since it means model fit is statistically worse.\n\nAfter checking the absolute model fit statistics displayed by the `summary()` function, determine whether the likelihood ratio test is statistically significant.\n\n**Would the GFI for this model be considered acceptable?**\n\n**Steps**\n\n1. Set the `fit.indices` option to include several common fit indices.\n2. View the summary output from the theory-based CFA and look for the likelihood ratio test.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Set the options to include various fit indices so they will print\noptions(fit.indices = c(\"CFI\", \"GFI\", \"RMSEA\", \"BIC\"))\n\n# Use the summary function to view fit information and parameter estimates\nsummary(theory_CFA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#>  Model Chisquare =  2125.012   Df =  265 Pr(>Chisq) = 3.904482e-287\n#>  Goodness-of-fit index =  0.8596772\n#>  RMSEA index =  0.07644752   90% CI: (NA, NA)\n#>  Bentler CFI =  0.79528\n#>  BIC =  245.7003\n#> \n#>  Normalized Residuals\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -7.1364 -0.4162  0.8205  1.0165  2.4105  8.9602 \n#> \n#>  R-square for Endogenous Variables\n#>     A1     A2     A3     A4     A5     C1     C2     C3     C4     C5     E1 \n#> 0.1309 0.4453 0.5728 0.2792 0.4939 0.3194 0.3779 0.3062 0.5162 0.3498 0.3271 \n#>     E2     E3     E4     E5     N1     N2     N3     N4     N5     O1     O2 \n#> 0.5026 0.3935 0.4926 0.2704 0.6737 0.6436 0.5293 0.3535 0.2730 0.3192 0.1806 \n#>     O3     O4     O5 \n#> 0.5462 0.0605 0.2373 \n#> \n#>  Parameter Estimates\n#>             Estimate   Std Error  z value    Pr(>|z|)                  \n#> lam[A1:AGE] -0.5003102 0.04273721 -11.706663  1.178268e-31 A1 <--- AGE \n#> lam[A2:AGE]  0.7735145 0.03277037  23.604083 3.499425e-123 A2 <--- AGE \n#> lam[A3:AGE]  0.9871081 0.03569264  27.655789 2.377456e-168 A3 <--- AGE \n#> lam[A4:AGE]  0.7839078 0.04397367  17.826755  4.381271e-71 A4 <--- AGE \n#> lam[A5:AGE]  0.8763640 0.03480203  25.181406 6.403528e-140 A5 <--- AGE \n#> lam[C1:CON] -0.7179917 0.03852030 -18.639306  1.542280e-77 C1 <--- CON \n#> lam[C2:CON] -0.8288968 0.04032918 -20.553277  7.193968e-94 C2 <--- CON \n#> lam[C3:CON] -0.6993354 0.03844849 -18.188890  6.320748e-74 C3 <--- CON \n#> lam[C4:CON]  0.9965166 0.04039549  24.669005 2.301419e-134 C4 <--- CON \n#> lam[C5:CON]  0.9606090 0.04889508  19.646333  6.214296e-86 C5 <--- CON \n#> lam[E1:EXT]  0.9318520 0.04758241  19.583959  2.118954e-85 E1 <--- EXT \n#> lam[E2:EXT]  1.1378676 0.04458053  25.523870 1.071181e-143 E2 <--- EXT \n#> lam[E3:EXT] -0.8368783 0.03821732 -21.897880 2.721522e-106 E3 <--- EXT \n#> lam[E4:EXT] -1.0362915 0.04112249 -25.200114 3.994187e-140 E4 <--- EXT \n#> lam[E5:EXT] -0.6856466 0.03914460 -17.515740  1.086637e-68 E5 <--- EXT \n#> lam[N1:NEU]  1.2797952 0.03970836  32.229866 6.737040e-228 N1 <--- NEU \n#> lam[N2:NEU]  1.2214331 0.03911436  31.227232 4.549791e-214 N2 <--- NEU \n#> lam[N3:NEU]  1.1589884 0.04234850  27.367873 6.618175e-165 N3 <--- NEU \n#> lam[N4:NEU]  0.9345088 0.04420387  21.140882  3.347664e-99 N4 <--- NEU \n#> lam[N5:NEU]  0.8419380 0.04645599  18.123346  2.085181e-73 N5 <--- NEU \n#> lam[O1:OPE]  0.6519278 0.03736728  17.446487  3.660654e-68 O1 <--- OPE \n#> lam[O2:OPE] -0.6671608 0.05165164 -12.916548  3.630703e-38 O2 <--- OPE \n#> lam[O3:OPE]  0.8875609 0.03925883  22.607929 3.621353e-113 O3 <--- OPE \n#> lam[O4:OPE]  0.2952155 0.04039263   7.308647  2.698447e-13 O4 <--- OPE \n#> lam[O5:OPE] -0.6551728 0.04386574 -14.935864  1.925616e-50 O5 <--- OPE \n#> C[AGE,CON]  -0.3607261 0.03385340 -10.655536  1.642960e-26 CON <--> AGE\n#> C[AGE,EXT]  -0.6946870 0.02392544 -29.035492 2.346349e-185 EXT <--> AGE\n#> C[AGE,NEU]  -0.2365861 0.03363982  -7.032920  2.022556e-12 NEU <--> AGE\n#> C[AGE,OPE]   0.3454547 0.03598314   9.600459  7.958900e-22 OPE <--> AGE\n#> C[CON,EXT]   0.3623520 0.03401881  10.651519  1.715435e-26 EXT <--> CON\n#> C[CON,NEU]   0.2536479 0.03405696   7.447757  9.494061e-14 NEU <--> CON\n#> C[CON,OPE]  -0.3267677 0.03703495  -8.823226  1.112085e-18 OPE <--> CON\n#> C[EXT,NEU]   0.3069664 0.03275712   9.370982  7.186087e-21 NEU <--> EXT\n#> C[EXT,OPE]  -0.4604294 0.03382689 -13.611342  3.428870e-42 OPE <--> EXT\n#> C[NEU,OPE]  -0.1358940 0.03695809  -3.676976  2.360150e-04 OPE <--> NEU\n#> V[A1]        1.6617194 0.07026679  23.648718 1.216715e-123 A1 <--> A1  \n#> V[A2]        0.7452676 0.03760052  19.820675  1.974421e-87 A2 <--> A2  \n#> V[A3]        0.7266219 0.04365130  16.646054  3.232265e-62 A3 <--> A3  \n#> V[A4]        1.5862359 0.07118863  22.282153 5.504705e-110 A4 <--> A4  \n#> V[A5]        0.7871077 0.04191550  18.778439  1.133635e-78 A5 <--> A5  \n#> V[C1]        1.0983347 0.05234746  20.981625  9.653832e-98 C1 <--> C1  \n#> V[C2]        1.1310071 0.05679677  19.913231  3.124988e-88 C2 <--> C2  \n#> V[C3]        1.1083511 0.05228726  21.197345  1.010372e-99 C3 <--> C3  \n#> V[C4]        0.9307071 0.05660271  16.442800  9.445175e-61 C4 <--> C4  \n#> V[C5]        1.7154550 0.08387349  20.452887  5.662315e-93 C5 <--> C5  \n#> V[E1]        1.7861750 0.08219225  21.731673 1.029820e-104 E1 <--> E1  \n#> V[E2]        1.2814174 0.06862867  18.671752  8.404993e-78 E2 <--> E2  \n#> V[E3]        1.0796099 0.05193791  20.786548  5.728637e-96 E3 <--> E3  \n#> V[E4]        1.1061819 0.05852220  18.901920  1.099765e-79 E4 <--> E4  \n#> V[E5]        1.2681975 0.05662174  22.397712 4.143130e-111 E5 <--> E5  \n#> V[N1]        0.7931706 0.05082964  15.604489  6.785132e-55 N1 <--> N1  \n#> V[N2]        0.8261871 0.04954791  16.674508  2.008578e-62 N2 <--> N2  \n#> V[N3]        1.1943430 0.06052636  19.732609  1.131749e-86 N3 <--> N3  \n#> V[N4]        1.5971318 0.07181979  22.238046 1.472290e-109 N4 <--> N4  \n#> V[N5]        1.8874384 0.08221274  22.957980 1.226417e-116 N5 <--> N5  \n#> V[O1]        0.9062908 0.04645331  19.509715  9.078659e-85 O1 <--> O1  \n#> V[O2]        2.0196781 0.09065767  22.278073 6.029636e-110 O2 <--> O2  \n#> V[O3]        0.6544146 0.05309750  12.324771  6.663243e-35 O3 <--> O3  \n#> V[O4]        1.3521928 0.05664338  23.872035 5.980116e-126 O4 <--> O4  \n#> V[O5]        1.3798579 0.06476901  21.304293 1.035785e-100 O5 <--> O5  \n#> \n#>  Iterations =  25\n```\n:::\n:::\n\n\n> *Question*\n> ---\n> <br>\n> <br>\n> ⬜ Yes, it is above 0.9.<br>\n> ⬜ Yes, it is below 0.9.<br>\n> ⬜ No, it is above 0.9.<br>\n> ✅ No, it is below 0.9.<br>\n\nThat's right - the GFI is less than 0.9, which means it's below the commonly used criterion.\n\n## Relative fit statistics\n\nIn this exercise, we'll use the relative fit statistics from two models to compare their fit. When you have two models that both fit well, relative fit statistics can help you decide which model to select. Since the bfi dataset has a strong theory underlying it - the big five personality factors are commonly accepted and studied - we would like to see the theory-based model fit better than our exploratory EFA-based model.\n\n**Which model is preferred according to its BIC?**\n\n**Steps**\n\n1. First, let's use the CFA syntax we created from the EFA results to run an EFA-based CFA.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run a CFA using the EFA syntax you created earlier\nEFA_CFA <- sem::sem(EFA_syn, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(EFA_syn, data = bfi_CFA): -198 observations removed due to\n#> missingness\n```\n:::\n:::\n\n\n2. Next, display the BIC by calling its list element within the summary output.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run a CFA using the EFA syntax you created earlier\nEFA_CFA <- sem(EFA_syn, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(EFA_syn, data = bfi_CFA): -198 observations removed due to\n#> missingness\n```\n:::\n\n```{.r .cell-code}\n# Locate the BIC in the fit statistics of the summary output\nsummary(EFA_CFA)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 416.7049\n```\n:::\n:::\n\n\n3. Finally, look at both models' BICs to see which model fits better.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run a CFA using the EFA syntax you created earlier\nEFA_CFA <- sem(EFA_syn, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(EFA_syn, data = bfi_CFA): -198 observations removed due to\n#> missingness\n```\n:::\n\n```{.r .cell-code}\n# Locate the BIC in the fit statistics of the summary output\nsummary(EFA_CFA)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 416.7049\n```\n:::\n\n```{.r .cell-code}\n# Compare EFA_CFA BIC to the BIC from the CFA based on theory\nsummary(theory_CFA)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 245.7003\n```\n:::\n:::\n\n\n> *Question*\n> ---\n> <br>\n> <br>\n> ✅ The theory-based model, because its BIC is lower.<br>\n> ⬜ The theory-based model, because its BIC is higher.<br>\n> ⬜ The EFA-based model, because its BIC is lower.<br>\n> ⬜ The EFA-based model, because its BIC is higher.<br>\n\nThat's right! The theory-based model is preferred due to its lower BIC.\n\n# 4. Refining your measure and/or model\n\nThis chapter will reinforce the difference between EFAs and CFAs and offer suggestions for improving your model and/or measure.\n\n## EFA vs. CFA revisited\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. EFA vs. CFA revisited**\n\nThis chapter is going to be about improving model fit, but first, let's revisit the differences between EFAs and CFAs. When looking at results to think about model improvement, it is especially important to keep this distinction in mind.\n\n**2. Review of the differences between EFA &amp; CFA**\n\nEFAs and CFAs are mathematically and conceptually different. EFAs estimate all possible item/factor relationships in order to help you identify patterns in the data. This is useful when you don't have a well-developed theory. In contrast, CFAs only estimate the variable/factor relationships that you specify in order to test your theory. CFA results are generally what you'll want to publish when you're developing a new measure.\n\n**3. Differences in factor loadings**\n\nThese differences in models, combined with the fact that you are using different halves of the dataset for your EFAs and CFAs, mean that your estimated factor loadings will differ, even for the same item/factor relationships.Let's look at the first five lines of output from the EFA and CFA estimated loadings. You'll notice that the output looks different: the EFA loadings are a matrix showing every item/factor combination, while the CFA loadings are listed one on each row.\n\n**4. Comparing factor loadings**\n\nAnother important thing to be attentive to is the factor naming conventions. You'll see in the CFA output that the first loading for factor 4 and item A1 is actually the relationship between A1 and MR5. These differences are due to the rotation involved in EFA estimation. So, the first row of the CFA output, which shows a loading of -0-point-503, is equivalent to row 1, column 4 in the EFA output, which shows a loading of -0-point-390 after rounding.\n\n**5. Differences in factor scores**\n\nThe factor scores also differ between the EFA and CFA. To investigate this, the first step is to get the factor scores from each model. This is a bit more involved with the CFA results, as you'll want to use the fscores() function. Here, you'll see we're calculating factor scores from the CFA we ran based on the EFA results in order to ensure maximum comparability. We're also using the bfi_EFA dataset - again, for maximum comparability.With the EFA results, you can just access the scores list element in the model object. Remember that this model was created using the bfi_EFA dataset, so if there were no differences between the model parameters, these scores would be identical.\n\n**6. Differences in factor scores, visualized**\n\nTo visualize the differences in factor scores obtained from the EFA and CFA, we can use a density plot. This is similar to how we plotted factor scores in Chapter 1, but now we'll add a second density curve using lines().As you can see from this graph, the score distributions are not identical, which indicates that the model parameters are different. The factor scores from the EFA parameters are represented by the blue density curve, and factor scores from the CFA parameters are represented by the red density curve. Notice how the blue EFA distribution is flatter and less normal than the CFA distribution.\n\n**7. Let's practice!**\n\nOkay, we've reviewed the differences between EFAs and CFAs and examined some evidence showing those differences. Now it's time for you to put this into action.\n\n## Differences in estimated factor loadings\n\nThe differences between EFAs and CFAs are evident when examining the factor loadings. Not only are the procedures mathematically different, but the number of estimated parameters is also different. By default, EFAs estimate all possible item/factor pairs, while CFAs only estimate specified item/factor relationships. Let's take a look at a few of those estimated loadings and see how they differ.\n\n**Which of these loadings represent the EFA and CFA estimates of the relationship between item A1 and the fourth factor (named MR5)?**\n\n**Steps**\n\n1. Check out the first five rows of `loadings` in `EFA_model`. Remember that the EFA estimates all possible item/factor pairs.\n2. Look at the first five loadings in `EFA_model`. This output only estimates specified item/factor relationships.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# View the first five rows of the EFA loadings\nEFA_model$loadings[1:5,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>            MR2        MR1          MR3        MR5          MR4\n#> A1  0.22965525 0.17360529  0.080832386 -0.4338577 -0.048734165\n#> A2 -0.02868022 0.01579386  0.080301996  0.6223671  0.008891873\n#> A3 -0.03017499 0.13687937  0.012476336  0.6622945  0.036893172\n#> A4 -0.07386009 0.06421236  0.190715338  0.4183947 -0.165074440\n#> A5 -0.09340458 0.29762789 -0.005808171  0.4950286  0.035823302\n```\n:::\n\n```{.r .cell-code}\n# View the first five loadings from the CFA estimated from the EFA results\nsummary(EFA_CFA)$coeff[1:5,]\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Estimate\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Std Error\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"z value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>|z|)\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\" \"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"-0.5005400\",\"2\":\"0.04285300\",\"3\":\"-11.68040\",\"4\":\"1.605472e-31\",\"5\":\"A1 <--- MR5\",\"_rn_\":\"F4A1\"},{\"1\":\"0.7719727\",\"2\":\"0.03292066\",\"3\":\"23.44949\",\"4\":\"1.337712e-121\",\"5\":\"A2 <--- MR5\",\"_rn_\":\"F4A2\"},{\"1\":\"0.9931577\",\"2\":\"0.03584291\",\"3\":\"27.70863\",\"4\":\"5.496247e-169\",\"5\":\"A3 <--- MR5\",\"_rn_\":\"F4A3\"},{\"1\":\"0.7820261\",\"2\":\"0.04412703\",\"3\":\"17.72216\",\"4\":\"2.828634e-70\",\"5\":\"A4 <--- MR5\",\"_rn_\":\"F4A4\"},{\"1\":\"0.8729503\",\"2\":\"0.03499902\",\"3\":\"24.94214\",\"4\":\"2.598988e-137\",\"5\":\"A5 <--- MR5\",\"_rn_\":\"F4A5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n> *Question*\n> ---\n> <br>\n> <br>\n> ⬜ 0.24 &amp; -0.50<br>\n> ✅ -0.39 &amp; -0.50<br>\n> ⬜ -0.39 &amp; -0.08<br>\n> ⬜ 0.24 &amp; 0.82<br>\n\nYep! These are the EFA and CFA estimates of the loading of item A1 on factor MR5 (which is the fourth factor after rotation).\n\n## Plotting differences in persons' factor scores\n\nFactor loadings aren't the only parameters that differ between EFA and CFA results. Individuals' factor scores also differ when they are calculated from the EFA or CFA parameters. To illustrate this, we'll look at how factor scores for individuals in the `bfi_EFA` dataset differ when they are calculated from the EFA model versus from the CFA model by examining those scores' density plots.\n\n**Steps**\n\n1. First, save the scores from the `scores` named list element from the `EFA_model` object into a new object. Since this model was created from the `bfi_EFA` dataset, the factor scores are from that dataset.\n2. Next, use `fscores()` to calculate and extract individuals' factor scores using the CFA model object with the same `bfi_EFA` dataset.\n3. Finally, use the `plot()` and `lines()` functions to graph the densities of the individuals' factor scores on the first factor.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extracting factor scores from the EFA model\nEFA_scores <- EFA_model$scores\n\n# Calculating factor scores by applying the CFA parameters to the EFA dataset\nCFA_scores <- fscores(EFA_CFA, data = bfi_EFA)\n\n# Comparing factor scores from the EFA and CFA results from the bfi_EFA dataset\nplot(density(EFA_scores[,1], na.rm = TRUE), \n    xlim = c(-3, 3), ylim = c(0, 1), col = \"blue\")\nlines(density(CFA_scores[,1], na.rm = TRUE), \n    xlim = c(-3, 3), ylim = c(0, 1), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](factor_analysis_in_r_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nExcellent job! You'll notice these density plots look different - that's because individuals' estimated factor scores are different depending on which model you used to calculate them.\n\n## Adding loadings to improve fit\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Adding loadings to improve model fit**\n\nNow that we've reviewed the differences between EFAs and CFAs, we'll look at how to improve model fit.\n\n**2. When to make adjustments**\n\nWhile EFAs estimate all variable/factor loadings, CFAs only estimate the loadings you specify. If you run your CFA and get disappointing fit statistics, adding additional loadings is one way to improve model fit.\n\n**3. Adding loadings to the syntax**\n\nOne way to select loadings to add is to look at the results of an EFA with the number of factors dictated by your theory. I've done this for you and picked a couple promising item/factor relationships to add: the fourth Neuroticism item could load to the Extraversion factor, and the third Extraversion item could load to the Neuroticism factor. Both of these proposed relationships hinge on a correlation between the Neuroticism and Extraversion factors. If we look at the summary stats from the original, theory-based CFA, we can see that these factors have a small positive correlation, so this isn't a totally implausible adjustment to the theory. Of course, in a real data application, you'll want to carefully evaluate the theoretical implications of adding loadings.\n\n**4. Adding new loadings to the syntax**\n\nHere's a graphical representation of what adding loadings looks like. You're effectively saying that the Neuroticism factor can predict responses to item E3 and the Extraversion factor can predict responses to item N4. These new relationships are shown here by bold green arrows.\n\n**5. Adding new loadings to the syntax**\n\nThe first step to adding new loadings is to alter the syntax for the CFA. You can do this by adding the new relationships to the equations used to create the syntax. You can see that we've added item N4 to the Extraversion factor, which is abbreviated EXT in our syntax, and we've also added item E3 to the Neuroticism factor, which is abbreviated NEU.Next, you'll feed those equations into the cfa() function to create syntax compatible with the sem() function used to run the CFA.Once you've got the syntax set up, you can go ahead and plug that revised syntax into the sem() function to run the CFA. As before, remember to continue using the bfi_CFA dataset, since this is still a CFA.\n\n**6. Comparing the original and revised models**\n\nNow, let's conduct a likelihood ratio test. As you may remember, this tests to see whether the two models fit statistically differently. When you are testing a model against the null model, you want this to be non-significant; however, when you are comparing two specified models, a significant result indicates that one model fits significantly better and is preferred. As the legend at the bottom shows, the stars after the p-value indicate a statistically significant difference, so this is a good result!\n\n**7. Comparing the original and revised models**\n\nAnother fit index to consider is the CFI. The higher value corresponds to the better-fitting model. You can see that the revised model's CFI is higher here.\n\n**8. Comparing the original and revised models**\n\nThe final fit index we'll look at will be the RMSEA. The first value in the output shows the RMSEA value. The next values are the bounds of the 90% confidence interval, which are NA here due to how the model is specified.The important thing here is to identify the lower RMSEA, which is associated with the revised model. As you'll recall, you ideally want the RMSEA to be less than 0.05. Neither of these are, but at least you're a bit closer with the revised model!For more information about calculating and interpreting these and other fit indices, check out this website.\n\n**9. Let's practice!**\n\nNow that you've seen a demonstration of adding loadings and comparing models, it's time to try it for yourself!\n\n## Add loadings to improve fit\n\nAdding more item/factor relationships will always improve your model fit because more parameters are always going to fit the data more precisely. However, when adding loadings, you want to be sure they are justifiable by theory. A good place to start is with loadings that were strong in an EFA with the same number of factors (on a separate dataset, of course!) - you can consider whether those suggested item/factor relationships might be justified by theory. If they are supported by your theory, you can add them to your CFA.\n\n**Steps**\n\n1. First, let's add some of the other strong factor loadings from the EFA. The fourth neuroticism item could load to Extraversion, and the third extraversion item could load to Neuroticism.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add some plausible item/factor loadings to the syntax\ntheory_syn_add <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5, N4\nNEU: N1, N2, N3, N4, N5, E3\nOPE: O1, O2, O3, O4, O5\n\"\n```\n:::\n\n\n2. Next, use the `cfa()` function to convert your series of equations into `sem()`-compatible syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Add some plausible item/factor loadings to the syntax\ntheory_syn_add <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5, N4\nNEU: N1, N2, N3, N4, N5, E3\nOPE: O1, O2, O3, O4, O5\n\"\n\n# Convert your equations to sem-compatible syntax\ntheory_syn2 <- cfa(text = theory_syn_add, reference.indicators = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> NOTE: adding 25 variances to the model\n```\n:::\n:::\n\n\n3. Finally, run a CFA using your new syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add some plausible item/factor loadings to the syntax\ntheory_syn_add <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5, N4\nNEU: N1, N2, N3, N4, N5, E3\nOPE: O1, O2, O3, O4, O5\n\"\n\n# Convert your equations to sem-compatible syntax\ntheory_syn2 <- cfa(text = theory_syn_add, reference.indicators = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> NOTE: adding 25 variances to the model\n```\n:::\n\n```{.r .cell-code}\n# Run a CFA with the revised syntax\ntheory_CFA_add <- sem(model = theory_syn2, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(model = theory_syn2, data = bfi_CFA): -198 observations\n#> removed due to missingness\n```\n:::\n:::\n\n\nGreat! You've added item/factor loadings for some conceivable relationships.\n\n## Compare original model to model with added loadings\n\nOnce you have a revised version of the theory-based model with added loadings, it's time to compare it to the original theory-based model. By adding loadings, you are actually saying that those items measure more than one construct, so you would want to carefully evaluate the additions' impacts on your theory. For this exercise, we'll conduct a likelihood ratio test, then compare the CFIs and RMSEAs.\n\n**Steps**\n\n1. Use the `anova()` function to conduct a likelihood ratio test to see whether the models fit significantly differently.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Conduct a likelihood ratio test\nanova(theory_CFA, theory_CFA_add)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Model Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Model Chisq\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LR Chisq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>Chisq)\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"265\",\"2\":\"2125.012\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"_rn_\":\"theory_CFA\"},{\"1\":\"263\",\"2\":\"2009.898\",\"3\":\"2\",\"4\":\"115.1136\",\"5\":\"1.007862e-25\",\"_rn_\":\"theory_CFA_add\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n2. Check out the fit indices for each model to see which fits better.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Conduct a likelihood ratio test\nanova(theory_CFA, theory_CFA_add)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Model Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Model Chisq\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LR Chisq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>Chisq)\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"265\",\"2\":\"2125.012\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"_rn_\":\"theory_CFA\"},{\"1\":\"263\",\"2\":\"2009.898\",\"3\":\"2\",\"4\":\"115.1136\",\"5\":\"1.007862e-25\",\"_rn_\":\"theory_CFA_add\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Compare the comparative fit indices - higher is better!\nsummary(theory_CFA)$CFI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.79528\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_add)$CFI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.8077297\n```\n:::\n:::\n\n\n3. Investigate the RMSEAs for each model to see which fits better.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Conduct a likelihood ratio test\nanova(theory_CFA, theory_CFA_add)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Model Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Model Chisq\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LR Chisq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>Chisq)\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"265\",\"2\":\"2125.012\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"_rn_\":\"theory_CFA\"},{\"1\":\"263\",\"2\":\"2009.898\",\"3\":\"2\",\"4\":\"115.1136\",\"5\":\"1.007862e-25\",\"_rn_\":\"theory_CFA_add\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Compare the comparative fit indices - higher is better!\nsummary(theory_CFA)$CFI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.79528\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_add)$CFI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.8077297\n```\n:::\n\n```{.r .cell-code}\n# Compare the RMSEA values - lower is better!\nsummary(theory_CFA)$RMSEA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.07644752         NA         NA 0.90000000\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_add)$RMSEA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.07436772         NA         NA 0.90000000\n```\n:::\n:::\n\n\nYep, the revised CFA fits better! In real life, you would carefully consider whether the new loadings make sense with your theory.\n\n## Evaluate added loadings with relative fit stats\n\nIn the previous exercise, you used several absolute fit statistics to compare the original theory-based CFA to the version with added item loadings. You can also use relative fit statistics like the BIC to compare models like these. \n\n**According to the BIC values, which model is preferred?**\n\n**Steps**\n\n1. Check out the models' BIC values to see which one is preferred.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compare BIC values\nsummary(theory_CFA)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 245.7003\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_add)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 144.7702\n```\n:::\n:::\n\n\n> *Question*\n> ---\n> <br>\n> <br>\n> ⬜ The original model is preferred due to its lower BIC.<br>\n> ⬜ The original model is preferred due to its higher BIC.<br>\n> ✅ The model with added loadings is preferred due to its lower BIC.<br>\n> ⬜ The model with added loadings is preferred due to its higher BIC.<br>\n\nExactly! The model with added loadings has a lower BIC, which indicates it is the preferred model.\n\n## Improving fit by removing loadings\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Removing loadings to improve fit**\n\nThe last lesson was about adding loadings to improve fit. You can also remove loadings to improve fit by getting rid of weak item/factor relationships.\n\n**2. What does it mean to remove a loading?**\n\nFor this lesson, you'll learn how to remove a loading by deleting the loading of item O4 to the Openness factor. If you inspect the CFA results, you'll notice that this was the weakest loading, with a value of 0-point-287.\n\n**3. What does it mean to remove a loading?**\n\nHere's what deleting loadings from the model looks like conceptually. In this example, you are effectively not only deleting the loading, you are also excluding the O4 item from the model since there are no other loadings associated with that item. By deleting this loading, you are effectively revising the measure by removing that item.\n\n**4. Removing a loading in the syntax**\n\nThe process for changing up the syntax and rerunning the CFA is just like when you were adding an item. First, you'll remove item O4 from the equations used to create the original syntax. This actually tells the package not to estimate the relationship between item O4 and the Openness factor. Then, as before, use the cfa() function to convert those equations to sem-compatible syntax.\n\n**5. Running the revised CFA**\n\nYou'll notice when you run the CFA with the revised syntax; you get a new warning. Since you've effectively removed item O4 from your model, the sem() function doesn't like that it's still in your input dataset. Don't worry - it's just a warning, not an error. It's difficult to turn this off, so just be sure to review the warning messages and make sure nothing unexpected pops up.\n\n**6. Comparing the original and revised models**\n\nYou may recall that when you added loadings, the first type of comparison was to run a likelihood ratio test. Since you've discarded an item, though, the models aren't run on the same dataset, and the likelihood ratio test can't be run.Instead, let's first look at the fit indices. As you'll see, the model with the deleted item and loading has a higher CFI and is therefore preferred.\n\n**7. Comparing the original and revised models**\n\nYou can also still consult the RMSEAs. The difference here isn't as noticeable, but you can see that the RMSEA is a teeny bit lower for the revised model. Again, for more information about calculating and interpreting fit indices, check out this website, which covers the indices in this lesson as well as several others.\n\n**8. Let's practice!**\n\nOkay! This is conceptually very different from adding loadings, but the code required is pretty similar. Let's go try this out.\n\n## Remove loadings to improve fit\n\nRemoving weak item/factor relationships will typically improve your model fit because you're estimating only meaningful parameters. However, when removing loadings, you want to be sure you are okay with removing the item from your measure. Removing an item's loading effectively means that item is no longer included in your measure, and scores on that item won't be considered in the analysis.\n\n**Steps**\n\n1. First, let's remove the weakest factor loading from the CFA, which is the fourth Openness item's loading on its factor.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Remove the weakest factor loading from the syntax\ntheory_syn_del <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5\nNEU: N1, N2, N3, N4, N5\nOPE: O1, O2, O3, O5\n\"\n```\n:::\n\n\n2. Next, convert your series of equations into `sem()` compatible syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Remove the weakest factor loading from the syntax\ntheory_syn_del <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5\nNEU: N1, N2, N3, N4, N5\nOPE: O1, O2, O3, O5\n\"\n\n# Convert your equations to sem-compatible syntax\ntheory_syn3 <- cfa(text = theory_syn_del, reference.indicators = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> NOTE: adding 24 variances to the model\n```\n:::\n:::\n\n\n3. Finally, run a CFA using your new syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Remove the weakest factor loading from the syntax\ntheory_syn_del <- \"\nAGE: A1, A2, A3, A4, A5\nCON: C1, C2, C3, C4, C5\nEXT: E1, E2, E3, E4, E5\nNEU: N1, N2, N3, N4, N5\nOPE: O1, O2, O3, O5\n\"\n\n# Convert your equations to sem-compatible syntax\ntheory_syn3 <- cfa(text = theory_syn_del, reference.indicators = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> NOTE: adding 24 variances to the model\n```\n:::\n\n```{.r .cell-code}\n# Run a CFA with the revised syntax\ntheory_CFA_del <- sem(model = theory_syn3, data = bfi_CFA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(model = theory_syn3, data = bfi_CFA): -198 observations\n#> removed due to missingness\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in sem.semmod(model = theory_syn3, data = bfi_CFA): The following observed variables are in the input covariance or raw-moment matrix but do not appear in the model:\n#> O4\n```\n:::\n:::\n\n\nGreat! You've removed the weakest item/factor loading.\n\n## Compare original model to model with deleted loadings\n\nOnce you have a revised version with that weak loading deleted, it's time to compare it to the original theory-based model. If you were the developer of this measure, you would probably hope that the original version would fit better, since you would have carefully constructed items to measure your theoretical constructs. In practice, though, it can be tricky to know whether or not items are actually performing well, and sometimes you might want to drop items to create a stronger measure. For this exercise, we'll compare the CFIs and RMSEAs.\n\n**Steps**\n\n1. Check out the absolute fit indices for each model to see which fits better.\n2. Investigate the RMSEAs for each model to see which fits better.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compare the comparative fit indices - higher is better!\nsummary(theory_CFA)$CFI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.79528\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_del)$CFI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.8093357\n```\n:::\n\n```{.r .cell-code}\n# Compare the RMSEA values - lower is better!\nsummary(theory_CFA)$RMSEA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.07644752         NA         NA 0.90000000\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_del)$RMSEA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.07626226         NA         NA 0.90000000\n```\n:::\n:::\n\n\nOnce again, the revised CFA fits better! In real life, you would carefully consider whether you are okay with excluding that loading and dropping that item.\n\n## Evaluate deleted loadings with relative fit stats\n\nIn the previous exercise, you used several absolute fit statistics to compare the original theory-based CFA to the version with added item loadings. You can also use relative fit statistics like the BIC to compare models like these. \n\n**According to the BIC values, which model is preferred?**\n\n**Steps**\n\n1. Check out the models' BIC values to see which one is preferred.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compare BIC values\nsummary(theory_CFA)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 245.7003\n```\n:::\n\n```{.r .cell-code}\nsummary(theory_CFA_del)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 216.1527\n```\n:::\n:::\n\n\n> *Question*\n> ---\n> <br>\n> <br>\n> ⬜ The original model is preferred due to its lower BIC.<br>\n> ⬜ The original model is preferred due to its higher BIC.<br>\n> ✅ The model with a deleted loading is preferred due to its lower BIC.<br>\n> ⬜ The model with a deleted loading is preferred due to its higher BIC.<br>\n\nExactly! The model with a deleted loading has a lower BIC, which indicates it is the preferred model.\n\n## Wrap-Up Video\n\nTheory. Coming soon ...\n<p class=\"dc-cookie-banner-text\">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            \n\n\n\n**1. Now you can conduct and interpret EFAs and CFAs!**\n\nAll right, that's it for this course!\n\n**2. Things you can do**\n\nAlong the way, you learned several skills that will help you through the process of developing, refining, and reporting results from a measure. You can conduct both uni- and multi-dimensional EFAs, which help you explore the structure of your data when you don't have a theory. You also learned how to conduct CFAs to confirm your model, either from a structure based on EFA results or from a structure based on a theory. Finally, you learned how to interpret both absolute and relative fit statistics to compare models and make adjustments if needed.\n\n**3. More information**\n\nAs you may have guessed, there's a lot more to factor analysis than we had the time to cover here. For more information, check out the documentation for the psych and sem packages or pick up a book on multivariate analysis. I recommend Applied Multivariate Statistical Analysis by Johnson &amp; Wichern, which will walk you through the mathematics of these analyses while maintaining a focus on application.The next level to this sort of analysis is structural equation modeling, which allows you to estimate hierarchical relationships between latent variables. There's a great DataCamp course that you can take next if you're interested!\n\n**4. Congrats!**\n\nSo, finally, congrats, and thanks for taking this course.\n\n",
    "supporting": [
      "factor_analysis_in_r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}