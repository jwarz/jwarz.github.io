{
  "hash": "36f25deb53ab76c31f015e4f57e5fba2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Working with Web Data in R\"\nauthor: \"Joschka Schwarz\"\n---\n\n\n\n\nMost of the useful data in the world, from economic data to news content to geographic information, lives somewhere on the internet - and this course will teach you how to access it.\\n\\nYou'll explore how to work with APIs (computer-readable interfaces to websites), access data from Wikipedia and other sources, and build your own simple API client. For those occasions where APIs are not available, you'll find out how to use R to scrape information out of web pages. In the process you'll learn how to get data out of even the most stubborn website, and how to turn it into a format ready for further analysis. The packages you'll use and learn your way around are rvest, httr, xml2 and jsonlite, along with particular API client packages like WikipediR and pageviews.\n\n# 1. Downloading Files and Using API Clients\n\nSometimes getting data off the internet is very, very simple - it's stored in a format that R can handle and just lives on a server somewhere, or it's in a more complex format and perhaps part of an API but there's an R package designed to make using it a piece of cake. This chapter will explore how to download and read in static files, and how to use APIs when pre-existing clients are available.\n\n## Introduction: Working With Web Data in R\n\nTheory. Coming soon ...\n\n**1. Introduction: Working With Web Data in R**\n\nHi, I'm Oliver Keyes, and I'm Charlotte Wickham. Welcome to Working with Web Data in R.\n\n**2. Working with Web Data in R**\n\nIt turns out that the Internet is a great place to find datasets, and this course teaches you how to get those datasets into R in order to analyze them.You'll start with the simplest cases: simply downloading files to your machine, and using existing packages specifically designed for pulling in web data.Next you'll use the tidyverse package httr to query web application programming interfaces using the GET() and POST() commands.After that, you'll learn how to work with JSON and XML, the two most common data formats used by websites. Typically, datasets in R are rectangular, so they can be stored in a data frame or matrix. JSON and XML are both nested data structures, so you'll learn new techniques to explore them, including XPath, the XML query language.Finally, you'll learn to use Cascading Style Sheets, or CSS, to navigate HTML pages and extract their data.\n\n**3. Importing data from a URL**\n\nFirst, let's begin with the easy case. Many functions in base R that are used for importing data accept a URL, so you can directly import the data from its location on the Internet. For example, to retrieve a CSV file that you found on the Internet, you can still call read dot csv().The only change to your code is that rather than passing a path to a local file is that you pass a URL.\n\n**4. Downloading data from a URL**\n\nSince downloading data from the Internet every time that you want to use it can be very slow and tedious, especially for large datasets, R provides the function download dot file() to copy Internet-based files to your machine. This takes two arguments: the URL where the file lives, and a path to somewhere on your local file system that you want to download the data to.\n\n**5. Let's practice!**\n\nNow let's try some examples.\n\n## Downloading files and reading them into R\n\nIn this first exercise we're going to look at reading already-formatted datasets - CSV or TSV files, with which you'll no doubt be familiar! - into R from the internet. This is a lot easier than it might sound because R's file-reading functions accept not just file paths, but also URLs.\n\n**Steps**\n\nThe URLs to those files are in your R session as `csv_url` and `tsv_url`.\n\n1. Read the CSV file stored at `csv_url` into R using `read.csv()`. Assign the result to `csv_data`.\n2. Do the same for the TSV file stored at `tsv_url` using `read.delim()`. Assign the result to `tsv_data`.\n3. Examine each object using `head()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Here are the URLs! As you can see they're just normal strings\ncsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv\"\ntsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv\"\n\n# Read a file in from the CSV URL and assign it to csv_data\ncsv_data <- read.csv(file = csv_url)\n\n# Read a file in from the TSV URL and assign it to tsv_data\ntsv_data <- read.delim(file = tsv_url)\n\n# Examine the objects with head()\nhead(csv_data)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"weight\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"feed\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"179\",\"2\":\"horsebean\",\"_rn_\":\"1\"},{\"1\":\"160\",\"2\":\"horsebean\",\"_rn_\":\"2\"},{\"1\":\"136\",\"2\":\"horsebean\",\"_rn_\":\"3\"},{\"1\":\"227\",\"2\":\"horsebean\",\"_rn_\":\"4\"},{\"1\":\"217\",\"2\":\"horsebean\",\"_rn_\":\"5\"},{\"1\":\"168\",\"2\":\"horsebean\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nhead(tsv_data)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"weight\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"feed\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"179\",\"2\":\"horsebean\",\"_rn_\":\"1\"},{\"1\":\"160\",\"2\":\"horsebean\",\"_rn_\":\"2\"},{\"1\":\"136\",\"2\":\"horsebean\",\"_rn_\":\"3\"},{\"1\":\"227\",\"2\":\"horsebean\",\"_rn_\":\"4\"},{\"1\":\"217\",\"2\":\"horsebean\",\"_rn_\":\"5\"},{\"1\":\"168\",\"2\":\"horsebean\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nExcellent work! As you can see, `read.csv()` and `read.delim()` work nicely with URLs, reading the contents\\nin just as they would if the URLs were instead paths to files on your computer.\n\n## Saving raw files to disk\n\nSometimes just reading the file in from the web is enough, but often you'll want to store it locally so that you\ncan refer back to it. This also lets you avoid having to spend the start of every analysis session twiddling your thumbs while particularly large files download.\n\nHelpfully, R has <a href=\"https://www.rdocumentation.org/packages/utils/topics/download.file\">`download.file()`</a>, a function that lets you do just that: download a file to a location of your choice on your computer. It takes two arguments; `url`, indicating the URL to read from, and `destfile`, the destination to write the downloaded file to. In this case, we've pre-defined the URL - once again, it's `csv_url`.\n\n**Steps**\n\n1. Download the file at `csv_url` with `download.file()`, naming the destination file `\"feed_data.csv\"`.\n2. Read `\"feed_data.csv\"` into R with `read.csv()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Download the file with download.file()\ndownload.file(url = csv_url, destfile = \"data/feed_data.csv\")\n\n# Read it in with read.csv()\ncsv_data <- read.csv(file = \"data/feed_data.csv\")\n```\n:::\n\n\nGood job. Now the file has been downloaded, you can refer back to it however many times you like without having to grab it from the internet anew each time.\n\n## Saving formatted files to disk\n\nWhether you're downloading the raw files with `download.file()` or using `read.csv()` and its sibling functions, at some point you're probably going to find the need to modify your input data, and then save the modified data to disk so you don't lose the changes.\n\nYou could use `write.table()`, but then you have to worry about accidentally writing out data in a format R can't read back in. An easy way to avoid this risk is to use <a href=\"https://www.rdocumentation.org/packages/base/topics/saveRDS\">`saveRDS()`</a> and <a href=\"https://www.rdocumentation.org/packages/base/topics/readRDS\">`readRDS()`</a>, which save R objects in an R-specific file format, with the data structure intact. That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in. `saveRDS()` takes two arguments, `object`, pointing to the R object to save and `file` pointing to where to save it to. `readRDS()` expects `file`, referring to the path to the RDS file to read in.\n\nIn this example we're going to modify the data you already read in, which is predefined as `csv_data`, and write the modified version out to a file before reading it in again.\n\n**Steps**\n\n1. Modify `csv_data` to add the column `square_weight`, containing the square of the `weight` column.\n2. Save it to disk as `\"modified_feed_data.RDS\"` with `saveRDS()`.\n3. Read it back in as `modified_feed_data` with `readRDS()`.\n4. Examine `modified_feed_data`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add a new column: square_weight\ncsv_data$square_weight <- (csv_data$weight ^ 2)\n\n# Save it to disk with saveRDS()\nsaveRDS(object = csv_data, file = \"data/modified_feed_data.RDS\")\n\n# Read it back in with readRDS()\nmodified_feed_data <- readRDS(file = \"data/modified_feed_data.RDS\")\n\n# Examine modified_feed_data\nstr(modified_feed_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> 'data.frame':\t71 obs. of  3 variables:\n#>  $ weight       : int  179 160 136 227 217 168 108 124 143 140 ...\n#>  $ feed         : chr  \"horsebean\" \"horsebean\" \"horsebean\" \"horsebean\" ...\n#>  $ square_weight: num  32041 25600 18496 51529 47089 ...\n```\n\n\n:::\n:::\n\n\nWell done. As you can see if you examine `modified_feed_data`, the changes you made before saving it to disk are reflected in the data.\n\n## Understanding Application Programming Interfaces\n\nTheory. Coming soon ...\n\n\n**1. Understanding Application Programming Interfaces**\n\nSo far you've used techniques for downloading static files and keeping them around, but most data on the Internet doesn't take that form. It's simply not practical: if you've got complex data, or simply a lot of it, you'd have to waste time and space constantly copying data into CSV files in the background every time something changes. For websites like Wikipedia and Facebook, there are hundreds of millions of changes every day, so it doesn't really make sense as an approach.\n\n**2. Application Programming Interfaces**\n\nInstead, people often make data available behind Application Programming Interfaces, or APIs. These are programs designed to let code interact with a website, in the same way you interact with a web page, and they're used pretty much everywhere. One of the places they're used is in making data available. Instead of a website having to constantly write data to CSVs, it can just hook an API up to its database. Instead of the user having to type in a new URL each time, they can make small, consistent modifications that change the instructions the API is given for what to send back.\n\n**3. API Clients**\n\nWhat those URLs and instructions look like is something we'll discuss later in the course, because one of the great things about APIs on the web is they make it trivial to write API clients: software libraries in various languages that make it possible for programmers or data scientists to interact with the API without ever having to care about how it's structured, or do the heavy lifting of cleaning up the data.\n\n**4. Using API Clients**\n\nNine times out of ten, if you're looking to get data out of an API, R probably has a client for it, that is, an R package that contains functions to retrieve that data.One simple, useful trick is to Google \"CRAN\" and then the name of the website to see if one exists. And if there is a client, you should rely on it as much as possible, because much of the time it's a lot easier than writing your own.\n\n**5. pageviews**\n\nNow you're going see an example of this using the pageviews package, which lets you see the number of views of Wikipedia pages. All you have to do is to call the function article_pageviews(), passing the name of a Wikipedia article.You're calling an R function, just like any other, and do not need to worry about the details of the API.\n\n**6. Let's practice!**\n\nNow it's your turn to try an example.\n\n## API test\n\nIn the last video you were introduced to Application Programming Interfaces, or APIs, along with their intended purpose (as the computer equivalent of the visible web page that you and I might interact with) and their utility for data retrieval. \n\n> *Question*\n> ---\n> What are APIs for?<br>\n> <br>\n> ⬜ Making parts of a website available to people.<br>\n> ⬜ Making parts of a website available to puppies.<br>\n> ✅ Making parts of a website available to computers.<br>\n\nCorrect! Websites let you and I interpret and interface with a server, APIs allow computers to do the same.\n\n## Using API clients\n\nSo we know that APIs are server components to make it easy for your code to interact with a service and get data from it. We also know that R features many \"clients\" - packages that wrap around connections to APIs so you don't have to worry about the details.\n\nLet's look at a really simple API client - the `pageviews` package, which acts as a client to Wikipedia's API of pageview data. As with other R API clients, it's formatted as a package, and lives on CRAN - the central repository of R packages. The goal here is just to show how simple clients are to use: they look just like other R code, because they *are* just like other R code.\n\n**Steps**\n\n1. Load the package `pageviews`.\n2. Use the `article_pageviews()` function to get the pageviews for the article \"Hadley Wickham\". \n3. Examine the resulting object.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load pageviews\nlibrary(pageviews)\n\n# Get the pageviews for \"Hadley Wickham\"\nhadley_pageviews <- article_pageviews(project = \"en.wikipedia\", article = \"Hadley Wickham\")\n\n# Examine the resulting object\nstr(hadley_pageviews)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> 'data.frame':\t1 obs. of  8 variables:\n#>  $ project    : chr \"wikipedia\"\n#>  $ language   : chr \"en\"\n#>  $ article    : chr \"Hadley_Wickham\"\n#>  $ access     : chr \"all-access\"\n#>  $ agent      : chr \"all-agents\"\n#>  $ granularity: chr \"daily\"\n#>  $ date       : POSIXct, format: \"2015-10-01\"\n#>  $ views      : num 53\n```\n\n\n:::\n:::\n\n\nNicely done. Hopefully this has demonstrated the crucial (and rather lovely) thing about API clients; they behave just like any other R package.\n\n## Access tokens and APIs\n\nTheory. Coming soon ...\n\n\n**1. Access Tokens and APIs**\n\nAs you can see from the previous exercise, API clients are super easy to use: they're just R packages. In the background, they're communicating over the Internet with the API, but that's nicely abstracted away: clients mean you don't have to care what the API is doing, or how it's structured. You can just write R code.\n\n**2. API etiquette**\n\nThe people who maintain the API, however, absolutely care what your client is doing. If many people are simultaneously trying to get a massive amount of data out of it, that can risk overwhelming the system. As a result, some APIs have access tokens: little per-user or per-session keys that identify the person making the API requests, making it easy for the operators to limit you or shut you down if you're causing problems with their service.\n\n**3. Getting access tokens**\n\nGetting an access token is normally pretty simple, and tends to require registration with the API you're trying to use. An example, and one we're going to play around with, is Wordnik, an online dictionary service that contains a ton of interesting metadata about English-language words. Their service requires an access token to use, which you gain by filling out a form that explains what you're trying to do and giving them your email address so they know who to yell at if you break their service.\n\n**4. birdnik**\n\nNow you'll try an example using the birdnik package, which wraps the Wordnik API. This gives you information about English words, such as the frequency that it is used.The API key is just a random string of letters and numbers that is unique to you. To use it, you just pass it as an argument to the birdnik functions.\n\n**5. Let's practice!**\n\nLet's try an example.\n\n## Using access tokens\n\nAs we discussed in the last video, it's common for APIs to require *access tokens* - unique keys that verify you're authorised to use a service. They're usually pretty easy to use with an API client.\n\nTo show how they work, and how easy it can be, we're going to use the R client for the Wordnik dictionary and word use service - 'birdnik' - and an API token we prepared earlier. Birdnik is fairly simple (I wrote it!) and lets you get all sorts of interesting information about word usage in published works. For example, to get the frequency of the use of the word \"chocolate\", you would write:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nword_frequency(api_key, \"chocolate\")\n```\n:::\n\n\nIn this exercise we're going to look at the word \"vector\" (since it's a common word in R!) using a pre-existing API key (stored as `api_key`)\n\n**Steps**\n\n1. Load the package `birdnik`.\n2. Using the pre-existing API key and `word_frequency()`, get the frequency of the word `\"vector\"` in Wordnik's database. Assign the results to `vector_frequency`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Api key is stored loacally\napi_key <- Sys.getenv(\"API_KEY\")\n\n# Load birdnik\nlibrary(birdnik)\n\n# Get the word frequency for \"vector\", using api_key to access it\nvector_frequency <- word_frequency(key = api_key, words = \"vector\")\n```\n:::\n\n\nWell done. Even with API keys, using API clients is pretty convenient!\n\n# 2. Using httr to interact with APIs directly\n\nIf an API client doesn't exist, it's up to you to communicate directly with the API. But don't worry, the package `httr` makes this really straightforward. In this chapter you'll learn how to make web requests from R, how to examine the responses you get back and some best practices for doing this in a responsible way.\n\n## GET and POST requests in theory\n\nTheory. Coming soon ...\n\n\n**1. GET and POST requests in theory**\n\nSometimes you'll run into situations where you don't have an API client available, and at that point you need to be thinking about how to handle API interactions yourself. The first part of that is understanding HTTP and HTTPS - the basics, at least - and how communication with web APIs works. With that in mind, we'll be discussing web requests as a concept, and how R implements them.\n\n**2. HTTP requests**\n\nInteractions with an API on the Internet are best understood as a conversation between two parties; the client - that's you - and the server. The first part of that conversation is the client explaining what it wants to happen, and in HTTP that's divided up into what are known as \"request methods\": different classes of requests, each meaning a different thing.\n\n**3. GET and POST**\n\nThe most common is GET requests, and they're exactly what they sound like: the client saying \"hey, get me this thing.\" If you're asking for data, you're most likely looking to make GET requests. The other common method is POST requests, which are, instead of \"get me this thing,\" \"here is this thing\" - they're you giving something to the server. That's how file uploading works, and is sometimes part of an authentication process to a server.So to use a made-up example; suppose you want to ask for a dataset, modify it locally, and then send it back to the server. You'd ask for the dataset with a GET request, tweak it, and then send it back with a POST request.\n\n**4. Other types**\n\nThere are other types of request too - for example, HEAD requests, which don't return content but do give you metadata about the content, and DELETE requests which (just as the name suggests) asks the server to remove a particular thing - but you probably won't have to work with those.Instead, let's focus on GET and POST requests in R, using the wonderful httr package, which makes interacting with servers and their APIs pretty easy.\n\n**5. Making GET requests with httr**\n\nTo make a GET request, you call the GET() function, passing the URL to get things from. This returns a response object. To get the data contents from this, there is one more step, namely to call the function content().\n\n**6. Making POST requests with httr**\n\nSimilarly, to make a POST request, you call the POST() function. In this case, you don't need to extract contents because you are sending things to somewhere else, and that's something that the recipient needs to worry about.\n\n**7. Let's practice!**\n\nNow you're going to try some examples using httpbin, which is a service for testing HTTP requests.\n\n## GET requests in practice\n\nTo start with you're going to make a GET request. As discussed in the video, this is a request that asks the server to give you a particular piece of data or content (usually specified in the URL). These make up the majority of the requests you'll make in a data science context, since most of the time you'll be getting data from servers, not giving it to them.\n\nTo do this you'll use the `httr` package, written by Hadley Wickham (of course), which makes HTTP requests extremely easy. You're going to make a very simple GET request, and then inspect the output to see what it looks like.\n\n**Steps**\n\n1. Load the `httr` package.\n2. Use the `GET()` function to make a request to `http://httpbin.org/get`, saving the result to `get_result`.\n3. Print `get_result` to inspect it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the httr package\nlibrary(httr)\n\n# Make a GET request to http://httpbin.org/get\nget_result <- GET(url = \"http://httpbin.org/get\")\n\n# Print it to inspect it\nget_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Response [http://httpbin.org/get]\n#>   Date: 2024-05-05 10:16\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 364 B\n#> {\n#>   \"args\": {}, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#>     \"Host\": \"httpbin.org\", \n#>     \"User-Agent\": \"libcurl/8.4.0 r-curl/5.2.1 httr/1.4.7\", \n#>     \"X-Amzn-Trace-Id\": \"Root=1-66375c61-22785e02677bff1f50457d84\"\n#>   }, \n#>   \"origin\": \"2.58.73.242\", \n#> ...\n```\n\n\n:::\n:::\n\n\nNicely done. As you can see from inspecting the output, there are a lot of parts of a HTTP response. Most of them you don't have to worry about right now; some, like the status code and the content, we'll cover later.\n\n## POST requests in practice\n\nNext we'll look at POST requests, also made through httr, with the function (you've guessed it) <a href=\"https://www.rdocumentation.org/packages/httr/topics/POST\">`POST()`</a>. Rather than asking the server to give you something, as in GET requests, a POST request asks the server to accept something *from* you. They're commonly used for things like file upload, or authentication. As a result of their use for uploading things, `POST()` accepts not just a `url` but also a `body` argument containing whatever you want to give to the server.\n\nYou'll make a very simple POST request, just uploading a piece of text, and then inspect the output to see what it looks like.\n\n**Steps**\n\n1. Load the `httr` package.\n2. Make a POST request with the URL `http://httpbin.org/post` and the body `\"this is a test\"`, saving the result to `post_result`.\n3. Print `post_result` to inspect it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the httr package\nlibrary(httr)\n\n# Make a POST request to http://httpbin.org/post with the body \"this is a test\"\npost_result <- POST(url = \"http://httpbin.org/post\", body = \"this is a test\")\n\n# Print it to inspect it\npost_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Response [http://httpbin.org/post]\n#>   Date: 2024-05-05 10:16\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 471 B\n#> {\n#>   \"args\": {}, \n#>   \"data\": \"this is a test\", \n#>   \"files\": {}, \n#>   \"form\": {}, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#>     \"Content-Length\": \"14\", \n#>     \"Host\": \"httpbin.org\", \n#> ...\n```\n\n\n:::\n:::\n\n\nNicely done. The output for POST requests looks pretty similar to that for GET requests, although (in this case) the body of your message is included - `this is a test`. Again, we'll dig into certain elements of the output in just a bit.\n\n## Extracting the response\n\nMaking requests is all well and good, but it's also not why you're here. What we really want to do is get the data the server sent back, which can be done with httr's `content()` function. You pass it an object returned from a `GET` (or `POST`, or `DELETE`, or…) call, and it spits out whatever the server actually sent in an R-compatible structure.\n\nWe're going to demonstrate that now, using a slightly more complicated URL than before - in fact, using a URL from the Wikimedia pageviews system you dealt with through the `pageviews` package, which is stored as `url`. Without looking *too* much at the structure for the time being (we'll get to that later) this request asks for the number of pageviews to the English-language Wikipedia's \"Hadley Wickham\" article on 1 and 2 January 2017.\n\n**Steps**\n\n1. Make a GET request using the `url` object as the URL. Save the results as `pageview_response`.\n2. Call `content()` on `pageview_response` to retrieve the data the server has sent back. Save the data as `pageview_data`.\n3. Examine `pageview_data` with `str()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nurl <- \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102\"\n\n# Make a GET request to url and save the results\npageview_response <- GET(url)\n\n# Call content() to retrieve the data the server sent back\npageview_data <- content(pageview_response)\n\n# Examine the results with str()\nstr(pageview_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> List of 1\n#>  $ items:List of 2\n#>   ..$ :List of 7\n#>   .. ..$ project    : chr \"en.wikipedia\"\n#>   .. ..$ article    : chr \"Hadley_Wickham\"\n#>   .. ..$ granularity: chr \"daily\"\n#>   .. ..$ timestamp  : chr \"2017010100\"\n#>   .. ..$ access     : chr \"all-access\"\n#>   .. ..$ agent      : chr \"all-agents\"\n#>   .. ..$ views      : int 45\n#>   ..$ :List of 7\n#>   .. ..$ project    : chr \"en.wikipedia\"\n#>   .. ..$ article    : chr \"Hadley_Wickham\"\n#>   .. ..$ granularity: chr \"daily\"\n#>   .. ..$ timestamp  : chr \"2017010200\"\n#>   .. ..$ access     : chr \"all-access\"\n#>   .. ..$ agent      : chr \"all-agents\"\n#>   .. ..$ views      : int 86\n```\n\n\n:::\n:::\n\n\nGreat! As you can see, the result of extracting the content is a list, which is pretty common (but not uniform) to API responses. We'll talk through how to manipulate returned data later on.\n\n## Multiple Choice: GET and POST requests\n\nWe've now discussed multiple types of HTTP request - including GET requests, for retrieving data, and POST requests, for transmitting it - as well as how to extract the server response once a request is complete. \n\n> *Question*\n> ---\n> What sort of request would you use to send a server data?<br>\n> <br>\n> ⬜ GET request<br>\n> ✅ POST request<br>\n\n\"Correct! POST requests send data - GET requests retrieve it.\"\n\n## Graceful httr\n\nTheory. Coming soon ...\n\n\n**1. Graceful httr**\n\nYou've played around with basic httr usage: great. The next step is writing code that is \"graceful,\" by which I mean: code that can respond appropriately to errors, and code which can construct its own URLs in response to user input (and so be applied to lots of different situations).\n\n**2. Error handling**\n\nThe results of requests come back with an HTTP status code, which indicates what happened to the request.\n\n**3. Error handling**\n\nYou can see that in the body of a result, if you print it - take a look at this example.If it's got a value of 200, the request was completed; if it was a 404, the server has no idea where the thing you were asking for lives. There are a whole range of different status codes, most of which we are not going to dig into because there are a lot of them.\n\n**4. Understanding status codes**\n\nAs a general rule: if the code starts with 2 or 3, it's fine. If it starts with 4, your code is messing up. If it starts with 5, their code is messing up. You don't have to worry about much of the specifics unless you see such an error code, and in that case, Wikipedia has a convenient list of codes and their explanation.Still, you'll want to check for them, so you know if your request fell over, and httr has a function for just that, http_error(), which we'll explore shortly.\n\n**5. URL construction**\n\nThe next big thing is URL construction. Writing out the whole URL every time is all well and good, but what if you want to make different requests for different assets? What if you don't want to have to think about most of the URL, just the bit you're changing?Automatically constructing URLs - stitching together standard bits and the bits specific to your request - solves for this nicely. It means you don't have to think about the bits that never change, or type out the whole thing each time, and neither does anyone else using your code. You just put in the essential bits and off you go.\n\n**6. Directory-based URLs**\n\nAPI URLs tend to take one of two forms. The first is directory-based URLs: they use directories to represent different parameters. So if you have a URL that goes fakeurl-dot-com/api/peaches/thursday to find out the price of peaches on Thursday, fakeurl-dot-com/api/apples/tuesday would tell you (if the API's author is doing their job right) the price of apples on Tuesday. We saw an example of this with the pageviews package.These are really, really common, particularly with more modern APIs: the good news is they're also really easy to construct with the function paste(), which glues strings together.\n\n**7. Parameter-based URLs**\n\nAlternatively, you get parameter-based URLs. These work via key-value pairs in the URL: the URL would contain a base URL, followed by a question mark, then parameters written as key equals value, and separated with ampersands.With the fruit example, the URL would look contain fruit equals \"peaches\" and day equals \"thursday\".You can also use paste() to construct parameter-based URLs, but this is rather fiddly. Fortunately, the GET() function allows you to pass a named list of parameters to its query argument, and will handle all this construction for you.\n\n**8. Let's practice!**\n\nLet's try some examples.\n\n## Handling http failures\n\nAs mentioned, HTTP calls can go wrong. Handling that can be done with httr's <a href=\"https://www.rdocumentation.org/packages/httr/topics/http_error\">`http_error()`</a> function, which identifies whether a server response contains an error.\n\nIf the response does contain an error, calling `http_error()` over the response will produce `TRUE`; otherwise, `FALSE`. You can use this for really fine-grained control over results. For example, you could check whether the request contained an error, and (if so) issue a warning and re-try the request.\n\nFor now we'll try something a bit simpler - issuing a warning that something went wrong if `http_error()` returns `TRUE`, and printing the content if it doesn't.\n\n**Steps**\n\n1. Make a httr `GET()` request to the URL stored as `fake_url`, and store the result as `request_result`.\n2. If `http_error()` returns `TRUE`, use `warning()` to issue the warning \"The request failed\".\n3. If not, use `content()` (as demonstrated in previous exercises) to print the contents of the result.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfake_url <- \"http://google.com/fakepagethatdoesnotexist\"\n\n# Make the GET request\nrequest_result <- GET(fake_url)\n\n# Check request_result\nif (http_error(request_result)){\n  warning(\"The request failed\")\n} else {\n  content(request_result)\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: The request failed\n```\n\n\n:::\n:::\n\n\nWell done! Since the url mentioned in `fake_url` doesn't exist, the code threw the warning message you mentioned. Error handling is really important for writing robust code, and it looks like you've got a good handle on it.\n\n## Constructing queries (Part I)\n\nAs briefly discussed in the previous video, the actual API query (which tells the API what you want to do) tends to be in one of the two forms. The first is directory-based, where values are separated by `/` marks within the URL. The second is parameter-based, where all the values exist at the end of the URL and take the form of `key=value`.\n\nConstructing directory-based URLs can be done via <a href=\"https://www.rdocumentation.org/packages/base/topics/paste\">`paste()`</a>, which takes an unlimited number of strings, along with a separator, as `sep`. So to construct `http://swapi.co/api/vehicles/12` we'd call:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npaste(\"http://swapi.co\", \"api\", \"vehicles\", \"12\", sep = \"/\")\n```\n:::\n\n\nLet's do that now! We'll cover parameter-based URLs later. In the mean time we can play with SWAPI, mentioned above, which is an API chock full of star wars data. This time, rather than a vehicle, we'll look for a person.\n\n**Steps**\n\n1. `httr` is loaded in your workspace.\n\n    * Construct a directory-based API URL to `http://swapi.co/api`, looking for person `1` in `people`.\n    * Assign the URL to `directory_url`.\n    * Use `GET` to make an API call with `directory_url`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Construct a directory-based API URL to `http://swapi.co/api`,\n# looking for person `1` in `people`\ndirectory_url <- paste(\"http://swapi.co/api\", \"people\", \"1\", sep = \"/\")\n\n# Make a GET call with it\nresult <- GET(directory_url)\n```\n:::\n\n\nGood work! Constructing these kinds of queries is very simple, but also extremely important. Now you know how to write automated or semi-automated code for more modern APIs.\n\n## Constructing queries (Part II)\n\nAs mentioned (albeit briefly) in the last exercise, there are also *parameter* based URLs, where all the query values exist at the end of the URL and take the form of `key=value` - they look something like `http://fakeurl.com/foo.php?country=spain&amp;food=goulash`\n\nConstructing parameter-based URLs can *also* be done with `paste()`, but the easiest way to do it is with `GET()` and `POST()` themselves, which accept a `query` argument consisting of a list of keys and values. So, to continue with the food-based examples, we could construct `fakeurl.com/api.php?fruit=peaches&amp;day=thursday` with:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGET(\"fakeurl.com/api.php\", query = list(fruit = \"peaches\", day = \"thursday\"))\n```\n:::\n\n\nIn this exercise you'll construct a call to `https://httpbin.org/get?nationality=americans&amp;country=antigua`.\n\n**Steps**\n\n1. Start by contructing the `query_params` list, with a `nationality` parameter of `\"americans\"` and a `country` parameter of `\"antigua\"`.\n2. Construct a parameter-based call to `https://httpbin.org/get`, using `GET()` passing `query_params` to the `query` arugment.\n3. Print the response `parameter_response`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create list with nationality and country elements\nquery_params <- list(nationality = \"americans\", \n    country = \"antigua\")\n    \n# Make parameter-based call to httpbin, with query_params\nparameter_response <- GET(\"https://httpbin.org/get\", query = query_params)\n\n# Print parameter_response\nparameter_response\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Response [https://httpbin.org/get?nationality=americans&country=antigua]\n#>   Date: 2024-05-05 10:16\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 464 B\n#> {\n#>   \"args\": {\n#>     \"country\": \"antigua\", \n#>     \"nationality\": \"americans\"\n#>   }, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#>     \"Host\": \"httpbin.org\", \n#>     \"User-Agent\": \"libcurl/8.4.0 r-curl/5.2.1 httr/1.4.7\", \n#> ...\n```\n\n\n:::\n:::\n\n\nNice job!  Did you notice the parameters you passed in were listed in the `args` of the response?\n\n## Respectful API usage\n\nTheory. Coming soon ...\n\n\n**1. Respectful API Usage**\n\nOkay, so you know how to make requests to APIs, and you know how to construct requests to APIs. Now it's important to discuss how to query APIs in a way that not only works, but also avoids annoying the people who run them. A while back you covered access tokens - unique keys you use to authenticate to an API - and talked about how one use of them was to allow the API's developers to cut you off if you misbehave. In this video we're going to talk about how to behave so this doesn't happen.\n\n**2. User agents**\n\nWhen you make an API query, the person at the other end doesn't know you from Adam. You could be anyone! But if something is going wrong they might need to contact you, or at least know vaguely what software you're using. For this reason, it's possible to send what's known as a user agent - a piece of text that identifies the user - as part of any request. I usually try to make sure that a user agent contains my email address, if it's something I'm just running on my own. That way if there's a problem with my code, the API developers can contact me and take issue.Previously, you called GET() and POST() with a single argument, the URL. To include your personal details, you also need to specify the config argument. This should be the result of a call to the function user_agent(), which takes a string containing whatever details about yourself that you wish to provide.\n\n**3. Rate limiting**\n\nMany APIs have a deliberate limit on how many requests you can send in a given time period to avoid ever running into service issues. Exceed that limit, you get blocked; stay under it, you don't.The way to avoid this is, well, making sure there's an interval between your requests, which is known as rate limiting. If you're only allowed 60 requests a minute, for example, you'd intentionally write your code so that it guaranteed one second would pass between requests - at which point you don't have to worry. Doing that in R is pretty simple using the Sys dot sleep() function. This simply tells R to do nothing for a specified time period.With all of this, you should be pretty set: you'll know how to make requests, how to structure them, and how to automate them in a way that doesn't ruin some far-away developer's day.\n\n**4. Let's practice!**\n\nTime to try some examples.\n\n## Using user agents\n\nAs discussed in the video, informative user-agents are a good way of being respectful of the developers running the API you're interacting with. They make it easy for them to contact you in the event something goes wrong. I always try to include:\n\n1. My email address;\n2. A URL for the project the code is a part of, if it's got a URL.\n\nBuilding user agents is done by passing a call to `user_agent()` into the `GET()` or `POST()` request; something like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGET(\"http://url.goes.here/\", user_agent(\"somefakeemail@domain.com http://project.website\"))\n```\n:::\n\n\nIn the event you don't have a website, a short one-sentence description of what the project is about serves pretty well.\n\n**Steps**\n\n1. Make a `GET()` request to `url`.\n2. Include a user agent that has a fake email address `\"my@email.address\"` followed by the sentence `\"this is a test\"`.\n3. Assign the response to `server_response`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Do not change the url\nurl <- \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100\"\n\n# Add the email address and the test sentence inside user_agent()\nserver_response <- GET(url, user_agent(\"my@email.address this is a test\"))\n```\n:::\n\n\nGood work! From your end, the request looks exactly the same with or without a user agent, but for the server the difference can be vital.\n\n## Rate-limiting\n\nThe next stage of respectful API usage is rate-limiting: making sure you only make a certain number of requests to the server in a given time period. Your limit will vary from server to server, but the implementation is always pretty much the same and involves a call to <a href=\"https://www.rdocumentation.org/packages/base/topics/Sys.sleep\">`Sys.sleep()`</a>. This function takes one argument, a number, which represents the number of seconds to \"sleep\" (pause) the R session for. So if you call `Sys.sleep(15)`, it'll pause for 15 seconds before allowing further code to run.\n\nAs you can imagine, this is really useful for rate-limiting. If you are only allowed 4 requests a minute? No problem! Just pause for 15 seconds between each request and you're guaranteed to never exceed it. Let's demonstrate now by putting together a little loop that sends multiple requests on a 5-second time delay. We'll use `httpbin.org`\n's APIs, which allow you to test different HTTP libraries.\n\n**Steps**\n\n1. Construct a vector of 2 URLs, `http://httpbin.org/status/404` and `http://httpbin.org/status/301`.\n2. Write a for-loop that sends a `GET()` request to each one.\n3. Ensure that the for-loop uses `Sys.sleep()` to delay for 5 seconds between request.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Construct a vector of 2 URLs\nurls <- c(\"http://httpbin.org/status/404\",\n          \"http://httpbin.org/status/301\")\n\nfor(url in urls){\n    # Send a GET request to url\n    result <- GET(url)\n    # Delay for 5 seconds between requests\n    Sys.sleep(5)\n}\n```\n:::\n\n\nNice work. With this and the user agents we covered in the last exercise, you know enough to avoid annoying servers - and thus snarling up your code\n\n## Tying it all together\n\nUsing everything that you learned in the chapter, let's make a simple replica of one of the 'pageviews' functions - building queries, sending GET requests (with an appropriate user agent) and handling the output in a fault-tolerant way. You'll build this function up piece by piece in this exercise.\n\nTo output an error, you will use the function <a href=\"https://www.rdocumentation.org/packages/base/topics/stop\">`stop()`</a>, which takes a string as an argument, *stops* the execution of the program, and outputs the string as an error.  You can try it right now by running `stop(\"This is an error\")`.\n\n**Steps**\n\n1. First, get the function to construct the url.  \n\n    * In the call to `paste()`, add `article_title` as the second argument to construct `url`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    # Include article title\n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  ) \n  url\n}\n```\n:::\n\n\n2. Now, make the request.\n\n    * Use `GET()` to request `url` with a user agent `\"my@email.com this is a test\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  ) \n  # Get the webpage   \n  response <- GET(url, config = user_agent(\"my@email.com this is a test\")) \n  response\n}\n```\n:::\n\n\n3. Now, add an error check.\n\n    * Check the response for errors with `http_error()`, throwing an error of `\"the request failed\"` with `stop()` if there was one.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  )   \n  response <- GET(url, user_agent(\"my@email.com this is a test\")) \n  # Is there an HTTP error?\n  if(http_error(response)){ \n    # Throw an R error\n    stop(\"the request failed\") \n  }\n  response\n}\n```\n:::\n\n\n4. Finally, instead of returning `response`, return the `content()` of the response.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  )   \n  response <- GET(url, user_agent(\"my@email.com this is a test\")) \n  # Is there an HTTP error?\n  if(http_error(response)){ \n    # Throw an R error\n    stop(\"the request failed\") \n  }\n  # Return the response's content\n  content(response)\n}\n```\n:::\n\n\nCongratulations! You've finished the chapter, and should now have a good handle on how to work with httr. Our next chapter will cover digging into the results you get back from your requests.\n\n# 3. Handling JSON and XML\n\nSometimes data is a TSV or nice plaintext output. Sometimes it's XML and/or JSON. This chapter walks you through what JSON and XML are, how to convert them into R-like objects, and how to extract data from them. You'll practice by examining the revision history for a Wikipedia article retrieved from the Wikipedia API using httr, xml2 and jsonlite.\n\n## JSON\n\nTheory. Coming soon ...\n\n\n**1. JSON**\n\nIn the previous chapter you learned how to ask for data from an API.  In this chapter you'll learn about two of the most common data formats that APIs return.Let's start with JSON, a particularly common format for storing and transmitting data on the web.  Later in the chapter you'll learn about another format: XML.\n\n**2. JSON (JavaScript Object Notation)**\n\nMuch like a CSV file, a JSON data file is just a plain text file, but with special conventions for describing data structures.  Unlike a CSV, JSON can accommodate data that is more complicated than a simple table and that is what makes it so useful.All JSON files are made up of only two kinds of structures: objects and arrays. Objects are collections of named values. In JSON an object starts with a left brace and ends with a right brace. Then a named value consists of a name in quotes followed by a colon, then the value.  Name value pairs are separated by a comma.Arrays are an ordered list of values. An array begins with a left bracket and ends with a right bracket, and values are separated by a comma.In both objects and arrays a value can be a string in double quotes, a number, \"true\", \"false\", \"null\", or another object or array.   Because objects and arrays can occur anywhere a value can, complicated hierarchical structures are easily represented.\n\n**3. An example JSON data set**\n\nLet's take a look at this JSON example that describes a couple of movies.  It consists of an array with two elements: each an object.  Each object has two named values: title and year.\n\n**4. Identifying a JSON response**\n\nHow do you know if an API has given you JSON data as a response? Usually you'll already have a pretty good idea based on the API documentation - it either always returns JSON, or you have specifically requested a JSON object. However, it's always worth checking you got what you expected.First check the type as reported in the header of the response using the http_type() function.  This should take the value \"application/json\".\n\n**5. Identifying a JSON response**\n\nThen you can examine the content of the response as plain text, by using the content() function. Wrapping this in writeLines() will give you a printout of the content which you should be able to recognize as JSON.\n\n**6. Let's practice!**\n\nTime to try some examples.\n\n## Can you spot JSON?\n\nHere is some information on a couple of fictional Jasons stored in different formats.\n\nA:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.json .cell-code}\n<jason>\n  <person>\n    <first_name>Jason</first_name>\n    <last_name>Bourne</last_name>\n    <occupation>Spy</occupation>\n  </person>\n  <person>\n    <first_name>Jason</first_name>\n    <last_name>Voorhees</last_name>\n    <occupation>Mass murderer</occupation>\n  </person>\n</jason>\n```\n:::\n\n\nB:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.json .cell-code}\nfirst_name, last_name, occupation\nJason, Bourne, Spy\nJason, Voorhees, Mass murderer\n```\n:::\n\n\nC:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.json .cell-code}\n[{ \n   \"first_name\": \"Jason\",\n   \"last_name\": \"Bourne\",\n   \"occupation\": \"Spy\"\n },\n{\n  \"first_name\": \"Jason\",\n  \"last_name\": \"Voorhees\",\n  \"occupation\": \"Mass murderer\"\n}]\n```\n:::\n\n\n> *Question*\n> ---\n>  Which one is JSON?<br>\n> <br>\n> ⬜ A<br>\n> ⬜ B<br>\n> ✅ C<br>\n\nYou got it! This JSON represents an array of two objects, each a different character called Jason.\n\n## Parsing JSON\n\nWhile JSON is a useful format for sharing data, your first step will often be to parse it into an R object, so you can manipulate it with R.\n\nThe <a href=\"https://www.rdocumentation.org/packages/httr/topics/content\">`content()`</a> function in `httr` retrieves the content from a request.  It takes an `as` argument that specifies the type of output to return.  You've already seen that `as = \"text\"` will return the content as a character string which is useful for checking the content is as you expect.\n\nIf you don't specify `as`, the default `as = \"parsed\"` is used. In this case the type of `content()` will be guessed based on the header and `content()` will choose an appropriate parsing function.  For JSON this function is <a href=\"https://www.rdocumentation.org/packages/jsonlite/topics/fromJSON\">`fromJSON()`</a> from the `jsonlite` package.  If you know your response is JSON, you may want to use `fromJSON()` directly.\n\nTo practice, you'll retrieve some revision history from the Wikipedia API, check it is JSON, then parse it into a list two ways.\n\n**Steps**\n\n1. Get the revision history for the Wikipedia article for `\"Hadley Wickham\"`, by calling `rev_history(\"Hadley Wickham\")` (a function we have written for you), store it in `resp_json`.\n2. Check the `http_type()` of `resp_json`, to confirm the API returned a JSON object.\n3. You can't always trust a header, so check the content looks like JSON by calling `content()` on `resp_json` with an additional argument, `as = \"text\"`.\n4. Parse `resp_json` using `content()` by explicitly setting `as = \"parsed\"`.\n5. Parse the returned text (from step 3) with `fromJSON()` .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get revision history for \"Hadley Wickham\"\nrevhist_url <- \"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=json&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\"\nresp_json <- GET(revhist_url)\n\n# Check http_type() of resp_json\nhttp_type(resp_json)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"application/json\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Examine returned text with content()\ncontent(resp_json, as = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"{\\\"continue\\\":{\\\"rvcontinue\\\":\\\"20150528042700|664370232\\\",\\\"continue\\\":\\\"||\\\"},\\\"warnings\\\":{\\\"main\\\":{\\\"*\\\":\\\"Subscribe to the mediawiki-api-announce mailing list at <https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/> for notice of API deprecations and breaking changes. Use [[Special:ApiFeatureUsage]] to see usage of deprecated features by your application.\\\"},\\\"revisions\\\":{\\\"*\\\":\\\"Because \\\\\\\"rvslots\\\\\\\" was not specified, a legacy format has been used for the output. This format is deprecated, and in the future the new format will always be used.\\\"}},\\\"query\\\":{\\\"pages\\\":{\\\"41916270\\\":{\\\"pageid\\\":41916270,\\\"ns\\\":0,\\\"title\\\":\\\"Hadley Wickham\\\",\\\"revisions\\\":[{\\\"user\\\":\\\"214.28.226.251\\\",\\\"anon\\\":\\\"\\\",\\\"timestamp\\\":\\\"2015-01-14T17:12:45Z\\\",\\\"contentformat\\\":\\\"text/x-wiki\\\",\\\"contentmodel\\\":\\\"wikitext\\\",\\\"comment\\\":\\\"\\\",\\\"*\\\":\\\"'''Hadley Mary Helen Wickham III''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\\\\\"about\\\\\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\\\n\\\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\\\\\"about\\\\\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\\\"},{\\\"user\\\":\\\"73.183.151.193\\\",\\\"anon\\\":\\\"\\\",\\\"timestamp\\\":\\\"2015-01-15T15:49:34Z\\\",\\\"contentformat\\\":\\\"text/x-wiki\\\",\\\"contentmodel\\\":\\\"wikitext\\\",\\\"comment\\\":\\\"\\\",\\\"*\\\":\\\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\\\\\"about\\\\\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\\\n\\\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\\\\\"about\\\\\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\\\"},{\\\"user\\\":\\\"FeanorStar7\\\",\\\"timestamp\\\":\\\"2015-01-24T16:34:31Z\\\",\\\"contentformat\\\":\\\"text/x-wiki\\\",\\\"contentmodel\\\":\\\"wikitext\\\",\\\"comment\\\":\\\"/* External links */ add LCCN and cats\\\",\\\"*\\\":\\\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\\\\\"about\\\\\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\\\n\\\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\\\\\"about\\\\\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\\\"},{\\\"user\\\":\\\"KasparBot\\\",\\\"timestamp\\\":\\\"2015-04-26T19:18:17Z\\\",\\\"contentformat\\\":\\\"text/x-wiki\\\",\\\"contentmodel\\\":\\\"wikitext\\\",\\\"comment\\\":\\\"authority control moved to wikidata\\\",\\\"*\\\":\\\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\\\\\"about\\\\\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\\\n\\\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\\\\\"about\\\\\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\\\"},{\\\"user\\\":\\\"Spkal\\\",\\\"timestamp\\\":\\\"2015-05-06T18:24:57Z\\\",\\\"contentformat\\\":\\\"text/x-wiki\\\",\\\"contentmodel\\\":\\\"wikitext\\\",\\\"comment\\\":\\\"/* Bibliography */  Added his new book, R Packages\\\",\\\"*\\\":\\\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\\\\\"about\\\\\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\\\n\\\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\\\\\"about\\\\\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\\\"}]}}}}\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Parse response with content()\ncontent(resp_json, as = \"parsed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $continue\n#> $continue$rvcontinue\n#> [1] \"20150528042700|664370232\"\n#> \n#> $continue$continue\n#> [1] \"||\"\n#> \n#> \n#> $warnings\n#> $warnings$main\n#> $warnings$main$`*`\n#> [1] \"Subscribe to the mediawiki-api-announce mailing list at <https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/> for notice of API deprecations and breaking changes. Use [[Special:ApiFeatureUsage]] to see usage of deprecated features by your application.\"\n#> \n#> \n#> $warnings$revisions\n#> $warnings$revisions$`*`\n#> [1] \"Because \\\"rvslots\\\" was not specified, a legacy format has been used for the output. This format is deprecated, and in the future the new format will always be used.\"\n#> \n#> \n#> \n#> $query\n#> $query$pages\n#> $query$pages$`41916270`\n#> $query$pages$`41916270`$pageid\n#> [1] 41916270\n#> \n#> $query$pages$`41916270`$ns\n#> [1] 0\n#> \n#> $query$pages$`41916270`$title\n#> [1] \"Hadley Wickham\"\n#> \n#> $query$pages$`41916270`$revisions\n#> $query$pages$`41916270`$revisions[[1]]\n#> $query$pages$`41916270`$revisions[[1]]$user\n#> [1] \"214.28.226.251\"\n#> \n#> $query$pages$`41916270`$revisions[[1]]$anon\n#> [1] \"\"\n#> \n#> $query$pages$`41916270`$revisions[[1]]$timestamp\n#> [1] \"2015-01-14T17:12:45Z\"\n#> \n#> $query$pages$`41916270`$revisions[[1]]$contentformat\n#> [1] \"text/x-wiki\"\n#> \n#> $query$pages$`41916270`$revisions[[1]]$contentmodel\n#> [1] \"wikitext\"\n#> \n#> $query$pages$`41916270`$revisions[[1]]$comment\n#> [1] \"\"\n#> \n#> $query$pages$`41916270`$revisions[[1]]$`*`\n#> [1] \"'''Hadley Mary Helen Wickham III''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n#> \n#> \n#> $query$pages$`41916270`$revisions[[2]]\n#> $query$pages$`41916270`$revisions[[2]]$user\n#> [1] \"73.183.151.193\"\n#> \n#> $query$pages$`41916270`$revisions[[2]]$anon\n#> [1] \"\"\n#> \n#> $query$pages$`41916270`$revisions[[2]]$timestamp\n#> [1] \"2015-01-15T15:49:34Z\"\n#> \n#> $query$pages$`41916270`$revisions[[2]]$contentformat\n#> [1] \"text/x-wiki\"\n#> \n#> $query$pages$`41916270`$revisions[[2]]$contentmodel\n#> [1] \"wikitext\"\n#> \n#> $query$pages$`41916270`$revisions[[2]]$comment\n#> [1] \"\"\n#> \n#> $query$pages$`41916270`$revisions[[2]]$`*`\n#> [1] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n#> \n#> \n#> $query$pages$`41916270`$revisions[[3]]\n#> $query$pages$`41916270`$revisions[[3]]$user\n#> [1] \"FeanorStar7\"\n#> \n#> $query$pages$`41916270`$revisions[[3]]$timestamp\n#> [1] \"2015-01-24T16:34:31Z\"\n#> \n#> $query$pages$`41916270`$revisions[[3]]$contentformat\n#> [1] \"text/x-wiki\"\n#> \n#> $query$pages$`41916270`$revisions[[3]]$contentmodel\n#> [1] \"wikitext\"\n#> \n#> $query$pages$`41916270`$revisions[[3]]$comment\n#> [1] \"/* External links */ add LCCN and cats\"\n#> \n#> $query$pages$`41916270`$revisions[[3]]$`*`\n#> [1] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n#> \n#> \n#> $query$pages$`41916270`$revisions[[4]]\n#> $query$pages$`41916270`$revisions[[4]]$user\n#> [1] \"KasparBot\"\n#> \n#> $query$pages$`41916270`$revisions[[4]]$timestamp\n#> [1] \"2015-04-26T19:18:17Z\"\n#> \n#> $query$pages$`41916270`$revisions[[4]]$contentformat\n#> [1] \"text/x-wiki\"\n#> \n#> $query$pages$`41916270`$revisions[[4]]$contentmodel\n#> [1] \"wikitext\"\n#> \n#> $query$pages$`41916270`$revisions[[4]]$comment\n#> [1] \"authority control moved to wikidata\"\n#> \n#> $query$pages$`41916270`$revisions[[4]]$`*`\n#> [1] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n#> \n#> \n#> $query$pages$`41916270`$revisions[[5]]\n#> $query$pages$`41916270`$revisions[[5]]$user\n#> [1] \"Spkal\"\n#> \n#> $query$pages$`41916270`$revisions[[5]]$timestamp\n#> [1] \"2015-05-06T18:24:57Z\"\n#> \n#> $query$pages$`41916270`$revisions[[5]]$contentformat\n#> [1] \"text/x-wiki\"\n#> \n#> $query$pages$`41916270`$revisions[[5]]$contentmodel\n#> [1] \"wikitext\"\n#> \n#> $query$pages$`41916270`$revisions[[5]]$comment\n#> [1] \"/* Bibliography */  Added his new book, R Packages\"\n#> \n#> $query$pages$`41916270`$revisions[[5]]$`*`\n#> [1] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Parse returned text with fromJSON()\nlibrary(jsonlite)\nfromJSON(content(resp_json, as = \"text\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $continue\n#> $continue$rvcontinue\n#> [1] \"20150528042700|664370232\"\n#> \n#> $continue$continue\n#> [1] \"||\"\n#> \n#> \n#> $warnings\n#> $warnings$main\n#> $warnings$main$`*`\n#> [1] \"Subscribe to the mediawiki-api-announce mailing list at <https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/> for notice of API deprecations and breaking changes. Use [[Special:ApiFeatureUsage]] to see usage of deprecated features by your application.\"\n#> \n#> \n#> $warnings$revisions\n#> $warnings$revisions$`*`\n#> [1] \"Because \\\"rvslots\\\" was not specified, a legacy format has been used for the output. This format is deprecated, and in the future the new format will always be used.\"\n#> \n#> \n#> \n#> $query\n#> $query$pages\n#> $query$pages$`41916270`\n#> $query$pages$`41916270`$pageid\n#> [1] 41916270\n#> \n#> $query$pages$`41916270`$ns\n#> [1] 0\n#> \n#> $query$pages$`41916270`$title\n#> [1] \"Hadley Wickham\"\n#> \n#> $query$pages$`41916270`$revisions\n#>             user anon            timestamp contentformat contentmodel\n#> 1 214.28.226.251      2015-01-14T17:12:45Z   text/x-wiki     wikitext\n#> 2 73.183.151.193      2015-01-15T15:49:34Z   text/x-wiki     wikitext\n#> 3    FeanorStar7 <NA> 2015-01-24T16:34:31Z   text/x-wiki     wikitext\n#> 4      KasparBot <NA> 2015-04-26T19:18:17Z   text/x-wiki     wikitext\n#> 5          Spkal <NA> 2015-05-06T18:24:57Z   text/x-wiki     wikitext\n#>                                              comment\n#> 1                                                   \n#> 2                                                   \n#> 3             /* External links */ add LCCN and cats\n#> 4                authority control moved to wikidata\n#> 5 /* Bibliography */  Added his new book, R Packages\n#>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            *\n#> 1 '''Hadley Mary Helen Wickham III''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\"about\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\"about\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\n#> 2                '''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\"about\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\"about\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\n#> 3                '''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\"about\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\"about\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\n#> 4                '''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\"about\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\"about\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\n#> 5                '''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\"about\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\"about\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\n```\n\n\n:::\n:::\n\n\nGreat work! You may have noticed the output from `content()` is pretty long and hard to understand. Don't worry, that is just the nature of nested data, you'll learn a couple of tricks for dealing with it next. However, it will be helpful to know that this response contains 5 revisions.\n\n## Manipulating JSON\n\nTheory. Coming soon ...\n\n\n**1. Manipulating JSON**\n\nYou might have noticed a lot of output scrolling past when you parsed your JSON data. What kind of object was being returned?\n\n**2. Movies example**\n\nLet's take a look using the movies JSON from the previous video. Here's the JSON.\n\n**3. Movies example**\n\nIn R, I'll just store that in a string called movies_json.\n\n**4. Movies example**\n\nNow, look what happens when we parse that with fromJSON().  I've set the simplifyVector argument to FALSE to emulate what happens when you use the content() function without any arguments.What we get back is a list!  Lists are the natural R object for storing JSON data because they can store hierarchical data just like JSON.  fromJSON() will always return a list.  It converts any JSON objects (remember those are the ones with curly braces) to named lists, and any JSON arrays (the ones with square brackets) to unnamed lists.\n\n**5. Simplifying the output (I)**\n\nfromJSON() also provides extra simplification through some of its arguments.  For example, if simplifyVector is true, any arrays of just numbers or strings will be converted to vectors.\n\n**6. Simplifying the output (II)**\n\nTake a look at what happens when you set simplifyDataFrame to TRUE.fromJSON() will convert any arrays of objects to data frames.\n\n**7. Extracting data from JSON (I)**\n\nNow imagine you want to extract all the titles of the movies. One option is to rely on fromJSON() to simplify to a dataframe and pull out the relevant column.\n\n**8. Extracting data from JSON (II)**\n\nThe other is to use the list form and iterate over each element.  Over the next couple of exercises you'll explore doing this two ways: first, with a dedicated package rlist, and second, using functions from base R and the tidyverse.\n\n**9. Let's practice!**\n\n\n\n## Manipulating parsed JSON\n\nAs you saw in the video, the output from parsing JSON is a list. One way to extract relevant data from that list is to use a package specifically designed for manipulating lists, <a href=\"https://www.rdocumentation.org/packages/rlist\">`rlist`</a>.\n\n`rlist` provides two particularly useful functions for selecting and combining elements from a list: <a href=\"https://www.rdocumentation.org/packages/rlist/topics/list.select\">`list.select()`</a> and <a href=\"https://www.rdocumentation.org/packages/rlist/topics/list.stack\">`list.stack()`</a>. `list.select()` extracts sub-elements by name from each element in a list. For example using the parsed movies data from the video (`movies_list`), we might ask for the `title` and `year` elements from each element:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlist.select(movies_list, title, year)\n```\n:::\n\n\nThe result is still a list, that is where `list.stack()` comes in. It will stack the elements of a list into a data frame.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlist.stack(\n    list.select(movies_list, title, year)\n)\n```\n:::\n\n\nIn this exercise you'll use these `rlist` functions to create a data frame with the user and timestamp for each revision.\n\n**Steps**\n\n1. First, you'll need to figure out where the revisions are. Examine the output from the `str()` call. *Can you see where the list of 5 revisions is?* \n2. Store the revisions in `revs`.\n3. Use `list.select()` to pull out the `user` and `timestamp` elements from each revision, store in `user_time`.\n4. Print `user_time` to verify it's a list with one element for each revision.\n5. Use `list.stack()` to stack the lists into a data frame.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load rlist\nlibrary(rlist)\n\n# Examine output of this code\nstr(content(resp_json), max.level = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> List of 3\n#>  $ continue:List of 2\n#>   ..$ rvcontinue: chr \"20150528042700|664370232\"\n#>   ..$ continue  : chr \"||\"\n#>  $ warnings:List of 2\n#>   ..$ main     :List of 1\n#>   .. ..$ *: chr \"Subscribe to the mediawiki-api-announce mailing list at <https://lists.wikimedia.org/postorius/lists/mediawiki-\"| __truncated__\n#>   ..$ revisions:List of 1\n#>   .. ..$ *: chr \"Because \\\"rvslots\\\" was not specified, a legacy format has been used for the output. This format is deprecated,\"| __truncated__\n#>  $ query   :List of 1\n#>   ..$ pages:List of 1\n#>   .. ..$ 41916270:List of 4\n#>   .. .. ..$ pageid   : int 41916270\n#>   .. .. ..$ ns       : int 0\n#>   .. .. ..$ title    : chr \"Hadley Wickham\"\n#>   .. .. ..$ revisions:List of 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Store revision list\nrevs <- content(resp_json)$query$pages$`41916270`$revisions\n\n# Extract the user element\nuser_time <- list.select(revs, user, timestamp)\n\n# Print user_time\nuser_time \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [[1]]\n#> [[1]]$user\n#> [1] \"214.28.226.251\"\n#> \n#> [[1]]$timestamp\n#> [1] \"2015-01-14T17:12:45Z\"\n#> \n#> \n#> [[2]]\n#> [[2]]$user\n#> [1] \"73.183.151.193\"\n#> \n#> [[2]]$timestamp\n#> [1] \"2015-01-15T15:49:34Z\"\n#> \n#> \n#> [[3]]\n#> [[3]]$user\n#> [1] \"FeanorStar7\"\n#> \n#> [[3]]$timestamp\n#> [1] \"2015-01-24T16:34:31Z\"\n#> \n#> \n#> [[4]]\n#> [[4]]$user\n#> [1] \"KasparBot\"\n#> \n#> [[4]]$timestamp\n#> [1] \"2015-04-26T19:18:17Z\"\n#> \n#> \n#> [[5]]\n#> [[5]]$user\n#> [1] \"Spkal\"\n#> \n#> [[5]]$timestamp\n#> [1] \"2015-05-06T18:24:57Z\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Stack to turn into a data frame\nlist.stack(user_time)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"user\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"timestamp\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"214.28.226.251\",\"2\":\"2015-01-14T17:12:45Z\"},{\"1\":\"73.183.151.193\",\"2\":\"2015-01-15T15:49:34Z\"},{\"1\":\"FeanorStar7\",\"2\":\"2015-01-24T16:34:31Z\"},{\"1\":\"KasparBot\",\"2\":\"2015-04-26T19:18:17Z\"},{\"1\":\"Spkal\",\"2\":\"2015-05-06T18:24:57Z\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nPerfect! [`rlist`](https://www.rdocumentation.org/packages/rlist) is designed to make working with lists easy, so if find you are working with JSON data a lot, you should explore more of its functionality.\n\n## Reformatting JSON\n\nOf course you don't have to use `rlist`. You can achieve the same thing by using functions from base R or the tidyverse.  In this exercise you'll repeat the task of extracting the username and timestamp using the <a href=\"https://www.rdocumentation.org/packages/dplyr\">`dplyr`</a> package which is part of the tidyverse.\n\nConceptually, you'll take the list of revisions, stack them into a data frame, then pull out the relevant columns.\n\n`dplyr`'s <a href=\"https://www.rdocumentation.org/packages/dplyr/topics/bind_rows\">`bind_rows()`</a> function takes a list and turns it into a data frame. Then you can use <a href=\"https://www.rdocumentation.org/packages/dplyr/topics/select\">`select()`</a> to extract the relevant columns. And of course if we can make use of the `%>%` (pipe) operator to chain them all together.\n\nTry it!\n\n**Steps**\n\n1. Pipe the list of revisions into  `bind_rows()`.\n2. Use `select()` to extract the `user` and `timestamp` columns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load dplyr\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n#> Attaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\n# Pull out revision list\nrevs <- content(resp_json)$query$pages$`41916270`$revisions\n\n# Extract user and timestamp\nrevs %>%\n  bind_rows() %>%\n  select(user, timestamp)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"user\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"timestamp\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"214.28.226.251\",\"2\":\"2015-01-14T17:12:45Z\"},{\"1\":\"73.183.151.193\",\"2\":\"2015-01-15T15:49:34Z\"},{\"1\":\"FeanorStar7\",\"2\":\"2015-01-24T16:34:31Z\"},{\"1\":\"KasparBot\",\"2\":\"2015-04-26T19:18:17Z\"},{\"1\":\"Spkal\",\"2\":\"2015-05-06T18:24:57Z\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nYou nailed it!\n\n## XML structure\n\nTheory. Coming soon ...\n\n\n**1. XML structure**\n\nXML is another popular format for transmitting data over APIs. Like JSON, it's a plain text format, but it has quite a different structure.\n\n**2. Movies in XML**\n\nLet's take a look at the same movies data you saw in JSON, in XML.The structure of an XML file can be divided into markup and content.  Markup describes the structure of the data, whereas content is the data itself.  Most markup is in the form of what are know as tags.  Tags begin with a less than sign and end with a greater than sign. In this data you can see movies, movie, title and year tags. They generally occur in pairs with a start tag containing just the tag name, and the end tag having a forward slash between the less than sign and the tag name.\n\n**3. Tags can have attributes**\n\nBeyond the tag name, a tag can also contain attributes in the form of name value pairs.  An alternative way of providing the same data might be to use attributes instead of tags. Here instead of a year tag, the year is provided in a year attribute of the title tag.There's no standard on how data should be stored in XML, but usually attributes are reserved for metadata, that is data about the data, and content is used for data.\n\n**4. The hierarchy of XML elements**\n\nSo, in this case since the year is data about a movie, we should probably stick to the original format.\n\n**5. The hierarchy of XML elements**\n\nThe key to navigating XML is understanding its hierarchical structure.  An XML element is everything from and including a start tag to and including the end tag. The content of an element is everything between the tags, including other elements.For example, the first movie element contains two more elements:\n\n**6. The hierarchy of XML elements**\n\na title element and a year element.\n\n**7. The hierarchy of XML elements**\n\nThe contents of the title element is simply the text \"A New Hope\".\n\n**8. Understanding XML as a tree**\n\nYou can think of an XML document as a tree where the nodes are elements or text. You can describe the relationships between the nodes just like you would a family tree.\n\n**9. Understanding XML as a tree**\n\nYou would say that this title element is a child of the first movie element because it is contained inside the first movie element.  Equivalently, you could say the first movie element is the parent of this title element.\n\n**10. Understanding XML as a tree**\n\nThese title and year elements are siblings, because they share the same parent: movie.\n\n**11. Understanding XML as a tree**\n\nJust like the two movie elements are siblings because they share the same parent: movies.\n\n**12. Let's practice!**\n\nOver the next few exercises you'll use the xml2 package to parse XML, and examine its structure with the xml_structure() function.\n\n## Do you understand XML structure?\n\nTake a look at this XML document:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.xml .cell-code}\n<jason>\n  <person type = \"fictional\">\n    <first_name>\n      Jason\n    </first_name>\n    <last_name>\n      Bourne\n    </last_name>\n    <occupation>\n      Spy\n    </occupation>\n  </person>\n</jason>\n```\n:::\n\n\n> *Question*\n> ---\n> Which of the following is **false**?<br>\n> <br>\n> ⬜ The contents of the `first_name` element is the text `Jason`.<br>\n> ⬜ The `type` attribute of the `person` element is `\"fictional\"`.<br>\n> ⬜ The `person` element is a child of the `jason` element.<br>\n> ✅ The `last_name` element is a child of the `first_name` element.<br>\n\nYou got it, the `last_name` element is a **sibling** of the `first_name` element.\n\n## Examining XML documents\n\nJust like JSON, you should first verify the response is indeed XML with <a href=\"https://www.rdocumentation.org/packages/httr/topics/http_type\">`http_type()`</a> and by examining the result of `content(r, as = \"text\")`.  Then you can turn the response into an XML document object with <a href=\"https://www.rdocumentation.org/packages/xml2/topics/read_xml\">`read_xml()`</a>.\n\nOne benefit of using the XML document object is the available functions that help you explore and manipulate the document.  For example <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_structure\">`xml_structure()`</a> will print a representation of the XML document that emphasizes the hierarchical structure by displaying the elements without the data.\n\nIn this exercise you'll grab the same revision history you've been working with as XML, and take a look at it with `xml_structure()`.\n\n**Steps**\n\n1. Get the XML version of the revision history for the Wikipedia article for `\"Hadley Wickham\"`, by calling `rev_history(\"Hadley Wickham\", format = \"xml\")`, store it in `resp_xml`.\n2. Check the response type of `resp_xml` to confirm the API returned an XML object.\n3. You can't always trust a header, so check the content looks like XML by calling `content()` on `resp_xml` with `as = \"text\"`, store in `rev_text`.\n4. Turn `rev_text` into an XML object with `read_xml()` from the `xml2` package, store as `rev_xml`.\n5. Call `xml_structure()` on `rev_xml` to see the structure of the returned XML.  *Can you see where the revisions are?*\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load xml2\nlibrary(xml2)\n\n# Get XML revision history\nrevhist_xml_url <- \"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=xml&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\"\n\nresp_xml <- GET(revhist_xml_url)\n\n# Check response is XML\nhttp_type(resp_xml)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"text/xml\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Examine returned text with content()\nrev_text <- content(resp_xml, as = \"text\")\nrev_text\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"<?xml version=\\\"1.0\\\"?><api><continue rvcontinue=\\\"20150528042700|664370232\\\" continue=\\\"||\\\" /><warnings><main xml:space=\\\"preserve\\\">Subscribe to the mediawiki-api-announce mailing list at &lt;https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/&gt; for notice of API deprecations and breaking changes. Use [[Special:ApiFeatureUsage]] to see usage of deprecated features by your application.</main><revisions xml:space=\\\"preserve\\\">Because \\\"rvslots\\\" was not specified, a legacy format has been used for the output. This format is deprecated, and in the future the new format will always be used.</revisions></warnings><query><pages><page _idx=\\\"41916270\\\" pageid=\\\"41916270\\\" ns=\\\"0\\\" title=\\\"Hadley Wickham\\\"><revisions><rev user=\\\"214.28.226.251\\\" anon=\\\"\\\" timestamp=\\\"2015-01-14T17:12:45Z\\\" contentformat=\\\"text/x-wiki\\\" contentmodel=\\\"wikitext\\\" comment=\\\"\\\" xml:space=\\\"preserve\\\">'''Hadley Mary Helen Wickham III''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]&lt;ref&gt;{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}&lt;/ref&gt; and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].&lt;ref name=\\\"about\\\"&gt;{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}&lt;/ref&gt; He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.&lt;ref&gt;{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}&lt;/ref&gt; In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.&lt;ref&gt;{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}&lt;/ref&gt;\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.&lt;ref name=\\\"about\\\" /&gt;&lt;ref&gt;{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}&lt;/ref&gt;</rev><rev user=\\\"73.183.151.193\\\" anon=\\\"\\\" timestamp=\\\"2015-01-15T15:49:34Z\\\" contentformat=\\\"text/x-wiki\\\" contentmodel=\\\"wikitext\\\" comment=\\\"\\\" xml:space=\\\"preserve\\\">'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]&lt;ref&gt;{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}&lt;/ref&gt; and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].&lt;ref name=\\\"about\\\"&gt;{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}&lt;/ref&gt; He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.&lt;ref&gt;{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}&lt;/ref&gt; In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.&lt;ref&gt;{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}&lt;/ref&gt;\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.&lt;ref name=\\\"about\\\" /&gt;&lt;ref&gt;{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}&lt;/ref&gt;</rev><rev user=\\\"FeanorStar7\\\" timestamp=\\\"2015-01-24T16:34:31Z\\\" contentformat=\\\"text/x-wiki\\\" contentmodel=\\\"wikitext\\\" comment=\\\"/* External links */ add LCCN and cats\\\" xml:space=\\\"preserve\\\">'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]&lt;ref&gt;{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}&lt;/ref&gt; and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].&lt;ref name=\\\"about\\\"&gt;{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}&lt;/ref&gt; He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.&lt;ref&gt;{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}&lt;/ref&gt; In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.&lt;ref&gt;{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}&lt;/ref&gt;\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.&lt;ref name=\\\"about\\\" /&gt;&lt;ref&gt;{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}&lt;/ref&gt;</rev><rev user=\\\"KasparBot\\\" timestamp=\\\"2015-04-26T19:18:17Z\\\" contentformat=\\\"text/x-wiki\\\" contentmodel=\\\"wikitext\\\" comment=\\\"authority control moved to wikidata\\\" xml:space=\\\"preserve\\\">'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]&lt;ref&gt;{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}&lt;/ref&gt; and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].&lt;ref name=\\\"about\\\"&gt;{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}&lt;/ref&gt; He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.&lt;ref&gt;{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}&lt;/ref&gt; In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.&lt;ref&gt;{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}&lt;/ref&gt;\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.&lt;ref name=\\\"about\\\" /&gt;&lt;ref&gt;{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}&lt;/ref&gt;</rev><rev user=\\\"Spkal\\\" timestamp=\\\"2015-05-06T18:24:57Z\\\" contentformat=\\\"text/x-wiki\\\" contentmodel=\\\"wikitext\\\" comment=\\\"/* Bibliography */  Added his new book, R Packages\\\" xml:space=\\\"preserve\\\">'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]&lt;ref&gt;{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}&lt;/ref&gt; and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].&lt;ref name=\\\"about\\\"&gt;{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}&lt;/ref&gt; He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.&lt;ref&gt;{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}&lt;/ref&gt; In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.&lt;ref&gt;{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}&lt;/ref&gt;\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.&lt;ref name=\\\"about\\\" /&gt;&lt;ref&gt;{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}&lt;/ref&gt;</rev></revisions></page></pages></query></api>\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Turn rev_text into an XML document\nrev_xml <- read_xml(rev_text)\n\n# Examine the structure of rev_xml\nxml_structure(rev_xml)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <api>\n#>   <continue [rvcontinue, continue]>\n#>   <warnings>\n#>     <main [space]>\n#>       {text}\n#>     <revisions [space]>\n#>       {text}\n#>   <query>\n#>     <pages>\n#>       <page [_idx, pageid, ns, title]>\n#>         <revisions>\n#>           <rev [user, anon, timestamp, contentformat, contentmodel, comment, space]>\n#>             {text}\n#>           <rev [user, anon, timestamp, contentformat, contentmodel, comment, space]>\n#>             {text}\n#>           <rev [user, timestamp, contentformat, contentmodel, comment, space]>\n#>             {text}\n#>           <rev [user, timestamp, contentformat, contentmodel, comment, space]>\n#>             {text}\n#>           <rev [user, timestamp, contentformat, contentmodel, comment, space]>\n#>             {text}\n```\n\n\n:::\n:::\n\n\nBrilliant! [`xml_structure()`](https://www.rdocumentation.org/packages/xml2/topics/xml_structure) helps you understand the structure of your document, without overwhelming you with content.\n\n## XPATHs\n\nTheory. Coming soon ...\n\n\n**1. XPATHS**\n\nNow you have an overview of the structure of XML documents you need to learn how to extract data from them.  In this video you'll learn about XPath, a language for specifying nodes in an XML document.\n\n**2. Movies example**\n\nTake a look at this slightly altered XML description of the movies.  I've added another title node to the movies element that describes that these movies belong to the Star Wars franchise.  I've also added an episode attribute to each movie:\n\n**3. Movies example**\n\nI'll read it in to XML as movies_xml.\n\n**4. XPATHS**\n\nXPATHs look a bit like file paths because they use forward slashes to specify levels in the XML document tree.  Here's a simple one: /movies/movie/title. It describes all nodes that are called title that are inside a movie node, inside the movies node at the top of the document.  The xml_find_all() function in xml2 will find all nodes that match an XPath description in a given document.\n\n**5. XPATHS**\n\nLet's look for nodes using this XPath in the movies XML.You see we get back the two title nodes that correspond to our two movies.  The result is a special object called a node set.\n\n**6. XPATHS**\n\nIf we want to extract the data from these we can use the xml_text() function to extract the contents of each node as text.\n\n**7. Other XPATH Syntax**\n\nA double forward slash in a XPATH describes nodes at any level of the document, for example \"double forward slash title\" describes any nodes below the top of the document with the title tag.  You can see this picks up our two movie titles but also the title of our collection.\n\n**8. Other XPATH Syntax**\n\nYou can also use XPaths to find nodes based on attributes using the \"at\" symbol.  For example, //movie/@episode finds any episode attributes that are inside a movie node anywhere in the document.\n\n**9. Or...**\n\nAn alternative way to extract attributes is to use the xml_attr() and xml_attrs() functions on nodes sets, which you'll see in the exercises.\n\n**10. Wrap Up**\n\nTo wrap up, you can use XPaths to specify specific nodes in an XML document.  A single slash denotes a node at the current level, and a double slash a node anywhere at or below the current level, an \"at\" symbol can be used for an attribute.  To extract these nodes from a document in R use the xml_find_all() function.  Then parse the contents of the nodes to get an R object.  You just saw xml_text(), but there is also xml_double(), xml_integer(), and as_list().\n\n**11. Let's practice!**\n\nOK time to extract some data from XML.\n\n## Extracting XML data\n\nXPATHs are designed to specifying nodes in an XML document.  Remember `/node_name` specifies nodes at the current level that have the tag `node_name`, where as `//node_name` specifies nodes at any level below the current level that have the tag `node_name`.\n\n`xml2` provides the function <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_find_all\">`xml_find_all()`</a> to extract nodes that match a given XPATH.  For example, `xml_find_all(rev_xml, \"/api\")` will find all the nodes at the top level of the `rev_xml` document that have the tag `api`.  Try running that in the console. You'll get a nodeset of one node because there is only one node that satisfies that XPATH.\n\nThe object returned from `xml_find_all()` is a nodeset (think of it like  a list of nodes). To actually get data out of the nodes in the nodeset, you'll have to explicitly ask for it with <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_text\">`xml_text()`</a> (or <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_double\">`xml_double()`</a> or <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_integer\">`xml_integer()`</a>).\n\nUse what you know about the location of the revisions data in the returned XML document extract just the content of the revision.\n\n**Steps**\n\n1. Use `xml_find_all()` on `rev_xml` to find all the nodes that describe revisions by using the XPATH, `\"/api/query/pages/page/revisions/rev\"`.\n2. Use `xml_find_all()` on `rev_xml` to find all the nodes that are in a `rev` node anywhere in the document, store in `rev_nodes`.\n3. Extract the contents from each node in `rev_nodes`, by passing `rev_nodes` to `xml_text()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Find all nodes using XPATH \"/api/query/pages/page/revisions/rev\"\nxml_find_all(rev_xml, \"/api/query/pages/page/revisions/rev\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> {xml_nodeset (5)}\n#> [1] <rev user=\"214.28.226.251\" anon=\"\" timestamp=\"2015-01-14T17:12:45Z\" conte ...\n#> [2] <rev user=\"73.183.151.193\" anon=\"\" timestamp=\"2015-01-15T15:49:34Z\" conte ...\n#> [3] <rev user=\"FeanorStar7\" timestamp=\"2015-01-24T16:34:31Z\" contentformat=\"t ...\n#> [4] <rev user=\"KasparBot\" timestamp=\"2015-04-26T19:18:17Z\" contentformat=\"tex ...\n#> [5] <rev user=\"Spkal\" timestamp=\"2015-05-06T18:24:57Z\" contentformat=\"text/x- ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find all rev nodes anywhere in document\nrev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n# Use xml_text() to get text from rev_nodes\nxml_text(rev_nodes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"'''Hadley Mary Helen Wickham III''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n#> [2] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"               \n#> [3] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"               \n#> [4] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"               \n#> [5] \"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"\n```\n\n\n:::\n:::\n\n\nTerrific, you've written your first XPATH!  You'll get plenty of practice with them this chapter and the next.\n\n## Extracting XML attributes\n\nNot all the useful data will be in the content of a node, some might also be in the attributes of a node. To extract attributes from a nodeset, `xml2` provides <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_attrs\">`xml_attrs()`</a> and <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_attr\">`xml_attr()`</a>.\n\n`xml_attrs()` takes a nodeset and returns **all** of the attributes for every node in the nodeset. `xml_attr()` takes a nodeset and an additional argument `attr` to extract a single named argument from each node in the nodeset.\n\nIn this exercise you'll grab the `user` and `anon` attributes for each revision.  You'll see  <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_find_first\">`xml_find_first()`</a> in the sample code. It works just like <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_find_all\">`xml_find_all()`</a> but it only extracts the first node it finds.\n\n**Steps**\n\n1. We've extracted `rev_nodes` and `first_rev_node` in the document for you to explore the difference between `xml_attrs()` and `xml_attr()`.\n\n    * Use `xml_attrs()` on `first_rev_node` to see all the attributes of the first revision node.  \n    * Use `xml_attr()` on `first_rev_node` along with an appropriate `attr` argument to extract the `user` attribute from the first revision node.  \n    * Now use `xml_attr()` again, but this time on `rev_nodes` to extract the `user` attribute from all revision nodes.  \n    * Use `xml_attr()` on `rev_nodes` to extract the `anon` attribute from all revision nodes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# All rev nodes\nrev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n# The first rev node\nfirst_rev_node <- xml_find_first(rev_xml, \"//rev\")\n\n# Find all attributes with xml_attrs()\nxml_attrs(first_rev_node)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                   user                   anon              timestamp \n#>       \"214.28.226.251\"                     \"\" \"2015-01-14T17:12:45Z\" \n#>          contentformat           contentmodel                comment \n#>          \"text/x-wiki\"             \"wikitext\"                     \"\" \n#>                  space \n#>             \"preserve\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find user attribute with xml_attr()\nxml_attr(first_rev_node, \"user\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"214.28.226.251\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find user attribute for all rev nodes\nxml_attr(rev_nodes, \"user\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"214.28.226.251\" \"73.183.151.193\" \"FeanorStar7\"    \"KasparBot\"     \n#> [5] \"Spkal\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find anon attribute for all rev nodes\nxml_attr(rev_nodes, \"anon\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"\" \"\" NA NA NA\n```\n\n\n:::\n:::\n\n\nGood job!  Did you notice that if a node didn't have the `anon` attribute `xml_attr()` returned an `NA`?\n\n## Wrapup: returning nice API output\n\nHow might all this work together? A useful API function will retrieve results from an API **and** return them in a useful form.  In Chapter 2, you finished up by writing a function that retrieves data from an API that relied on <a href=\"https://www.rdocumentation.org/packages/httr/topics/content\">`content()`</a> to convert it to a useful form.  To write a more robust API function you shouldn't rely on `content()` but instead parse the data yourself.\n\nTo finish up this chapter you'll do exactly that: write `get_revision_history()` which retrieves the XML data for the revision history of page on Wikipedia, parses it, and returns it in a nice data frame.  \n\nSo that you can focus on the parts of the function that parse the return object, you'll see your function calls `rev_history()` to get the response from the API.  You can assume this function returns the raw response and follows the best practices you learnt in Chapter 2, like using a user agent, and checking the response status.\n\n**Steps**\n\n1. Fill in the `___` to finish the function definition.\n\n    * Use `read_xml()` to turn the `content()` of `rev_resp` as text into an XML object.\n    * Use `xml_find_all()` to find *all* the `rev` nodes in the XML.\n    * Parse out the `\"user\"` attribute from `rev_nodes`.\n    * Parse out the content from `rev_nodes` using  `xml_text()`.\n    * Finally, call `get_revision_history()` with `article_title = \"Hadley Wickham\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Adjust for title & format\n\nget_revision_history <- function(article_title){\n  # Get raw revision response\n  # rev_resp <- rev_history(article_title, format = \"xml\")\n  # Get XML revision history\n  revhist_xml_url <- \"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=xml&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\"\n\n  rev_resp <- GET(revhist_xml_url)\n  \n  # Turn the content() of rev_resp into XML\n  rev_xml <- read_xml(content(rev_resp, \"text\"))\n  \n  # Find revision nodes\n  rev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n  # Parse out usernames\n  user <- xml_attr(rev_nodes, \"user\")\n  \n  # Parse out timestamps\n  timestamp <- readr::parse_datetime(xml_attr(rev_nodes, \"timestamp\"))\n  \n  # Parse out content\n  content <- xml_text(rev_nodes)\n  \n  # Return data frame\n  data.frame(user = user,\n    timestamp = timestamp,\n    content = substr(content, 1, 40))\n}\n\n# Call function for \"Hadley Wickham\"\nget_revision_history(\"Hadley Wickham\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"user\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"timestamp\"],\"name\":[2],\"type\":[\"dttm\"],\"align\":[\"right\"]},{\"label\":[\"content\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"214.28.226.251\",\"2\":\"2015-01-14 17:12:45\",\"3\":\"'''Hadley Mary Helen Wickham III''' is a\"},{\"1\":\"73.183.151.193\",\"2\":\"2015-01-15 15:49:34\",\"3\":\"'''Hadley Wickham''' is a  [[statisticia\"},{\"1\":\"FeanorStar7\",\"2\":\"2015-01-24 16:34:31\",\"3\":\"'''Hadley Wickham''' is a  [[statisticia\"},{\"1\":\"KasparBot\",\"2\":\"2015-04-26 19:18:17\",\"3\":\"'''Hadley Wickham''' is a  [[statisticia\"},{\"1\":\"Spkal\",\"2\":\"2015-05-06 18:24:57\",\"3\":\"'''Hadley Wickham''' is a  [[statisticia\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nNice job!  Your function parsed the XML data, but you could have just as easily parsed the JSON data.\n\n# 4. Web scraping with XPATHs\n\nNow that we've covered the low-hanging fruit (\"it has an API, and a client\", \"it has an API\") it's time to talk about what to do when a website doesn't have any access mechanisms at all - when you have to rely on web scraping. This chapter will introduce you to the rvest web-scraping package, and  build on your previous knowledge of XML manipulation and XPATHs.\n\n## Web scraping 101\n\nTheory. Coming soon ...\n\n\n**1. Web Scraping 101**\n\nWe've discussed API usage and how to play around with the data you retrieve from the API. The problem is that sometimes websites don't have a formal API. This doesn't mean that you can't get data out of them, just that you have to take a different approach.That approach is called \"web scraping,\" and it consists of grabbing the raw HTML of a website and then extracting values from it. Web scraping is a bit messier than API use, and a bit more complex, but it's important to learn for occasions where you can't rely on an API's presence. One caveat is that there can be legal objections to doing it en-masse, so if it's something you're planning to do, make sure it's something the website owner is comfortable with.\n\n**2. Selectors**\n\nWeb scraping works by filtering the HTML of a web page to just the bits you want, using some kind of identifier. There are a lot of forms these identifiers can take, which we'll discuss later in the chapter and course, using example identifiers we have already extracted.When you get out of this course and start applying web scraping to your own problems, you'll have to work out the identifiers yourself. This can be done with a tool known as a selector: basically a browser plugin or extension which, when you mouse over an element of a page, tells you what categories it falls into for ID purposes.We're not going to use it in this course, but once you're done you should absolutely check it out - it makes things a heck of a lot easier.\n\n**3. rvest**\n\nTo ease the task of web scraping, there's a dedicated package, rvest. This provides utilities for everything from reading the HTML in to extracting elements from it, and is what we'll be exploring in this chapter and the next.To read an HTML page, call read_html(), passing the URL. There are a few advanced options that are useful for dealing with badly-formed pages, but usually this is the only argument that you need.\n\n**4. Parsing HTML**\n\nread_html() actually returns an XML document, so the XPath querying skills that you learned in the previous chapter will come in useful now. To retrieve a node from your document, call html_node() passing the document and an XPath string describing the node that you want.\n\n**5. Parsing HTML**\n\nFor example, calling read_html() on the Wikipedia page for R returns an XML document object,\n\n**6. Parsing HTML**\n\nthen calling html_node() with the document and the XPath double forwards slash \"ul\" returns a node of the first unordered list on that page.\n\n**7. Let's practice!**\n\nNow it's your turn to try some examples.\n\n## Reading HTML\n\nThe first step with web scraping is actually reading the HTML in. This can be done with a function from `xml2`, which is imported by `rvest` -  <a href=\"https://www.rdocumentation.org/packages/rvest/topics/read_html\">`read_html()`</a>. This accepts a single URL, and returns a big blob of XML that we can use further on.\n\nWe're going to experiment with that by grabbing Hadley Wickham's wikipedia page, with <a href=\"https://www.rdocumentation.org/packages/rvest\">`rvest`</a>, and then printing it just to see what the structure looks like.\n\n**Steps**\n\n1. Load the `rvest` package.\n2. Use `read_html()` to read the URL stored at `test_url`. Store the results as `test_xml`.\n3. Print `test_xml`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load rvest\nlibrary(rvest)\n\n# Hadley Wickham's Wikipedia page\ntest_url <- \"https://en.wikipedia.org/wiki/Hadley_Wickham\"\n\n# Read the URL stored as \"test_url\" with read_html()\ntest_xml <- read_html(test_url)\n\n# Print test_xml\ntest_xml\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> {html_document}\n#> <html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-clientpref-0 vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available\" lang=\"en\" dir=\"ltr\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n#> [2] <body class=\"skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ...\n```\n\n\n:::\n:::\n\n\nNicely done! As you can see, the XML document looks very similar to what we saw in the last chapter.\n\n## Extracting nodes by XPATH\n\nNow you've got a HTML page read into R. Great! But how do you get individual, identifiable pieces of it?\n\nThe answer is to use <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_node\">`html_node()`</a>, which extracts individual chunks of HTML from a HTML document. There are a couple of ways of identifying and filtering nodes, and for now we're going to use XPATHs: unique identifiers for individual pieces of a HTML document.\n\nThese can be retrieved using a browser gadget we'll talk about later - in the meanwhile the XPATH for the information box in the page you just downloaded is stored as `test_node_xpath`. We're going to retrieve the box from the HTML doc with `html_node()`, using `test_node_xpath` as the `xpath` argument.\n\n**Steps**\n\n1. Use `html_node()` to retrieve the node with the XPATH stored at `test_node_xpath` from `test_xml` document you grabbed in the last exercise.\n2. Print the first element of the results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_node_xpath <- '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]'\n\n# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`\nnode <- html_node(x = test_xml, xpath = test_node_xpath)\n\n# Print the first element of the result\nnode[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <pointer: 0x7fd80887b300>\n```\n\n\n:::\n:::\n\n\nGood job! XML nodes are the building block of an XML document - extracting them leads to everything else. At the moment, they're still kind of abstract objects: we'll dig into their contents later on.\n\n## HTML structure\n\nTheory. Coming soon ...\n\n\n**1. HTML Structure**\n\nAt this point you've grabbed HTML pages and extracted nodes from them - more importantly you've looked at the node contents which, if you're not familiar with HTML, look a lot like gibberish. In this video we're going to explore how HTML is structured, so it can at least be meaningful gibberish.\n\n**2. Tags**\n\nHTML is defined largely by the presence of content within \"tags.\" With a few exceptions, these are paired: there's a start tag and an end tag. The start tag is wrapped with a less than and greater than sign - the end tag with a less than, backslash and greater than sign. You might spot that this is just the same as with XML data, which Charlotte discussed in the last chapter.As an example, if you want to store the line of text \"this is a test\" with paragraph formatting, we'd use &lt;p&gt; this is a test &lt;/p&gt; - where the tag itself (p) tells the browser what kind of information it's dealing with (text) and the start and end tags tell the browser where that information - and the associated formatting - end and begin.\n\n**3. Attributes**\n\nYou can also have attributes in tags. For example, the tag for a link is \"a,\" which requires the attribute href - the link itself - before the text. So a link to Wikipedia, with the display text \"this is a test,\" would be &lt;a href = \"https://en-dot-wikipedia-dot-org/\"&gt; this is a test &lt;/a&gt;. Parameters can do a lot of useful things, and are often used to incorporate styling or formatting information, which is a thing we'll discuss in the next chapter.The important thing, though, is that the information in a piece of HTML is more than just a piece of text or a photo. It can be text - the actual displayed element. It can be an attribute - links, stylesheet information, or similar things. And it can be a name - the actual tag used.\n\n**4. Extracting information**\n\nrvest provides the functions html_text(), html_attr() and  html_name() to extract information stored in these three ways. For html_text(), and html_name() you just need to pass the html nodeset of interest. For html_attr() you'll also need to specify the attribute name.\n\n**5. Let's practice!**\n\nIn the following exercises you'll learn how to extract any and all of the above, as you find necessary.\n\n## Extracting names\n\nThe first thing we'll grab is a name, from the first element of the previously extracted table (now stored as `table_element`). We can do this with <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_name\">`html_name()`</a>. As you may recall from when you printed it, the element has the tag <code><table>...</table></code>, so we'd expect the name to be, well, `table`.\n\n**Steps**\n\n1. Extract the name of `table_element` using the function `html_name()`. Save it as `element_name`.\n2. Print `element_name`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# read html\ntable_element <- read_html(\"data/vcard.html\") |> html_node((\"table\"))\n\n# Extract the name of table_element\nelement_name <- html_name(table_element)\n\n# Print the name\nelement_name\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"table\"\n```\n\n\n:::\n:::\n\n\nNice work! You've started extracting components from HTML and XML nodes. The tag might not seem important (and most of the time, it's not) but it's a good first step, and the actual node contents (text, say) is something we'll move on to next.\n\n## Extracting values\n\nJust knowing the type of HTML object a node is isn't much use, though (although it can be very helpful). What we really want is to extract the actual *text* stored within the value.\n\nWe can do that with (shocker) <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_text\">`html_text()`</a>, another convenient `rvest` function that accepts a node and passes back the text inside it. For this we'll want a node *within* the extracted element - specifically, the one containing the page title. The xpath value for that node is stored as `second_xpath_val`.\n\nUsing this xpath value, extract the node within `table_element` that we want, and then use `html_text` to extract the text, before printing it.\n\n**Steps**\n\n1. Extract the element of `table_element` referred to by `second_xpath_val` and store it as `page_name`.\n2. Extract the text from `page_name` using `html_text()`, saving it as `page_title`.\n3. Print `page_title`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# xpath\nsecond_xpath_val <- '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"fn\", \" \" ))]'\n\n# Extract the element of table_element referred to by second_xpath_val and store it as page_name\npage_name <- html_node(x = table_element, xpath = second_xpath_val)\n\n# Extract the text from page_name\npage_title <- html_text(page_name)\n\n# Print page_title\npage_title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"Hadley Wickham\"\n```\n\n\n:::\n:::\n\n\nNice! Text extraction is most of what you're likely to do with XML - after all, the content of an XML tag is almost always the important value. If it's consistently a set of digits, say, you can always use as.integer() or as.numeric() to turn it from a string, into a number\n\n## Test: HTML reading and extraction\n\nTime for a quick test of what we've learned about HTML. \n\n> *Question*\n> ---\n> What would you use to extract the type of HTML tag a value is wrapped in?<br>\n> <br>\n> ⬜ `html_attr()`<br>\n> ✅ `html_name()`<br>\n\nCorrect! The type of tag is the tag name - so `html_name()` is the right function.\n\n## Reformatting Data\n\nTheory. Coming soon ...\n\n\n**1. Reformatting Data**\n\nSomething you briefly touched on in the last chapter is reformatting data so that it's rectangular - in other words, so it fits in a data frame. This is because, as I'm sure you know, data frames are the most commonly used and commonly supported ways of representing data in R.We're going to discuss it more here, in the context of web scraping, and will cover two things. The first is turning HTML tables into R data frames, and cleaning them up a bit: the second is turning arbitrary HTML data, like names and text, into data frames.\n\n**2. HTML tables**\n\nTables aren't just an R structure, they're also a structure in HTML. Every time you see something tabular in HTML, that's using a set of dedicated tags - starting with the table tag - which tell your browser \"this is a table\".HTML tables can be extracted directly with rvest, using the html_table() function. This produces a data frame. Much of the time that's all you need - the one exception is when the HTML table itself didn't have a header. In that case, the data frame will have default column names which are pretty hard to remember. This is easily fixed by assigning new memorable column names to it with the colnames() function.\n\n**3. Turning things into data.frames**\n\nMost HTML, though, as you've seen, isn't stored in tables. The good news is we can still turn them into data frames, just using the data dot frame() function. The vectors of text, names or attributes you've extracted are just like any other vectors, and so you can easily construct a data frame from them (assuming they're all the same length).\n\n**4. Let's practice!**\n\nYou'll be doing just that - and turning HTML tables into data frames - in the next exercises.\n\n## Extracting tables\n\nThe data from Wikipedia that we've been playing around with *can* be extracted bit by bit and cleaned up manually, but since it's a table, we have an easier way of turning it into an R object. `rvest` contains the function <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_table\">`html_table()`</a> which, as the name suggests, extracts tables. It accepts a node containing a table object, and outputs a data frame.\n\nLet's use it now: take the table we've extracted, and turn it into a data frame.\n\n**Steps**\n\n1. Turn `table_element` into a data frame and assign it to `wiki_table`.\n2. Print the resulting object.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Turn table_element into a data frame and assign it to wiki_table\nwiki_table <- html_table(table_element)\n\n# Print wiki_table\nwiki_table\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Hadley Wickham\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Hadley Wickham\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Hadley Wickham in 2015\",\"2\":\"Hadley Wickham in 2015\"},{\"1\":\"Born\",\"2\":\"Hadley Alexander Wickham\\\\n (1979-10-14) 14 October 1979 (age 42)Hamilton, New Zealand\"},{\"1\":\"Alma mater\",\"2\":\"University of Auckland (BSc, MSc)Iowa State University (PhD)\"},{\"1\":\"Known for\",\"2\":\"ggplot2[1]tidyverseR packages\"},{\"1\":\"Awards\",\"2\":\"COPSS Presidents' Award (2019)\\\\n\\\\nFellow of the American Statistical Association (2015)\"},{\"1\":\"Scientific career\",\"2\":\"Scientific career\"},{\"1\":\"Fields\",\"2\":\"Data science\\\\nVisualization\\\\n\\\\nStatistics[2]\"},{\"1\":\"Institutions\",\"2\":\"RStudio Inc.\\\\nUniversity of Auckland\\\\nStanford University\\\\nRice University\"},{\"1\":\"Thesis\",\"2\":\"Practical tools for exploring data and models (2008)\"},{\"1\":\"Doctoral advisors\",\"2\":\"Dianne Cook\\\\n\\\\nHeike Hofmann[3]\"},{\"1\":\"\",\"2\":\"\"},{\"1\":\"Website\",\"2\":\"hadley.nz\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWell done! Being able to extract tables directly is a massive speedup, since otherwise they're a ton of different nested tags.\n\n## Cleaning a data frame\n\nIn the last exercise, we looked at extracting tables with `html_table()`. The resulting data frame was pretty clean, but had two problems - first, the column names weren't descriptive, and second, there was an empty row.\n\nIn this exercise we're going to look at fixing both of those problems. First, column names. Column names can be cleaned up with the `colnames()` function. You call it on the object you want to rename, and then assign *to that call* a vector of new names.\n\nThe missing row, meanwhile, can be removed with the `subset()` function. `subset` takes an object, and a condition. For example, if you have a data frame `df` containing a column `x`, you could run \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsubset(df, !x == \"\")\n```\n:::\n\n\nto remove all rows from `df` consisting of empty strings (`\"\"`) in the column `x`.\n\n**Steps**\n\n1. <ol>\\n<li>Rename the columns of `wiki_table` to `\"key\"` and `\"value\"` using `colnames()`.</li>\\n<li>Remove the empty row from `wiki_table` using `subset()`, and assign the result to `cleaned_table`.</li>\\n<li>Print `cleaned_table`.</li>\\n</ol>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Rename the columns of wiki_table\ncolnames(wiki_table) <- c(\"key\", \"value\")\n\n# Remove the empty row from wiki_table\ncleaned_table <- subset(wiki_table, !key == \"\")\n\n# Print cleaned_table\ncleaned_table\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"key\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Hadley Wickham in 2015\",\"2\":\"Hadley Wickham in 2015\"},{\"1\":\"Born\",\"2\":\"Hadley Alexander Wickham\\\\n (1979-10-14) 14 October 1979 (age 42)Hamilton, New Zealand\"},{\"1\":\"Alma mater\",\"2\":\"University of Auckland (BSc, MSc)Iowa State University (PhD)\"},{\"1\":\"Known for\",\"2\":\"ggplot2[1]tidyverseR packages\"},{\"1\":\"Awards\",\"2\":\"COPSS Presidents' Award (2019)\\\\n\\\\nFellow of the American Statistical Association (2015)\"},{\"1\":\"Scientific career\",\"2\":\"Scientific career\"},{\"1\":\"Fields\",\"2\":\"Data science\\\\nVisualization\\\\n\\\\nStatistics[2]\"},{\"1\":\"Institutions\",\"2\":\"RStudio Inc.\\\\nUniversity of Auckland\\\\nStanford University\\\\nRice University\"},{\"1\":\"Thesis\",\"2\":\"Practical tools for exploring data and models (2008)\"},{\"1\":\"Doctoral advisors\",\"2\":\"Dianne Cook\\\\n\\\\nHeike Hofmann[3]\"},{\"1\":\"Website\",\"2\":\"hadley.nz\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWell done! Cleaning up data, or 'munging', is a really common thing to have to do, particularly when someone else picked how it's formatted. If you can scrape data and clean it, you can do anything.\n\n# 5. CSS Web Scraping and Final Case Study\n\nCSS path-based web scraping is a far-more-pleasant alternative to using XPATHs.  You'll start this chapter by learning about CSS, and how to leverage it for web scraping.   Then, you'll work through a final case study that combines everything you've learnt so  far to write a function that queries an API, parses the response and returns data in a nice form.\n\n## CSS web scraping in theory\n\nTheory. Coming soon ...\n\n\n**1. CSS**\n\nBy this point you should already be somewhat familiar with web scraping thanks to chapter 4, which discusses web scraping using XPATH values. Next up is web scraping with CSS!CSS - Cascading Style Sheets - is basically a way of adding design information (font size, color, or border width) to HTML web pages. In order to make this information easy to change and non-duplicative, CSS information is associated with 'classes' or 'ids' which can then be applied to whichever HTML elements the developer wants.\n\n**2. CSS example**\n\nSo if we wanted two types of link - one black, one red - we'd create two different CSS classes with different font color choices,\n\n**3. CSS example**\n\nand then apply different classes to different HTML tags. And you can see us doing that in the slide.\n\n**4. CSS versus XPATH**\n\nCSS-based scraping looks for these class names. Because particular classes are strongly associated with particular UI objects (such as 'links in the main body', or a particular table) it's really easy to select or filter content based on them. It looks a lot like the XPATH-based scraping in the last chapter, but with one major difference: CSS-based techniques often get you a whole set of items that meet conditions, rather than one specific item. We're going to be using modified versions of the exercises from the last chapter so you can see the differences and similarities.Just as before, you'll be using the html_node() function to retrieve nodes from the document. This time, however rather than passing the xpath argument, you'll be passing the css argument.\n\n**5. Let's practice!**\n\nHave a go at some examples.\n\n## Using CSS to scrape nodes\n\nAs mentioned in the video, CSS is a way to add design information to HTML, that instructs the browser on how to display the content. You can leverage these design instructions to identify content on the page.\n\nYou've already used <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_node\">`html_node()`</a>, but it's more common with CSS selectors to use <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_nodes\">`html_nodes()`</a> since you'll often want more than one node returned.  Both functions allow you to specify a `css` argument to use a CSS selector, instead of specifying the `xpath` argument.\n\nWhat do CSS selectors look like? Try these examples to see a few possibilities.\n\n**Steps**\n\n1. We've read in the same HTML page from Chapter 4, the Wikipedia page for Hadley Wickham, into `test_xml`.\n\n    * Use the CSS selector `\"table\"` to select all elements that are a  `table` tag.\n    * Use the CSS selector `\".infobox\"` to select all elements that have the attribute `class = \"infobox\"`.\n    * Use the CSS selector `\"#firstHeading\"` to select all elements that have the attribute `id = \"firstHeading\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# read xml\ntest_xml <- read_html(\"data/test.html\")\n\n# Select the table elements\nhtml_nodes(test_xml, css = \"table\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> {xml_nodeset (3)}\n#> [1] <table class=\"infobox biography vcard\"><tbody>\\n<tr><th colspan=\"2\" class ...\n#> [2] <table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style ...\n#> [3] <table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Select elements with class = \"infobox\"\nhtml_nodes(test_xml, css = \".infobox\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> {xml_nodeset (1)}\n#> [1] <table class=\"infobox biography vcard\"><tbody>\\n<tr><th colspan=\"2\" class ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Select elements with id = \"firstHeading\"\nhtml_nodes(test_xml, css = \"#firstHeading\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> {xml_nodeset (1)}\n#> [1] <h1 id=\"firstHeading\" class=\"firstHeading mw-first-heading\">Hadley Wickha ...\n```\n\n\n:::\n:::\n\n\nNice work! Did you notice the special prefixes needed for classes and ids?\n\n## Scraping names\n\nYou might have noticed in the previous exercise, to select elements with a certain class, you add a `.` in front of the class name. If you need to select an element based on its id, you add a `#` in front of the id name.\n\nFor example if this element was inside your HTML document:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.html .cell-code}\n<h1 class = \"heading\" id = \"intro\">\n  Introduction\n</h1>\n```\n:::\n\n\nYou could select it by its class using the CSS selector `\".heading\"`, or by its id using the CSS selector `\"#intro\"`.\n\nOnce you've selected an element with a CSS selector, you can get the element tag name just like you did with XPATH selectors, with  <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_name\">`html_name()`</a>. Try it!\n\n**Steps**\n\n1. The infobox you extracted in Chapter 4 has the class `infobox`. Use `html_nodes()` and the appropriate CSS selector to extract the infobox element to `infobox_element`.\n2. Use `html_name()` to extract the tag name of `infobox_element` and store it in `element_name`.\n3. Print `element_name`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract element with class infobox\ninfobox_element <- html_nodes(test_xml, css = \".infobox\")\n\n# Get tag name of infobox_element\nelement_name <- html_name(infobox_element)\n\n# Print element_name\nelement_name\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"table\"\n```\n\n\n:::\n:::\n\n\nTerrific!  This is the same element you selected in Chapter 4 with an XPATH statement, and unsurprisingly it has the same tag, it's a `table`.\n\n## Scraping text\n\nOf course you can get the contents of a node extracted using a CSS selector too, with <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_text\">`html_text()`</a>. \n\nCan you put the pieces together to get the page title like you did in Chapter 4?\n\n**Steps**\n\n1. The infobox HTML element is stored in `infobox_element` in your workspace.\n\n    * Use `html_node()` to extract the element from `infobox_element` with the CSS class `fn`.\n    * Use `html_text()` to extract the contents of `page_name`.\n    * Print `page_title`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract element with class fn\npage_name <- html_node(x = infobox_element, css = \".fn\")\n\n# Get contents of page_name\npage_title <- html_text(page_name)\n\n# Print page_title\npage_title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"Hadley Wickham\"\n```\n\n\n:::\n:::\n\n\nPerfect! Why do you think the class for this element is `fn`?  I suspect it's short for **f**ull **n**ame.\n\n## Test: CSS web scraping\n\nTake a look at the chunk of HTML being read into `test`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest <- read_html('\n   <h1 class = \"main\">Hello world!</h1>\n   ')\n```\n:::\n\n\n> *Question*\n> ---\n> How would you extract the text `Hello world!` using `rvest` and CSS selectors?<br>\n> <br>\n> ⬜ `html_name(html_node(test, css = \".main\"))`<br>\n> ⬜ `html_name(html_node(test, css = \"#main\"))`<br>\n> ⬜ `html_text(html_node(test, css = \"#main\"))`<br>\n> ✅ `html_text(html_node(test, css = \".main\"))`<br>\n\nCorrect! You want the contents of the tag with class `main`.\n\n## Final case study: Introduction\n\nTheory. Coming soon ...\n\n\n**1. Final case study: Introduction**\n\nIf you're watching this, you've almost completed the course! Or you've skipped ahead. Either way, congratulations - you're nearly done. The only thing left is a final exercise which puts together everything you've learned so far.\n\n**2. What we'll cover**\n\nWe're going to tie everything you've learned together by having you get the (XML) content of a Wikipedia article out of the Wikipedia API.This has four steps: First you retrieve the data through the API, then you extract an infobox - which is the little metadata box in the top right of a lot of articles,  thirdly you turn the information in it into a data frame, and finally you wrap this in a function for reproducibility.\n\n**3. Let's practice!**\n\nEverything you've learned so far should show you nicely how to do it! Onwards to the finish!\n\n## API calls\n\nYour first step is to use the Wikipedia API to get the page contents for a specific page.  We'll continue to work with the Hadley Wickham page, but as your last exercise, you'll make it more general.\n\nTo get the content of a page from the Wikipedia API you need to use a parameter based URL.  The URL you want is \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhttps://en.wikipedia.org/w/api.php?action=parse&amp;page=Hadley%20Wickham&amp;format=xml\n```\n:::\n\n\nwhich specifies that you want the parsed content (i.e the HTML) for the \"Hadley Wickham\" page, and the API response should be XML.\n\nIn this exercise you'll make the request with `GET()` and parse the XML response with `content()`.\n\n**Steps**\n\n1. We've already defined `base_url` for you.\n\n    * Create a list for the query parameters, setting `action = \"parse\"`, `page = \"Hadley Wickham\"` and `format = \"xml\"`.\n    * Use `GET()` to call the API by specifying `url` and `query`.\n    * Parse the response using `content()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load httr\nlibrary(httr)\n\n# The API url\nbase_url <- \"https://en.wikipedia.org/w/api.php\"\n\n# Set query parameters\nquery_params <- list(action = \"parse\", \n  page = \"Hadley Wickham\", \n  format = \"xml\")\n\n# Get data from API\nresp <- GET(url = base_url, query = query_params)\n    \n# Parse response\nresp_xml <- content(resp)\n```\n:::\n\n\nGood work! You now have a response, but can you find the HTML for the page in that response?\n\n## Extracting information\n\nNow we have a response from the API, we need to extract the HTML for the page from it. It turns out the HTML is stored in the contents of the XML response.<br>\nTake a look, by using `xml_text()` to pull out the text from the XML response:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxml_text(resp_xml)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"<div class=\\\"mw-content-ltr mw-parser-output\\\" lang=\\\"en\\\" dir=\\\"ltr\\\"><div class=\\\"shortdescription nomobile noexcerpt noprint searchaux\\\" style=\\\"display:none\\\">New Zealand statistician</div>\\n<style data-mw-deduplicate=\\\"TemplateStyles:r1218072481\\\">.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}html.skin-theme-clientpref-night .mw-parser-output .infobox-full-data div{background:#1f1f23!important;color:#f8f9fa}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .infobox-full-data div{background:#1f1f23!important;color:#f8f9fa}}</style><table class=\\\"infobox biography vcard\\\"><tbody><tr><th colspan=\\\"2\\\" class=\\\"infobox-above\\\"><div class=\\\"fn\\\">Hadley Wickham</div></th></tr><tr><td colspan=\\\"2\\\" class=\\\"infobox-image\\\"><span class=\\\"mw-default-size\\\" typeof=\\\"mw:File/Frameless\\\"><a href=\\\"/wiki/File:Hadley-wickham2016-02-04.jpg\\\" class=\\\"mw-file-description\\\"><img src=\\\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Hadley-wickham2016-02-04.jpg/220px-Hadley-wickham2016-02-04.jpg\\\" decoding=\\\"async\\\" width=\\\"220\\\" height=\\\"330\\\" class=\\\"mw-file-element\\\" srcset=\\\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Hadley-wickham2016-02-04.jpg/330px-Hadley-wickham2016-02-04.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Hadley-wickham2016-02-04.jpg/440px-Hadley-wickham2016-02-04.jpg 2x\\\" data-file-width=\\\"1000\\\" data-file-height=\\\"1500\\\" /></a></span><div class=\\\"infobox-caption\\\">Hadley Wickham in 2015</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Born</th><td class=\\\"infobox-data\\\"><div style=\\\"display:inline\\\" class=\\\"nickname\\\">Hadley Alexander Wickham</div><br /><span style=\\\"display:none\\\"> (<span class=\\\"bday\\\">1979-10-14</span>) </span>14 October 1979<span class=\\\"noprint ForceAgeToShow\\\"> (age&#160;44)</span><br /><div style=\\\"display:inline\\\" class=\\\"birthplace\\\"><a href=\\\"/wiki/Hamilton,_New_Zealand\\\" title=\\\"Hamilton, New Zealand\\\">Hamilton, New Zealand</a></div></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Alma&#160;mater</th><td class=\\\"infobox-data\\\"><a href=\\\"/wiki/University_of_Auckland\\\" title=\\\"University of Auckland\\\">University of Auckland</a> (BSc, MSc)<br /><a href=\\\"/wiki/Iowa_State_University\\\" title=\\\"Iowa State University\\\">Iowa State University</a> (PhD)</td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Known&#160;for</th><td class=\\\"infobox-data\\\"><a href=\\\"/wiki/Ggplot2\\\" title=\\\"Ggplot2\\\">ggplot2</a><sup id=\\\"cite_ref-ggplot_3-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-ggplot-3\\\">&#91;3&#93;</a></sup><br /><a href=\\\"/wiki/Tidyverse\\\" title=\\\"Tidyverse\\\">tidyverse</a><br /><a href=\\\"/wiki/R_packages\\\" class=\\\"mw-redirect\\\" title=\\\"R packages\\\">R packages</a></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Awards</th><td class=\\\"infobox-data\\\"><style data-mw-deduplicate=\\\"TemplateStyles:r1126788409\\\">.mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}</style><div class=\\\"plainlist\\\">\\n<ul><li><a href=\\\"/wiki/COPSS_Presidents%27_Award\\\" title=\\\"COPSS Presidents&#39; Award\\\">COPSS Presidents' Award</a> (2019)</li>\\n<li><a href=\\\"/wiki/Fellow_of_the_American_Statistical_Association\\\" class=\\\"mw-redirect\\\" title=\\\"Fellow of the American Statistical Association\\\">Fellow of the American Statistical Association</a> (2015)</li></ul>\\n</div></td></tr><tr><td colspan=\\\"2\\\" class=\\\"infobox-full-data\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1218072481\\\"><b>Scientific career</b></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Fields</th><td class=\\\"infobox-data category\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1126788409\\\"><div class=\\\"plainlist\\\">\\n<ul><li><a href=\\\"/wiki/Data_science\\\" title=\\\"Data science\\\">Data science</a></li>\\n<li><a href=\\\"/wiki/Data_visualization\\\" class=\\\"mw-redirect\\\" title=\\\"Data visualization\\\">Visualization</a></li>\\n<li><a href=\\\"/wiki/Statistics\\\" title=\\\"Statistics\\\">Statistics</a><sup id=\\\"cite_ref-gs_1-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-gs-1\\\">&#91;1&#93;</a></sup></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Institutions</th><td class=\\\"infobox-data\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1126788409\\\"><div class=\\\"plainlist\\\">\\n<ul><li><a href=\\\"/w/index.php?title=Posit_Software&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Posit Software (page does not exist)\\\">Posit</a>, PBC (former RStudio Inc.)</li>\\n<li><a href=\\\"/wiki/University_of_Auckland\\\" title=\\\"University of Auckland\\\">University of Auckland</a></li>\\n<li><a href=\\\"/wiki/Stanford_University\\\" title=\\\"Stanford University\\\">Stanford University</a></li>\\n<li><a href=\\\"/wiki/Rice_University\\\" title=\\\"Rice University\\\">Rice University</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\"><a href=\\\"/wiki/Thesis\\\" title=\\\"Thesis\\\">Thesis</a></th><td class=\\\"infobox-data\\\"><i><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://dx.doi.org/10.31274/rtd-180813-16852\\\">Practical tools for exploring data and models</a></i>&#160;<span style=\\\"font-size:97%;\\\">(2008)</span></td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\"><a href=\\\"/wiki/Doctoral_advisor\\\" title=\\\"Doctoral advisor\\\">Doctoral advisors</a></th><td class=\\\"infobox-data\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1126788409\\\"><div class=\\\"plainlist\\\">\\n<ul><li><a href=\\\"/wiki/Dianne_Cook_(statistician)\\\" title=\\\"Dianne Cook (statistician)\\\">Dianne Cook</a></li>\\n<li><a href=\\\"/wiki/Heike_Hofmann\\\" title=\\\"Heike Hofmann\\\">Heike Hofmann</a><sup id=\\\"cite_ref-mathgene_2-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-mathgene-2\\\">&#91;2&#93;</a></sup></li></ul>\\n</div></td></tr><tr style=\\\"display:none\\\"><td colspan=\\\"2\\\">\\n</td></tr><tr><th scope=\\\"row\\\" class=\\\"infobox-label\\\">Website</th><td class=\\\"infobox-data\\\"><span class=\\\"url\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://hadley.nz/\\\">hadley<wbr />.nz</a></span></td></tr></tbody></table>\\n<p><b>Hadley Alexander Wickham</b> (born 14 October 1979) is a <a href=\\\"/wiki/New_Zealand\\\" title=\\\"New Zealand\\\">New Zealand</a> <a href=\\\"/wiki/Statistician\\\" title=\\\"Statistician\\\">statistician</a> known for his work on <a href=\\\"/wiki/Open-source_software\\\" title=\\\"Open-source software\\\">open-source software</a> for the <a href=\\\"/wiki/R_(programming_language)\\\" title=\\\"R (programming language)\\\">R statistical programming environment</a>. He is the <a href=\\\"/wiki/Chief_scientific_officer\\\" title=\\\"Chief scientific officer\\\">chief scientist</a> at <a href=\\\"/w/index.php?title=Posit,_PBC&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Posit, PBC (page does not exist)\\\">Posit, PBC</a> and an adjunct professor of statistics at the <a href=\\\"/wiki/University_of_Auckland\\\" title=\\\"University of Auckland\\\">University of Auckland</a>, <a href=\\\"/wiki/Stanford_University\\\" title=\\\"Stanford University\\\">Stanford University</a>, and <a href=\\\"/wiki/Rice_University\\\" title=\\\"Rice University\\\">Rice University</a>. His work includes the <a href=\\\"/wiki/Data_visualisation\\\" class=\\\"mw-redirect\\\" title=\\\"Data visualisation\\\">data visualisation</a> system <a href=\\\"/wiki/Ggplot2\\\" title=\\\"Ggplot2\\\">ggplot2</a> and the <a href=\\\"/wiki/Tidyverse\\\" title=\\\"Tidyverse\\\">tidyverse</a>, a collection of <a href=\\\"/wiki/R_package\\\" title=\\\"R package\\\">R packages</a> for <a href=\\\"/wiki/Data_science\\\" title=\\\"Data science\\\">data science</a> based on the concept of <a href=\\\"/wiki/Tidy_data\\\" class=\\\"mw-redirect\\\" title=\\\"Tidy data\\\">tidy data</a>.\\n</p>\\n<meta property=\\\"mw:PageProp/toc\\\" />\\n<h2><span class=\\\"mw-headline\\\" id=\\\"Education_and_career\\\">Education and career</span><span class=\\\"mw-editsection\\\"><span class=\\\"mw-editsection-bracket\\\">[</span><a href=\\\"/w/index.php?title=Hadley_Wickham&amp;action=edit&amp;section=1\\\" title=\\\"Edit section: Education and career\\\"><span>edit</span></a><span class=\\\"mw-editsection-bracket\\\">]</span></span></h2>\\n<p>Wickham was born in <a href=\\\"/wiki/Hamilton,_New_Zealand\\\" title=\\\"Hamilton, New Zealand\\\">Hamilton, New Zealand</a>. He received a Bachelors degree in <a href=\\\"/wiki/Human_Biology\\\" class=\\\"mw-redirect\\\" title=\\\"Human Biology\\\">Human Biology</a> and a masters degree in statistics at the <a href=\\\"/wiki/University_of_Auckland\\\" title=\\\"University of Auckland\\\">University of Auckland</a> in 1999–2004 and his <a href=\\\"/wiki/PhD\\\" class=\\\"mw-redirect\\\" title=\\\"PhD\\\">PhD</a> at <a href=\\\"/wiki/Iowa_State_University\\\" title=\\\"Iowa State University\\\">Iowa State University</a> in 2008 supervised by <a href=\\\"/wiki/Di_Cook\\\" class=\\\"mw-redirect\\\" title=\\\"Di Cook\\\">Di Cook</a> and <a href=\\\"/wiki/Heike_Hofmann\\\" title=\\\"Heike Hofmann\\\">Heike Hofmann</a>.<sup id=\\\"cite_ref-mathgene_2-1\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-mathgene-2\\\">&#91;2&#93;</a></sup><sup id=\\\"cite_ref-4\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-4\\\">&#91;4&#93;</a></sup> He is the <a href=\\\"/wiki/Chief_scientific_officer\\\" title=\\\"Chief scientific officer\\\">chief scientist</a> at <a href=\\\"/w/index.php?title=Posit,_PBC&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Posit, PBC (page does not exist)\\\">Posit, PBC</a> (formerly <a href=\\\"/wiki/RStudio\\\" title=\\\"RStudio\\\">RStudio</a>)<sup id=\\\"cite_ref-5\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-5\\\">&#91;5&#93;</a></sup> and an adjunct professor of statistics at the <a href=\\\"/wiki/University_of_Auckland\\\" title=\\\"University of Auckland\\\">University of Auckland</a>, <a href=\\\"/wiki/Stanford_University\\\" title=\\\"Stanford University\\\">Stanford University</a>, and <a href=\\\"/wiki/Rice_University\\\" title=\\\"Rice University\\\">Rice University</a>.<sup id=\\\"cite_ref-University_of_Auckland_Adjunct_Professorship_-_Dr_Hadley_Alexander_Wickham_-_Honorary,_Academic_6-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-University_of_Auckland_Adjunct_Professorship_-_Dr_Hadley_Alexander_Wickham_-_Honorary,_Academic-6\\\">&#91;6&#93;</a></sup><sup id=\\\"cite_ref-Stanford_University_-_Adjunct_Professor,_Institute_for_Computational_and_Mathematical_Engineering_(ICME)_7-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-Stanford_University_-_Adjunct_Professor,_Institute_for_Computational_and_Mathematical_Engineering_(ICME)-7\\\">&#91;7&#93;</a></sup><sup id=\\\"cite_ref-about_8-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-about-8\\\">&#91;8&#93;</a></sup>\\n</p><p>Wickham is a prominent and active member of the <a href=\\\"/wiki/R_(programming_language)\\\" title=\\\"R (programming language)\\\">R</a> user community and has developed several notable and widely used packages including <a href=\\\"/wiki/Ggplot2\\\" title=\\\"Ggplot2\\\">ggplot2</a>, plyr, <a href=\\\"/wiki/Dplyr\\\" title=\\\"Dplyr\\\">dplyr</a> and reshape2.<sup id=\\\"cite_ref-about_8-1\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-about-8\\\">&#91;8&#93;</a></sup><sup id=\\\"cite_ref-9\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-9\\\">&#91;9&#93;</a></sup> Wickham's data analysis packages for R are collectively known as the <a href=\\\"/wiki/Tidyverse\\\" title=\\\"Tidyverse\\\">tidyverse</a>.<sup id=\\\"cite_ref-10\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-10\\\">&#91;10&#93;</a></sup> According to Wickham's <a href=\\\"/wiki/Tidy_data\\\" class=\\\"mw-redirect\\\" title=\\\"Tidy data\\\">tidy data</a> approach, each <a href=\\\"/wiki/Variable_(mathematics)\\\" title=\\\"Variable (mathematics)\\\">variable</a> should be a <a href=\\\"/wiki/Column_(database)\\\" title=\\\"Column (database)\\\">column</a>, each observation should be a <a href=\\\"/wiki/Row_(database)\\\" title=\\\"Row (database)\\\">row</a>, and each type of observational unit should be a <a href=\\\"/wiki/Table_(database)\\\" title=\\\"Table (database)\\\">table</a>.<sup id=\\\"cite_ref-11\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-11\\\">&#91;11&#93;</a></sup>\\n</p>\\n<h3><span class=\\\"mw-headline\\\" id=\\\"Honors_and_awards\\\">Honors and awards</span><span class=\\\"mw-editsection\\\"><span class=\\\"mw-editsection-bracket\\\">[</span><a href=\\\"/w/index.php?title=Hadley_Wickham&amp;action=edit&amp;section=2\\\" title=\\\"Edit section: Honors and awards\\\"><span>edit</span></a><span class=\\\"mw-editsection-bracket\\\">]</span></span></h3>\\n<p>In 2006 he was awarded the <a href=\\\"/wiki/John_Chambers_(statistician)\\\" title=\\\"John Chambers (statistician)\\\">John Chambers</a> Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<sup id=\\\"cite_ref-12\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-12\\\">&#91;12&#93;</a></sup> Wickham was named a Fellow by the <a href=\\\"/wiki/American_Statistical_Association\\\" title=\\\"American Statistical Association\\\">American Statistical Association</a> in 2015 for \\\"pivotal contributions to statistical practice through innovative and pioneering research in statistical graphics and computing\\\".<sup id=\\\"cite_ref-asa-fellow_13-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-asa-fellow-13\\\">&#91;13&#93;</a></sup> Wickham was awarded the international <a href=\\\"/wiki/COPSS_Presidents%27_Award\\\" title=\\\"COPSS Presidents&#39; Award\\\">COPSS Presidents' Award</a> in 2019 for \\\"influential work in statistical computing, visualisation, graphics, and data analysis\\\" including \\\"making statistical thinking and computing accessible to a large audience\\\".<sup id=\\\"cite_ref-14\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-14\\\">&#91;14&#93;</a></sup>\\n</p>\\n<h2><span class=\\\"mw-headline\\\" id=\\\"Personal_life\\\">Personal life</span><span class=\\\"mw-editsection\\\"><span class=\\\"mw-editsection-bracket\\\">[</span><a href=\\\"/w/index.php?title=Hadley_Wickham&amp;action=edit&amp;section=3\\\" title=\\\"Edit section: Personal life\\\"><span>edit</span></a><span class=\\\"mw-editsection-bracket\\\">]</span></span></h2>\\n<p>Wickham's sister Charlotte Wickham is also a statistician.<sup id=\\\"cite_ref-hz_15-0\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-hz-15\\\">&#91;15&#93;</a></sup>\\n</p>\\n<h2><span class=\\\"mw-headline\\\" id=\\\"Publications\\\">Publications</span><span class=\\\"mw-editsection\\\"><span class=\\\"mw-editsection-bracket\\\">[</span><a href=\\\"/w/index.php?title=Hadley_Wickham&amp;action=edit&amp;section=4\\\" title=\\\"Edit section: Publications\\\"><span>edit</span></a><span class=\\\"mw-editsection-bracket\\\">]</span></span></h2>\\n<p>Wickham's publications<sup id=\\\"cite_ref-gs_1-1\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-gs-1\\\">&#91;1&#93;</a></sup> include:\\n</p>\\n<ul><li><style data-mw-deduplicate=\\\"TemplateStyles:r1215172403\\\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\\\"\\\\\\\"\\\"\\\"\\\\\\\"\\\"\\\"'\\\"\\\"'\\\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\\\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\\\")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\\\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\\\")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\\\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\\\")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url(\\\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\\\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}}</style><cite id=\\\"CITEREFWickhamGrolemund2017\\\" class=\\\"citation book cs1\\\">Wickham, Hadley; Grolemund, Garrett (2017). <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://r4ds.had.co.nz/\\\"><i>R for Data Science&#160;: Import, Tidy, Transform, Visualize, and Model Data</i></a>. Sebastopol, CA: O'Reilly Media. <a href=\\\"/wiki/ISBN_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"ISBN (identifier)\\\">ISBN</a>&#160;<a href=\\\"/wiki/Special:BookSources/978-1491910399\\\" title=\\\"Special:BookSources/978-1491910399\\\"><bdi>978-1491910399</bdi></a>. <a href=\\\"/wiki/OCLC_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"OCLC (identifier)\\\">OCLC</a>&#160;<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.worldcat.org/oclc/968213225\\\">968213225</a>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=R+for+Data+Science+%3A+Import%2C+Tidy%2C+Transform%2C+Visualize%2C+and+Model+Data&amp;rft.place=Sebastopol%2C+CA&amp;rft.pub=O%27Reilly+Media&amp;rft.date=2017&amp;rft_id=info%3Aoclcnum%2F968213225&amp;rft.isbn=978-1491910399&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rft.au=Grolemund%2C+Garrett&amp;rft_id=http%3A%2F%2Fr4ds.had.co.nz%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham,_Hadley2015\\\" class=\\\"citation book cs1\\\">Wickham, Hadley (2015). <i>R Packages</i>. Sebastopol, CA: O'Reilly Media, Inc. <a href=\\\"/wiki/ISBN_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"ISBN (identifier)\\\">ISBN</a>&#160;<a href=\\\"/wiki/Special:BookSources/978-1491910597\\\" title=\\\"Special:BookSources/978-1491910597\\\"><bdi>978-1491910597</bdi></a>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=R+Packages&amp;rft.place=Sebastopol%2C+CA&amp;rft.pub=O%27Reilly+Media%2C+Inc&amp;rft.date=2015&amp;rft.isbn=978-1491910597&amp;rft.au=Wickham%2C+Hadley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham,_Hadley2014\\\" class=\\\"citation book cs1\\\">Wickham, Hadley (2014). <i>Advanced R</i>. New York: Chapman &amp; Hall/CRC The R Series. <a href=\\\"/wiki/ISBN_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"ISBN (identifier)\\\">ISBN</a>&#160;<a href=\\\"/wiki/Special:BookSources/978-1466586963\\\" title=\\\"Special:BookSources/978-1466586963\\\"><bdi>978-1466586963</bdi></a>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Advanced+R&amp;rft.place=New+York&amp;rft.pub=Chapman+%26+Hall%2FCRC+The+R+Series&amp;rft.date=2014&amp;rft.isbn=978-1466586963&amp;rft.au=Wickham%2C+Hadley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2011\\\" class=\\\"citation journal cs1\\\">Wickham, Hadley (2011). <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.18637%2Fjss.v040.i01\\\">\\\"The split-apply-combine strategy for data analysis\\\"</a>. <i>Journal of Statistical Software</i>. <b>40</b> (1): 1–29. <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<span class=\\\"id-lock-free\\\" title=\\\"Freely accessible\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.18637%2Fjss.v040.i01\\\">10.18637/jss.v040.i01</a></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Statistical+Software&amp;rft.atitle=The+split-apply-combine+strategy+for+data+analysis&amp;rft.volume=40&amp;rft.issue=1&amp;rft.pages=1-29&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.18637%2Fjss.v040.i01&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.18637%252Fjss.v040.i01&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2010\\\" class=\\\"citation journal cs1\\\">Wickham, Hadley (2010). \\\"A layered grammar of graphics\\\". <i>Journal of Computational and Graphical Statistics</i>. <b>19</b> (1): 3–28. <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.1198%2Fjcgs.2009.07098\\\">10.1198/jcgs.2009.07098</a>. <a href=\\\"/wiki/S2CID_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"S2CID (identifier)\\\">S2CID</a>&#160;<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://api.semanticscholar.org/CorpusID:58971746\\\">58971746</a>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Computational+and+Graphical+Statistics&amp;rft.atitle=A+layered+grammar+of+graphics&amp;rft.volume=19&amp;rft.issue=1&amp;rft.pages=3-28&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1198%2Fjcgs.2009.07098&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A58971746%23id-name%3DS2CID&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2010\\\" class=\\\"citation journal cs1\\\">Wickham, Hadley (2010). <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.32614%2FRJ-2010-012\\\">\\\"stringr: modern, consistent string processing\\\"</a>. <i><a href=\\\"/wiki/The_R_Journal\\\" title=\\\"The R Journal\\\">The R Journal</a></i>. <b>2</b> (2): 3–28. <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<span class=\\\"id-lock-free\\\" title=\\\"Freely accessible\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.32614%2FRJ-2010-012\\\">10.32614/RJ-2010-012</a></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+R+Journal&amp;rft.atitle=stringr%3A+modern%2C+consistent+string+processing&amp;rft.volume=2&amp;rft.issue=2&amp;rft.pages=3-28&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.32614%2FRJ-2010-012&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.32614%252FRJ-2010-012&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham,_Hadley2009\\\" class=\\\"citation book cs1\\\">Wickham, Hadley (2009). <i>ggplot2: Elegant Graphics for Data Analysis (Use R!)</i>. New York: Springer. <a href=\\\"/wiki/ISBN_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"ISBN (identifier)\\\">ISBN</a>&#160;<a href=\\\"/wiki/Special:BookSources/978-0387981406\\\" title=\\\"Special:BookSources/978-0387981406\\\"><bdi>978-0387981406</bdi></a>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=ggplot2%3A+Elegant+Graphics+for+Data+Analysis+%28Use+R%21%29&amp;rft.place=New+York&amp;rft.pub=Springer&amp;rft.date=2009&amp;rft.isbn=978-0387981406&amp;rft.au=Wickham%2C+Hadley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span><sup id=\\\"cite_ref-ggplot_3-1\\\" class=\\\"reference\\\"><a href=\\\"#cite_note-ggplot-3\\\">&#91;3&#93;</a></sup></li>\\n<li><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2007\\\" class=\\\"citation journal cs1\\\">Wickham, Hadley (2007). <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.18637%2Fjss.v021.i12\\\">\\\"Reshaping data with the reshape package\\\"</a>. <i>Journal of Statistical Software</i>. <b>21</b> (12): 1–20. <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<span class=\\\"id-lock-free\\\" title=\\\"Freely accessible\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.18637%2Fjss.v021.i12\\\">10.18637/jss.v021.i12</a></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Statistical+Software&amp;rft.atitle=Reshaping+data+with+the+reshape+package&amp;rft.volume=21&amp;rft.issue=12&amp;rft.pages=1-20&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.18637%2Fjss.v021.i12&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.18637%252Fjss.v021.i12&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></li></ul>\\n<h2><span class=\\\"mw-headline\\\" id=\\\"References\\\">References</span><span class=\\\"mw-editsection\\\"><span class=\\\"mw-editsection-bracket\\\">[</span><a href=\\\"/w/index.php?title=Hadley_Wickham&amp;action=edit&amp;section=5\\\" title=\\\"Edit section: References\\\"><span>edit</span></a><span class=\\\"mw-editsection-bracket\\\">]</span></span></h2>\\n<style data-mw-deduplicate=\\\"TemplateStyles:r1217336898\\\">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class=\\\"reflist\\\">\\n<div class=\\\"mw-references-wrap mw-references-columns\\\"><ol class=\\\"references\\\">\\n<li id=\\\"cite_note-gs-1\\\"><span class=\\\"mw-cite-backlink\\\">^ <a href=\\\"#cite_ref-gs_1-0\\\"><sup><i><b>a</b></i></sup></a> <a href=\\\"#cite_ref-gs_1-1\\\"><sup><i><b>b</b></i></sup></a></span> <span class=\\\"reference-text\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://scholar.google.com/citations?user=YA43PbsAAAAJ\\\">Hadley Wickham</a> publications indexed by <a href=\\\"/wiki/Google_Scholar\\\" title=\\\"Google Scholar\\\">Google Scholar</a> <span class=\\\"mw-valign-text-top noprint\\\" typeof=\\\"mw:File/Frameless\\\"><a href=\\\"https://www.wikidata.org/wiki/Q16251925#P1960\\\" title=\\\"Edit this at Wikidata\\\"><img alt=\\\"Edit this at Wikidata\\\" src=\\\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\" decoding=\\\"async\\\" width=\\\"10\\\" height=\\\"10\\\" class=\\\"mw-file-element\\\" srcset=\\\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x\\\" data-file-width=\\\"20\\\" data-file-height=\\\"20\\\" /></a></span></span>\\n</li>\\n<li id=\\\"cite_note-mathgene-2\\\"><span class=\\\"mw-cite-backlink\\\">^ <a href=\\\"#cite_ref-mathgene_2-0\\\"><sup><i><b>a</b></i></sup></a> <a href=\\\"#cite_ref-mathgene_2-1\\\"><sup><i><b>b</b></i></sup></a></span> <span class=\\\"reference-text\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://mathgenealogy.org/id.php?id=145799\\\">Hadley Wickham</a> at the <a href=\\\"/wiki/Mathematics_Genealogy_Project\\\" title=\\\"Mathematics Genealogy Project\\\">Mathematics Genealogy Project</a> <span class=\\\"mw-valign-text-top noprint\\\" typeof=\\\"mw:File/Frameless\\\"><a href=\\\"https://www.wikidata.org/wiki/Q16251925#P549\\\" title=\\\"Edit this at Wikidata\\\"><img alt=\\\"Edit this at Wikidata\\\" src=\\\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\" decoding=\\\"async\\\" width=\\\"10\\\" height=\\\"10\\\" class=\\\"mw-file-element\\\" srcset=\\\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x\\\" data-file-width=\\\"20\\\" data-file-height=\\\"20\\\" /></a></span></span>\\n</li>\\n<li id=\\\"cite_note-ggplot-3\\\"><span class=\\\"mw-cite-backlink\\\">^ <a href=\\\"#cite_ref-ggplot_3-0\\\"><sup><i><b>a</b></i></sup></a> <a href=\\\"#cite_ref-ggplot_3-1\\\"><sup><i><b>b</b></i></sup></a></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2011\\\" class=\\\"citation journal cs1\\\">Wickham, Hadley (2011). \\\"ggplot2\\\". <i>Wiley Interdisciplinary Reviews: Computational Statistics</i>. <b>3</b> (2): 180–185. <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.1002%2Fwics.147\\\">10.1002/wics.147</a>. <a href=\\\"/wiki/ISSN_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"ISSN (identifier)\\\">ISSN</a>&#160;<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.worldcat.org/issn/1939-5108\\\">1939-5108</a>. <a href=\\\"/wiki/S2CID_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"S2CID (identifier)\\\">S2CID</a>&#160;<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://api.semanticscholar.org/CorpusID:247702774\\\">247702774</a>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wiley+Interdisciplinary+Reviews%3A+Computational+Statistics&amp;rft.atitle=ggplot2&amp;rft.volume=3&amp;rft.issue=2&amp;rft.pages=180-185&amp;rft.date=2011&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A247702774%23id-name%3DS2CID&amp;rft.issn=1939-5108&amp;rft_id=info%3Adoi%2F10.1002%2Fwics.147&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-4\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-4\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2008\\\" class=\\\"citation thesis cs1\\\">Wickham, Hadley Alexander (2008). <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://lib.dr.iastate.edu/rtd/15639/\\\"><i>Practical tools for exploring data and models</i></a>. <i>iastate.edu</i> (PhD). Iowa State University. <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<span class=\\\"id-lock-free\\\" title=\\\"Freely accessible\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.31274%2Frtd-180813-16852\\\">10.31274/rtd-180813-16852</a></span>. <a href=\\\"/wiki/OCLC_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"OCLC (identifier)\\\">OCLC</a>&#160;<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.worldcat.org/oclc/247410260\\\">247410260</a>. <link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><a href=\\\"/wiki/ProQuest_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"ProQuest (identifier)\\\">ProQuest</a>&#160;<a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://search.proquest.com/docview/194000416\\\">194000416</a><span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2019-02-14</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Practical+tools+for+exploring+data+and+models&amp;rft.inst=Iowa+State+University&amp;rft.date=2008&amp;rft_id=info%3Aoclcnum%2F247410260&amp;rft_id=info%3Adoi%2F10.31274%2Frtd-180813-16852&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley+Alexander&amp;rft_id=https%3A%2F%2Flib.dr.iastate.edu%2Frtd%2F15639%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-5\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-5\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.rstudio.com/authors/hadley-wickham/\\\">\\\"Hadley Wickham\\\"</a>. <i>RStudio</i><span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2023-05-05</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=RStudio&amp;rft.atitle=Hadley+Wickham&amp;rft_id=https%3A%2F%2Fwww.rstudio.com%2Fauthors%2Fhadley-wickham%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-University_of_Auckland_Adjunct_Professorship_-_Dr_Hadley_Alexander_Wickham_-_Honorary,_Academic-6\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-University_of_Auckland_Adjunct_Professorship_-_Dr_Hadley_Alexander_Wickham_-_Honorary,_Academic_6-0\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.stat.auckland.ac.nz/people/hwic004\\\">\\\"University of Auckland\\\"</a><span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2017-09-03</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=University+of+Auckland&amp;rft_id=https%3A%2F%2Fwww.stat.auckland.ac.nz%2Fpeople%2Fhwic004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-Stanford_University_-_Adjunct_Professor,_Institute_for_Computational_and_Mathematical_Engineering_(ICME)-7\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-Stanford_University_-_Adjunct_Professor,_Institute_for_Computational_and_Mathematical_Engineering_(ICME)_7-0\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://profiles.stanford.edu/hadley-wickham\\\">\\\"Hadley Wickham's Profile - Stanford Profiles\\\"</a><span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2017-09-03</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Hadley+Wickham%27s+Profile+-+Stanford+Profiles&amp;rft_id=https%3A%2F%2Fprofiles.stanford.edu%2Fhadley-wickham&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span> <sup class=\\\"noprint Inline-Template\\\"><span style=\\\"white-space: nowrap;\\\">&#91;<i><a href=\\\"/wiki/Wikipedia:Link_rot\\\" title=\\\"Wikipedia:Link rot\\\"><span title=\\\"&#160;Dead link tagged July 2021\\\">dead link</span></a></i>&#93;</span></sup></span>\\n</li>\\n<li id=\\\"cite_note-about-8\\\"><span class=\\\"mw-cite-backlink\\\">^ <a href=\\\"#cite_ref-about_8-0\\\"><sup><i><b>a</b></i></sup></a> <a href=\\\"#cite_ref-about_8-1\\\"><sup><i><b>b</b></i></sup></a></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://www.rstudio.com/about/\\\">\\\"About - RStudio\\\"</a><span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2014-08-13</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=About+-+RStudio&amp;rft_id=http%3A%2F%2Fwww.rstudio.com%2Fabout%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-9\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-9\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/\\\">\\\"Top 100 R Packages for 2013 (Jan-May)!\\\"</a>. R-statistics blog. 13 June 2013<span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2014-08-12</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Top+100+R+Packages+for+2013+%28Jan-May%29%21&amp;rft.pub=R-statistics+blog&amp;rft.date=2013-06-13&amp;rft_id=http%3A%2F%2Fwww.r-statistics.com%2F2013%2F06%2Ftop-100-r-packages-for-2013-jan-may%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-10\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-10\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://blog.revolutionanalytics.com/2016/09/tidyverse.html\\\">\\\"Welcome to the Tidyverse\\\"</a>. Revolution Analytics<span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2016-09-21</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Welcome+to+the+Tidyverse&amp;rft.pub=Revolution+Analytics&amp;rft_id=http%3A%2F%2Fblog.revolutionanalytics.com%2F2016%2F09%2Ftidyverse.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-11\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-11\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite id=\\\"CITEREFWickham2014\\\" class=\\\"citation journal cs1\\\">Wickham, Hadley (2014). <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.18637%2Fjss.v059.i10\\\">\\\"Tidy Data\\\"</a>. <i>Journal of Statistical Software</i>. <b>59</b> (10). <a href=\\\"/wiki/Doi_(identifier)\\\" class=\\\"mw-redirect\\\" title=\\\"Doi (identifier)\\\">doi</a>:<span class=\\\"id-lock-free\\\" title=\\\"Freely accessible\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://doi.org/10.18637%2Fjss.v059.i10\\\">10.18637/jss.v059.i10</a></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Statistical+Software&amp;rft.atitle=Tidy+Data&amp;rft.volume=59&amp;rft.issue=10&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.18637%2Fjss.v059.i10&amp;rft.aulast=Wickham&amp;rft.aufirst=Hadley&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.18637%252Fjss.v059.i10&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-12\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-12\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://web.archive.org/web/20170731231612/http://stat-computing.org/awards/jmc/winners.html\\\">\\\"John Chambers Award Past winners\\\"</a>. ASA Sections on Statistical Computing, Statistical Graphics. Archived from <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://stat-computing.org/awards/jmc/winners.html\\\">the original</a> on 2017-07-31<span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">2014-08-12</span></span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=John+Chambers+Award+Past+winners&amp;rft.pub=ASA+Sections+on+Statistical+Computing%2C+Statistical+Graphics&amp;rft_id=http%3A%2F%2Fstat-computing.org%2Fawards%2Fjmc%2Fwinners.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-asa-fellow-13\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-asa-fellow_13-0\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://web.archive.org/web/20160304081541/http://www.amstat.org/newsroom/pressreleases/2015-ASANames62NewFellows.pdf\\\">\\\"ASA names 62 fellows\\\"</a> <span class=\\\"cs1-format\\\">(PDF)</span>. <i>American Statistical Association</i>. Archived from <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.amstat.org/newsroom/pressreleases/2015-ASANames62NewFellows.pdf\\\">the original</a> <span class=\\\"cs1-format\\\">(PDF)</span> on 4 March 2016<span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">14 November</span> 2015</span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=American+Statistical+Association&amp;rft.atitle=ASA+names+62+fellows&amp;rft_id=https%3A%2F%2Fwww.amstat.org%2Fnewsroom%2Fpressreleases%2F2015-ASANames62NewFellows.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-14\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-14\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=12254723\\\">\\\"Kiwi wins prestigious international statistics award for his outstanding contributions to the profession\\\"</a><span class=\\\"reference-accessdate\\\">. Retrieved <span class=\\\"nowrap\\\">1 August</span> 2019</span>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Kiwi+wins+prestigious+international+statistics+award+for+his+outstanding+contributions+to+the+profession&amp;rft_id=https%3A%2F%2Fwww.nzherald.co.nz%2Fnz%2Fnews%2Farticle.cfm%3Fc_id%3D1%26objectid%3D12254723&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n<li id=\\\"cite_note-hz-15\\\"><span class=\\\"mw-cite-backlink\\\"><b><a href=\\\"#cite_ref-hz_15-0\\\">^</a></b></span> <span class=\\\"reference-text\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1215172403\\\"><cite class=\\\"citation web cs1\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://hadley.nz/\\\">\\\"Hadley Wickham\\\"</a>. <i>hadley.nz</i>.</cite><span title=\\\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=hadley.nz&amp;rft.atitle=Hadley+Wickham&amp;rft_id=http%3A%2F%2Fhadley.nz%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHadley+Wickham\\\" class=\\\"Z3988\\\"></span></span>\\n</li>\\n</ol></div></div>\\n<h2><span class=\\\"mw-headline\\\" id=\\\"External_links\\\">External links</span><span class=\\\"mw-editsection\\\"><span class=\\\"mw-editsection-bracket\\\">[</span><a href=\\\"/w/index.php?title=Hadley_Wickham&amp;action=edit&amp;section=6\\\" title=\\\"Edit section: External links\\\"><span>edit</span></a><span class=\\\"mw-editsection-bracket\\\">]</span></span></h2>\\n<ul><li>On the web\\n<ul><li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://twitter.com/hadleywickham\\\">twitter</a></li>\\n<li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://github.com/hadley\\\">github</a></li></ul></li>\\n<li>interviews\\n<ul><li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://datascience.la/a-conversation-with-hadley-wickham-the-user-2014-interview/\\\">Interview</a> <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://web.archive.org/web/20230201000144/https://datascience.la/a-conversation-with-hadley-wickham-the-user-2014-interview/\\\">Archived</a> 2023-02-01 at the <a href=\\\"/wiki/Wayback_Machine\\\" title=\\\"Wayback Machine\\\">Wayback Machine</a> by Datascience.LA at UseR! 2014</li>\\n<li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://statr.me/2013/09/a-conversation-with-hadley-wickham/\\\">Interview</a> by Yixuan Qiu (2013)</li>\\n<li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://peadarcoyle.wordpress.com/2015/08/02/interview-with-a-data-scientist-hadley-wickham/\\\">Interview</a> by Models are</li></ul></li>\\n<li>talks\\n<ul><li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://strataconf.com/strata2014/public/schedule/speaker/131906\\\">Speaker Hadley Wickham Strata 2014 - O'Reilly Conferences, February 11 - 13, 2014, Santa Clara, CA</a> <a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://web.archive.org/web/20140223174908/http://strataconf.com/strata2014/public/schedule/speaker/131906\\\">Archived</a> 2014-02-23 at the <a href=\\\"/wiki/Wayback_Machine\\\" title=\\\"Wayback Machine\\\">Wayback Machine</a></li>\\n<li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.youtube.com/watch?v=LOXe6Eu59As\\\">Interview</a> at Strata 2014 Illuminating and Wrong</li>\\n<li><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.youtube.com/watch?v=1POb5fx_m3I\\\">Ihaka Lecture Series 2017: Expressing yourself with R</a></li></ul></li></ul>\\n<div class=\\\"navbox-styles\\\"><style data-mw-deduplicate=\\\"TemplateStyles:r1129693374\\\">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\\\": \\\"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\\\" · \\\";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\\\" (\\\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\\\")\\\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\\\" \\\"counter(listitem)\\\"\\\\a0 \\\"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\\\" (\\\"counter(listitem)\\\"\\\\a0 \\\"}</style><style data-mw-deduplicate=\\\"TemplateStyles:r1061467846\\\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div role=\\\"navigation\\\" class=\\\"navbox\\\" aria-labelledby=\\\"R_(programming_language)\\\" style=\\\"padding:3px\\\"><table class=\\\"nowraplinks mw-collapsible autocollapse navbox-inner\\\" style=\\\"border-spacing:0;background:transparent;color:inherit\\\"><tbody><tr><th scope=\\\"col\\\" class=\\\"navbox-title\\\" colspan=\\\"3\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1129693374\\\"><style data-mw-deduplicate=\\\"TemplateStyles:r1063604349\\\">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\\\"[ \\\"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\\\" ]\\\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class=\\\"navbar plainlinks hlist navbar-mini\\\"><ul><li class=\\\"nv-view\\\"><a href=\\\"/wiki/Template:R_(programming_language)\\\" title=\\\"Template:R (programming language)\\\"><abbr title=\\\"View this template\\\" style=\\\";;background:none transparent;border:none;box-shadow:none;padding:0;\\\">v</abbr></a></li><li class=\\\"nv-talk\\\"><a href=\\\"/wiki/Template_talk:R_(programming_language)\\\" title=\\\"Template talk:R (programming language)\\\"><abbr title=\\\"Discuss this template\\\" style=\\\";;background:none transparent;border:none;box-shadow:none;padding:0;\\\">t</abbr></a></li><li class=\\\"nv-edit\\\"><a href=\\\"/wiki/Special:EditPage/Template:R_(programming_language)\\\" title=\\\"Special:EditPage/Template:R (programming language)\\\"><abbr title=\\\"Edit this template\\\" style=\\\";;background:none transparent;border:none;box-shadow:none;padding:0;\\\">e</abbr></a></li></ul></div><div id=\\\"R_(programming_language)\\\" style=\\\"font-size:114%;margin:0 4em\\\"><a href=\\\"/wiki/R_(programming_language)\\\" title=\\\"R (programming language)\\\">R (programming language)</a></div></th></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\"><a href=\\\"/wiki/R_(programming_language)#Programming_features\\\" title=\\\"R (programming language)\\\">Features</a></th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Sweave\\\" title=\\\"Sweave\\\">Sweave</a></li></ul>\\n</div></td><td class=\\\"noviewer navbox-image\\\" rowspan=\\\"7\\\" style=\\\"width:1px;padding:0 0 0 2px\\\"><div><span typeof=\\\"mw:File\\\"><a href=\\\"/wiki/File:R_logo.svg\\\" class=\\\"mw-file-description\\\"><img src=\\\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/80px-R_logo.svg.png\\\" decoding=\\\"async\\\" width=\\\"80\\\" height=\\\"62\\\" class=\\\"mw-file-element\\\" srcset=\\\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/120px-R_logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/160px-R_logo.svg.png 2x\\\" data-file-width=\\\"724\\\" data-file-height=\\\"561\\\" /></a></span></div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\"><a href=\\\"/wiki/R_(programming_language)#Implementations\\\" title=\\\"R (programming language)\\\">Implementations</a></th><td class=\\\"navbox-list-with-group navbox-list navbox-even hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Distributed_R\\\" title=\\\"Distributed R\\\">Distributed R</a></li>\\n<li><a href=\\\"/wiki/Microsoft_R_Open\\\" class=\\\"mw-redirect\\\" title=\\\"Microsoft R Open\\\">Microsoft R Open</a> (Revolution R Open)</li>\\n<li><a href=\\\"/wiki/Renjin\\\" title=\\\"Renjin\\\">Renjin</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\"><a href=\\\"/wiki/R_package\\\" title=\\\"R package\\\">Packages</a></th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Bibliometrix\\\" title=\\\"Bibliometrix\\\">Bibliometrix</a></li>\\n<li><a href=\\\"/wiki/Easystats\\\" title=\\\"Easystats\\\">easystats</a></li>\\n<li><a href=\\\"/wiki/Quantitative_Discourse_Analysis_Package\\\" title=\\\"Quantitative Discourse Analysis Package\\\">qdap</a></li>\\n<li><a href=\\\"/wiki/Lumi_(software)\\\" title=\\\"Lumi (software)\\\">lumi</a></li>\\n<li><a href=\\\"/wiki/RGtk2\\\" title=\\\"RGtk2\\\">RGtk2</a></li>\\n<li><a href=\\\"/wiki/Rhea_(pipeline)\\\" title=\\\"Rhea (pipeline)\\\">Rhea</a></li>\\n<li><a href=\\\"/wiki/Rmetrics\\\" title=\\\"Rmetrics\\\">Rmetrics</a></li>\\n<li><a href=\\\"/wiki/Rnn_(software)\\\" title=\\\"Rnn (software)\\\">rnn</a></li>\\n<li><a href=\\\"/wiki/RQDA\\\" title=\\\"RQDA\\\">RQDA</a></li>\\n<li><a href=\\\"/wiki/Shiny_(software)\\\" title=\\\"Shiny (software)\\\">shiny</a></li>\\n<li><a href=\\\"/wiki/SimpleITK\\\" title=\\\"SimpleITK\\\">SimpleITK</a></li>\\n<li><a href=\\\"/wiki/Statcheck\\\" title=\\\"Statcheck\\\">Statcheck</a></li>\\n<li><b><a href=\\\"/wiki/Tidyverse\\\" title=\\\"Tidyverse\\\">tidyverse</a></b>\\n<ul><li><a href=\\\"/wiki/Ggplot2\\\" title=\\\"Ggplot2\\\">ggplot2</a></li>\\n<li><a href=\\\"/wiki/Dplyr\\\" title=\\\"Dplyr\\\">dplyr</a></li>\\n<li><a href=\\\"/wiki/Knitr\\\" title=\\\"Knitr\\\">knitr</a></li></ul></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\"><a href=\\\"/wiki/R_(programming_language)#Interfaces\\\" title=\\\"R (programming language)\\\">Interfaces</a></th><td class=\\\"navbox-list-with-group navbox-list navbox-even hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Bio7\\\" title=\\\"Bio7\\\">Bio7</a></li>\\n<li><a href=\\\"/wiki/Emacs_Speaks_Statistics\\\" title=\\\"Emacs Speaks Statistics\\\">Emacs Speaks Statistics</a></li>\\n<li><a href=\\\"/wiki/Java_GUI_for_R\\\" title=\\\"Java GUI for R\\\">Java GUI for R</a></li>\\n<li><a href=\\\"/wiki/KH_Coder\\\" title=\\\"KH Coder\\\">KH Coder</a></li>\\n<li><a href=\\\"/wiki/Rattle_GUI\\\" title=\\\"Rattle GUI\\\">Rattle GUI</a></li>\\n<li><a href=\\\"/wiki/R_Commander\\\" title=\\\"R Commander\\\">R Commander</a></li>\\n<li><a href=\\\"/wiki/RExcel\\\" title=\\\"RExcel\\\">RExcel</a></li>\\n<li><a href=\\\"/wiki/RKWard\\\" title=\\\"RKWard\\\">RKWard</a></li>\\n<li><a href=\\\"/wiki/RStudio\\\" title=\\\"RStudio\\\">RStudio</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">People</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Roger_Bivand\\\" title=\\\"Roger Bivand\\\">Roger Bivand</a></li>\\n<li><a href=\\\"/wiki/Jenny_Bryan\\\" title=\\\"Jenny Bryan\\\">Jenny Bryan</a></li>\\n<li><a href=\\\"/wiki/John_Chambers_(statistician)\\\" title=\\\"John Chambers (statistician)\\\">John Chambers</a></li>\\n<li><a href=\\\"/wiki/Peter_Dalgaard\\\" title=\\\"Peter Dalgaard\\\">Peter Dalgaard</a></li>\\n<li><a href=\\\"/wiki/Dirk_Eddelbuettel\\\" title=\\\"Dirk Eddelbuettel\\\">Dirk Eddelbuettel</a></li>\\n<li><a href=\\\"/wiki/Robert_Gentleman_(statistician)\\\" title=\\\"Robert Gentleman (statistician)\\\">Robert Gentleman</a></li>\\n<li><a href=\\\"/wiki/Ross_Ihaka\\\" title=\\\"Ross Ihaka\\\">Ross Ihaka</a></li>\\n<li><a href=\\\"/wiki/Thomas_Lumley_(statistician)\\\" title=\\\"Thomas Lumley (statistician)\\\">Thomas Lumley</a></li>\\n<li><a href=\\\"/wiki/Brian_D._Ripley\\\" title=\\\"Brian D. Ripley\\\">Brian D. Ripley</a></li>\\n<li><a href=\\\"/wiki/Julia_Silge\\\" title=\\\"Julia Silge\\\">Julia Silge</a></li>\\n<li><a href=\\\"/wiki/Luke_Tierney\\\" title=\\\"Luke Tierney\\\">Luke Tierney</a></li>\\n<li><a class=\\\"mw-selflink selflink\\\">Hadley Wickham</a></li>\\n<li><a href=\\\"/wiki/Yihui_Xie\\\" title=\\\"Yihui Xie\\\">Yihui Xie</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Organisations</th><td class=\\\"navbox-list-with-group navbox-list navbox-even hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/R_Consortium\\\" class=\\\"mw-redirect\\\" title=\\\"R Consortium\\\">R Consortium</a></li>\\n<li><a href=\\\"/wiki/Revolution_Analytics\\\" title=\\\"Revolution Analytics\\\">Revolution Analytics</a></li>\\n<li><a href=\\\"/wiki/R-Ladies\\\" title=\\\"R-Ladies\\\">R-Ladies</a></li>\\n<li><a href=\\\"/wiki/RStudio\\\" title=\\\"RStudio\\\">RStudio</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Publications</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><i><a href=\\\"/wiki/The_R_Journal\\\" title=\\\"The R Journal\\\">The R Journal</a></i></li></ul>\\n</div></td></tr></tbody></table></div>\\n<div class=\\\"navbox-styles\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1129693374\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1061467846\\\"></div><div role=\\\"navigation\\\" class=\\\"navbox\\\" aria-labelledby=\\\"Visualization_of_technical_information\\\" style=\\\"padding:3px\\\"><table class=\\\"nowraplinks mw-collapsible autocollapse navbox-inner\\\" style=\\\"border-spacing:0;background:transparent;color:inherit\\\"><tbody><tr><th scope=\\\"col\\\" class=\\\"navbox-title\\\" colspan=\\\"2\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1129693374\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1063604349\\\"><div class=\\\"navbar plainlinks hlist navbar-mini\\\"><ul><li class=\\\"nv-view\\\"><a href=\\\"/wiki/Template:Visualization\\\" title=\\\"Template:Visualization\\\"><abbr title=\\\"View this template\\\" style=\\\";;background:none transparent;border:none;box-shadow:none;padding:0;\\\">v</abbr></a></li><li class=\\\"nv-talk\\\"><a href=\\\"/wiki/Template_talk:Visualization\\\" title=\\\"Template talk:Visualization\\\"><abbr title=\\\"Discuss this template\\\" style=\\\";;background:none transparent;border:none;box-shadow:none;padding:0;\\\">t</abbr></a></li><li class=\\\"nv-edit\\\"><a href=\\\"/wiki/Special:EditPage/Template:Visualization\\\" title=\\\"Special:EditPage/Template:Visualization\\\"><abbr title=\\\"Edit this template\\\" style=\\\";;background:none transparent;border:none;box-shadow:none;padding:0;\\\">e</abbr></a></li></ul></div><div id=\\\"Visualization_of_technical_information\\\" style=\\\"font-size:114%;margin:0 4em\\\"><a href=\\\"/wiki/Visualization_(graphics)\\\" title=\\\"Visualization (graphics)\\\">Visualization</a> of technical information</div></th></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Fields</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Biological_data_visualization\\\" title=\\\"Biological data visualization\\\">Biological data visualization</a></li>\\n<li><a href=\\\"/wiki/Chemical_imaging\\\" title=\\\"Chemical imaging\\\">Chemical imaging</a></li>\\n<li><a href=\\\"/wiki/Crime_mapping\\\" title=\\\"Crime mapping\\\">Crime mapping</a></li>\\n<li><a href=\\\"/wiki/Data_visualization\\\" class=\\\"mw-redirect\\\" title=\\\"Data visualization\\\">Data visualization</a></li>\\n<li><a href=\\\"/wiki/Visualization_(graphics)\\\" title=\\\"Visualization (graphics)\\\">Educational visualization</a></li>\\n<li><a href=\\\"/wiki/Flow_visualization\\\" title=\\\"Flow visualization\\\">Flow visualization</a></li>\\n<li><a href=\\\"/wiki/Geovisualization\\\" title=\\\"Geovisualization\\\">Geovisualization</a></li>\\n<li><a href=\\\"/wiki/Information_visualization\\\" class=\\\"mw-redirect\\\" title=\\\"Information visualization\\\">Information visualization</a></li>\\n<li><a href=\\\"/wiki/Mathematical_diagram\\\" title=\\\"Mathematical diagram\\\">Mathematical visualization</a></li>\\n<li><a href=\\\"/wiki/Medical_imaging\\\" title=\\\"Medical imaging\\\">Medical imaging</a></li>\\n<li><a href=\\\"/wiki/Molecular_graphics\\\" title=\\\"Molecular graphics\\\">Molecular graphics</a></li>\\n<li><a href=\\\"/wiki/Visualization_(graphics)\\\" title=\\\"Visualization (graphics)\\\">Product visualization</a></li>\\n<li><a href=\\\"/wiki/Scientific_visualization\\\" title=\\\"Scientific visualization\\\">Scientific visualization</a></li>\\n<li><a href=\\\"/wiki/Social_visualization\\\" title=\\\"Social visualization\\\">Social visualization</a></li>\\n<li><a href=\\\"/wiki/Software_visualization\\\" title=\\\"Software visualization\\\">Software visualization</a></li>\\n<li><a href=\\\"/wiki/Technical_drawing\\\" title=\\\"Technical drawing\\\">Technical drawing</a></li>\\n<li><a href=\\\"/wiki/User_interface_design\\\" title=\\\"User interface design\\\">User interface design</a></li>\\n<li><a href=\\\"/wiki/Visual_culture\\\" title=\\\"Visual culture\\\">Visual culture</a></li>\\n<li><a href=\\\"/wiki/Volume_rendering\\\" title=\\\"Volume rendering\\\">Volume visualization</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Image <br />types</th><td class=\\\"navbox-list-with-group navbox-list navbox-even hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Chart\\\" title=\\\"Chart\\\">Chart</a></li>\\n<li><a href=\\\"/wiki/Diagram\\\" title=\\\"Diagram\\\">Diagram</a></li>\\n<li><a href=\\\"/wiki/Engineering_drawing\\\" title=\\\"Engineering drawing\\\">Engineering drawing</a></li>\\n<li><a href=\\\"/wiki/Graph_of_a_function\\\" title=\\\"Graph of a function\\\">Graph of a function</a></li>\\n<li><a href=\\\"/wiki/Ideogram\\\" title=\\\"Ideogram\\\">Ideogram</a></li>\\n<li><a href=\\\"/wiki/Map\\\" title=\\\"Map\\\">Map</a></li>\\n<li><a href=\\\"/wiki/Photograph\\\" title=\\\"Photograph\\\">Photograph</a></li>\\n<li><a href=\\\"/wiki/Pictogram\\\" title=\\\"Pictogram\\\">Pictogram</a></li>\\n<li><a href=\\\"/wiki/Plot_(graphics)\\\" title=\\\"Plot (graphics)\\\">Plot</a></li>\\n<li><a href=\\\"/wiki/Sankey_diagram\\\" title=\\\"Sankey diagram\\\">Sankey diagram</a></li>\\n<li><a href=\\\"/wiki/Schematic\\\" title=\\\"Schematic\\\">Schematic</a></li>\\n<li><a href=\\\"/wiki/Skeletal_formula\\\" title=\\\"Skeletal formula\\\">Skeletal formula</a></li>\\n<li><a href=\\\"/wiki/Statistical_graphics\\\" title=\\\"Statistical graphics\\\">Statistical graphics</a></li>\\n<li><a href=\\\"/wiki/Table_(information)\\\" title=\\\"Table (information)\\\">Table</a></li>\\n<li><a href=\\\"/wiki/Technical_drawing\\\" title=\\\"Technical drawing\\\">Technical drawings</a></li>\\n<li><a href=\\\"/wiki/Technical_illustration\\\" title=\\\"Technical illustration\\\">Technical illustration</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">People</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\"></div><table class=\\\"nowraplinks navbox-subgroup\\\" style=\\\"border-spacing:0\\\"><tbody><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Pre-19th century</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Edmond_Halley\\\" title=\\\"Edmond Halley\\\">Edmond Halley</a></li>\\n<li><a href=\\\"/wiki/Charles-Ren%C3%A9_de_Fourcroy\\\" title=\\\"Charles-René de Fourcroy\\\">Charles-René de Fourcroy</a></li>\\n<li><a href=\\\"/wiki/Joseph_Priestley\\\" title=\\\"Joseph Priestley\\\">Joseph Priestley</a></li>\\n<li><a href=\\\"/wiki/Gaspard_Monge\\\" title=\\\"Gaspard Monge\\\">Gaspard Monge</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">19th century</th><td class=\\\"navbox-list-with-group navbox-list navbox-even\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Charles_Dupin\\\" title=\\\"Charles Dupin\\\">Charles Dupin</a></li>\\n<li><a href=\\\"/wiki/Adolphe_Quetelet\\\" title=\\\"Adolphe Quetelet\\\">Adolphe Quetelet</a></li>\\n<li><a href=\\\"/wiki/Andr%C3%A9-Michel_Guerry\\\" title=\\\"André-Michel Guerry\\\">André-Michel Guerry</a></li>\\n<li><a href=\\\"/wiki/William_Playfair\\\" title=\\\"William Playfair\\\">William Playfair</a></li>\\n<li><a href=\\\"/wiki/August_Kekul%C3%A9\\\" title=\\\"August Kekulé\\\">August Kekulé</a></li>\\n<li><a href=\\\"/wiki/Charles_Joseph_Minard\\\" title=\\\"Charles Joseph Minard\\\">Charles Joseph Minard</a></li>\\n<li><a href=\\\"/w/index.php?title=Luigi_Perozzo&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Luigi Perozzo (page does not exist)\\\">Luigi Perozzo</a></li>\\n<li><a href=\\\"/wiki/Francis_Amasa_Walker\\\" title=\\\"Francis Amasa Walker\\\">Francis Amasa Walker</a></li>\\n<li><a href=\\\"/wiki/John_Venn\\\" title=\\\"John Venn\\\">John Venn</a></li>\\n<li><a href=\\\"/wiki/Oliver_Byrne_(mathematician)\\\" title=\\\"Oliver Byrne (mathematician)\\\">Oliver Byrne</a></li>\\n<li><a href=\\\"/wiki/Matthew_Henry_Phineas_Riall_Sankey\\\" title=\\\"Matthew Henry Phineas Riall Sankey\\\">Matthew Sankey</a></li>\\n<li><a href=\\\"/wiki/Charles_Booth_(social_reformer)\\\" title=\\\"Charles Booth (social reformer)\\\">Charles Booth</a></li>\\n<li><a href=\\\"/w/index.php?title=Georg_von_Mayr&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Georg von Mayr (page does not exist)\\\">Georg von Mayr</a></li>\\n<li><a href=\\\"/wiki/John_Snow\\\" title=\\\"John Snow\\\">John Snow</a></li>\\n<li><a href=\\\"/wiki/Florence_Nightingale\\\" title=\\\"Florence Nightingale\\\">Florence Nightingale</a></li>\\n<li><a href=\\\"/wiki/Karl_Wilhelm_Pohlke\\\" title=\\\"Karl Wilhelm Pohlke\\\">Karl Wilhelm Pohlke</a></li>\\n<li><a href=\\\"/wiki/Toussaint_Loua\\\" title=\\\"Toussaint Loua\\\">Toussaint Loua</a></li>\\n<li><a href=\\\"/wiki/Francis_Galton\\\" title=\\\"Francis Galton\\\">Francis Galton</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Early 20th century</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Edward_Walter_Maunder\\\" title=\\\"Edward Walter Maunder\\\">Edward Walter Maunder</a></li>\\n<li><a href=\\\"/wiki/Otto_Neurath\\\" title=\\\"Otto Neurath\\\">Otto Neurath</a></li>\\n<li><a href=\\\"/wiki/W._E._B._Du_Bois\\\" title=\\\"W. E. B. Du Bois\\\">W. E. B. Du Bois</a></li>\\n<li><a href=\\\"/wiki/Henry_Gantt\\\" title=\\\"Henry Gantt\\\">Henry Gantt</a></li>\\n<li><a href=\\\"/wiki/Arthur_Lyon_Bowley\\\" title=\\\"Arthur Lyon Bowley\\\">Arthur Lyon Bowley</a></li>\\n<li><a href=\\\"/wiki/Howard_G._Funkhouser\\\" title=\\\"Howard G. Funkhouser\\\">Howard G. Funkhouser</a></li>\\n<li><a href=\\\"/wiki/John_B._Peddle\\\" title=\\\"John B. Peddle\\\">John B. Peddle</a></li>\\n<li><a href=\\\"/wiki/Ejnar_Hertzsprung\\\" title=\\\"Ejnar Hertzsprung\\\">Ejnar Hertzsprung</a></li>\\n<li><a href=\\\"/wiki/Henry_Norris_Russell\\\" title=\\\"Henry Norris Russell\\\">Henry Norris Russell</a></li>\\n<li><a href=\\\"/wiki/Max_O._Lorenz\\\" title=\\\"Max O. Lorenz\\\">Max O. Lorenz</a></li>\\n<li><a href=\\\"/wiki/Fritz_Kahn\\\" title=\\\"Fritz Kahn\\\">Fritz Kahn</a></li>\\n<li><a href=\\\"/wiki/Harry_Beck\\\" title=\\\"Harry Beck\\\">Harry Beck</a></li>\\n<li><a href=\\\"/wiki/Erwin_Raisz\\\" title=\\\"Erwin Raisz\\\">Erwin Raisz</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Mid 20th century</th><td class=\\\"navbox-list-with-group navbox-list navbox-even\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Jacques_Bertin\\\" title=\\\"Jacques Bertin\\\">Jacques Bertin</a></li>\\n<li><a href=\\\"/wiki/Rudolf_Modley\\\" title=\\\"Rudolf Modley\\\">Rudolf Modley</a></li>\\n<li><a href=\\\"/wiki/Arthur_H._Robinson\\\" title=\\\"Arthur H. Robinson\\\">Arthur H. Robinson</a></li>\\n<li><a href=\\\"/wiki/John_Tukey\\\" title=\\\"John Tukey\\\">John Tukey</a></li>\\n<li><a href=\\\"/wiki/Mary_Eleanor_Spear\\\" title=\\\"Mary Eleanor Spear\\\">Mary Eleanor Spear</a></li>\\n<li><a href=\\\"/wiki/Edgar_Anderson\\\" title=\\\"Edgar Anderson\\\">Edgar Anderson</a></li>\\n<li><a href=\\\"/wiki/Howard_T._Fisher\\\" title=\\\"Howard T. Fisher\\\">Howard T. Fisher</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Late 20th century</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Borden_Dent\\\" title=\\\"Borden Dent\\\">Borden Dent</a></li>\\n<li><a href=\\\"/wiki/Nigel_Holmes\\\" title=\\\"Nigel Holmes\\\">Nigel Holmes</a></li>\\n<li><a href=\\\"/wiki/William_S._Cleveland\\\" title=\\\"William S. Cleveland\\\">William S. Cleveland</a></li>\\n<li><a href=\\\"/wiki/George_G._Robertson\\\" title=\\\"George G. Robertson\\\">George G. Robertson</a></li>\\n<li><a href=\\\"/wiki/Bruce_H._McCormick\\\" title=\\\"Bruce H. McCormick\\\">Bruce H. McCormick</a></li>\\n<li><a href=\\\"/wiki/Catherine_Plaisant\\\" title=\\\"Catherine Plaisant\\\">Catherine Plaisant</a></li>\\n<li><a href=\\\"/wiki/Stuart_Card\\\" title=\\\"Stuart Card\\\">Stuart Card</a></li>\\n<li><a href=\\\"/wiki/Pat_Hanrahan\\\" title=\\\"Pat Hanrahan\\\">Pat Hanrahan</a></li>\\n<li><a href=\\\"/wiki/Edward_Tufte\\\" title=\\\"Edward Tufte\\\">Edward Tufte</a></li>\\n<li><a href=\\\"/wiki/Ben_Shneiderman\\\" title=\\\"Ben Shneiderman\\\">Ben Shneiderman</a></li>\\n<li><a href=\\\"/wiki/Michael_Friendly\\\" title=\\\"Michael Friendly\\\">Michael Friendly</a></li>\\n<li><a href=\\\"/wiki/Howard_Wainer\\\" title=\\\"Howard Wainer\\\">Howard Wainer</a></li>\\n<li><a href=\\\"/wiki/Clifford_A._Pickover\\\" title=\\\"Clifford A. Pickover\\\">Clifford A. Pickover</a></li>\\n<li><a href=\\\"/wiki/Lawrence_J._Rosenblum\\\" title=\\\"Lawrence J. Rosenblum\\\">Lawrence J. Rosenblum</a></li>\\n<li><a href=\\\"/wiki/Thomas_A._DeFanti\\\" title=\\\"Thomas A. DeFanti\\\">Thomas A. DeFanti</a></li>\\n<li><a href=\\\"/wiki/George_Furnas\\\" title=\\\"George Furnas\\\">George Furnas</a></li>\\n<li><a href=\\\"/wiki/Sheelagh_Carpendale\\\" title=\\\"Sheelagh Carpendale\\\">Sheelagh Carpendale</a></li>\\n<li><a href=\\\"/wiki/Cynthia_Brewer\\\" title=\\\"Cynthia Brewer\\\">Cynthia Brewer</a></li>\\n<li><a href=\\\"/wiki/Miriah_Meyer\\\" title=\\\"Miriah Meyer\\\">Miriah Meyer</a></li>\\n<li><a href=\\\"/wiki/Jock_D._Mackinlay\\\" title=\\\"Jock D. Mackinlay\\\">Jock D. Mackinlay</a></li>\\n<li><a href=\\\"/wiki/Alan_MacEachren\\\" title=\\\"Alan MacEachren\\\">Alan MacEachren</a></li>\\n<li><a href=\\\"/wiki/David_Goodsell\\\" title=\\\"David Goodsell\\\">David Goodsell</a></li>\\n<li><a href=\\\"/wiki/Kwan-Liu_Ma\\\" title=\\\"Kwan-Liu Ma\\\">Kwan-Liu Ma</a></li>\\n<li><a href=\\\"/wiki/Michael_Maltz\\\" class=\\\"mw-redirect\\\" title=\\\"Michael Maltz\\\">Michael Maltz</a></li>\\n<li><a href=\\\"/wiki/Leland_Wilkinson\\\" title=\\\"Leland Wilkinson\\\">Leland Wilkinson</a></li>\\n<li><a href=\\\"/wiki/Alfred_Inselberg\\\" title=\\\"Alfred Inselberg\\\">Alfred Inselberg</a></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Early 21st century</th><td class=\\\"navbox-list-with-group navbox-list navbox-even\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Ben_Fry\\\" title=\\\"Ben Fry\\\">Ben Fry</a></li>\\n<li><a href=\\\"/wiki/Hans_Rosling\\\" title=\\\"Hans Rosling\\\">Hans Rosling</a></li>\\n<li><a href=\\\"/wiki/Christopher_R._Johnson\\\" title=\\\"Christopher R. Johnson\\\">Christopher R. Johnson</a></li>\\n<li><a href=\\\"/wiki/David_McCandless\\\" title=\\\"David McCandless\\\">David McCandless</a></li>\\n<li><a href=\\\"/wiki/Mauro_Martino\\\" title=\\\"Mauro Martino\\\">Mauro Martino</a></li>\\n<li><a href=\\\"/wiki/John_Maeda\\\" title=\\\"John Maeda\\\">John Maeda</a></li>\\n<li><a href=\\\"/wiki/Tamara_Munzner\\\" title=\\\"Tamara Munzner\\\">Tamara Munzner</a></li>\\n<li><a href=\\\"/wiki/Jeffrey_Heer\\\" title=\\\"Jeffrey Heer\\\">Jeffrey Heer</a></li>\\n<li><a href=\\\"/wiki/Gordon_Kindlmann\\\" title=\\\"Gordon Kindlmann\\\">Gordon Kindlmann</a></li>\\n<li><a href=\\\"/wiki/Hanspeter_Pfister\\\" title=\\\"Hanspeter Pfister\\\">Hanspeter Pfister</a></li>\\n<li><a href=\\\"/wiki/Manuel_Lima\\\" title=\\\"Manuel Lima\\\">Manuel Lima</a></li>\\n<li><a href=\\\"/wiki/Aaron_Koblin\\\" title=\\\"Aaron Koblin\\\">Aaron Koblin</a></li>\\n<li><a href=\\\"/w/index.php?title=Martin_Krzywinski&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Martin Krzywinski (page does not exist)\\\">Martin Krzywinski</a></li>\\n<li><a href=\\\"/wiki/Bang_Wong\\\" title=\\\"Bang Wong\\\">Bang Wong</a></li>\\n<li><a href=\\\"/wiki/Jessica_Hullman\\\" title=\\\"Jessica Hullman\\\">Jessica Hullman</a></li>\\n<li><a class=\\\"mw-selflink selflink\\\">Hadley Wickham</a></li>\\n<li><a href=\\\"/w/index.php?title=Polo_Chau&amp;action=edit&amp;redlink=1\\\" class=\\\"new\\\" title=\\\"Polo Chau (page does not exist)\\\">Polo Chau</a></li>\\n<li><a href=\\\"/wiki/Fernanda_Vi%C3%A9gas\\\" title=\\\"Fernanda Viégas\\\">Fernanda Viégas</a></li>\\n<li><a href=\\\"/wiki/Martin_M._Wattenberg\\\" title=\\\"Martin M. Wattenberg\\\">Martin Wattenberg</a></li>\\n<li><a href=\\\"/wiki/Claudio_Silva_(computer_scientist)\\\" title=\\\"Claudio Silva (computer scientist)\\\">Claudio Silva</a></li>\\n<li><a href=\\\"/wiki/Ade_Olufeko\\\" class=\\\"mw-redirect\\\" title=\\\"Ade Olufeko\\\">Ade Olufeko</a></li>\\n<li><a href=\\\"/wiki/Moritz_Stefaner\\\" title=\\\"Moritz Stefaner\\\">Moritz Stefaner</a></li></ul>\\n</div></td></tr></tbody></table><div></div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Related <br />topics</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd hlist\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><a href=\\\"/wiki/Cartography\\\" title=\\\"Cartography\\\">Cartography</a></li>\\n<li><a href=\\\"/wiki/Chartjunk\\\" title=\\\"Chartjunk\\\">Chartjunk</a></li>\\n<li><a href=\\\"/wiki/Color_coding_in_data_visualization\\\" title=\\\"Color coding in data visualization\\\">Color coding</a></li>\\n<li><a href=\\\"/wiki/Computer_graphics\\\" title=\\\"Computer graphics\\\">Computer graphics</a>\\n<ul><li><a href=\\\"/wiki/Computer_graphics_(computer_science)\\\" title=\\\"Computer graphics (computer science)\\\">in computer science</a></li></ul></li>\\n<li><a href=\\\"/wiki/CPK_coloring\\\" title=\\\"CPK coloring\\\">CPK coloring</a></li>\\n<li><a href=\\\"/wiki/Graph_drawing\\\" title=\\\"Graph drawing\\\">Graph drawing</a></li>\\n<li><a href=\\\"/wiki/Graphic_design\\\" title=\\\"Graphic design\\\">Graphic design</a></li>\\n<li><a href=\\\"/wiki/Graphic_organizer\\\" title=\\\"Graphic organizer\\\">Graphic organizer</a></li>\\n<li><a href=\\\"/wiki/Imaging_science\\\" class=\\\"mw-redirect\\\" title=\\\"Imaging science\\\">Imaging science</a></li>\\n<li><a href=\\\"/wiki/Infographic\\\" title=\\\"Infographic\\\">Information graphics</a></li>\\n<li><a href=\\\"/wiki/Information_science\\\" title=\\\"Information science\\\">Information science</a></li>\\n<li><a href=\\\"/wiki/Misleading_graph\\\" title=\\\"Misleading graph\\\">Misleading graph</a></li>\\n<li><a href=\\\"/wiki/Neuroimaging\\\" title=\\\"Neuroimaging\\\">Neuroimaging</a></li>\\n<li><a href=\\\"/wiki/Patent_drawing\\\" title=\\\"Patent drawing\\\">Patent drawing</a></li>\\n<li><a href=\\\"/wiki/Scientific_modelling\\\" title=\\\"Scientific modelling\\\">Scientific modelling</a></li>\\n<li><a href=\\\"/wiki/Spatial_analysis\\\" title=\\\"Spatial analysis\\\">Spatial analysis</a></li>\\n<li><a href=\\\"/wiki/Visual_analytics\\\" title=\\\"Visual analytics\\\">Visual analytics</a></li>\\n<li><a href=\\\"/wiki/Visual_perception\\\" title=\\\"Visual perception\\\">Visual perception</a></li>\\n<li><a href=\\\"/wiki/Volume_cartography\\\" title=\\\"Volume cartography\\\">Volume cartography</a></li>\\n<li><a href=\\\"/wiki/Volume_rendering\\\" title=\\\"Volume rendering\\\">Volume rendering</a></li>\\n<li><a href=\\\"/wiki/Information_art\\\" title=\\\"Information art\\\">Information art</a></li></ul>\\n</div></td></tr></tbody></table></div>\\n<div class=\\\"navbox-styles\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1129693374\\\"><link rel=\\\"mw-deduplicated-inline-style\\\" href=\\\"mw-data:TemplateStyles:r1061467846\\\"></div><div role=\\\"navigation\\\" class=\\\"navbox authority-control\\\" aria-labelledby=\\\"Authority_control_databases_frameless&amp;#124;text-top&amp;#124;10px&amp;#124;alt=Edit_this_at_Wikidata&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q16251925#identifiers&amp;#124;class=noprint&amp;#124;Edit_this_at_Wikidata\\\" style=\\\"padding:3px\\\"><table class=\\\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\\\" style=\\\"border-spacing:0;background:transparent;color:inherit\\\"><tbody><tr><th scope=\\\"col\\\" class=\\\"navbox-title\\\" colspan=\\\"2\\\"><div id=\\\"Authority_control_databases_frameless&amp;#124;text-top&amp;#124;10px&amp;#124;alt=Edit_this_at_Wikidata&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q16251925#identifiers&amp;#124;class=noprint&amp;#124;Edit_this_at_Wikidata\\\" style=\\\"font-size:114%;margin:0 4em\\\"><a href=\\\"/wiki/Help:Authority_control\\\" title=\\\"Help:Authority control\\\">Authority control databases</a> <span class=\\\"mw-valign-text-top noprint\\\" typeof=\\\"mw:File/Frameless\\\"><a href=\\\"https://www.wikidata.org/wiki/Q16251925#identifiers\\\" title=\\\"Edit this at Wikidata\\\"><img alt=\\\"Edit this at Wikidata\\\" src=\\\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\" decoding=\\\"async\\\" width=\\\"10\\\" height=\\\"10\\\" class=\\\"mw-file-element\\\" srcset=\\\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x\\\" data-file-width=\\\"20\\\" data-file-height=\\\"20\\\" /></a></span></div></th></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">International</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://isni.org/isni/0000000384728447\\\">ISNI</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://viaf.org/viaf/278010996\\\">VIAF</a></span></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">National</th><td class=\\\"navbox-list-with-group navbox-list navbox-even\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://authority.bibsys.no/authority/rest/authorities/html/9056659\\\">Norway</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://d-nb.info/gnd/1068547812\\\">Germany</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://olduli.nli.org.il/F/?func=find-b&amp;local_base=NLX10&amp;find_code=UID&amp;request=987007605524705171\\\">Israel</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://id.loc.gov/authorities/nb2009023756\\\">United States</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://lod.nl.go.kr/resource/KAC201803195\\\">Korea</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"http://data.bibliotheken.nl/id/thes/p33357284X\\\">Netherlands</a></span></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Academics</th><td class=\\\"navbox-list-with-group navbox-list navbox-odd\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://dblp.org/pid/60/5760\\\">DBLP</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://scholar.google.com/citations?user=YA43PbsAAAAJ\\\">Google Scholar</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://mathscinet.ams.org/mathscinet/MRAuthorID/884658\\\">MathSciNet</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.mathgenealogy.org/id.php?id=145799\\\">Mathematics Genealogy Project</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://orcid.org/0000-0003-4757-117X\\\">ORCID</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.scopus.com/authid/detail.uri?authorId=23092619300\\\">Scopus</a></span></li>\\n<li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://zbmath.org/authors/?q=ai:wickham.hadley\\\">zbMATH</a></span></li></ul>\\n</div></td></tr><tr><th scope=\\\"row\\\" class=\\\"navbox-group\\\" style=\\\"width:1%\\\">Other</th><td class=\\\"navbox-list-with-group navbox-list navbox-even\\\" style=\\\"width:100%;padding:0\\\"><div style=\\\"padding:0 0.25em\\\">\\n<ul><li><span class=\\\"uid\\\"><a rel=\\\"nofollow\\\" class=\\\"external text\\\" href=\\\"https://www.idref.fr/146674480\\\">IdRef</a></span></li></ul>\\n</div></td></tr></tbody></table></div>\\n<!-- \\nNewPP limit report\\nParsed by mw‐web.codfw.main‐7576cb65d‐hbqgk\\nCached time: 20240501123547\\nCache expiry: 2592000\\nReduced expiry: false\\nComplications: [vary‐revision‐sha1, show‐toc]\\nCPU time usage: 0.567 seconds\\nReal time usage: 0.770 seconds\\nPreprocessor visited node count: 3364/1000000\\nPost‐expand include size: 103262/2097152 bytes\\nTemplate argument size: 6202/2097152 bytes\\nHighest expansion depth: 17/100\\nExpensive parser function count: 18/500\\nUnstrip recursion depth: 1/20\\nUnstrip post‐expand size: 96720/5000000 bytes\\nLua time usage: 0.373/10.000 seconds\\nLua memory usage: 7488673/52428800 bytes\\nNumber of Wikibase entities loaded: 1/400\\n-->\\n<!--\\nTransclusion expansion time report (%,ms,calls,template)\\n100.00%  648.720      1 -total\\n 27.43%  177.964      1 Template:Infobox_scientist\\n 20.52%  133.104      1 Template:Reflist\\n 15.33%   99.437      4 Template:Cite_book\\n 12.52%   81.217      3 Template:Navbox\\n 11.84%   76.776      1 Template:R_(programming_language)\\n 11.30%   73.280      1 Template:Short_description\\n  6.51%   42.247      1 Template:Authority_control\\n  6.26%   40.585      1 Template:Birth_date_and_age\\n  5.98%   38.788      2 Template:Pagetype\\n-->\\n\\n<!-- Saved in parser cache with key enwiki:pcache:idhash:41916270-0!canonical and timestamp 20240501123547 and revision id 1214743386. Rendering was triggered because: page-view\\n -->\\n</div>Hadley Wickhamハドリー・ウィッカムHadley Wickham哈德利·威克漢姆All_articles_with_dead_external_linksArticles_with_dead_external_links_from_July_2021Articles_with_short_descriptionShort_description_matches_WikidataArticles_with_hCardsWebarchive_template_wayback_linksArticles_with_ISNI_identifiersArticles_with_VIAF_identifiersArticles_with_BIBSYS_identifiersArticles_with_GND_identifiersArticles_with_J9U_identifiersArticles_with_LCCN_identifiersArticles_with_NLK_identifiersArticles_with_NTA_identifiersArticles_with_DBLP_identifiersArticles_with_Google_Scholar_identifiersArticles_with_MATHSN_identifiersArticles_with_MGP_identifiersArticles_with_ORCID_identifiersArticles_with_Scopus_identifiersArticles_with_ZBMATH_identifiersArticles_with_SUDOC_identifiers1979_birthsLiving_peopleNew_Zealand_computer_scientistsNew_Zealand_statisticiansNew_Zealand_expatriates_in_the_United_StatesData_scientistsFellows_of_the_American_Statistical_AssociationR_(programming_language)_peopleCategory:Articles with dead external links from July 2021Category:Articles with BIBSYS identifiersCategory:Articles with DBLP identifiersCategory:Articles with GND identifiersCategory:Articles with Google Scholar identifiersCategory:Articles with ISNI identifiersCategory:Articles with J9U identifiersCategory:Articles with LCCN identifiersCategory:Articles with MATHSN identifiersCategory:Articles with MGP identifiersCategory:Articles with NLK identifiersCategory:Articles with NTA identifiersCategory:Articles with ORCID identifiersCategory:Articles with Scopus identifiersCategory:Articles with SUDOC identifiersCategory:Articles with VIAF identifiersCategory:Articles with ZBMATH identifiersTemplate:R (programming language)Template:VisualizationAaron KoblinAde OlufekoAdolphe QueteletAlan MacEachrenAlfred InselbergAmerican Statistical AssociationAndré-Michel GuerryArthur H. RobinsonArthur Lyon BowleyAugust KekuléBang WongBen FryBen ShneidermanBibliometrixBio7Biological data visualizationBorden DentBrian D. RipleyBruce H. McCormickCOPSS Presidents' AwardCPK coloringCartographyCatherine PlaisantCharles-René de FourcroyCharles Booth (social reformer)Charles DupinCharles Joseph MinardChartChartjunkChemical imagingChief scientific officerChristopher R. JohnsonClaudio Silva (computer scientist)Clifford A. PickoverColor coding in data visualizationColumn (database)Computer graphicsComputer graphics (computer science)Crime mappingCynthia BrewerData scienceData visualisationData visualizationDavid GoodsellDavid McCandlessDi CookDiagramDianne Cook (statistician)Dirk EddelbuettelDistributed RDoctoral advisorDoi (identifier)DplyrEasystatsEdgar AndersonEdmond HalleyEdward TufteEdward Walter MaunderEjnar HertzsprungEmacs Speaks StatisticsEngineering drawingErwin RaiszFellow of the American Statistical AssociationFernanda ViégasFlorence NightingaleFlow visualizationFrancis Amasa WalkerFrancis GaltonFritz KahnGaspard MongeGeorge FurnasGeorge G. RobertsonGeovisualizationGgplot2Google ScholarGordon KindlmannGraph drawingGraph of a functionGraphic designGraphic organizerHamilton, New ZealandHans RoslingHanspeter PfisterHarry BeckHeike HofmannHenry GanttHenry Norris RussellHoward G. FunkhouserHoward T. FisherHoward WainerHuman BiologyISBN (identifier)ISSN (identifier)IdeogramImaging scienceInfographicInformation artInformation scienceInformation visualizationIowa State UniversityJacques BertinJava GUI for RJeffrey HeerJenny BryanJessica HullmanJock D. MackinlayJohn B. PeddleJohn Chambers (statistician)John MaedaJohn SnowJohn TukeyJohn VennJoseph PriestleyJulia SilgeKH CoderKarl Wilhelm PohlkeKnitrKwan-Liu MaLawrence J. RosenblumLeland WilkinsonLuke TierneyLumi (software)Manuel LimaMapMartin M. WattenbergMary Eleanor SpearMathematical diagramMathematics Genealogy ProjectMatthew Henry Phineas Riall SankeyMauro MartinoMax O. LorenzMedical imagingMichael FriendlyMichael MaltzMicrosoft R OpenMiriah MeyerMisleading graphMolecular graphicsMoritz StefanerNeuroimagingNew ZealandNigel HolmesOCLC (identifier)Oliver Byrne (mathematician)Open-source softwareOtto NeurathPat HanrahanPatent drawingPeter DalgaardPhDPhotographPictogramPlot (graphics)ProQuest (identifier)Quantitative Discourse Analysis PackageR-LadiesRExcelRGtk2RKWardRQDARStudioR (programming language)R CommanderR ConsortiumR packageR packagesRattle GUIRenjinRevolution AnalyticsRhea (pipeline)Rice UniversityRmetricsRnn (software)Robert Gentleman (statistician)Roger BivandRoss IhakaRow (database)Rudolf ModleyS2CID (identifier)Sankey diagramSchematicScientific modellingScientific visualizationSheelagh CarpendaleShiny (software)SimpleITKSkeletal formulaSocial visualizationSoftware visualizationSpatial analysisStanford UniversityStatcheckStatistical graphicsStatisticianStatisticsStuart CardSweaveTable (database)Table (information)Tamara MunznerTechnical drawingTechnical illustrationThe R JournalThesisThomas A. DeFantiThomas Lumley (statistician)Tidy dataTidyverseToussaint LouaUniversity of AucklandUser interface designVariable (mathematics)Visual analyticsVisual cultureVisual perceptionVisualization (graphics)Volume cartographyVolume renderingW. E. B. Du BoisWayback MachineWilliam PlayfairWilliam S. ClevelandYihui XiePosit SoftwarePosit, PBCLuigi PerozzoGeorg von MayrMartin KrzywinskiPolo ChauWikipedia:Link rotTemplate talk:R (programming language)Template talk:VisualizationHelp:Authority controlTemplate:Short descriptionTemplate:PagetypeTemplate:Main otherTemplate:Short description/lowercasecheckTemplate:SDcatTemplate:Infobox scientistTemplate:PlainlistTemplate:Plainlist/styles.cssTemplate:InfoboxTemplate:If emptyTemplate:ResizeTemplate:Template otherTemplate:Birth date and ageTemplate:URLTemplate:Infobox personTemplate:Br separated entriesTemplate:Pluralize from textTemplate:CountTemplate:Unbulleted listTemplate:Wikidata imageTemplate:Cite bookTemplate:Cite journalTemplate:ReflistTemplate:Reflist/styles.cssTemplate:Google scholar idTemplate:Google Scholar IDTemplate:First wordTemplate:PAGENAMEBASETemplate:EditAtWikidataTemplate:MathGenealogyTemplate:Cite thesisTemplate:ProQuestTemplate:Catalog lookup linkTemplate:Cite webTemplate:DeadTemplate:Dead linkTemplate:FixTemplate:Category handlerTemplate:Fix/categoryTemplate:WebarchiveTemplate:R (programming language)Template:NavboxTemplate:Hlist/styles.cssTemplate:VisualizationTemplate:Authority controlModule:PagetypeModule:Pagetype/configModule:ArgumentsModule:YesnoModule:Wikitext ParsingModule:Pagetype/softredirectModule:Pagetype/setindexModule:Pagetype/disambiguationModule:Disambiguation/templatesModule:Check for unknown parametersModule:StringModule:SDcatModule:Template wrapperModule:InfoboxModule:If emptyModule:Infobox/styles.cssModule:AgeModule:DateModule:WdModule:Wd/i18nModule:URLModule:InfoboxImageModule:Separated entriesModule:TableToolsModule:Detect singularModule:TextModule:ParameterCountModule:ListModule:Check for clobbered parametersModule:Citation/CS1Module:Citation/CS1/ConfigurationModule:Citation/CS1/WhitelistModule:Citation/CS1/UtilitiesModule:Citation/CS1/Date validationModule:Citation/CS1/IdentifiersModule:Citation/CS1/COinSModule:Citation/CS1/styles.cssModule:EditAtWikidataModule:Catalog lookup linkModule:UnsubstModule:Category handlerModule:Category handler/dataModule:Category handler/configModule:Category handler/sharedModule:Category handler/blacklistModule:Namespace detect/dataModule:Namespace detect/configModule:WebarchiveModule:Webarchive/dataModule:NavboxModule:NavbarModule:Navbar/configurationModule:Navbox/configurationModule:Navbox/styles.cssModule:Navbar/styles.cssModule:Authority controlModule:Authority control/configModule:Authority control/auxiliaryHadley WickhamOOjs_UI_icon_edit-ltr-progressive.svgHadley-wickham2016-02-04.jpgR_logo.svghttps://www.wikidata.org/wiki/Q16251925#P1960https://www.wikidata.org/wiki/Q16251925#P549https://scholar.google.com/citations?user=YA43PbsAAAAJhttps://mathgenealogy.org/id.php?id=145799https://doi.org/10.1002%2Fwics.147https://www.worldcat.org/issn/1939-5108https://api.semanticscholar.org/CorpusID:247702774https://lib.dr.iastate.edu/rtd/15639/https://doi.org/10.31274%2Frtd-180813-16852https://www.worldcat.org/oclc/247410260https://search.proquest.com/docview/194000416https://www.rstudio.com/authors/hadley-wickham/https://www.stat.auckland.ac.nz/people/hwic004https://profiles.stanford.edu/hadley-wickhamhttp://www.rstudio.com/about/http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/http://blog.revolutionanalytics.com/2016/09/tidyverse.htmlhttps://doi.org/10.18637%2Fjss.v059.i10https://web.archive.org/web/20170731231612/http://stat-computing.org/awards/jmc/winners.htmlhttp://stat-computing.org/awards/jmc/winners.htmlhttps://web.archive.org/web/20160304081541/http://www.amstat.org/newsroom/pressreleases/2015-ASANames62NewFellows.pdfhttps://www.amstat.org/newsroom/pressreleases/2015-ASANames62NewFellows.pdfhttps://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&objectid=12254723http://hadley.nz/https://www.wikidata.org/wiki/Q16251925#identifiershttp://dx.doi.org/10.31274/rtd-180813-16852http://r4ds.had.co.nz/https://www.worldcat.org/oclc/968213225https://doi.org/10.18637%2Fjss.v040.i01https://doi.org/10.1198%2Fjcgs.2009.07098https://api.semanticscholar.org/CorpusID:58971746https://doi.org/10.32614%2FRJ-2010-012https://doi.org/10.18637%2Fjss.v021.i12https://twitter.com/hadleywickhamhttps://github.com/hadleyhttp://datascience.la/a-conversation-with-hadley-wickham-the-user-2014-interview/https://web.archive.org/web/20230201000144/https://datascience.la/a-conversation-with-hadley-wickham-the-user-2014-interview/http://statr.me/2013/09/a-conversation-with-hadley-wickham/https://peadarcoyle.wordpress.com/2015/08/02/interview-with-a-data-scientist-hadley-wickham/http://strataconf.com/strata2014/public/schedule/speaker/131906https://web.archive.org/web/20140223174908/http://strataconf.com/strata2014/public/schedule/speaker/131906https://www.youtube.com/watch?v=LOXe6Eu59Ashttps://www.youtube.com/watch?v=1POb5fx_m3Ihttps://isni.org/isni/0000000384728447https://viaf.org/viaf/278010996https://authority.bibsys.no/authority/rest/authorities/html/9056659https://d-nb.info/gnd/1068547812http://olduli.nli.org.il/F/?func=find-b&local_base=NLX10&find_code=UID&request=987007605524705171https://id.loc.gov/authorities/nb2009023756https://lod.nl.go.kr/resource/KAC201803195http://data.bibliotheken.nl/id/thes/p33357284Xhttps://dblp.org/pid/60/5760https://mathscinet.ams.org/mathscinet/MRAuthorID/884658https://www.mathgenealogy.org/id.php?id=145799https://orcid.org/0000-0003-4757-117Xhttps://www.scopus.com/authid/detail.uri?authorId=23092619300https://zbmath.org/authors/?q=ai:wickham.hadleyhttps://www.idref.fr/146674480New Zealand statistician1Wickham, HadleyHadley-wickham2016-02-04.jpgQ16251925\"\n```\n\n\n:::\n:::\n\n\nIn this exercise, you'll read this text as HTML, then extract the relevant nodes to get the infobox and page title.\n\n**Steps**\n\n1. Code from the previous exercise has already been run, so you have `resp_xml` available in your workspace.\n\n    * Use `read_html()` to read the contents of the XML response (`xml_text(resp_xml)`) as HTML.\n    * Use `html_node()` to extract the infobox element (having the class `infobox`) from `page_html` with a CSS selector.\n    * Use `html_node()` to extract the page title element (having the class `fn`) from `infobox_element` with a CSS selector.\n    * Extract the title text from `page_name` with `html_text()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load rvest\nlibrary(rvest)\n\n# Read page contents as HTML\npage_html <- read_html(xml_text(resp_xml))\n\n# Extract infobox element\ninfobox_element <- html_node(x = page_html, css =\".infobox\")\n\n# Extract page name element from infobox\npage_name <- html_node(x = infobox_element, css = \".fn\")\n\n# Extract page name as text\npage_title <- html_text(page_name)\n```\n:::\n\n\nFantastic! You have the info you need you just need to return it in a nice format.\n\n## Normalising information\n\nNow it's time to put together the information in a nice format.  You've already seen you can use `html_table()` to parse the infobox into a data frame. But one piece of important information is missing from that table: who the information is about!  \n\nIn this exercise, you'll parse the infobox in a data frame, and add a row for the full name of the subject.\n\n**Steps**\n\n1. No need to repeat all the table parsing code from Chapter 4, we've already added it to your script.\n\n    * Create a new data frame where `key` is the string `\"Full name\"` and `value` is our previously stored `page_title`.\n    * Combine `name_df` with `cleaned_table` using `rbind()` and assign it to `wiki_table2`. \n    * Print `wiki_table2`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Your code from earlier exercises\nwiki_table <- html_table(infobox_element)\ncolnames(wiki_table) <- c(\"key\", \"value\")\ncleaned_table <- subset(wiki_table, !key == \"\")\n\n# Create a dataframe for full name\nname_df <- data.frame(key = \"Full name\", value = page_title)\n\n# Combine name_df with cleaned_table\nwiki_table2 <- rbind(name_df, cleaned_table)\n\n# Print wiki_table\nwiki_table2\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"key\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Full name\",\"2\":\"Hadley Wickham\"},{\"1\":\"Hadley Wickham in 2015\",\"2\":\"Hadley Wickham in 2015\"},{\"1\":\"Born\",\"2\":\"Hadley Alexander Wickham (1979-10-14) 14 October 1979 (age 44)Hamilton, New Zealand\"},{\"1\":\"Alma mater\",\"2\":\"University of Auckland (BSc, MSc)Iowa State University (PhD)\"},{\"1\":\"Known for\",\"2\":\"ggplot2[3]tidyverseR packages\"},{\"1\":\"Awards\",\"2\":\".mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}\\\\nCOPSS Presidents' Award (2019)\\\\nFellow of the American Statistical Association (2015)\"},{\"1\":\"Scientific career\",\"2\":\"Scientific career\"},{\"1\":\"Fields\",\"2\":\"Data science\\\\nVisualization\\\\nStatistics[1]\"},{\"1\":\"Institutions\",\"2\":\"Posit, PBC (former RStudio Inc.)\\\\nUniversity of Auckland\\\\nStanford University\\\\nRice University\"},{\"1\":\"Thesis\",\"2\":\"Practical tools for exploring data and models (2008)\"},{\"1\":\"Doctoral advisors\",\"2\":\"Dianne Cook\\\\nHeike Hofmann[2]\"},{\"1\":\"Website\",\"2\":\"hadley.nz\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nAwesome! But what if we wanted to do this for someone else with  a page on Wikipedia?  You'll do just that in the next exercise.\n\n## Reproducibility\n\nNow you've figured out the process for requesting and parsing the infobox for the Hadley Wickham page, it's time to turn it into a function that does the same thing for anyone.\n\nYou've already done all the hard work! In the sample script we've just copied all your code from the previous three exercises, with only one change: we've wrapped it in the function definition syntax, and chosen the name `get_infobox()` for this function.\n\nIt doesn't quite work yet, the argument `title` isn't used inside the function.  In this exercise you'll fix that, then test it out with some other personalities.\n\n**Steps**\n\n1. Fix the function, by replacing the string `\"Hadley Wickham\"` with `title`, so that the title argument of the function will be used for the query.\n2. Test `get_infobox()` with `title = \"Hadley Wickham\"`.\n3. Now, try getting the infobox for `\"Ross Ihaka\"`.\n4. Finally, try getting the infobox for `\"Grace Hopper\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(httr)\nlibrary(rvest)\nlibrary(xml2)\n\nget_infobox <- function(title){\n  base_url <- \"https://en.wikipedia.org/w/api.php\"\n  \n  # Change \"Hadley Wickham\" to title\n  query_params <- list(action = \"parse\", \n    page = title, \n    format = \"xml\")\n  \n  resp <- GET(url = base_url, query = query_params)\n  resp_xml <- content(resp)\n  \n  page_html <- read_html(xml_text(resp_xml))\n  infobox_element <- html_node(x = page_html, css =\".infobox\")\n  page_name <- html_node(x = infobox_element, css = \".fn\")\n  page_title <- html_text(page_name)\n  \n  wiki_table <- html_table(infobox_element)\n  colnames(wiki_table) <- c(\"key\", \"value\")\n  cleaned_table <- subset(wiki_table, !wiki_table$key == \"\")\n  name_df <- data.frame(key = \"Full name\", value = page_title)\n  wiki_table <- rbind(name_df, cleaned_table)\n  \n  wiki_table\n}\n\n# Test get_infobox with \"Hadley Wickham\"\nget_infobox(title = \"Hadley Wickham\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"key\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Full name\",\"2\":\"Hadley Wickham\"},{\"1\":\"Hadley Wickham in 2015\",\"2\":\"Hadley Wickham in 2015\"},{\"1\":\"Born\",\"2\":\"Hadley Alexander Wickham (1979-10-14) 14 October 1979 (age 44)Hamilton, New Zealand\"},{\"1\":\"Alma mater\",\"2\":\"University of Auckland (BSc, MSc)Iowa State University (PhD)\"},{\"1\":\"Known for\",\"2\":\"ggplot2[3]tidyverseR packages\"},{\"1\":\"Awards\",\"2\":\".mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}\\\\nCOPSS Presidents' Award (2019)\\\\nFellow of the American Statistical Association (2015)\"},{\"1\":\"Scientific career\",\"2\":\"Scientific career\"},{\"1\":\"Fields\",\"2\":\"Data science\\\\nVisualization\\\\nStatistics[1]\"},{\"1\":\"Institutions\",\"2\":\"Posit, PBC (former RStudio Inc.)\\\\nUniversity of Auckland\\\\nStanford University\\\\nRice University\"},{\"1\":\"Thesis\",\"2\":\"Practical tools for exploring data and models (2008)\"},{\"1\":\"Doctoral advisors\",\"2\":\"Dianne Cook\\\\nHeike Hofmann[2]\"},{\"1\":\"Website\",\"2\":\"hadley.nz\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Try get_infobox with \"Ross Ihaka\"\nget_infobox(title = \"Ross Ihaka\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"key\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Full name\",\"2\":\"Ross Ihaka\"},{\"1\":\"Ihaka at the 2010 New Zealand Open Source Awards\",\"2\":\"Ihaka at the 2010 New Zealand Open Source Awards\"},{\"1\":\"Born\",\"2\":\"George Ross Ihaka1954 (age 69–70)Waiuku, New Zealand\"},{\"1\":\"Alma mater\",\"2\":\"University of AucklandUniversity of California, Berkeley (PhD)\"},{\"1\":\"Known for\",\"2\":\"R programming language\"},{\"1\":\"Awards\",\"2\":\"Pickering Medal (2008)\"},{\"1\":\"Scientific career\",\"2\":\"Scientific career\"},{\"1\":\"Fields\",\"2\":\"Statistical computing\"},{\"1\":\"Institutions\",\"2\":\"University of Auckland\"},{\"1\":\"Thesis\",\"2\":\"Rūaumoko (1985)\"},{\"1\":\"Doctoral advisor\",\"2\":\"David R. Brillinger[1]\"},{\"1\":\"Website\",\"2\":\"www.stat.auckland.ac.nz/~ihaka/\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Try get_infobox with \"Grace Hopper\"\nget_infobox(title = \"Grace Hopper\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"key\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Full name\",\"2\":\"Grace Hopper\"},{\"1\":\"Photograph from 1984\",\"2\":\"Photograph from 1984\"},{\"1\":\"Born\",\"2\":\"Grace Brewster Murray(1906-12-09)December 9, 1906New York City, New York, U.S.\"},{\"1\":\"Died\",\"2\":\"January 1, 1992(1992-01-01) (aged 85)Arlington County, Virginia, U.S.\"},{\"1\":\"Resting place\",\"2\":\"Arlington National Cemetery\"},{\"1\":\"Alma mater\",\"2\":\"Vassar College (BA) Yale University (MS, PhD)\"},{\"1\":\"Spouse\",\"2\":\".mw-parser-output .marriage-line-margin2px{line-height:0;margin-bottom:-2px}.mw-parser-output .marriage-line-margin3px{line-height:0;margin-bottom:-3px}.mw-parser-output .marriage-display-ws{display:inline;white-space:nowrap}Vincent Foster Hopper\\\\n   ​ ​(m. 1930; div. 1945)​\"},{\"1\":\"Awards\",\"2\":\".mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}\\\\nDefense Distinguished Service Medal\\\\nLegion of Merit\\\\nMeritorious Service Medal\\\\nAmerican Campaign Medal\\\\nWorld War II Victory Medal\\\\nNational Defense Service Medal\\\\nArmed Forces Reserve Medal with two Hourglass Devices\\\\nNaval Reserve Medal\\\\nPresidential Medal of Freedom (posthumous)\"},{\"1\":\"Military career\",\"2\":\"Military career\"},{\"1\":\"Allegiance\",\"2\":\"United States\"},{\"1\":\"Service/branch\",\"2\":\"United States Navy\"},{\"1\":\"Years of service\",\"2\":\"1943–1986\"},{\"1\":\"Rank\",\"2\":\"Rear admiral (lower half)\"},{\"1\":\"Known for\",\"2\":\"FLOW-MATIC\\\\nCOBOL\"},{\"1\":\"Scientific career\",\"2\":\"Scientific career\"},{\"1\":\"Fields\",\"2\":\"Computer scienceMathematics\"},{\"1\":\"Institutions\",\"2\":\"Vassar College\\\\nHarvard University\\\\nEckert–Mauchly Computer Corporation\\\\nRemington Rand\\\\nDigital Equipment Corporation\"},{\"1\":\"Thesis\",\"2\":\"New Types of Irreducibility Criteria (1934)\"},{\"1\":\"Doctoral advisor\",\"2\":\"Øystein Ore\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWow, great work! You put together everything you've learn to make a useful API function.  The function isn't perfect: you may notice it fails rather ungracefully if you ask for a page that doesn't exist, or a person without an infobox. \n\n## Wrap Up\n\nTheory. Coming soon ...\n\n\n**1. Wrap Up**\n\nYou're now at the end of the course - congratulations, and thank you for all your hard work!\n\n**2. Wrap up**\n\nOver the last five chapters, we've covered a lot of topics, from downloading and reading flat files to both using and designing API clients.After that, we talked about web scraping, using both Cascading Style Sheets and XPaths, to get data out of websites that don't have an API in the first place.With all of that, you should find yourself knowing how to get data out of pretty much any website in pretty much any way - something that's vital in a time when more and more useful data is being accessed via the Internet.\n\n**3. Good luck!**\n\nThat's all from us. Hopefully you've found the course both valuable and interesting, and the knowledge you've gained will serve you well.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}