{
  "hash": "7c73951eea58029becbcac8dcd4dd2ea",
  "result": {
    "markdown": "---\ntitle: \"Working with Web Data in R\"\nauthor: \"Joschka Schwarz\"\ntoc-depth: 2\n---\n\n\n\n\n**Short Description**\n\nLearn how to efficiently import data from the web into R. Discover how to work with APIs, build your own API client, and access data from Wikipedia and other sources by using R to scrape information from web pages.\n\n**Long Description**\n\nMost of the useful data in the world, from economic data to news content to geographic information, lives somewhere on the internet - and this course will teach you how to access it.\\n\\nYou'll explore how to work with APIs (computer-readable interfaces to websites), access data from Wikipedia and other sources, and build your own simple API client. For those occasions where APIs are not available, you'll find out how to use R to scrape information out of web pages. In the process you'll learn how to get data out of even the most stubborn website, and how to turn it into a format ready for further analysis. The packages you'll use and learn your way around are rvest, httr, xml2 and jsonlite, along with particular API client packages like WikipediR and pageviews.\n\n# 1. Downloading Files and Using API Clients\n\nSometimes getting data off the internet is very, very simple - it's stored in a format that R can handle and just lives on a server somewhere, or it's in a more complex format and perhaps part of an API but there's an R package designed to make using it a piece of cake. This chapter will explore how to download and read in static files, and how to use APIs when pre-existing clients are available.\n\n## Introduction: Working With Web Data in R\n\nTheory. Coming soon ...\n\n**1. Introduction: Working With Web Data in R**\n\nHi, I'm Oliver Keyes, and I'm Charlotte Wickham. Welcome to Working with Web Data in R.\n\n**2. Working with Web Data in R**\n\nIt turns out that the Internet is a great place to find datasets, and this course teaches you how to get those datasets into R in order to analyze them.You'll start with the simplest cases: simply downloading files to your machine, and using existing packages specifically designed for pulling in web data.Next you'll use the tidyverse package httr to query web application programming interfaces using the GET() and POST() commands.After that, you'll learn how to work with JSON and XML, the two most common data formats used by websites. Typically, datasets in R are rectangular, so they can be stored in a data frame or matrix. JSON and XML are both nested data structures, so you'll learn new techniques to explore them, including XPath, the XML query language.Finally, you'll learn to use Cascading Style Sheets, or CSS, to navigate HTML pages and extract their data.\n\n**3. Importing data from a URL**\n\nFirst, let's begin with the easy case. Many functions in base R that are used for importing data accept a URL, so you can directly import the data from its location on the Internet. For example, to retrieve a CSV file that you found on the Internet, you can still call read dot csv().The only change to your code is that rather than passing a path to a local file is that you pass a URL.\n\n**4. Downloading data from a URL**\n\nSince downloading data from the Internet every time that you want to use it can be very slow and tedious, especially for large datasets, R provides the function download dot file() to copy Internet-based files to your machine. This takes two arguments: the URL where the file lives, and a path to somewhere on your local file system that you want to download the data to.\n\n**5. Let's practice!**\n\nNow let's try some examples.\n\n## Downloading files and reading them into R\n\nIn this first exercise we're going to look at reading already-formatted datasets - CSV or TSV files, with which you'll no doubt be familiar! - into R from the internet. This is a lot easier than it might sound because R's file-reading functions accept not just file paths, but also URLs.\n\n**Steps**\n\nThe URLs to those files are in your R session as `csv_url` and `tsv_url`.\n\n1. Read the CSV file stored at `csv_url` into R using `read.csv()`. Assign the result to `csv_data`.\n2. Do the same for the TSV file stored at `tsv_url` using `read.delim()`. Assign the result to `tsv_data`.\n3. Examine each object using `head()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here are the URLs! As you can see they're just normal strings\ncsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv\"\ntsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv\"\n\n# Read a file in from the CSV URL and assign it to csv_data\ncsv_data <- read.csv(file = csv_url)\n\n# Read a file in from the TSV URL and assign it to tsv_data\ntsv_data <- read.delim(file = tsv_url)\n\n# Examine the objects with head()\nhead(csv_data)\nhead(tsv_data)\n```\n:::\n\n\nExcellent work! As you can see, `read.csv()` and `read.delim()` work nicely with URLs, reading the contents\\nin just as they would if the URLs were instead paths to files on your computer.\n\n## Saving raw files to disk\n\nSometimes just reading the file in from the web is enough, but often you'll want to store it locally so that you\ncan refer back to it. This also lets you avoid having to spend the start of every analysis session twiddling your thumbs while particularly large files download.\n\nHelpfully, R has <a href=\"https://www.rdocumentation.org/packages/utils/topics/download.file\">`download.file()`</a>, a function that lets you do just that: download a file to a location of your choice on your computer. It takes two arguments; `url`, indicating the URL to read from, and `destfile`, the destination to write the downloaded file to. In this case, we've pre-defined the URL - once again, it's `csv_url`.\n\n**Steps**\n\n1. Download the file at `csv_url` with `download.file()`, naming the destination file `\"feed_data.csv\"`.\n2. Read `\"feed_data.csv\"` into R with `read.csv()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download the file with download.file()\ndownload.file(url = csv_url, destfile = \"data/feed_data.csv\")\n\n# Read it in with read.csv()\ncsv_data <- read.csv(file = \"data/feed_data.csv\")\n```\n:::\n\n\nGood job. Now the file has been downloaded, you can refer back to it however many times you like without having to grab it from the internet anew each time.\n\n## Saving formatted files to disk\n\nWhether you're downloading the raw files with `download.file()` or using `read.csv()` and its sibling functions, at some point you're probably going to find the need to modify your input data, and then save the modified data to disk so you don't lose the changes.\n\nYou could use `write.table()`, but then you have to worry about accidentally writing out data in a format R can't read back in. An easy way to avoid this risk is to use <a href=\"https://www.rdocumentation.org/packages/base/topics/saveRDS\">`saveRDS()`</a> and <a href=\"https://www.rdocumentation.org/packages/base/topics/readRDS\">`readRDS()`</a>, which save R objects in an R-specific file format, with the data structure intact. That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in. `saveRDS()` takes two arguments, `object`, pointing to the R object to save and `file` pointing to where to save it to. `readRDS()` expects `file`, referring to the path to the RDS file to read in.\n\nIn this example we're going to modify the data you already read in, which is predefined as `csv_data`, and write the modified version out to a file before reading it in again.\n\n**Steps**\n\n1. Modify `csv_data` to add the column `square_weight`, containing the square of the `weight` column.\n2. Save it to disk as `\"modified_feed_data.RDS\"` with `saveRDS()`.\n3. Read it back in as `modified_feed_data` with `readRDS()`.\n4. Examine `modified_feed_data`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add a new column: square_weight\ncsv_data$square_weight <- (csv_data$weight ^ 2)\n\n# Save it to disk with saveRDS()\nsaveRDS(object = csv_data, file = \"data/modified_feed_data.RDS\")\n\n# Read it back in with readRDS()\nmodified_feed_data <- readRDS(file = \"data/modified_feed_data.RDS\")\n\n# Examine modified_feed_data\nstr(modified_feed_data)\n```\n:::\n\n\nWell done. As you can see if you examine `modified_feed_data`, the changes you made before saving it to disk are reflected in the data.\n\n## Understanding Application Programming Interfaces\n\nTheory. Coming soon ...\n\n\n**1. Understanding Application Programming Interfaces**\n\nSo far you've used techniques for downloading static files and keeping them around, but most data on the Internet doesn't take that form. It's simply not practical: if you've got complex data, or simply a lot of it, you'd have to waste time and space constantly copying data into CSV files in the background every time something changes. For websites like Wikipedia and Facebook, there are hundreds of millions of changes every day, so it doesn't really make sense as an approach.\n\n**2. Application Programming Interfaces**\n\nInstead, people often make data available behind Application Programming Interfaces, or APIs. These are programs designed to let code interact with a website, in the same way you interact with a web page, and they're used pretty much everywhere. One of the places they're used is in making data available. Instead of a website having to constantly write data to CSVs, it can just hook an API up to its database. Instead of the user having to type in a new URL each time, they can make small, consistent modifications that change the instructions the API is given for what to send back.\n\n**3. API Clients**\n\nWhat those URLs and instructions look like is something we'll discuss later in the course, because one of the great things about APIs on the web is they make it trivial to write API clients: software libraries in various languages that make it possible for programmers or data scientists to interact with the API without ever having to care about how it's structured, or do the heavy lifting of cleaning up the data.\n\n**4. Using API Clients**\n\nNine times out of ten, if you're looking to get data out of an API, R probably has a client for it, that is, an R package that contains functions to retrieve that data.One simple, useful trick is to Google \"CRAN\" and then the name of the website to see if one exists. And if there is a client, you should rely on it as much as possible, because much of the time it's a lot easier than writing your own.\n\n**5. pageviews**\n\nNow you're going see an example of this using the pageviews package, which lets you see the number of views of Wikipedia pages. All you have to do is to call the function article_pageviews(), passing the name of a Wikipedia article.You're calling an R function, just like any other, and do not need to worry about the details of the API.\n\n**6. Let's practice!**\n\nNow it's your turn to try an example.\n\n## API test\n\nIn the last video you were introduced to Application Programming Interfaces, or APIs, along with their intended purpose (as the computer equivalent of the visible web page that you and I might interact with) and their utility for data retrieval. \n\n> *Question*\n> ---\n> What are APIs for?<br>\n> <br>\n> ⬜ Making parts of a website available to people.<br>\n> ⬜ Making parts of a website available to puppies.<br>\n> ✅ Making parts of a website available to computers.<br>\n\nCorrect! Websites let you and I interpret and interface with a server, APIs allow computers to do the same.\n\n## Using API clients\n\nSo we know that APIs are server components to make it easy for your code to interact with a service and get data from it. We also know that R features many \"clients\" - packages that wrap around connections to APIs so you don't have to worry about the details.\n\nLet's look at a really simple API client - the `pageviews` package, which acts as a client to Wikipedia's API of pageview data. As with other R API clients, it's formatted as a package, and lives on CRAN - the central repository of R packages. The goal here is just to show how simple clients are to use: they look just like other R code, because they *are* just like other R code.\n\n**Steps**\n\n1. Load the package `pageviews`.\n2. Use the `article_pageviews()` function to get the pageviews for the article \"Hadley Wickham\". \n3. Examine the resulting object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load pageviews\nlibrary(pageviews)\n\n# Get the pageviews for \"Hadley Wickham\"\nhadley_pageviews <- article_pageviews(project = \"en.wikipedia\", article = \"Hadley Wickham\")\n\n# Examine the resulting object\nstr(hadley_pageviews)\n```\n:::\n\n\nNicely done. Hopefully this has demonstrated the crucial (and rather lovely) thing about API clients; they behave just like any other R package.\n\n## Access tokens and APIs\n\nTheory. Coming soon ...\n\n\n**1. Access Tokens and APIs**\n\nAs you can see from the previous exercise, API clients are super easy to use: they're just R packages. In the background, they're communicating over the Internet with the API, but that's nicely abstracted away: clients mean you don't have to care what the API is doing, or how it's structured. You can just write R code.\n\n**2. API etiquette**\n\nThe people who maintain the API, however, absolutely care what your client is doing. If many people are simultaneously trying to get a massive amount of data out of it, that can risk overwhelming the system. As a result, some APIs have access tokens: little per-user or per-session keys that identify the person making the API requests, making it easy for the operators to limit you or shut you down if you're causing problems with their service.\n\n**3. Getting access tokens**\n\nGetting an access token is normally pretty simple, and tends to require registration with the API you're trying to use. An example, and one we're going to play around with, is Wordnik, an online dictionary service that contains a ton of interesting metadata about English-language words. Their service requires an access token to use, which you gain by filling out a form that explains what you're trying to do and giving them your email address so they know who to yell at if you break their service.\n\n**4. birdnik**\n\nNow you'll try an example using the birdnik package, which wraps the Wordnik API. This gives you information about English words, such as the frequency that it is used.The API key is just a random string of letters and numbers that is unique to you. To use it, you just pass it as an argument to the birdnik functions.\n\n**5. Let's practice!**\n\nLet's try an example.\n\n## Using access tokens\n\nAs we discussed in the last video, it's common for APIs to require *access tokens* - unique keys that verify you're authorised to use a service. They're usually pretty easy to use with an API client.\n\nTo show how they work, and how easy it can be, we're going to use the R client for the Wordnik dictionary and word use service - 'birdnik' - and an API token we prepared earlier. Birdnik is fairly simple (I wrote it!) and lets you get all sorts of interesting information about word usage in published works. For example, to get the frequency of the use of the word \"chocolate\", you would write:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_frequency(api_key, \"chocolate\")\n```\n:::\n\n\nIn this exercise we're going to look at the word \"vector\" (since it's a common word in R!) using a pre-existing API key (stored as `api_key`)\n\n**Steps**\n\n1. Load the package `birdnik`.\n2. Using the pre-existing API key and `word_frequency()`, get the frequency of the word `\"vector\"` in Wordnik's database. Assign the results to `vector_frequency`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Api key is stored loacally\napi_key <- Sys.getenv(\"API_KEY\")\n\n# Load birdnik\nlibrary(birdnik)\n\n# Get the word frequency for \"vector\", using api_key to access it\nvector_frequency <- word_frequency(key = api_key, words = \"vector\")\n```\n:::\n\n\nWell done. Even with API keys, using API clients is pretty convenient!\n\n# 2. Using httr to interact with APIs directly\n\nIf an API client doesn't exist, it's up to you to communicate directly with the API. But don't worry, the package `httr` makes this really straightforward. In this chapter you'll learn how to make web requests from R, how to examine the responses you get back and some best practices for doing this in a responsible way.\n\n## GET and POST requests in theory\n\nTheory. Coming soon ...\n\n\n**1. GET and POST requests in theory**\n\nSometimes you'll run into situations where you don't have an API client available, and at that point you need to be thinking about how to handle API interactions yourself. The first part of that is understanding HTTP and HTTPS - the basics, at least - and how communication with web APIs works. With that in mind, we'll be discussing web requests as a concept, and how R implements them.\n\n**2. HTTP requests**\n\nInteractions with an API on the Internet are best understood as a conversation between two parties; the client - that's you - and the server. The first part of that conversation is the client explaining what it wants to happen, and in HTTP that's divided up into what are known as \"request methods\": different classes of requests, each meaning a different thing.\n\n**3. GET and POST**\n\nThe most common is GET requests, and they're exactly what they sound like: the client saying \"hey, get me this thing.\" If you're asking for data, you're most likely looking to make GET requests. The other common method is POST requests, which are, instead of \"get me this thing,\" \"here is this thing\" - they're you giving something to the server. That's how file uploading works, and is sometimes part of an authentication process to a server.So to use a made-up example; suppose you want to ask for a dataset, modify it locally, and then send it back to the server. You'd ask for the dataset with a GET request, tweak it, and then send it back with a POST request.\n\n**4. Other types**\n\nThere are other types of request too - for example, HEAD requests, which don't return content but do give you metadata about the content, and DELETE requests which (just as the name suggests) asks the server to remove a particular thing - but you probably won't have to work with those.Instead, let's focus on GET and POST requests in R, using the wonderful httr package, which makes interacting with servers and their APIs pretty easy.\n\n**5. Making GET requests with httr**\n\nTo make a GET request, you call the GET() function, passing the URL to get things from. This returns a response object. To get the data contents from this, there is one more step, namely to call the function content().\n\n**6. Making POST requests with httr**\n\nSimilarly, to make a POST request, you call the POST() function. In this case, you don't need to extract contents because you are sending things to somewhere else, and that's something that the recipient needs to worry about.\n\n**7. Let's practice!**\n\nNow you're going to try some examples using httpbin, which is a service for testing HTTP requests.\n\n## GET requests in practice\n\nTo start with you're going to make a GET request. As discussed in the video, this is a request that asks the server to give you a particular piece of data or content (usually specified in the URL). These make up the majority of the requests you'll make in a data science context, since most of the time you'll be getting data from servers, not giving it to them.\n\nTo do this you'll use the `httr` package, written by Hadley Wickham (of course), which makes HTTP requests extremely easy. You're going to make a very simple GET request, and then inspect the output to see what it looks like.\n\n**Steps**\n\n1. Load the `httr` package.\n2. Use the `GET()` function to make a request to `http://httpbin.org/get`, saving the result to `get_result`.\n3. Print `get_result` to inspect it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the httr package\nlibrary(httr)\n\n# Make a GET request to http://httpbin.org/get\nget_result <- GET(url = \"http://httpbin.org/get\")\n\n# Print it to inspect it\nget_result\n```\n:::\n\n\nNicely done. As you can see from inspecting the output, there are a lot of parts of a HTTP response. Most of them you don't have to worry about right now; some, like the status code and the content, we'll cover later.\n\n## POST requests in practice\n\nNext we'll look at POST requests, also made through httr, with the function (you've guessed it) <a href=\"https://www.rdocumentation.org/packages/httr/topics/POST\">`POST()`</a>. Rather than asking the server to give you something, as in GET requests, a POST request asks the server to accept something *from* you. They're commonly used for things like file upload, or authentication. As a result of their use for uploading things, `POST()` accepts not just a `url` but also a `body` argument containing whatever you want to give to the server.\n\nYou'll make a very simple POST request, just uploading a piece of text, and then inspect the output to see what it looks like.\n\n**Steps**\n\n1. Load the `httr` package.\n2. Make a POST request with the URL `http://httpbin.org/post` and the body `\"this is a test\"`, saving the result to `post_result`.\n3. Print `post_result` to inspect it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the httr package\nlibrary(httr)\n\n# Make a POST request to http://httpbin.org/post with the body \"this is a test\"\npost_result <- POST(url = \"http://httpbin.org/post\", body = \"this is a test\")\n\n# Print it to inspect it\npost_result\n```\n:::\n\n\nNicely done. The output for POST requests looks pretty similar to that for GET requests, although (in this case) the body of your message is included - `this is a test`. Again, we'll dig into certain elements of the output in just a bit.\n\n## Extracting the response\n\nMaking requests is all well and good, but it's also not why you're here. What we really want to do is get the data the server sent back, which can be done with httr's `content()` function. You pass it an object returned from a `GET` (or `POST`, or `DELETE`, or…) call, and it spits out whatever the server actually sent in an R-compatible structure.\n\nWe're going to demonstrate that now, using a slightly more complicated URL than before - in fact, using a URL from the Wikimedia pageviews system you dealt with through the `pageviews` package, which is stored as `url`. Without looking *too* much at the structure for the time being (we'll get to that later) this request asks for the number of pageviews to the English-language Wikipedia's \"Hadley Wickham\" article on 1 and 2 January 2017.\n\n**Steps**\n\n1. Make a GET request using the `url` object as the URL. Save the results as `pageview_response`.\n2. Call `content()` on `pageview_response` to retrieve the data the server has sent back. Save the data as `pageview_data`.\n3. Examine `pageview_data` with `str()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102\"\n\n# Make a GET request to url and save the results\npageview_response <- GET(url)\n\n# Call content() to retrieve the data the server sent back\npageview_data <- content(pageview_response)\n\n# Examine the results with str()\nstr(pageview_data)\n```\n:::\n\n\nGreat! As you can see, the result of extracting the content is a list, which is pretty common (but not uniform) to API responses. We'll talk through how to manipulate returned data later on.\n\n## Multiple Choice: GET and POST requests\n\nWe've now discussed multiple types of HTTP request - including GET requests, for retrieving data, and POST requests, for transmitting it - as well as how to extract the server response once a request is complete. \n\n> *Question*\n> ---\n> What sort of request would you use to send a server data?<br>\n> <br>\n> ⬜ GET request<br>\n> ✅ POST request<br>\n\n\"Correct! POST requests send data - GET requests retrieve it.\"\n\n## Graceful httr\n\nTheory. Coming soon ...\n\n\n**1. Graceful httr**\n\nYou've played around with basic httr usage: great. The next step is writing code that is \"graceful,\" by which I mean: code that can respond appropriately to errors, and code which can construct its own URLs in response to user input (and so be applied to lots of different situations).\n\n**2. Error handling**\n\nThe results of requests come back with an HTTP status code, which indicates what happened to the request.\n\n**3. Error handling**\n\nYou can see that in the body of a result, if you print it - take a look at this example.If it's got a value of 200, the request was completed; if it was a 404, the server has no idea where the thing you were asking for lives. There are a whole range of different status codes, most of which we are not going to dig into because there are a lot of them.\n\n**4. Understanding status codes**\n\nAs a general rule: if the code starts with 2 or 3, it's fine. If it starts with 4, your code is messing up. If it starts with 5, their code is messing up. You don't have to worry about much of the specifics unless you see such an error code, and in that case, Wikipedia has a convenient list of codes and their explanation.Still, you'll want to check for them, so you know if your request fell over, and httr has a function for just that, http_error(), which we'll explore shortly.\n\n**5. URL construction**\n\nThe next big thing is URL construction. Writing out the whole URL every time is all well and good, but what if you want to make different requests for different assets? What if you don't want to have to think about most of the URL, just the bit you're changing?Automatically constructing URLs - stitching together standard bits and the bits specific to your request - solves for this nicely. It means you don't have to think about the bits that never change, or type out the whole thing each time, and neither does anyone else using your code. You just put in the essential bits and off you go.\n\n**6. Directory-based URLs**\n\nAPI URLs tend to take one of two forms. The first is directory-based URLs: they use directories to represent different parameters. So if you have a URL that goes fakeurl-dot-com/api/peaches/thursday to find out the price of peaches on Thursday, fakeurl-dot-com/api/apples/tuesday would tell you (if the API's author is doing their job right) the price of apples on Tuesday. We saw an example of this with the pageviews package.These are really, really common, particularly with more modern APIs: the good news is they're also really easy to construct with the function paste(), which glues strings together.\n\n**7. Parameter-based URLs**\n\nAlternatively, you get parameter-based URLs. These work via key-value pairs in the URL: the URL would contain a base URL, followed by a question mark, then parameters written as key equals value, and separated with ampersands.With the fruit example, the URL would look contain fruit equals \"peaches\" and day equals \"thursday\".You can also use paste() to construct parameter-based URLs, but this is rather fiddly. Fortunately, the GET() function allows you to pass a named list of parameters to its query argument, and will handle all this construction for you.\n\n**8. Let's practice!**\n\nLet's try some examples.\n\n## Handling http failures\n\nAs mentioned, HTTP calls can go wrong. Handling that can be done with httr's <a href=\"https://www.rdocumentation.org/packages/httr/topics/http_error\">`http_error()`</a> function, which identifies whether a server response contains an error.\n\nIf the response does contain an error, calling `http_error()` over the response will produce `TRUE`; otherwise, `FALSE`. You can use this for really fine-grained control over results. For example, you could check whether the request contained an error, and (if so) issue a warning and re-try the request.\n\nFor now we'll try something a bit simpler - issuing a warning that something went wrong if `http_error()` returns `TRUE`, and printing the content if it doesn't.\n\n**Steps**\n\n1. Make a httr `GET()` request to the URL stored as `fake_url`, and store the result as `request_result`.\n2. If `http_error()` returns `TRUE`, use `warning()` to issue the warning \"The request failed\".\n3. If not, use `content()` (as demonstrated in previous exercises) to print the contents of the result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_url <- \"http://google.com/fakepagethatdoesnotexist\"\n\n# Make the GET request\nrequest_result <- GET(fake_url)\n\n# Check request_result\nif (http_error(request_result)){\n  warning(\"The request failed\")\n} else {\n  content(request_result)\n}\n```\n:::\n\n\nWell done! Since the url mentioned in `fake_url` doesn't exist, the code threw the warning message you mentioned. Error handling is really important for writing robust code, and it looks like you've got a good handle on it.\n\n## Constructing queries (Part I)\n\nAs briefly discussed in the previous video, the actual API query (which tells the API what you want to do) tends to be in one of the two forms. The first is directory-based, where values are separated by `/` marks within the URL. The second is parameter-based, where all the values exist at the end of the URL and take the form of `key=value`.\n\nConstructing directory-based URLs can be done via <a href=\"https://www.rdocumentation.org/packages/base/topics/paste\">`paste()`</a>, which takes an unlimited number of strings, along with a separator, as `sep`. So to construct `http://swapi.co/api/vehicles/12` we'd call:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste(\"http://swapi.co\", \"api\", \"vehicles\", \"12\", sep = \"/\")\n```\n:::\n\n\nLet's do that now! We'll cover parameter-based URLs later. In the mean time we can play with SWAPI, mentioned above, which is an API chock full of star wars data. This time, rather than a vehicle, we'll look for a person.\n\n**Steps**\n\n1. `httr` is loaded in your workspace.\n\n    * Construct a directory-based API URL to `http://swapi.co/api`, looking for person `1` in `people`.\n    * Assign the URL to `directory_url`.\n    * Use `GET` to make an API call with `directory_url`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct a directory-based API URL to `http://swapi.co/api`,\n# looking for person `1` in `people`\ndirectory_url <- paste(\"http://swapi.co/api\", \"people\", \"1\", sep = \"/\")\n\n# Make a GET call with it\nresult <- GET(directory_url)\n```\n:::\n\n\nGood work! Constructing these kinds of queries is very simple, but also extremely important. Now you know how to write automated or semi-automated code for more modern APIs.\n\n## Constructing queries (Part II)\n\nAs mentioned (albeit briefly) in the last exercise, there are also *parameter* based URLs, where all the query values exist at the end of the URL and take the form of `key=value` - they look something like `http://fakeurl.com/foo.php?country=spain&amp;food=goulash`\n\nConstructing parameter-based URLs can *also* be done with `paste()`, but the easiest way to do it is with `GET()` and `POST()` themselves, which accept a `query` argument consisting of a list of keys and values. So, to continue with the food-based examples, we could construct `fakeurl.com/api.php?fruit=peaches&amp;day=thursday` with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGET(\"fakeurl.com/api.php\", query = list(fruit = \"peaches\", day = \"thursday\"))\n```\n:::\n\n\nIn this exercise you'll construct a call to `https://httpbin.org/get?nationality=americans&amp;country=antigua`.\n\n**Steps**\n\n1. Start by contructing the `query_params` list, with a `nationality` parameter of `\"americans\"` and a `country` parameter of `\"antigua\"`.\n2. Construct a parameter-based call to `https://httpbin.org/get`, using `GET()` passing `query_params` to the `query` arugment.\n3. Print the response `parameter_response`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create list with nationality and country elements\nquery_params <- list(nationality = \"americans\", \n    country = \"antigua\")\n    \n# Make parameter-based call to httpbin, with query_params\nparameter_response <- GET(\"https://httpbin.org/get\", query = query_params)\n\n# Print parameter_response\nparameter_response\n```\n:::\n\n\nNice job!  Did you notice the parameters you passed in were listed in the `args` of the response?\n\n## Respectful API usage\n\nTheory. Coming soon ...\n\n\n**1. Respectful API Usage**\n\nOkay, so you know how to make requests to APIs, and you know how to construct requests to APIs. Now it's important to discuss how to query APIs in a way that not only works, but also avoids annoying the people who run them. A while back you covered access tokens - unique keys you use to authenticate to an API - and talked about how one use of them was to allow the API's developers to cut you off if you misbehave. In this video we're going to talk about how to behave so this doesn't happen.\n\n**2. User agents**\n\nWhen you make an API query, the person at the other end doesn't know you from Adam. You could be anyone! But if something is going wrong they might need to contact you, or at least know vaguely what software you're using. For this reason, it's possible to send what's known as a user agent - a piece of text that identifies the user - as part of any request. I usually try to make sure that a user agent contains my email address, if it's something I'm just running on my own. That way if there's a problem with my code, the API developers can contact me and take issue.Previously, you called GET() and POST() with a single argument, the URL. To include your personal details, you also need to specify the config argument. This should be the result of a call to the function user_agent(), which takes a string containing whatever details about yourself that you wish to provide.\n\n**3. Rate limiting**\n\nMany APIs have a deliberate limit on how many requests you can send in a given time period to avoid ever running into service issues. Exceed that limit, you get blocked; stay under it, you don't.The way to avoid this is, well, making sure there's an interval between your requests, which is known as rate limiting. If you're only allowed 60 requests a minute, for example, you'd intentionally write your code so that it guaranteed one second would pass between requests - at which point you don't have to worry. Doing that in R is pretty simple using the Sys dot sleep() function. This simply tells R to do nothing for a specified time period.With all of this, you should be pretty set: you'll know how to make requests, how to structure them, and how to automate them in a way that doesn't ruin some far-away developer's day.\n\n**4. Let's practice!**\n\nTime to try some examples.\n\n## Using user agents\n\nAs discussed in the video, informative user-agents are a good way of being respectful of the developers running the API you're interacting with. They make it easy for them to contact you in the event something goes wrong. I always try to include:\n\n1. My email address;\n2. A URL for the project the code is a part of, if it's got a URL.\n\nBuilding user agents is done by passing a call to `user_agent()` into the `GET()` or `POST()` request; something like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGET(\"http://url.goes.here/\", user_agent(\"somefakeemail@domain.com http://project.website\"))\n```\n:::\n\n\nIn the event you don't have a website, a short one-sentence description of what the project is about serves pretty well.\n\n**Steps**\n\n1. Make a `GET()` request to `url`.\n2. Include a user agent that has a fake email address `\"my@email.address\"` followed by the sentence `\"this is a test\"`.\n3. Assign the response to `server_response`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Do not change the url\nurl <- \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100\"\n\n# Add the email address and the test sentence inside user_agent()\nserver_response <- GET(url, user_agent(\"my@email.address this is a test\"))\n```\n:::\n\n\nGood work! From your end, the request looks exactly the same with or without a user agent, but for the server the difference can be vital.\n\n## Rate-limiting\n\nThe next stage of respectful API usage is rate-limiting: making sure you only make a certain number of requests to the server in a given time period. Your limit will vary from server to server, but the implementation is always pretty much the same and involves a call to <a href=\"https://www.rdocumentation.org/packages/base/topics/Sys.sleep\">`Sys.sleep()`</a>. This function takes one argument, a number, which represents the number of seconds to \"sleep\" (pause) the R session for. So if you call `Sys.sleep(15)`, it'll pause for 15 seconds before allowing further code to run.\n\nAs you can imagine, this is really useful for rate-limiting. If you are only allowed 4 requests a minute? No problem! Just pause for 15 seconds between each request and you're guaranteed to never exceed it. Let's demonstrate now by putting together a little loop that sends multiple requests on a 5-second time delay. We'll use `httpbin.org`\n's APIs, which allow you to test different HTTP libraries.\n\n**Steps**\n\n1. Construct a vector of 2 URLs, `http://httpbin.org/status/404` and `http://httpbin.org/status/301`.\n2. Write a for-loop that sends a `GET()` request to each one.\n3. Ensure that the for-loop uses `Sys.sleep()` to delay for 5 seconds between request.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct a vector of 2 URLs\nurls <- c(\"http://httpbin.org/status/404\",\n          \"http://httpbin.org/status/301\")\n\nfor(url in urls){\n    # Send a GET request to url\n    result <- GET(url)\n    # Delay for 5 seconds between requests\n    Sys.sleep(5)\n}\n```\n:::\n\n\nNice work. With this and the user agents we covered in the last exercise, you know enough to avoid annoying servers - and thus snarling up your code\n\n## Tying it all together\n\nUsing everything that you learned in the chapter, let's make a simple replica of one of the 'pageviews' functions - building queries, sending GET requests (with an appropriate user agent) and handling the output in a fault-tolerant way. You'll build this function up piece by piece in this exercise.\n\nTo output an error, you will use the function <a href=\"https://www.rdocumentation.org/packages/base/topics/stop\">`stop()`</a>, which takes a string as an argument, *stops* the execution of the program, and outputs the string as an error.  You can try it right now by running `stop(\"This is an error\")`.\n\n**Steps**\n\n1. First, get the function to construct the url.  \n\n    * In the call to `paste()`, add `article_title` as the second argument to construct `url`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    # Include article title\n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  ) \n  url\n}\n```\n:::\n\n\n2. Now, make the request.\n\n    * Use `GET()` to request `url` with a user agent `\"my@email.com this is a test\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  ) \n  # Get the webpage   \n  response <- GET(url, config = user_agent(\"my@email.com this is a test\")) \n  response\n}\n```\n:::\n\n\n3. Now, add an error check.\n\n    * Check the response for errors with `http_error()`, throwing an error of `\"the request failed\"` with `stop()` if there was one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  )   \n  response <- GET(url, user_agent(\"my@email.com this is a test\")) \n  # Is there an HTTP error?\n  if(http_error(response)){ \n    # Throw an R error\n    stop(\"the request failed\") \n  }\n  response\n}\n```\n:::\n\n\n4. Finally, instead of returning `response`, return the `content()` of the response.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_pageviews <- function(article_title){\n  url <- paste(\n    \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n    article_title, \n    \"daily/2015100100/2015103100\", \n    sep = \"/\"\n  )   \n  response <- GET(url, user_agent(\"my@email.com this is a test\")) \n  # Is there an HTTP error?\n  if(http_error(response)){ \n    # Throw an R error\n    stop(\"the request failed\") \n  }\n  # Return the response's content\n  content(response)\n}\n```\n:::\n\n\nCongratulations! You've finished the chapter, and should now have a good handle on how to work with httr. Our next chapter will cover digging into the results you get back from your requests.\n\n# 3. Handling JSON and XML\n\nSometimes data is a TSV or nice plaintext output. Sometimes it's XML and/or JSON. This chapter walks you through what JSON and XML are, how to convert them into R-like objects, and how to extract data from them. You'll practice by examining the revision history for a Wikipedia article retrieved from the Wikipedia API using httr, xml2 and jsonlite.\n\n## JSON\n\nTheory. Coming soon ...\n\n\n**1. JSON**\n\nIn the previous chapter you learned how to ask for data from an API.  In this chapter you'll learn about two of the most common data formats that APIs return.Let's start with JSON, a particularly common format for storing and transmitting data on the web.  Later in the chapter you'll learn about another format: XML.\n\n**2. JSON (JavaScript Object Notation)**\n\nMuch like a CSV file, a JSON data file is just a plain text file, but with special conventions for describing data structures.  Unlike a CSV, JSON can accommodate data that is more complicated than a simple table and that is what makes it so useful.All JSON files are made up of only two kinds of structures: objects and arrays. Objects are collections of named values. In JSON an object starts with a left brace and ends with a right brace. Then a named value consists of a name in quotes followed by a colon, then the value.  Name value pairs are separated by a comma.Arrays are an ordered list of values. An array begins with a left bracket and ends with a right bracket, and values are separated by a comma.In both objects and arrays a value can be a string in double quotes, a number, \"true\", \"false\", \"null\", or another object or array.   Because objects and arrays can occur anywhere a value can, complicated hierarchical structures are easily represented.\n\n**3. An example JSON data set**\n\nLet's take a look at this JSON example that describes a couple of movies.  It consists of an array with two elements: each an object.  Each object has two named values: title and year.\n\n**4. Identifying a JSON response**\n\nHow do you know if an API has given you JSON data as a response? Usually you'll already have a pretty good idea based on the API documentation - it either always returns JSON, or you have specifically requested a JSON object. However, it's always worth checking you got what you expected.First check the type as reported in the header of the response using the http_type() function.  This should take the value \"application/json\".\n\n**5. Identifying a JSON response**\n\nThen you can examine the content of the response as plain text, by using the content() function. Wrapping this in writeLines() will give you a printout of the content which you should be able to recognize as JSON.\n\n**6. Let's practice!**\n\nTime to try some examples.\n\n## Can you spot JSON?\n\nHere is some information on a couple of fictional Jasons stored in different formats.\n\nA:\n\n\n::: {.cell}\n\n```{.json .cell-code}\n<jason>\n  <person>\n    <first_name>Jason</first_name>\n    <last_name>Bourne</last_name>\n    <occupation>Spy</occupation>\n  </person>\n  <person>\n    <first_name>Jason</first_name>\n    <last_name>Voorhees</last_name>\n    <occupation>Mass murderer</occupation>\n  </person>\n</jason>\n```\n:::\n\n\nB:\n\n\n::: {.cell}\n\n```{.json .cell-code}\nfirst_name, last_name, occupation\nJason, Bourne, Spy\nJason, Voorhees, Mass murderer\n```\n:::\n\n\nC:\n\n\n::: {.cell}\n\n```{.json .cell-code}\n[{ \n   \"first_name\": \"Jason\",\n   \"last_name\": \"Bourne\",\n   \"occupation\": \"Spy\"\n },\n{\n  \"first_name\": \"Jason\",\n  \"last_name\": \"Voorhees\",\n  \"occupation\": \"Mass murderer\"\n}]\n```\n:::\n\n\n> *Question*\n> ---\n>  Which one is JSON?<br>\n> <br>\n> ⬜ A<br>\n> ⬜ B<br>\n> ✅ C<br>\n\nYou got it! This JSON represents an array of two objects, each a different character called Jason.\n\n## Parsing JSON\n\nWhile JSON is a useful format for sharing data, your first step will often be to parse it into an R object, so you can manipulate it with R.\n\nThe <a href=\"https://www.rdocumentation.org/packages/httr/topics/content\">`content()`</a> function in `httr` retrieves the content from a request.  It takes an `as` argument that specifies the type of output to return.  You've already seen that `as = \"text\"` will return the content as a character string which is useful for checking the content is as you expect.\n\nIf you don't specify `as`, the default `as = \"parsed\"` is used. In this case the type of `content()` will be guessed based on the header and `content()` will choose an appropriate parsing function.  For JSON this function is <a href=\"https://www.rdocumentation.org/packages/jsonlite/topics/fromJSON\">`fromJSON()`</a> from the `jsonlite` package.  If you know your response is JSON, you may want to use `fromJSON()` directly.\n\nTo practice, you'll retrieve some revision history from the Wikipedia API, check it is JSON, then parse it into a list two ways.\n\n**Steps**\n\n1. Get the revision history for the Wikipedia article for `\"Hadley Wickham\"`, by calling `rev_history(\"Hadley Wickham\")` (a function we have written for you), store it in `resp_json`.\n2. Check the `http_type()` of `resp_json`, to confirm the API returned a JSON object.\n3. You can't always trust a header, so check the content looks like JSON by calling `content()` on `resp_json` with an additional argument, `as = \"text\"`.\n4. Parse `resp_json` using `content()` by explicitly setting `as = \"parsed\"`.\n5. Parse the returned text (from step 3) with `fromJSON()` .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get revision history for \"Hadley Wickham\"\nrevhist_url <- \"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=json&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\"\nresp_json <- GET(revhist_url)\n\n# Check http_type() of resp_json\nhttp_type(resp_json)\n\n# Examine returned text with content()\ncontent(resp_json, as = \"text\")\n\n# Parse response with content()\ncontent(resp_json, as = \"parsed\")\n\n# Parse returned text with fromJSON()\nlibrary(jsonlite)\nfromJSON(content(resp_json, as = \"text\"))\n```\n:::\n\n\nGreat work! You may have noticed the output from `content()` is pretty long and hard to understand. Don't worry, that is just the nature of nested data, you'll learn a couple of tricks for dealing with it next. However, it will be helpful to know that this response contains 5 revisions.\n\n## Manipulating JSON\n\nTheory. Coming soon ...\n\n\n**1. Manipulating JSON**\n\nYou might have noticed a lot of output scrolling past when you parsed your JSON data. What kind of object was being returned?\n\n**2. Movies example**\n\nLet's take a look using the movies JSON from the previous video. Here's the JSON.\n\n**3. Movies example**\n\nIn R, I'll just store that in a string called movies_json.\n\n**4. Movies example**\n\nNow, look what happens when we parse that with fromJSON().  I've set the simplifyVector argument to FALSE to emulate what happens when you use the content() function without any arguments.What we get back is a list!  Lists are the natural R object for storing JSON data because they can store hierarchical data just like JSON.  fromJSON() will always return a list.  It converts any JSON objects (remember those are the ones with curly braces) to named lists, and any JSON arrays (the ones with square brackets) to unnamed lists.\n\n**5. Simplifying the output (I)**\n\nfromJSON() also provides extra simplification through some of its arguments.  For example, if simplifyVector is true, any arrays of just numbers or strings will be converted to vectors.\n\n**6. Simplifying the output (II)**\n\nTake a look at what happens when you set simplifyDataFrame to TRUE.fromJSON() will convert any arrays of objects to data frames.\n\n**7. Extracting data from JSON (I)**\n\nNow imagine you want to extract all the titles of the movies. One option is to rely on fromJSON() to simplify to a dataframe and pull out the relevant column.\n\n**8. Extracting data from JSON (II)**\n\nThe other is to use the list form and iterate over each element.  Over the next couple of exercises you'll explore doing this two ways: first, with a dedicated package rlist, and second, using functions from base R and the tidyverse.\n\n**9. Let's practice!**\n\n\n\n## Manipulating parsed JSON\n\nAs you saw in the video, the output from parsing JSON is a list. One way to extract relevant data from that list is to use a package specifically designed for manipulating lists, <a href=\"https://www.rdocumentation.org/packages/rlist\">`rlist`</a>.\n\n`rlist` provides two particularly useful functions for selecting and combining elements from a list: <a href=\"https://www.rdocumentation.org/packages/rlist/topics/list.select\">`list.select()`</a> and <a href=\"https://www.rdocumentation.org/packages/rlist/topics/list.stack\">`list.stack()`</a>. `list.select()` extracts sub-elements by name from each element in a list. For example using the parsed movies data from the video (`movies_list`), we might ask for the `title` and `year` elements from each element:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.select(movies_list, title, year)\n```\n:::\n\n\nThe result is still a list, that is where `list.stack()` comes in. It will stack the elements of a list into a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.stack(\n    list.select(movies_list, title, year)\n)\n```\n:::\n\n\nIn this exercise you'll use these `rlist` functions to create a data frame with the user and timestamp for each revision.\n\n**Steps**\n\n1. First, you'll need to figure out where the revisions are. Examine the output from the `str()` call. *Can you see where the list of 5 revisions is?* \n2. Store the revisions in `revs`.\n3. Use `list.select()` to pull out the `user` and `timestamp` elements from each revision, store in `user_time`.\n4. Print `user_time` to verify it's a list with one element for each revision.\n5. Use `list.stack()` to stack the lists into a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load rlist\nlibrary(rlist)\n\n# Examine output of this code\nstr(content(resp_json), max.level = 4)\n\n# Store revision list\nrevs <- content(resp_json)$query$pages$`41916270`$revisions\n\n# Extract the user element\nuser_time <- list.select(revs, user, timestamp)\n\n# Print user_time\nuser_time \n\n# Stack to turn into a data frame\nlist.stack(user_time)\n```\n:::\n\n\nPerfect! [`rlist`](https://www.rdocumentation.org/packages/rlist) is designed to make working with lists easy, so if find you are working with JSON data a lot, you should explore more of its functionality.\n\n## Reformatting JSON\n\nOf course you don't have to use `rlist`. You can achieve the same thing by using functions from base R or the tidyverse.  In this exercise you'll repeat the task of extracting the username and timestamp using the <a href=\"https://www.rdocumentation.org/packages/dplyr\">`dplyr`</a> package which is part of the tidyverse.\n\nConceptually, you'll take the list of revisions, stack them into a data frame, then pull out the relevant columns.\n\n`dplyr`'s <a href=\"https://www.rdocumentation.org/packages/dplyr/topics/bind_rows\">`bind_rows()`</a> function takes a list and turns it into a data frame. Then you can use <a href=\"https://www.rdocumentation.org/packages/dplyr/topics/select\">`select()`</a> to extract the relevant columns. And of course if we can make use of the `%>%` (pipe) operator to chain them all together.\n\nTry it!\n\n**Steps**\n\n1. Pipe the list of revisions into  `bind_rows()`.\n2. Use `select()` to extract the `user` and `timestamp` columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load dplyr\nlibrary(dplyr)\n\n# Pull out revision list\nrevs <- content(resp_json)$query$pages$`41916270`$revisions\n\n# Extract user and timestamp\nrevs %>%\n  bind_rows() %>%\n  select(user, timestamp)\n```\n:::\n\n\nYou nailed it!\n\n## XML structure\n\nTheory. Coming soon ...\n\n\n**1. XML structure**\n\nXML is another popular format for transmitting data over APIs. Like JSON, it's a plain text format, but it has quite a different structure.\n\n**2. Movies in XML**\n\nLet's take a look at the same movies data you saw in JSON, in XML.The structure of an XML file can be divided into markup and content.  Markup describes the structure of the data, whereas content is the data itself.  Most markup is in the form of what are know as tags.  Tags begin with a less than sign and end with a greater than sign. In this data you can see movies, movie, title and year tags. They generally occur in pairs with a start tag containing just the tag name, and the end tag having a forward slash between the less than sign and the tag name.\n\n**3. Tags can have attributes**\n\nBeyond the tag name, a tag can also contain attributes in the form of name value pairs.  An alternative way of providing the same data might be to use attributes instead of tags. Here instead of a year tag, the year is provided in a year attribute of the title tag.There's no standard on how data should be stored in XML, but usually attributes are reserved for metadata, that is data about the data, and content is used for data.\n\n**4. The hierarchy of XML elements**\n\nSo, in this case since the year is data about a movie, we should probably stick to the original format.\n\n**5. The hierarchy of XML elements**\n\nThe key to navigating XML is understanding its hierarchical structure.  An XML element is everything from and including a start tag to and including the end tag. The content of an element is everything between the tags, including other elements.For example, the first movie element contains two more elements:\n\n**6. The hierarchy of XML elements**\n\na title element and a year element.\n\n**7. The hierarchy of XML elements**\n\nThe contents of the title element is simply the text \"A New Hope\".\n\n**8. Understanding XML as a tree**\n\nYou can think of an XML document as a tree where the nodes are elements or text. You can describe the relationships between the nodes just like you would a family tree.\n\n**9. Understanding XML as a tree**\n\nYou would say that this title element is a child of the first movie element because it is contained inside the first movie element.  Equivalently, you could say the first movie element is the parent of this title element.\n\n**10. Understanding XML as a tree**\n\nThese title and year elements are siblings, because they share the same parent: movie.\n\n**11. Understanding XML as a tree**\n\nJust like the two movie elements are siblings because they share the same parent: movies.\n\n**12. Let's practice!**\n\nOver the next few exercises you'll use the xml2 package to parse XML, and examine its structure with the xml_structure() function.\n\n## Do you understand XML structure?\n\nTake a look at this XML document:\n\n\n::: {.cell}\n\n```{.xml .cell-code}\n<jason>\n  <person type = \"fictional\">\n    <first_name>\n      Jason\n    </first_name>\n    <last_name>\n      Bourne\n    </last_name>\n    <occupation>\n      Spy\n    </occupation>\n  </person>\n</jason>\n```\n:::\n\n\n> *Question*\n> ---\n> Which of the following is **false**?<br>\n> <br>\n> ⬜ The contents of the `first_name` element is the text `Jason`.<br>\n> ⬜ The `type` attribute of the `person` element is `\"fictional\"`.<br>\n> ⬜ The `person` element is a child of the `jason` element.<br>\n> ✅ The `last_name` element is a child of the `first_name` element.<br>\n\nYou got it, the `last_name` element is a **sibling** of the `first_name` element.\n\n## Examining XML documents\n\nJust like JSON, you should first verify the response is indeed XML with <a href=\"https://www.rdocumentation.org/packages/httr/topics/http_type\">`http_type()`</a> and by examining the result of `content(r, as = \"text\")`.  Then you can turn the response into an XML document object with <a href=\"https://www.rdocumentation.org/packages/xml2/topics/read_xml\">`read_xml()`</a>.\n\nOne benefit of using the XML document object is the available functions that help you explore and manipulate the document.  For example <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_structure\">`xml_structure()`</a> will print a representation of the XML document that emphasizes the hierarchical structure by displaying the elements without the data.\n\nIn this exercise you'll grab the same revision history you've been working with as XML, and take a look at it with `xml_structure()`.\n\n**Steps**\n\n1. Get the XML version of the revision history for the Wikipedia article for `\"Hadley Wickham\"`, by calling `rev_history(\"Hadley Wickham\", format = \"xml\")`, store it in `resp_xml`.\n2. Check the response type of `resp_xml` to confirm the API returned an XML object.\n3. You can't always trust a header, so check the content looks like XML by calling `content()` on `resp_xml` with `as = \"text\"`, store in `rev_text`.\n4. Turn `rev_text` into an XML object with `read_xml()` from the `xml2` package, store as `rev_xml`.\n5. Call `xml_structure()` on `rev_xml` to see the structure of the returned XML.  *Can you see where the revisions are?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load xml2\nlibrary(xml2)\n\n# Get XML revision history\nrevhist_xml_url <- \"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=xml&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\"\n\nresp_xml <- GET(revhist_xml_url)\n\n# Check response is XML\nhttp_type(resp_xml)\n\n# Examine returned text with content()\nrev_text <- content(resp_xml, as = \"text\")\nrev_text\n\n# Turn rev_text into an XML document\nrev_xml <- read_xml(rev_text)\n\n# Examine the structure of rev_xml\nxml_structure(rev_xml)\n```\n:::\n\n\nBrilliant! [`xml_structure()`](https://www.rdocumentation.org/packages/xml2/topics/xml_structure) helps you understand the structure of your document, without overwhelming you with content.\n\n## XPATHs\n\nTheory. Coming soon ...\n\n\n**1. XPATHS**\n\nNow you have an overview of the structure of XML documents you need to learn how to extract data from them.  In this video you'll learn about XPath, a language for specifying nodes in an XML document.\n\n**2. Movies example**\n\nTake a look at this slightly altered XML description of the movies.  I've added another title node to the movies element that describes that these movies belong to the Star Wars franchise.  I've also added an episode attribute to each movie:\n\n**3. Movies example**\n\nI'll read it in to XML as movies_xml.\n\n**4. XPATHS**\n\nXPATHs look a bit like file paths because they use forward slashes to specify levels in the XML document tree.  Here's a simple one: /movies/movie/title. It describes all nodes that are called title that are inside a movie node, inside the movies node at the top of the document.  The xml_find_all() function in xml2 will find all nodes that match an XPath description in a given document.\n\n**5. XPATHS**\n\nLet's look for nodes using this XPath in the movies XML.You see we get back the two title nodes that correspond to our two movies.  The result is a special object called a node set.\n\n**6. XPATHS**\n\nIf we want to extract the data from these we can use the xml_text() function to extract the contents of each node as text.\n\n**7. Other XPATH Syntax**\n\nA double forward slash in a XPATH describes nodes at any level of the document, for example \"double forward slash title\" describes any nodes below the top of the document with the title tag.  You can see this picks up our two movie titles but also the title of our collection.\n\n**8. Other XPATH Syntax**\n\nYou can also use XPaths to find nodes based on attributes using the \"at\" symbol.  For example, //movie/@episode finds any episode attributes that are inside a movie node anywhere in the document.\n\n**9. Or...**\n\nAn alternative way to extract attributes is to use the xml_attr() and xml_attrs() functions on nodes sets, which you'll see in the exercises.\n\n**10. Wrap Up**\n\nTo wrap up, you can use XPaths to specify specific nodes in an XML document.  A single slash denotes a node at the current level, and a double slash a node anywhere at or below the current level, an \"at\" symbol can be used for an attribute.  To extract these nodes from a document in R use the xml_find_all() function.  Then parse the contents of the nodes to get an R object.  You just saw xml_text(), but there is also xml_double(), xml_integer(), and as_list().\n\n**11. Let's practice!**\n\nOK time to extract some data from XML.\n\n## Extracting XML data\n\nXPATHs are designed to specifying nodes in an XML document.  Remember `/node_name` specifies nodes at the current level that have the tag `node_name`, where as `//node_name` specifies nodes at any level below the current level that have the tag `node_name`.\n\n`xml2` provides the function <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_find_all\">`xml_find_all()`</a> to extract nodes that match a given XPATH.  For example, `xml_find_all(rev_xml, \"/api\")` will find all the nodes at the top level of the `rev_xml` document that have the tag `api`.  Try running that in the console. You'll get a nodeset of one node because there is only one node that satisfies that XPATH.\n\nThe object returned from `xml_find_all()` is a nodeset (think of it like  a list of nodes). To actually get data out of the nodes in the nodeset, you'll have to explicitly ask for it with <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_text\">`xml_text()`</a> (or <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_double\">`xml_double()`</a> or <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_integer\">`xml_integer()`</a>).\n\nUse what you know about the location of the revisions data in the returned XML document extract just the content of the revision.\n\n**Steps**\n\n1. Use `xml_find_all()` on `rev_xml` to find all the nodes that describe revisions by using the XPATH, `\"/api/query/pages/page/revisions/rev\"`.\n2. Use `xml_find_all()` on `rev_xml` to find all the nodes that are in a `rev` node anywhere in the document, store in `rev_nodes`.\n3. Extract the contents from each node in `rev_nodes`, by passing `rev_nodes` to `xml_text()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find all nodes using XPATH \"/api/query/pages/page/revisions/rev\"\nxml_find_all(rev_xml, \"/api/query/pages/page/revisions/rev\")\n\n# Find all rev nodes anywhere in document\nrev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n# Use xml_text() to get text from rev_nodes\nxml_text(rev_nodes)\n```\n:::\n\n\nTerrific, you've written your first XPATH!  You'll get plenty of practice with them this chapter and the next.\n\n## Extracting XML attributes\n\nNot all the useful data will be in the content of a node, some might also be in the attributes of a node. To extract attributes from a nodeset, `xml2` provides <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_attrs\">`xml_attrs()`</a> and <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_attr\">`xml_attr()`</a>.\n\n`xml_attrs()` takes a nodeset and returns **all** of the attributes for every node in the nodeset. `xml_attr()` takes a nodeset and an additional argument `attr` to extract a single named argument from each node in the nodeset.\n\nIn this exercise you'll grab the `user` and `anon` attributes for each revision.  You'll see  <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_find_first\">`xml_find_first()`</a> in the sample code. It works just like <a href=\"https://www.rdocumentation.org/packages/xml2/topics/xml_find_all\">`xml_find_all()`</a> but it only extracts the first node it finds.\n\n**Steps**\n\n1. We've extracted `rev_nodes` and `first_rev_node` in the document for you to explore the difference between `xml_attrs()` and `xml_attr()`.\n\n    * Use `xml_attrs()` on `first_rev_node` to see all the attributes of the first revision node.  \n    * Use `xml_attr()` on `first_rev_node` along with an appropriate `attr` argument to extract the `user` attribute from the first revision node.  \n    * Now use `xml_attr()` again, but this time on `rev_nodes` to extract the `user` attribute from all revision nodes.  \n    * Use `xml_attr()` on `rev_nodes` to extract the `anon` attribute from all revision nodes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# All rev nodes\nrev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n# The first rev node\nfirst_rev_node <- xml_find_first(rev_xml, \"//rev\")\n\n# Find all attributes with xml_attrs()\nxml_attrs(first_rev_node)\n\n# Find user attribute with xml_attr()\nxml_attr(first_rev_node, \"user\")\n\n# Find user attribute for all rev nodes\nxml_attr(rev_nodes, \"user\")\n\n# Find anon attribute for all rev nodes\nxml_attr(rev_nodes, \"anon\")\n```\n:::\n\n\nGood job!  Did you notice that if a node didn't have the `anon` attribute `xml_attr()` returned an `NA`?\n\n## Wrapup: returning nice API output\n\nHow might all this work together? A useful API function will retrieve results from an API **and** return them in a useful form.  In Chapter 2, you finished up by writing a function that retrieves data from an API that relied on <a href=\"https://www.rdocumentation.org/packages/httr/topics/content\">`content()`</a> to convert it to a useful form.  To write a more robust API function you shouldn't rely on `content()` but instead parse the data yourself.\n\nTo finish up this chapter you'll do exactly that: write `get_revision_history()` which retrieves the XML data for the revision history of page on Wikipedia, parses it, and returns it in a nice data frame.  \n\nSo that you can focus on the parts of the function that parse the return object, you'll see your function calls `rev_history()` to get the response from the API.  You can assume this function returns the raw response and follows the best practices you learnt in Chapter 2, like using a user agent, and checking the response status.\n\n**Steps**\n\n1. Fill in the `___` to finish the function definition.\n\n    * Use `read_xml()` to turn the `content()` of `rev_resp` as text into an XML object.\n    * Use `xml_find_all()` to find *all* the `rev` nodes in the XML.\n    * Parse out the `\"user\"` attribute from `rev_nodes`.\n    * Parse out the content from `rev_nodes` using  `xml_text()`.\n    * Finally, call `get_revision_history()` with `article_title = \"Hadley Wickham\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Adjust for title & format\n\nget_revision_history <- function(article_title){\n  # Get raw revision response\n  # rev_resp <- rev_history(article_title, format = \"xml\")\n  # Get XML revision history\n  revhist_xml_url <- \"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=xml&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\"\n\n  rev_resp <- GET(revhist_xml_url)\n  \n  # Turn the content() of rev_resp into XML\n  rev_xml <- read_xml(content(rev_resp, \"text\"))\n  \n  # Find revision nodes\n  rev_nodes <- xml_find_all(rev_xml, \"//rev\")\n\n  # Parse out usernames\n  user <- xml_attr(rev_nodes, \"user\")\n  \n  # Parse out timestamps\n  timestamp <- readr::parse_datetime(xml_attr(rev_nodes, \"timestamp\"))\n  \n  # Parse out content\n  content <- xml_text(rev_nodes)\n  \n  # Return data frame\n  data.frame(user = user,\n    timestamp = timestamp,\n    content = substr(content, 1, 40))\n}\n\n# Call function for \"Hadley Wickham\"\nget_revision_history(\"Hadley Wickham\")\n```\n:::\n\n\nNice job!  Your function parsed the XML data, but you could have just as easily parsed the JSON data.\n\n# 4. Web scraping with XPATHs\n\nNow that we've covered the low-hanging fruit (\"it has an API, and a client\", \"it has an API\") it's time to talk about what to do when a website doesn't have any access mechanisms at all - when you have to rely on web scraping. This chapter will introduce you to the rvest web-scraping package, and  build on your previous knowledge of XML manipulation and XPATHs.\n\n## Web scraping 101\n\nTheory. Coming soon ...\n\n\n**1. Web Scraping 101**\n\nWe've discussed API usage and how to play around with the data you retrieve from the API. The problem is that sometimes websites don't have a formal API. This doesn't mean that you can't get data out of them, just that you have to take a different approach.That approach is called \"web scraping,\" and it consists of grabbing the raw HTML of a website and then extracting values from it. Web scraping is a bit messier than API use, and a bit more complex, but it's important to learn for occasions where you can't rely on an API's presence. One caveat is that there can be legal objections to doing it en-masse, so if it's something you're planning to do, make sure it's something the website owner is comfortable with.\n\n**2. Selectors**\n\nWeb scraping works by filtering the HTML of a web page to just the bits you want, using some kind of identifier. There are a lot of forms these identifiers can take, which we'll discuss later in the chapter and course, using example identifiers we have already extracted.When you get out of this course and start applying web scraping to your own problems, you'll have to work out the identifiers yourself. This can be done with a tool known as a selector: basically a browser plugin or extension which, when you mouse over an element of a page, tells you what categories it falls into for ID purposes.We're not going to use it in this course, but once you're done you should absolutely check it out - it makes things a heck of a lot easier.\n\n**3. rvest**\n\nTo ease the task of web scraping, there's a dedicated package, rvest. This provides utilities for everything from reading the HTML in to extracting elements from it, and is what we'll be exploring in this chapter and the next.To read an HTML page, call read_html(), passing the URL. There are a few advanced options that are useful for dealing with badly-formed pages, but usually this is the only argument that you need.\n\n**4. Parsing HTML**\n\nread_html() actually returns an XML document, so the XPath querying skills that you learned in the previous chapter will come in useful now. To retrieve a node from your document, call html_node() passing the document and an XPath string describing the node that you want.\n\n**5. Parsing HTML**\n\nFor example, calling read_html() on the Wikipedia page for R returns an XML document object,\n\n**6. Parsing HTML**\n\nthen calling html_node() with the document and the XPath double forwards slash \"ul\" returns a node of the first unordered list on that page.\n\n**7. Let's practice!**\n\nNow it's your turn to try some examples.\n\n## Reading HTML\n\nThe first step with web scraping is actually reading the HTML in. This can be done with a function from `xml2`, which is imported by `rvest` -  <a href=\"https://www.rdocumentation.org/packages/rvest/topics/read_html\">`read_html()`</a>. This accepts a single URL, and returns a big blob of XML that we can use further on.\n\nWe're going to experiment with that by grabbing Hadley Wickham's wikipedia page, with <a href=\"https://www.rdocumentation.org/packages/rvest\">`rvest`</a>, and then printing it just to see what the structure looks like.\n\n**Steps**\n\n1. Load the `rvest` package.\n2. Use `read_html()` to read the URL stored at `test_url`. Store the results as `test_xml`.\n3. Print `test_xml`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load rvest\nlibrary(rvest)\n\n# Hadley Wickham's Wikipedia page\ntest_url <- \"https://en.wikipedia.org/wiki/Hadley_Wickham\"\n\n# Read the URL stored as \"test_url\" with read_html()\ntest_xml <- read_html(test_url)\n\n# Print test_xml\ntest_xml\n```\n:::\n\n\nNicely done! As you can see, the XML document looks very similar to what we saw in the last chapter.\n\n## Extracting nodes by XPATH\n\nNow you've got a HTML page read into R. Great! But how do you get individual, identifiable pieces of it?\n\nThe answer is to use <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_node\">`html_node()`</a>, which extracts individual chunks of HTML from a HTML document. There are a couple of ways of identifying and filtering nodes, and for now we're going to use XPATHs: unique identifiers for individual pieces of a HTML document.\n\nThese can be retrieved using a browser gadget we'll talk about later - in the meanwhile the XPATH for the information box in the page you just downloaded is stored as `test_node_xpath`. We're going to retrieve the box from the HTML doc with `html_node()`, using `test_node_xpath` as the `xpath` argument.\n\n**Steps**\n\n1. Use `html_node()` to retrieve the node with the XPATH stored at `test_node_xpath` from `test_xml` document you grabbed in the last exercise.\n2. Print the first element of the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_node_xpath <- '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]'\n\n# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`\nnode <- html_node(x = test_xml, xpath = test_node_xpath)\n\n# Print the first element of the result\nnode[[1]]\n```\n:::\n\n\nGood job! XML nodes are the building block of an XML document - extracting them leads to everything else. At the moment, they're still kind of abstract objects: we'll dig into their contents later on.\n\n## HTML structure\n\nTheory. Coming soon ...\n\n\n**1. HTML Structure**\n\nAt this point you've grabbed HTML pages and extracted nodes from them - more importantly you've looked at the node contents which, if you're not familiar with HTML, look a lot like gibberish. In this video we're going to explore how HTML is structured, so it can at least be meaningful gibberish.\n\n**2. Tags**\n\nHTML is defined largely by the presence of content within \"tags.\" With a few exceptions, these are paired: there's a start tag and an end tag. The start tag is wrapped with a less than and greater than sign - the end tag with a less than, backslash and greater than sign. You might spot that this is just the same as with XML data, which Charlotte discussed in the last chapter.As an example, if you want to store the line of text \"this is a test\" with paragraph formatting, we'd use &lt;p&gt; this is a test &lt;/p&gt; - where the tag itself (p) tells the browser what kind of information it's dealing with (text) and the start and end tags tell the browser where that information - and the associated formatting - end and begin.\n\n**3. Attributes**\n\nYou can also have attributes in tags. For example, the tag for a link is \"a,\" which requires the attribute href - the link itself - before the text. So a link to Wikipedia, with the display text \"this is a test,\" would be &lt;a href = \"https://en-dot-wikipedia-dot-org/\"&gt; this is a test &lt;/a&gt;. Parameters can do a lot of useful things, and are often used to incorporate styling or formatting information, which is a thing we'll discuss in the next chapter.The important thing, though, is that the information in a piece of HTML is more than just a piece of text or a photo. It can be text - the actual displayed element. It can be an attribute - links, stylesheet information, or similar things. And it can be a name - the actual tag used.\n\n**4. Extracting information**\n\nrvest provides the functions html_text(), html_attr() and  html_name() to extract information stored in these three ways. For html_text(), and html_name() you just need to pass the html nodeset of interest. For html_attr() you'll also need to specify the attribute name.\n\n**5. Let's practice!**\n\nIn the following exercises you'll learn how to extract any and all of the above, as you find necessary.\n\n## Extracting names\n\nThe first thing we'll grab is a name, from the first element of the previously extracted table (now stored as `table_element`). We can do this with <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_name\">`html_name()`</a>. As you may recall from when you printed it, the element has the tag <code><table>...</table></code>, so we'd expect the name to be, well, `table`.\n\n**Steps**\n\n1. Extract the name of `table_element` using the function `html_name()`. Save it as `element_name`.\n2. Print `element_name`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read html\ntable_element <- read_html(\"data/vcard.html\") |> html_node((\"table\"))\n\n# Extract the name of table_element\nelement_name <- html_name(table_element)\n\n# Print the name\nelement_name\n```\n:::\n\n\nNice work! You've started extracting components from HTML and XML nodes. The tag might not seem important (and most of the time, it's not) but it's a good first step, and the actual node contents (text, say) is something we'll move on to next.\n\n## Extracting values\n\nJust knowing the type of HTML object a node is isn't much use, though (although it can be very helpful). What we really want is to extract the actual *text* stored within the value.\n\nWe can do that with (shocker) <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_text\">`html_text()`</a>, another convenient `rvest` function that accepts a node and passes back the text inside it. For this we'll want a node *within* the extracted element - specifically, the one containing the page title. The xpath value for that node is stored as `second_xpath_val`.\n\nUsing this xpath value, extract the node within `table_element` that we want, and then use `html_text` to extract the text, before printing it.\n\n**Steps**\n\n1. Extract the element of `table_element` referred to by `second_xpath_val` and store it as `page_name`.\n2. Extract the text from `page_name` using `html_text()`, saving it as `page_title`.\n3. Print `page_title`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# xpath\nsecond_xpath_val <- '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"fn\", \" \" ))]'\n\n# Extract the element of table_element referred to by second_xpath_val and store it as page_name\npage_name <- html_node(x = table_element, xpath = second_xpath_val)\n\n# Extract the text from page_name\npage_title <- html_text(page_name)\n\n# Print page_title\npage_title\n```\n:::\n\n\nNice! Text extraction is most of what you're likely to do with XML - after all, the content of an XML tag is almost always the important value. If it's consistently a set of digits, say, you can always use as.integer() or as.numeric() to turn it from a string, into a number\n\n## Test: HTML reading and extraction\n\nTime for a quick test of what we've learned about HTML. \n\n> *Question*\n> ---\n> What would you use to extract the type of HTML tag a value is wrapped in?<br>\n> <br>\n> ⬜ `html_attr()`<br>\n> ✅ `html_name()`<br>\n\nCorrect! The type of tag is the tag name - so `html_name()` is the right function.\n\n## Reformatting Data\n\nTheory. Coming soon ...\n\n\n**1. Reformatting Data**\n\nSomething you briefly touched on in the last chapter is reformatting data so that it's rectangular - in other words, so it fits in a data frame. This is because, as I'm sure you know, data frames are the most commonly used and commonly supported ways of representing data in R.We're going to discuss it more here, in the context of web scraping, and will cover two things. The first is turning HTML tables into R data frames, and cleaning them up a bit: the second is turning arbitrary HTML data, like names and text, into data frames.\n\n**2. HTML tables**\n\nTables aren't just an R structure, they're also a structure in HTML. Every time you see something tabular in HTML, that's using a set of dedicated tags - starting with the table tag - which tell your browser \"this is a table\".HTML tables can be extracted directly with rvest, using the html_table() function. This produces a data frame. Much of the time that's all you need - the one exception is when the HTML table itself didn't have a header. In that case, the data frame will have default column names which are pretty hard to remember. This is easily fixed by assigning new memorable column names to it with the colnames() function.\n\n**3. Turning things into data.frames**\n\nMost HTML, though, as you've seen, isn't stored in tables. The good news is we can still turn them into data frames, just using the data dot frame() function. The vectors of text, names or attributes you've extracted are just like any other vectors, and so you can easily construct a data frame from them (assuming they're all the same length).\n\n**4. Let's practice!**\n\nYou'll be doing just that - and turning HTML tables into data frames - in the next exercises.\n\n## Extracting tables\n\nThe data from Wikipedia that we've been playing around with *can* be extracted bit by bit and cleaned up manually, but since it's a table, we have an easier way of turning it into an R object. `rvest` contains the function <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_table\">`html_table()`</a> which, as the name suggests, extracts tables. It accepts a node containing a table object, and outputs a data frame.\n\nLet's use it now: take the table we've extracted, and turn it into a data frame.\n\n**Steps**\n\n1. Turn `table_element` into a data frame and assign it to `wiki_table`.\n2. Print the resulting object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Turn table_element into a data frame and assign it to wiki_table\nwiki_table <- html_table(table_element)\n\n# Print wiki_table\nwiki_table\n```\n:::\n\n\nWell done! Being able to extract tables directly is a massive speedup, since otherwise they're a ton of different nested tags.\n\n## Cleaning a data frame\n\nIn the last exercise, we looked at extracting tables with `html_table()`. The resulting data frame was pretty clean, but had two problems - first, the column names weren't descriptive, and second, there was an empty row.\n\nIn this exercise we're going to look at fixing both of those problems. First, column names. Column names can be cleaned up with the `colnames()` function. You call it on the object you want to rename, and then assign *to that call* a vector of new names.\n\nThe missing row, meanwhile, can be removed with the `subset()` function. `subset` takes an object, and a condition. For example, if you have a data frame `df` containing a column `x`, you could run \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubset(df, !x == \"\")\n```\n:::\n\n\nto remove all rows from `df` consisting of empty strings (`\"\"`) in the column `x`.\n\n**Steps**\n\n1. <ol>\\n<li>Rename the columns of `wiki_table` to `\"key\"` and `\"value\"` using `colnames()`.</li>\\n<li>Remove the empty row from `wiki_table` using `subset()`, and assign the result to `cleaned_table`.</li>\\n<li>Print `cleaned_table`.</li>\\n</ol>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rename the columns of wiki_table\ncolnames(wiki_table) <- c(\"key\", \"value\")\n\n# Remove the empty row from wiki_table\ncleaned_table <- subset(wiki_table, !key == \"\")\n\n# Print cleaned_table\ncleaned_table\n```\n:::\n\n\nWell done! Cleaning up data, or 'munging', is a really common thing to have to do, particularly when someone else picked how it's formatted. If you can scrape data and clean it, you can do anything.\n\n# 5. CSS Web Scraping and Final Case Study\n\nCSS path-based web scraping is a far-more-pleasant alternative to using XPATHs.  You'll start this chapter by learning about CSS, and how to leverage it for web scraping.   Then, you'll work through a final case study that combines everything you've learnt so  far to write a function that queries an API, parses the response and returns data in a nice form.\n\n## CSS web scraping in theory\n\nTheory. Coming soon ...\n\n\n**1. CSS**\n\nBy this point you should already be somewhat familiar with web scraping thanks to chapter 4, which discusses web scraping using XPATH values. Next up is web scraping with CSS!CSS - Cascading Style Sheets - is basically a way of adding design information (font size, color, or border width) to HTML web pages. In order to make this information easy to change and non-duplicative, CSS information is associated with 'classes' or 'ids' which can then be applied to whichever HTML elements the developer wants.\n\n**2. CSS example**\n\nSo if we wanted two types of link - one black, one red - we'd create two different CSS classes with different font color choices,\n\n**3. CSS example**\n\nand then apply different classes to different HTML tags. And you can see us doing that in the slide.\n\n**4. CSS versus XPATH**\n\nCSS-based scraping looks for these class names. Because particular classes are strongly associated with particular UI objects (such as 'links in the main body', or a particular table) it's really easy to select or filter content based on them. It looks a lot like the XPATH-based scraping in the last chapter, but with one major difference: CSS-based techniques often get you a whole set of items that meet conditions, rather than one specific item. We're going to be using modified versions of the exercises from the last chapter so you can see the differences and similarities.Just as before, you'll be using the html_node() function to retrieve nodes from the document. This time, however rather than passing the xpath argument, you'll be passing the css argument.\n\n**5. Let's practice!**\n\nHave a go at some examples.\n\n## Using CSS to scrape nodes\n\nAs mentioned in the video, CSS is a way to add design information to HTML, that instructs the browser on how to display the content. You can leverage these design instructions to identify content on the page.\n\nYou've already used <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_node\">`html_node()`</a>, but it's more common with CSS selectors to use <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_nodes\">`html_nodes()`</a> since you'll often want more than one node returned.  Both functions allow you to specify a `css` argument to use a CSS selector, instead of specifying the `xpath` argument.\n\nWhat do CSS selectors look like? Try these examples to see a few possibilities.\n\n**Steps**\n\n1. We've read in the same HTML page from Chapter 4, the Wikipedia page for Hadley Wickham, into `test_xml`.\n\n    * Use the CSS selector `\"table\"` to select all elements that are a  `table` tag.\n    * Use the CSS selector `\".infobox\"` to select all elements that have the attribute `class = \"infobox\"`.\n    * Use the CSS selector `\"#firstHeading\"` to select all elements that have the attribute `id = \"firstHeading\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read xml\ntest_xml <- read_html(\"data/test.html\")\n\n# Select the table elements\nhtml_nodes(test_xml, css = \"table\")\n\n# Select elements with class = \"infobox\"\nhtml_nodes(test_xml, css = \".infobox\")\n\n# Select elements with id = \"firstHeading\"\nhtml_nodes(test_xml, css = \"#firstHeading\")\n```\n:::\n\n\nNice work! Did you notice the special prefixes needed for classes and ids?\n\n## Scraping names\n\nYou might have noticed in the previous exercise, to select elements with a certain class, you add a `.` in front of the class name. If you need to select an element based on its id, you add a `#` in front of the id name.\n\nFor example if this element was inside your HTML document:\n\n\n::: {.cell}\n\n```{.html .cell-code}\n<h1 class = \"heading\" id = \"intro\">\n  Introduction\n</h1>\n```\n:::\n\n\nYou could select it by its class using the CSS selector `\".heading\"`, or by its id using the CSS selector `\"#intro\"`.\n\nOnce you've selected an element with a CSS selector, you can get the element tag name just like you did with XPATH selectors, with  <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_name\">`html_name()`</a>. Try it!\n\n**Steps**\n\n1. The infobox you extracted in Chapter 4 has the class `infobox`. Use `html_nodes()` and the appropriate CSS selector to extract the infobox element to `infobox_element`.\n2. Use `html_name()` to extract the tag name of `infobox_element` and store it in `element_name`.\n3. Print `element_name`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract element with class infobox\ninfobox_element <- html_nodes(test_xml, css = \".infobox\")\n\n# Get tag name of infobox_element\nelement_name <- html_name(infobox_element)\n\n# Print element_name\nelement_name\n```\n:::\n\n\nTerrific!  This is the same element you selected in Chapter 4 with an XPATH statement, and unsurprisingly it has the same tag, it's a `table`.\n\n## Scraping text\n\nOf course you can get the contents of a node extracted using a CSS selector too, with <a href=\"https://www.rdocumentation.org/packages/rvest/topics/html_text\">`html_text()`</a>. \n\nCan you put the pieces together to get the page title like you did in Chapter 4?\n\n**Steps**\n\n1. The infobox HTML element is stored in `infobox_element` in your workspace.\n\n    * Use `html_node()` to extract the element from `infobox_element` with the CSS class `fn`.\n    * Use `html_text()` to extract the contents of `page_name`.\n    * Print `page_title`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract element with class fn\npage_name <- html_node(x = infobox_element, css = \".fn\")\n\n# Get contents of page_name\npage_title <- html_text(page_name)\n\n# Print page_title\npage_title\n```\n:::\n\n\nPerfect! Why do you think the class for this element is `fn`?  I suspect it's short for **f**ull **n**ame.\n\n## Test: CSS web scraping\n\nTake a look at the chunk of HTML being read into `test`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- read_html('\n   <h1 class = \"main\">Hello world!</h1>\n   ')\n```\n:::\n\n\n> *Question*\n> ---\n> How would you extract the text `Hello world!` using `rvest` and CSS selectors?<br>\n> <br>\n> ⬜ `html_name(html_node(test, css = \".main\"))`<br>\n> ⬜ `html_name(html_node(test, css = \"#main\"))`<br>\n> ⬜ `html_text(html_node(test, css = \"#main\"))`<br>\n> ✅ `html_text(html_node(test, css = \".main\"))`<br>\n\nCorrect! You want the contents of the tag with class `main`.\n\n## Final case study: Introduction\n\nTheory. Coming soon ...\n\n\n**1. Final case study: Introduction**\n\nIf you're watching this, you've almost completed the course! Or you've skipped ahead. Either way, congratulations - you're nearly done. The only thing left is a final exercise which puts together everything you've learned so far.\n\n**2. What we'll cover**\n\nWe're going to tie everything you've learned together by having you get the (XML) content of a Wikipedia article out of the Wikipedia API.This has four steps: First you retrieve the data through the API, then you extract an infobox - which is the little metadata box in the top right of a lot of articles,  thirdly you turn the information in it into a data frame, and finally you wrap this in a function for reproducibility.\n\n**3. Let's practice!**\n\nEverything you've learned so far should show you nicely how to do it! Onwards to the finish!\n\n## API calls\n\nYour first step is to use the Wikipedia API to get the page contents for a specific page.  We'll continue to work with the Hadley Wickham page, but as your last exercise, you'll make it more general.\n\nTo get the content of a page from the Wikipedia API you need to use a parameter based URL.  The URL you want is \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhttps://en.wikipedia.org/w/api.php?action=parse&amp;page=Hadley%20Wickham&amp;format=xml\n```\n:::\n\n\nwhich specifies that you want the parsed content (i.e the HTML) for the \"Hadley Wickham\" page, and the API response should be XML.\n\nIn this exercise you'll make the request with `GET()` and parse the XML response with `content()`.\n\n**Steps**\n\n1. We've already defined `base_url` for you.\n\n    * Create a list for the query parameters, setting `action = \"parse\"`, `page = \"Hadley Wickham\"` and `format = \"xml\"`.\n    * Use `GET()` to call the API by specifying `url` and `query`.\n    * Parse the response using `content()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load httr\nlibrary(httr)\n\n# The API url\nbase_url <- \"https://en.wikipedia.org/w/api.php\"\n\n# Set query parameters\nquery_params <- list(action = \"parse\", \n  page = \"Hadley Wickham\", \n  format = \"xml\")\n\n# Get data from API\nresp <- GET(url = base_url, query = query_params)\n    \n# Parse response\nresp_xml <- content(resp)\n```\n:::\n\n\nGood work! You now have a response, but can you find the HTML for the page in that response?\n\n## Extracting information\n\nNow we have a response from the API, we need to extract the HTML for the page from it. It turns out the HTML is stored in the contents of the XML response.<br>\nTake a look, by using `xml_text()` to pull out the text from the XML response:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxml_text(resp_xml)\n```\n:::\n\n\nIn this exercise, you'll read this text as HTML, then extract the relevant nodes to get the infobox and page title.\n\n**Steps**\n\n1. Code from the previous exercise has already been run, so you have `resp_xml` available in your workspace.\n\n    * Use `read_html()` to read the contents of the XML response (`xml_text(resp_xml)`) as HTML.\n    * Use `html_node()` to extract the infobox element (having the class `infobox`) from `page_html` with a CSS selector.\n    * Use `html_node()` to extract the page title element (having the class `fn`) from `infobox_element` with a CSS selector.\n    * Extract the title text from `page_name` with `html_text()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load rvest\nlibrary(rvest)\n\n# Read page contents as HTML\npage_html <- read_html(xml_text(resp_xml))\n\n# Extract infobox element\ninfobox_element <- html_node(x = page_html, css =\".infobox\")\n\n# Extract page name element from infobox\npage_name <- html_node(x = infobox_element, css = \".fn\")\n\n# Extract page name as text\npage_title <- html_text(page_name)\n```\n:::\n\n\nFantastic! You have the info you need you just need to return it in a nice format.\n\n## Normalising information\n\nNow it's time to put together the information in a nice format.  You've already seen you can use `html_table()` to parse the infobox into a data frame. But one piece of important information is missing from that table: who the information is about!  \n\nIn this exercise, you'll parse the infobox in a data frame, and add a row for the full name of the subject.\n\n**Steps**\n\n1. No need to repeat all the table parsing code from Chapter 4, we've already added it to your script.\n\n    * Create a new data frame where `key` is the string `\"Full name\"` and `value` is our previously stored `page_title`.\n    * Combine `name_df` with `cleaned_table` using `rbind()` and assign it to `wiki_table2`. \n    * Print `wiki_table2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code from earlier exercises\nwiki_table <- html_table(infobox_element)\ncolnames(wiki_table) <- c(\"key\", \"value\")\ncleaned_table <- subset(wiki_table, !key == \"\")\n\n# Create a dataframe for full name\nname_df <- data.frame(key = \"Full name\", value = page_title)\n\n# Combine name_df with cleaned_table\nwiki_table2 <- rbind(name_df, cleaned_table)\n\n# Print wiki_table\nwiki_table2\n```\n:::\n\n\nAwesome! But what if we wanted to do this for someone else with  a page on Wikipedia?  You'll do just that in the next exercise.\n\n## Reproducibility\n\nNow you've figured out the process for requesting and parsing the infobox for the Hadley Wickham page, it's time to turn it into a function that does the same thing for anyone.\n\nYou've already done all the hard work! In the sample script we've just copied all your code from the previous three exercises, with only one change: we've wrapped it in the function definition syntax, and chosen the name `get_infobox()` for this function.\n\nIt doesn't quite work yet, the argument `title` isn't used inside the function.  In this exercise you'll fix that, then test it out with some other personalities.\n\n**Steps**\n\n1. Fix the function, by replacing the string `\"Hadley Wickham\"` with `title`, so that the title argument of the function will be used for the query.\n2. Test `get_infobox()` with `title = \"Hadley Wickham\"`.\n3. Now, try getting the infobox for `\"Ross Ihaka\"`.\n4. Finally, try getting the infobox for `\"Grace Hopper\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\nlibrary(rvest)\nlibrary(xml2)\n\nget_infobox <- function(title){\n  base_url <- \"https://en.wikipedia.org/w/api.php\"\n  \n  # Change \"Hadley Wickham\" to title\n  query_params <- list(action = \"parse\", \n    page = title, \n    format = \"xml\")\n  \n  resp <- GET(url = base_url, query = query_params)\n  resp_xml <- content(resp)\n  \n  page_html <- read_html(xml_text(resp_xml))\n  infobox_element <- html_node(x = page_html, css =\".infobox\")\n  page_name <- html_node(x = infobox_element, css = \".fn\")\n  page_title <- html_text(page_name)\n  \n  wiki_table <- html_table(infobox_element)\n  colnames(wiki_table) <- c(\"key\", \"value\")\n  cleaned_table <- subset(wiki_table, !wiki_table$key == \"\")\n  name_df <- data.frame(key = \"Full name\", value = page_title)\n  wiki_table <- rbind(name_df, cleaned_table)\n  \n  wiki_table\n}\n\n# Test get_infobox with \"Hadley Wickham\"\nget_infobox(title = \"Hadley Wickham\")\n\n# Try get_infobox with \"Ross Ihaka\"\nget_infobox(title = \"Ross Ihaka\")\n\n# Try get_infobox with \"Grace Hopper\"\nget_infobox(title = \"Grace Hopper\")\n```\n:::\n\n\nWow, great work! You put together everything you've learn to make a useful API function.  The function isn't perfect: you may notice it fails rather ungracefully if you ask for a page that doesn't exist, or a person without an infobox. \n\n## Wrap Up\n\nTheory. Coming soon ...\n\n\n**1. Wrap Up**\n\nYou're now at the end of the course - congratulations, and thank you for all your hard work!\n\n**2. Wrap up**\n\nOver the last five chapters, we've covered a lot of topics, from downloading and reading flat files to both using and designing API clients.After that, we talked about web scraping, using both Cascading Style Sheets and XPaths, to get data out of websites that don't have an API in the first place.With all of that, you should find yourself knowing how to get data out of pretty much any website in pretty much any way - something that's vital in a time when more and more useful data is being accessed via the Internet.\n\n**3. Good luck!**\n\nThat's all from us. Hopefully you've found the course both valuable and interesting, and the knowledge you've gained will serve you well.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}