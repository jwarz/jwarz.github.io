---
title: "Foundations of Probability in R"
author: "Joschka Schwarz"
---

```{r}
#| include: false
source(here::here("R/setup-ggplot2-tie.R"))
options(dplyr.summarise.inform = FALSE)
```

Probability is the study of making predictions about random phenomena. In this course, you'll learn about the concepts of random variables, distributions, and conditioning, using the example of coin flips. You'll also gain intuition for how to solve probability problems through random simulation. These principles will help you understand statistical inference and can be applied to draw conclusions from data.

# 1. The binomial distribution

One of the simplest and most common examples of a random phenomenon is a coin flip: an event that is either "yes" or "no" with some probability. Here you'll learn about the binomial distribution, which describes the behavior of a combination of yes/no trials and how to predict and simulate its behavior.

## Flipping coins in R

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Flipping coins in R**

Many DataCamp courses teach the process of statistical inference. That's the process where you have some observed data, and you use it to build an underlying model. This is essential in data analysis, but it's only part of a useful statistical understanding. Probability is the study of how data can be generated from a model. This serves as the foundation for inference, and understanding both of these directions will make you a better statistician.I'm Dave Robinson, and I'll be your instructor for this course, where we'll be talking about one of the simplest models for generating random data: a coin flip. By exploring coin flipping with the R programming language, you'll learn the basic laws and methods of probability.

**2. Flipping a coin**

Each time I flip a fair coin, it has a 50% chance of being heads and a 50% chance of being tails. Before I look at this coin, it is a random variable. We'll call one case of simulating from this random variable, a "draw".

**3. Flipping a coin in R**

If you don't have a coin handy, you can use R to simulate this random event. Specifically, you can use the rbinom() function. This is named because it's a random draw from a binomial distribution, which you'll learn about in a moment.rbinom() takes three arguments: first is the number of random draws we're doing: a draw is a single outcome from a random variable. Second is the number of coins we're flipping on each draw, which is also just one. Third is the probability of a "heads", which for a fair coin is 50%.There are two possible outcomes of this function: 0 and 1. In this case we got a result of "1"- throughout this course, we're going to interpret a 1 as "heads". (Recall that the starting bracket one bracket simply indicates that this is a vector in R: we can completely ignore it).This is a random process, which means that if run the same line of code again, we might get 0, which we will interpret to mean tails. If we ran it a third time, we could get either 1, heads, or 0, tails, unpredictably.

**4. Flipping multiple coins**

In this course we're usually not going to be flipping one coin, we're going to be flipping multiple. It's a hassle to run the same line of code many times to flip a sequence of coins, so you can flip many in a row by changing the first argument. For example, here we flipped ten coins to get a series of draws. In this case the results were six ones, or "heads", and four zeroes, or "tails."Let's run that again. This time, the ten flips resulted in only three heads, and seven tails. Each time we do a set of flips, we'll see a different outcome.Right now each draw has one coin flip. Rather than counting the heads in this way, we can take just one draw, and use the second argument to specify how many coins should be flipped within each. At this point the function will simply return the number of heads. Here, we did one random draw and got a result of 4.We could also say that we want to do multiple draws, and flip multiple coins within each, by setting both of the arguments. In this case, we're saying we want to perform 10 draws, and flip ten coins within each draw. This shows us that when we flip 10 coins, the resulting number of heads might be 3, 6, 5, or so on. Notice that the resulting number of heads tends to be between 3 and 8, and that the most common result is 5: that will be relevant later.

**5. Unfair coins**

Until now we've been talking about a coin that has a 50% chance of heads and a 50% chance of tails. But not all random events have an equal likelihood of happening. In this course we'll often work with "biased" coins: ones that are more likely to result in heads or in tails. We can control this probability by setting the third parameter.When the third parameter is point-8, that means each flip has an 80% chance of resulting in heads. When we flip one of these unfair coins ten times, we notice the number of heads tends to be around seven to nine. When the third parameter is point-2, each flip has only a 20% chance of being heads, and out of ten flips, each of our draws has around 1 to 3 heads.

**6. Binomial distribution**

Consider this process of flipping a number of biased coins and counting the resulting heads. Each outcome, X, is a random variable that follows a binomial distribution. A probability distribution is a mathematical description of the possible outcomes of a random variable. We describe the binomial distribution based on two parameters: the size, or number of flips, and p, the probability that each is heads.In this course, you'll learn to reason and make predictions about this distribution, based on these two parameters.

**7. Let's practice!**



## Simulating coin flips

In these exercises, you'll practice using the `rbinom()` function, which generates random "flips" that are either 1 ("heads") or 0 ("tails").

**Steps**

1. With one line of code, simulate 10 coin flips, each with a 30% chance of coming up 1 ("heads").
2. What kind of values do you see?

```{r}
# Generate 10 separate random flips with probability .3
rbinom(10, 1, .3)
```

That's much easier than actually flipping all those coins!

## Simulating draws from a binomial

In the last exercise, you simulated 10 separate coin flips, each with a 30% chance of heads. Thus, with `rbinom(10, 1, .3)` you ended up with 10 outcomes that were either 0 ("tails") or 1 ("heads").

But by changing the second argument of `rbinom()` (currently `1`), you can flip multiple coins within each draw. Thus, each outcome will end up being a number between 0 and 10, showing the number of flips that were heads in that trial.

**Steps**

1. Use the `rbinom()` function to simulate 100 separate occurrences of flipping 10 coins, where each coin has a 30% chance of coming up heads.
2. What kind of values do you see?

```{r}
# Generate 100 occurrences of flipping 10 coins, each with 30% probability
rbinom(100, 10, .3)
```

Wow, that's a lot of flips in almost no time at all!

## Density and cumulative density

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Density and cumulative density**

When you flip a fair coin ten times, what's the most likely number of heads? Well, since heads and tails are equally likely, you can probably figure out that the most likely outcome is that 5 come up heads, 5 tails. Say I offer you a bet: if it is exactly that result, I'll pay you a dollar, otherwise you'll pay me a dollar. Should you take the bet?

**2. Simulating many outcomes**

To answer this, we'll have to find the probability a binomial variable X with these parameters- ten flips, each with a 50% probability- results in an outcome of 5. We would express this as "Pr X equals 5."One way to find out is to simulate many draws from X- say, a hundred thousand: and then see how common each outcome is. As you saw in the last exercises, you can choose the number of simulations to perform by setting the first argument of rbinom. This resulting variable flips then contains the results of these one hundred thousand draws.We can't print out all these results, at least not in a way we'll understand them. Instead, we can visualize them in a graph.This plot is called a histogram: each bar shows the relative frequency of one outcome, from 0, 1, 2 all the way through 10. Histograms are a common way to examine a probability distribution, and we'll be using them frequently throughout the course. Notice that out of these hundred thousand draws, about twenty five thousand are equal to 5.

**3. Finding density with simulation**

There's a useful trick in R for calculating the fraction equal to 5 directly. The expression flips == 5 compares each item in the vector to 5. We can then use the mean() function to find the fraction of comparisons that are TRUE. This works because the mean function treats TRUE as 1 and FALSE as 0. Thus, "mean flips == 5" gives the fraction of values equal to 5. You're going to be using this trick with mean a lot in these exercises whenever you estimate values through simulation.In this case, we found that the fraction of outcomes equal to 5 was point-2463: that is, there's a 24.6% chance. This is called the density of the binomial at that point.

**4. Calculating exact probability density**

Simulation is a very useful way to understand a distribution and to answer questions about its behavior. But in the case of the binomial distribution, R also provides a way to calculate the exact probability density, using the dbinom function. dbinom takes three arguments: the outcome we're estimating the density at, 5, the number of coins, 10, and the probability of each being heads, point-5.Notice that this gives a result of point-246: this confirms the result from our simulation, that the probability is about 24.6%.Similarly, we could calculate the probability density of getting exactly six heads, or all ten coins being heads, by changing the first argument. Finding a probability through both simulation and exact calculation will be a common task in this course.

**5. Cumulative density**

So now you know not to take my bet: more likely than not, I won't get exactly 5 heads out of 10. What if I offer a new one? I'll pay you a dollar if 4 or fewer come up heads, otherwise you have to pay me.This describes the cumulative density of the binomial, the probability X is less than or equal to 4, and the process for calculating it is similar to calculating the density.You can estimate it using simulation: we'd generate a hundred thousand draws from the binomial distribution, then instead of using "equals equals", we would do "mean flips less than or equal to 4". We can see that this was true of about 37.7% of the simulations.Much like the density, R provides a function to get the exact cumulative density of the binomial. Rather than dbinom, use pbinom. This result confirms that the probability is about 37.7% that a binomial with ten flips gets 4 or fewer heads. In other words, you still shouldn't take my bet.In your exercises, you'll find the density and the cumulative density of several other binomial distributions, using both the simulation approach and the dbinom and pbinom functions.

**6. Let's practice!**



## Calculating density of a binomial

If you flip 10 coins each with a 30% probability of coming up heads, what is the probability exactly 2 of them are heads?

**Steps**

1. Answer the above question using the `dbinom()` function. This function takes almost the same arguments as `rbinom()`. The second and third arguments are `size` and `prob`, but now the first argument is `x` instead of `n`. Use `x` to specify where you want to evaluate the binomial density.
2. Confirm your answer using the `rbinom()` function by creating a simulation of 10,000 trials. Put this all on one line by wrapping the `mean()` function around the `rbinom()` function.

```{r}
# Calculate the probability that 2 are heads using dbinom
dbinom(2, 10, .3)

# Confirm your answer with a simulation using rbinom
mean(rbinom(10000, 10, .3) == 2)
```

Awesome job on this exercise!

## Calculating cumulative density of a binomial

If you flip ten coins that each have a 30% probability of heads, what is the probability **at least** five are heads?

**Steps**

1. Answer the above question using the `pbinom()` function. (Note that you can compute the probability that the number of heads is less than or equal to 4, then take 1 - that probability).
2. Confirm your answer with a simulation of 10,000 trials by finding the number of trials that result in 5 or more heads.

```{r}
# Calculate the probability that at least five coins are heads
1 - pbinom(4, 10, .3)

# Confirm your answer with a simulation of 10,000 trials
mean(rbinom(10000, 10, .3) >= 5)
```

Good work simulating!

## Varying the number of trials

In the last exercise you tried flipping ten coins with a 30% probability of heads to find the probability **at least** five are heads. You found that the exact answer was `1 - pbinom(4, 10, .3)` = 0.1502683, then confirmed with 10,000 simulated trials.

Did you need all 10,000 trials to get an accurate answer? Would your answer have been more accurate with more trials?

**Steps**

1. Try answering this question with simulations of 100, 1,000, 10,000, 100,000 trials.
2. Which is the closest to the exact answer?

```{r}
# Here is how you computed the answer in the last problem
mean(rbinom(10000, 10, .3) >= 5)

# Try now with 100, 1000, 10,000, and 100,000 trials
mean(rbinom(100, 10, .3) >= 5)
mean(rbinom(1000, 10, .3) >= 5)
mean(rbinom(10000, 10, .3) >= 5)
mean(rbinom(100000, 10, .3) >= 5)
```

Awesome work on all those simulations!

## Expected value and variance

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Expected value and variance**

When we talk about a probability distribution, we're often interested in summarizing it into a few descriptive statistics.

**2. Properties of a distribution**

Two of the most interesting properties are where the distribution is centered, and how widely spread out it is. We describe these with the expected value and the variance.

**3. Expected value**

The expected value is the mean of the distribution. If you imagine we drew an infinite number of values from the distribution, the expected value is what the average of all those would be. This puts it right at the center of a distribution.Let's try to find the expected value of the binomial distribution with size 10, and probability point-5. We can't draw an infinite number of values, but we can draw a lot of them.As you've done in the exercises, we can use rbinom to simulate one hundred thousand draws with size 10 and probability point-5, then use the mean() function to take the average of these draws. We see the average is very close to 5. That's the "center" of the distribution if we displayed it as a histogram.If we tried sampling from a binomial with size 100 and probability point-2, we find that the mean is very close to 20.As you might notice from these examples, there's a general rule: we can get the expected value of a binomial distribution by multiplying the size (or the number of coins), by the probability each is heads.

**4. Variance**

The expected value measures the center of the distribution, but we also want a measure of how spread out the results are. Statisticians use the variance to measure this. Variance is the average squared distance of each value from the mean of the sample. The variance isn't quite as intuitive as the mean, but it has useful mathematical properties that will become clear in this course.R provides the var() function to calculate the variance from a particular sample. So we could simulate 100,000 draws of a binomial distribution with size 10 and probability point-5, then use var to find the variance of that distribution. We'd see that the variance is very close to 2-point-5. We saw in the last slide that the mean of this distribution is 5, so that means 2-point-5 is the average squared distance between 5 and one random draw.The variance of the binomial distribution in general follows a particular rule, which is that the variance is the size times p times 1 - p. So for example, the variance of the binomial with parameters 10 and point-5 is 10 times /5 time 1 minus point-5: which is 2-point-5, just as we saw in the simulation.We could try this with a second binomial distribution, with size 100 and probability point-2, and we'd see that the variance is 16, the same value we'd get by multiplying 100 times point-2 times 1 - point-2.Just like the expected value, simulation gives us a way to estimate properties of the distribution by drawing many values, while mathematical rules can sometimes also give you an exact answer.

**5. Rules for expected value and variance**

We thus have two rules for the properties of the binomial distribution. The expected value of the binomial is the size times p, and the variance of the binomial is the size times p times 1 - p.

**6. Let's practice!**



## Calculating the expected value

What is the expected value of a binomial distribution where 25 coins are flipped, each having a 30% chance of heads?

**Steps**

1. Calculate this using the exact formula you learned in the lecture: the expected value of the binomial is size * p. Print this result to the screen.
2. Confirm with a simulation of 10,000 draws from the binomial.

```{r}
# Calculate the expected value using the exact formula
25 * .3

# Confirm with a simulation using rbinom
mean(rbinom(10000, 25, .3))
```

Great work! Do you prefer using the simulation or the exact formula to find the mean?

## Calculating the variance

What is the variance of a binomial distribution where 25 coins are flipped, each having a 30% chance of heads?

**Steps**

1. Calculate this using the exact formula you learned in the lecture: the variance of the binomial is size * p * (1 - p). Print this result to the screen.
2. Confirm with a simulation of 10,000 trials.

```{r}
# Calculate the variance using the exact formula
25 * .3 * (1 - .3)

# Confirm with a simulation using rbinom
var(rbinom(10000, 25, .3))
```

Awesome job! You can simulate the variance just like you can with the mean.

# 2. Laws of probability

In this chapter you'll learn to combine multiple probabilities, such as the probability two events both happen or that at least one happens, and confirm each with random simulations. You'll also learn some of the properties of adding and multiplying random variables.

## Probability of event A and event B

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Probability of event A and event B**

So far we've been using coin flips as a simple example of random phenomena. But let's step back from that to talk about what a coin flip represents: a random variable that is either "yes" or "no."

**2. Event A: "Coin is heads"**

For example, A could represent "a coin flip results in heads." A then either happens- it's heads- or it doesn't- it's tails: with some probability. Throughout this chapter, we'll refer to coin flips and events interchangeably - we could say "A is an event with probability 20%" or "A is a coin with probability 20% of being heads." In reality, these events could represent the probability it is snowing outside, or the probability that someone will click an advertisement on a website. And in this chapter, you'll learn some of the mathematical laws that govern these kinds of random events and let us make predictions about them.

**3. Events A and B: Two Different Coins**

Now let's consider if we have two events, A and B. Event A is the result of one flip, either 1 for heads or 0 for tails, each with some probability, and event B is the result of the second flip.Now suppose you want to know the probability of A and B: that is, the probability that both flips are heads?

**4. Probability of A and B**

Well, consider this in terms of nested branches. First, you flip coin A. There's some probability that it results in heads, and some probability it results in tails. This branches into two possibilities, A = 1 or A = 0.Second, you flip coin B. This causes both to branch again, separately. Only that one branch where both A and B resulted in 1, or heads, counts as A AND B. We represent that by multiplying those two probabilities, to represent the probability of choosing one branch after the other. The probability of A and B is the probability of A times the probability of B.Note that this is true only if events A and B are independent: that is, if the result of A doesn't affect the probability of B. That is generally true of two different coin flips, and of all the cases we'll examine in this chapter.

**5. Simulating two coins**

To confirm this, let's try a simulation of many coin flips. We can simulate 100,000 flips of coin A. Notice that we've set the parameters so that each draw has only one flip, and each flip has a 50% chance of being heads. We can then simulate 100,000 flips of coin B separately.Once we have all the flips, we can compare all the pairs of flips. R lets you combine them with the "AND" operator, a single ampersand. This will compare each corresponding flip in A and B, and result in TRUE if and only if both A and B are true. We thus get a sequence of TRUEs and FALSEs for each of the 100,000 pairs.Once we have these, we can take the mean to find the percentage that are TRUE, just as we did in the simulations last chapter. In this case, we see that A and B were both true in about 25% of the simulations. This is the probability that two fair coins will both result in heads. This confirms our rule: the probability both A and B are true is the probability of A times the probability of B, or point-5 times point-5 = point-25.We could use the same simulation approach if the coins weren't fair: that is, if A and B didn't each have a 50% probability.For example, we could set event A to have a 10% probability, by setting the third argument of rbinom to point-1, and set event B to have a probability of 70%, or point-7.When we again combine A and B, we see that about 7% of the pairs are both true. This once again matches what we'd expect: the probability of A and B is point-1 times point-7. You'll be able to apply and simulate this rule in the exercises, and learn more about other rules for combining probabilities throughout this chapter.

**6. Let's practice!**



## Solving for probability of A and B

If events A and B are independent, and A has a 40% chance of happening, and event B has a 20% chance of happening, what is the probability they will both happen?

> *Question*
> ---
> ???<br>
> <br>
> ✅ 8%<br>
> ⬜ 12%<br>
> ⬜ 20%<br>
> ⬜ 60%<br>

That's right! You're a probability whiz!

## Simulating the probability of A and B

You can also use simulation to estimate the probability of two events both happening.

**Steps**

1. Randomly simulate 100,000 flips of coin A, each of which has a 40% chance of being heads. Save this as a variable `A`.
2. Randomly simulate 100,000 flips of coin B, each of which has a 20% chance of being heads. Save this as a variable `B`.
3. Use the "and" operator (`&`) to combine the variables `A` and `B` to estimate the probability that both A and B are heads.

```{r}
# Simulate 100,000 flips of a coin with a 40% chance of heads
A <- rbinom(100000, 1, .4)

# Simulate 100,000 flips of a coin with a 20% chance of heads
B <- rbinom(100000, 1, .2)

# Estimate the probability both A and B are heads
mean(A & B)
```

You're a whiz at this simulation stuff!

## Simulating the probability of A, B, and C

Randomly simulate 100,000 flips of A (40% chance), B (20% chance), and C (70% chance). What fraction of the time do all three coins come up heads?

**Steps**

1. You've already simulated A and B. Now simulate 100,000 flips of coin C, where each has a 70% chance of coming up heads.
2. Use `A`, `B`, and `C` to estimate the probability that all three coins would come up heads.

```{r}
# You've already simulated 100,000 flips of coins A and B
A <- rbinom(100000, 1, .4)
B <- rbinom(100000, 1, .2)

# Simulate 100,000 flips of coin C (70% chance of heads)
C <- rbinom(100000, 1, .7)

# Estimate the probability A, B, and C are all heads
mean(A & B & C)
```

Great job! That was much easier than using a nasty formula.

## Probability of A or B

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Probability of A or B**

In the last exercises, you learned a rule for finding the probability that event A and event B happened. Now you'll learn how to calculate the probability that either event A or B happens. For example, suppose I flip this coin twice: what is the probability that at least one of the flips is heads?

**2. Probability of A or B**

You can imagine this as two overlapping circles in a Venn diagram. One circle represents whether event A happens, such as the first flip being heads, and one whether B happens, such as the second flip being heads. The probability that either happens is the total region.To find that overall probability, you can start by adding up the areas of the two circles. But then you'll be double counting the region where they overlap where they both happen, so you have to subtract that as well.This means that the probability of A or B happening is the probability of A, plus the probability of B, minus the overlap- the probability of both A and B. In the last lessons you learned that the probability of A and B, as long as A and B are independent, is the probability of A times the probability of B.So if A and B are independent flips of a fair coin, we can take the probability of the first being heads - 50%- plus the probability of the second being heads - 50% - and subtract the probability they both are heads - point-5 times point-5.

**3. Simulating two events**

Let's try this in a simulation. We can start by simulating one hundred thousand fair coin flips for A, and one hundred thousand fair coin flips for B. Then we want to compare pairs of them.To do this, we'll introduce the "or" logical operator, which in R is represented by a vertical pipe. On most keyboards you'll find that pipe above the return or enter key. Then much as we did for "and", we'll take the mean to estimate the probability of either happening.Here you can see that it's about 75%. This matches the number we earlier got from the formula: point-5 plus point-5 minus point-5 times point-5.This also works if our coins are biased. If event A has a 20% probability, and event B has a 60% probability, what is the probability that either event happens? This simulation shows that it's about 68%. This also matches what we would get from our rule: point-2 plus point-6 minus point-2 times point-6.

**4. Three coins**

One advantage of a simulation approach is that it extends to cases where mathematical solutions would get cumbersome. For example, suppose we have three coins, and want to know the probability that any of them three of them is heads. that is, the probability A or B or C.The formula for three events is a bit complicated- and you don't need to memorize it. But if you've simulated the events A, B, and C, you can simply combine all three with an "or" operator and then take the mean. You could even do this with four events, or five events. You'll try a problem like this in the exercises.

**5. Let's practice!**



## Solving for probability of A or B

If coins A and B are independent, and A has a 60% chance of coming up heads, and event B has a 10% chance of coming up heads, what is the probability either A or B will come up heads?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ 6%<br>
> ⬜ 50%<br>
> ✅ 64%<br>
> ⬜ 70%<br>

That's right, great job!

## Simulating probability of A or B

In the last multiple choice exercise you found that there was a 64% chance that either coin A (60% chance) or coin B (10% chance) would come up heads. Now you'll confirm that answer using simulation.

**Steps**

1. Use `rbinom()` to simulate 100,000 flips of coin A, each having a 60% chance of being heads.
2. Use `rbinom()` to simulate 100,000 flips of coin B, each having a 10% chance of being heads.
3. Use these to estimate the probability that A or B is heads.

```{r}
# Simulate 100,000 flips of a coin with a 60% chance of heads
A <- rbinom(100000, 1, .6)

# Simulate 100,000 flips of a coin with a 10% chance of heads
B <- rbinom(100000, 1, .1)

# Estimate the probability either A or B is heads
mean(A | B)
```

Great work! How does the simulated probability compare to the one you calculated in the last exercise?

## Probability either variable is less than or equal to 4

Suppose X is a random Binom(10, .6) variable (10 flips of a coin with 60% chance of heads) and Y is a random Binom(10, .7) variable (10 flips of a coin with a 70% chance of heads), and they are independent.

What is the probability that either of the variables is less than or equal to 4?

**Steps**

1. Simulate 100,000 draws from each of X (10 coins, 60% chance of heads) and Y (10 coins, 70% chance of heads) binomial variables, saving them as `X` and `Y` respectively.
2. Use these simulations to estimate the probability that either X or Y is less than or equal to 4.
3. Use the `pbinom()` function to calculate the exact probability that X is less than or equal to 4, then the probability that Y is less than or equal to 4.
4. Combine these two exact probabilities to calculate the exact probability that either variable is less than or equal to 4.

```{r}
# Use rbinom to simulate 100,000 draws from each of X and Y
X <- rbinom(100000, 10, .6)
Y <- rbinom(100000, 10, .7)

# Estimate the probability either X or Y is <= to 4
mean(X <= 4 | Y <= 4)

# Use pbinom to calculate the probabilities separately
prob_X_less <- pbinom(4, 10, .6)
prob_Y_less <- pbinom(4, 10, .7)

# Combine these to calculate the exact probability either <= 4
prob_X_less + prob_Y_less - prob_X_less * prob_Y_less
```

Awesome! Simulation is a great way of avoiding difficult calculations.

## Multiplying random variables

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Multiplying random variables**

Imagine I flip this fair coin ten times and count the number of heads. Then I take that number and triple it. What could you tell me about the resulting number? You don't know exactly what it is, but could you tell me its mean, or its variance?Just like there are laws of probability for combining events, there are laws for manipulating random variables. In these next lessons you'll learn to multiply random variables with a constant, or combine them together.

**2. Multiplying a random variable**

Suppose X is a random variable containing the result of flipping a fair coin ten times. We could take draws from X- one draw could be 5, another could be 7, another could be 4, each representing ten flips of a coin.In probability, it's important to get into the habit of manipulating random variables kind of like they were algebraic symbols. So we could imagine defining a new random variable Y, which is 3 times X. If X were 5, Y would be 15. If X were 7, Y would be 21.Now we want to know the properties of Y, such as its expected value and variance. To do that, imagine the histogram of X, then compare it to the histogram of X times 3 = Y. Notice that the shape of Y is the same, but it is both larger, and more spread out. So we would expect both the expected value and the variance to increase.

**3. Simulation: Effect of multiplying on expected value**

Let's see what the exact effect of multiplying by 3 is through simulation. Try taking one hundred thousand draws from X- a binomial with 10 flips of a fair coin. We can take the mean and check that the expected value is about 5: that's 10 flips times the probability .5.To get a sample from Y, we multiply our sample of X by three. Note that 3 times X will multiply every individual value by 3. Thus, we went from one hundred thousand draws of X, to one hundred thousand draws from Y.We can find the expected value of Y by taking the mean, and we see that it's about 15. Thus, when we multiplied a random variable by 3, we also multiplied the expected value by 3. This makes sense because we can see that the distribution has roughly the same shape, it's just three times larger.This is a general rule: when you multiply a random variable by a constant k, you also multiply the expected value by k.

**4. Simulation: Effect of multiplying on variance**

We can also examine what happens to the variance. The variance of X is about 2-point-5: recall that that's the size times p times 1 minus p.Now when we multiply it by 3 to get Y, the variance becomes 22-point-5: it's increased by a factor of 9.Why is it nine? Because that's three squared, and the variance is the average squared distance of values from the mean. So when the distribution became three times wider, the variance increases by nine.

**5. Rules of manipulating random variables**

This gives us two general rules for the properties of random variables when they're multiplied by a constant.You're learning about the binomial in these chapters, but these properties hold true no matter what the distribution the random variable follows, so they're useful in many applications of probability.

**6. Let's practice!**



## Expected value of multiplying a random variable

If X is a binomial with size 50 and p = .4, what is the expected value of 3*X?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ 1.2<br>
> ⬜ 20<br>
> ✅ 60<br>
> ⬜ 150<br>

Great, you got it!

## Simulating multiplying a random variable

In this exercise you'll use simulation to confirm the rule you just learned about how multiplying a random variable by a constant effects its expected value.

**Steps**

1. Simulate 100,000 draws of X, a binomial random variable with size 20 and p = .1. Save this as `X`
2. Use this simulation to estimate the expected value of X.
3. Use this simulation to estimate the expected value of 5*X, as well.

```{r}
# Simulate 100,000 draws of a binomial with size 20 and p = .1
X <- rbinom(100000, 20, .1)

# Estimate the expected value of X
mean(X)

# Estimate the expected value of 5 * X
mean(5 * X)
```

Fantastic! You're a pro at this simulation stuff!

## Variance of a multiplied random variable

In the last exercise you simulated X from a binomial with size 20 and p = .1 and now you'll use this same simulation to explore the variance.

**Steps**

1. Use this simulation to estimate the variance of X.
2. Estimate the variance of 5 * X

```{r}
# X is simulated from 100,000 draws of a binomial with size 20 and p = .1
X <- rbinom(100000, 20, .1)

# Estimate the variance of X
var(X)

# Estimate the variance of 5 * X
var(5 * X)
```

Stellar work! How does the variance compare to the mean?

## Adding two random variables

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Adding two random variables together**

In the last lesson you learned how to multiply one random variable by a constant. Now you'll learn about adding multiple random variables together.

**2. Adding two random variables**

Suppose we defined two random variables, X and Y. X is the result of flipping ten coins that each have a point-5 probability of heads, and Y is the result of flipping one hundred coins that each have point-2 probability of heads. Assume these are independent random variables: you flipped the coins separately.Now suppose we add those two random variables together to get a random variable Z. If we get six heads in X and 22 in Y, Z would be equal to 28.Here's a histogram of the distributions of X, Y, and Z. Notice that Z is both larger and more spread out than either X or Y. While the distribution looks somewhat similar to X and Y, Z doesn't actually follow a binomial distribution, but we can still make some predictions about its properties.

**3. Simulation: expected value of X + Y**

We can simulate Z to find out its properties., First simulate one hundred thousand draws from X, each with 10 flips of a fair coin. As we've seen before, the expected value of X is about 5. We also simulate a hundred thousand flips from Y. You can see that the expected value of Y is about 20, which we could have predicted with 100 times point-2.Now we create a variable Z, which is X plus Y. When we take the mean, we see that the expected value of Z is about 25. Notice that that's 5 plus 20: the sum of each of the variable's means.This is a general rule: the expected value of X plus Y is the expected value of X plus the expected value of Y.

**4. Simulation: variance of X + Y**

What about the variance of Z? We see in the histogram that it's more spread out than either X or Y. And indeed, the variance of our simulated X is about 2-point-5, and the variance of the simulated Y is about 16, and the variance of Z is about 18-point-5. Notice that that's the variance of X plus the variance of Y.So this follows a rule much like the expected value: the variance of the sum of two independent random variables is the sum of their variances.

**5. Rules for combining random variables**

This gives us our two general rules for the properties of the sum of random variables.  One note is that the rule for the expected value is true even if X and Y aren't independent- that is, even if outcome of one influences the probabilities of the other. However, the rule for adding together variances is true only if X and Y are independent.This is important in probability, but note that your exercises will only involve summing independent variables.

**6. Let's practice!**



## Solving for the sum of two binomial variables

If X is drawn from a binomial with size 20 and p = .3, and Y from size 40 and p = .1, what is the expected value (mean) of X + Y?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ 4<br>
> ⬜ 6<br>
> ✅ 10<br>
> ⬜ 60<br>

Great work!

## Simulating adding two binomial variables

In the last multiple choice exercise, you found the expected value of the sum of two binomials. In this problem you'll use a simulation to confirm your answer.

**Steps**

1. Simulate 100,000 draws from X, a binomial with size 20 and p = .3, and Y, with size 40 and p = .1.
2. Use this simulation to estimate the expected value of X + Y.

```{r}
# Simulate 100,000 draws of X (size 20, p = .3) and Y (size 40, p = .1)
X <- rbinom(100000, 20, .3) 
Y <- rbinom(100000, 40, .1)

# Estimate the expected value of X + Y
mean(X + Y)
```

Great job! The fact that you can add the means makes stuff much simpler.

## Simulating variance of sum of two binomial variables

In the last multiple choice exercise, you examined the expected value of the sum of two binomials. Here you'll estimate the variance.

**Steps**

1. Use your simulation of the variables X and Y to estimate the variance of `X + Y`.
2. Use your simulation to estimate the variance of `3 * X + Y`.

```{r}
# Simulation from last exercise of 100,000 draws from X and Y
X <- rbinom(100000, 20, .3) 
Y <- rbinom(100000, 40, .1)

# Find the variance of X + Y
var(X + Y)

# Find the variance of 3 * X + Y
var(3 * X + Y)
```

Great simulating! Remember this rule only works when X and Y are independent.

# 3. Bayesian statistics

Bayesian statistics is a mathematically rigorous method for updating your beliefs based on evidence. In this chapter, you'll learn to apply Bayes' theorem to draw conclusions about whether a coin is fair or biased, and back it up with simulations.

## Updating with evidence

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Updating from evidence**

Throughout this course we've been talking about biased coins: coins that have a chance of heads that isn't 50%. And that knowledge is going to come in handy, because I'm worried I could have a biased coin here.My friend Nick and I made a bet on this coin- heads I pay him, tails he pays me. It's Nick's coin, though, and I'm not sure I trust him, because I think he might have given me a trick coin that so that it comes up heads 75% of the time. How can we tell whether this coin is fair, or biased?

**2. 20 flips of a coin**

Well, we could run an experiment. Suppose after flipping twenty times, we see 14 heads, and 6 tails. Now that we've seen that, do we believe that the coin is fair or biased, and could we give a probability?We're describing the process of updating our beliefs after seeing evidence, which is at the heart of Bayesian statistics. This chapter will talk about Bayesian statistics in terms of determining whether a coin is fair or biased based on evidence.

**3. Two piles of 50,000 coins**

Let's imagine that before we ran the experiment, we think there is a fifty percent chance the coin is fair, and fifty percent chance it's biased. Picture that as two piles of coins: one of 50,000 fair coins, one of 50,000 biased coins.Imagine that you took every coin from each of the two piles, flipped it 20 times, and recorded the results. You would get two histograms like this: one for the fair coins, one for the biased coins.Here's the trick of Bayesian statistics: when we see 14 heads out of 20, and we know that it's either a fair or biased coin, we know that we're in one of those red bars in the histogram. All we need to know is which.Let's run this as a simulation. First go to every coin in the 50,000 fair coins, flip it twenty times, and record the number of heads. Then we find out how many were 14s. There were nearly two thousand coins that resulted in 14 heads. So it is possible to get that from a fair coin.We do the same to see how many biased coins resulted in 14 heads. Notice that this time we changed the probability to .75.The original piles were of equal size, but a lot more of the biased coins ended up resulting in 14 heads. Notice that the red bar is a lot taller in the "biased" histogram than the fair histogram. And we can add them up to see that between the two piles, there were a total of 10 thousand two hundred and sixty coins that resulted in 14 heads- that is, the total of the two red bars.Now we can get the conditional probability: the probability the coin is biased given the condition that we got 14 heads. Note the vertical line in the probability means "given" in this context. We take the number of biased coins that resulted in 14,  and divide it by the total number of coins that resulted in 14. This gives us an 82% chance the coin is biased.We originally thought there was a 50% probability the coin was biased (that is, the piles were equal size), but conditional on seeing 14 heads, the probability has been updated to 82%. If we'd seen, say, 9 heads out of 20, which is more likely to come from a fair coin than a biased coin, the probability would have been updated in the opposite direction.In the exercises, you'll run more simulations like this to find conditional probabilities based on other outcomes.

**4. Let's practice!**



## Updating

Suppose you have a coin that is equally likely to be fair (50% heads) or biased (75% heads). You then flip the coin 20 times and see 11 heads.

Without doing any math, which do you now think is more likely- that the coin is fair, or that the coin is biased?

> *Question*
> ---
> ???<br>
> <br>
> ✅ More likely that the coin is fair<br>
> ⬜ More likely that the coin is biased<br>
> ⬜ Still equally likely<br>

Great! That's exactly right.

## Updating with simulation

We see 11 out of 20 flips from a coin that is either fair (50% chance of heads) or biased (75% chance of heads). How likely is it that the coin is fair? Answer this by simulating 50,000 fair coins and 50,000 biased coins.

**Steps**

1. Simulate 50,000 cases of flipping 20 coins from a fair coin (50% chance of heads), as well as from a biased coin (75% chance of heads). Save these variables as `fair` and `biased` respectively.
2. Find the number of fair coins where exactly 11/20 came up heads, then the number of biased coins where exactly 11/20 came up heads. Save them as `fair_11` and `biased_11` respectively.
3. Find the fraction of all coins that came up heads 11 times that were fair coins- this is the posterior probability that a coin with 11/20 is fair.

```{r}
# Simulate 50000 cases of flipping 20 coins from fair and from biased
fair <- rbinom(50000, 20, .5)
biased <- rbinom(50000, 20, .75)

# How many fair cases, and how many biased, led to exactly 11 heads?
fair_11 <- sum(fair == 11)
biased_11 <- sum(biased == 11)

# Find the fraction of fair coins that are 11 out of all coins that were 11
fair_11 / (fair_11 + biased_11)
```

Great job! How does this compare with your intuition?

## Updating after 16 heads

Suppose that when you flip a different coin (that could either be fair or biased) 20 times, you see 16 heads.

Without doing any math, which do you now think is more likely- that this coin is fair, or that it's biased?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ More likely that the coin is fair.<br>
> ✅ More likely that the coin is biased.<br>
> ⬜ Still equally likely.<br>

Great thinking!

## Updating with simulation after 16 heads

We see 16 out of 20 flips from a coin that is either fair (50% chance of heads) or biased (75% chance of heads). How likely is it that the coin is fair?

**Steps**

1. Simulate 50,000 cases of flipping 20 coins from a fair coin (50% chance of heads), as well as from a biased coin (75% chance of heads). Save these variables as `fair` and `biased` respectively.
2. Find the number of fair coins where exactly 16/20 came up heads, then the number of biased coins where exactly 16/20 came up heads. Save them as `fair_16` and `biased_16` respectively.
3. Print the fraction of all coins that came up heads 16 times that were fair coins- this is the posterior probability that a coin with 16/20 is fair.

```{r}
# Simulate 50000 cases of flipping 20 coins from fair and from biased
fair <- rbinom(50000, 20, .5)
biased <- rbinom(50000, 20, .75)

# How many fair cases, and how many biased, led to exactly 16 heads?
fair_16 <- sum(fair == 16)
biased_16 <- sum(biased == 16)

# Find the fraction of fair coins that are 16 out of all coins that were 16
fair_16 / (fair_16 + biased_16)
```

Fantastic work! How does this compare with your intuition from the last exercise?

## Prior probability

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Prior probability**

In these exercises we've been determining whether a coin we got from Nick is fair or biased. We've treated it as if before we saw any flips, there's a 50% chance that the coin is fair, and a 50% chance that the coin is biased towards heads.But let's say I generally trust Nick. When he gave me this coin, I figured he probably wasn't trying to trick me. I'm just testing the coin to be completely safe.Let's say that when Nick gives me the coin, I think there's only 10% chance that it's biased, and a 90% chance that it's fair. This is called a prior probability, and it's an important part of Bayesian statistics.

**2. Differently sized piles**

Let's go back to the two piles of coins from the last lesson: one of the piles is fair, one of them biased, and we flip each one of them 20 times. We know that Nick got his coin from one of these two piles. And by comparing the sizes of the red bars- the cases where a coin resulted in exactly 14 heads- we were able to find a conditional probability.However, this time, instead of having two equally sized piles, let's start with 90,000 coins in the fair pile and only 10,000 biased coins in the other pile. This represents our prior probability, giving the fair coins an advantage. Notice that in the resulting histograms, the relative height of the red bars- those with 14 heads- has changed. In fact, even though each of the fair coins individually was less likely to result in 14 heads than a biased coin was, there were more fair coins that ended up with 14 than biased ones.

**3. Simulating with differently sized piles**

We can simulate these piles to find the probability that a coin with 14 heads is biased, given our prior probability of 10%.We first simulate 90,000 draws from the binomial where the coins are fair: notice that we changed the first argument, the number of draws, to 90,000. We see that three thousand, four hundred and ten of these draws resulted in 14 heads.We then simulate only 10,000 draws of biased coins, each with a 75% chance of heads. We see that one thousand seven hundred and six of them led to 14 heads.Now we can find our conditional probability. Given that we are using one of those coins that resulted in 14 heads, what fraction of those were biased? We find that it's about 33%. We originally thought there was a 10% chance the coin was biased, but after seeing 14 heads out of 20 flips, our probability has updated to 33%.This simulation approach works even if there are more than two possibilities for the probability of heads. For example, you could start with three piles of coins, one that has a 25% chance, one that has a 50% chance, and one that has a 75% chance. You'll try an example like that in the exercises.

**4. Let's practice!**



## Updating with priors

We see 14 out of 20 flips are heads, and start with a 80% chance the coin is fair and a 20% chance it is biased to 75%.

You'll solve this case with simulation, by starting with a "bucket" of 10,000 coins, where 8,000 are fair and 2,000 are biased, and flipping each of them 20 times.

**Steps**

1. Simulate 8,000 trials of flipping a fair coin 20 times and 2,000 trials of flipping a biased coin 20 times. Save them as `fair_flips` and `biased_flips`, respectively.
2. Find the number of cases that resulted in 14 heads from each coin, saving them as `fair_14` and `biased_14` respectively.
3. Find the fraction of all coins that resulted in 14 heads that were fair: this is an estimate of the posterior probability that the coin is fair.

```{r}
# Simulate 8000 cases of flipping a fair coin, and 2000 of a biased coin
fair_flips <- rbinom(8000, 20, .5)
biased_flips <- rbinom(2000, 20, .75)

# Find the number of cases from each coin that resulted in 14/20
fair_14 <- sum(fair_flips == 14)
biased_14 <- sum(biased_flips == 14)

# Use these to estimate the posterior probability
fair_14 / (fair_14 + biased_14)
```

Awesome! How did adding a prior into the mix change your outcome?

## Updating with three coins

Suppose instead of a coin being either fair or biased, there are three possibilities: that the coin is fair (50% heads), low (25% heads), and high (75% heads). There is a 80% chance it is fair, a 10% chance it is biased low, and a 10% chance it is biased high.

You see 14/20 flips are heads. What is the probability that the coin is fair?

**Steps**

1. Use the `rbinom()` function to simulate 80,000 draws from the fair coin, 10,000 draws from the high coin, and 10,000 draws from the low coin, with each draw containing 20 flips. Save them as `flips_fair`, `flips_high`, and `flips_low`, respectively.
2. For each of these types, compute the number of coins that resulted in 14. Save them as `fair_14`, `high_14`, and `low_14`, respectively.
3. Find the posterior probability that the coin was fair, by dividing the number of fair coins resulting in 14 from the total number of coins resulting in 14.

```{r}
# Simulate 80,000 draws from fair coin, 10,000 from each of high and low coins
flips_fair <- rbinom(80000, 20, .5)
flips_high <- rbinom(10000, 20, .75)
flips_low <- rbinom(10000, 20, .25)

# Compute the number of coins that resulted in 14 heads from each of these piles
fair_14 <- sum(flips_fair == 14)
high_14 <- sum(flips_high == 14)
low_14 <- sum(flips_low == 14)

# Compute the posterior probability that the coin was fair
fair_14 / (fair_14 + low_14 + high_14)
```

Wow! Great work! Adding another coin doesn't make things too much harder, does it?

## Bayes' theorem

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. Bayes' theorem**

Throughout this chapter, we've used simulation to estimate a conditional probability that a coin is fair or biased.

**2. Probabilities**

For example, we looked at a distribution of 90,000 fair coins and 10,000 biased coins, and saw how many coins from each pile resulted in 14 heads. We saw about three thousand resulting in 14 heads among the fair coins, and about fifteen hundred among the biased coins.But what we're really working with in these simulations is probability densities. Consider this histogram in terms of the probability a coin out of the one hundred thousand we're flipping ends up with that outcome.In this lesson, rather than simulating many coins to determine whether a coin is biased, we'll use probability density- specifically the dbinom function- to find an exact answer. In the process, we'll introduce one of the most important equations in probability: Bayes' Theorem.

**3. Probability of fair coin with 14 heads**

Recall that we used simulation to find the number of fair coins resulting in 14, out of the 90 thousand in the original pile. Now, rather than simulating it with rbinom, let's find the exact probability of 14 heads with the dbinom function. But notice we also multiply it with the prior probability that the coin is fair, .9. That's the equivalent of the "90,000" in the rbinom line: it calculates the probability the coin is both fair and resulted in 14 heads.We also simulated 10,000 biased coins and found the number of times there were 14 heads out of 20 flips. We could replace that with dbinom using a .75 probability, and multiplying it by the .1 prior probability that a coin was biased. These get the exact probabilities shown in red in the histograms.

**4. Conditional probability**

Now that we have those probabilities, we can combine them the same way we did the coins.To find the probability a coin is biased conditional on it resulting in 14 heads out of 20, we look at the probability it is both 14 heads and biased out of the total probability that it resulted in 14 heads. This is the equivalent of looking at "both of the red bars", and asking which red bar the coin is in.As we did in the last slide, we can compute each of those probabilities by multiplying the probability density by the prior probability. In the numerator we're multiplying the probability a biased coin would give 14 heads, by the prior probability the coin is biased.You're now able to calculate this in R, by using dbinom to calculate each of the densities, multiplying each by the prior probabilities, and then putting them into a fraction.  You'll be able to try this in the exercises.

**5. Bayes' Theorem**

This exact solution is an application of Bayes' Theorem. Bayes' Theorem is usually written in terms of finding the probability of event A given event B when you knew the probability of event B given event A. In this problem we explored in this chapter, event A is that the coin is biased, and event B is that it resulted in 14 heads out of 20. This is why Bayes Theorem was useful: we knew the probability of getting 14 heads given that the coin is biased, but we needed to convert it to the probability that a coin is biased given that it resulted in 14 heads.We could have just started the chapter with this equation, but our simulation showed what the numerator and denominator really represent, by imagining what fraction of all coins resulting in 14 heads were biased. This will help you understand and apply the theorem to calculate conditional probabilities in the future.

**6. Let's practice!**



## Updating with Bayes theorem

In this chapter, you used simulation to estimate the posterior probability that a coin that resulted in 11 heads out of 20 is fair. Now you'll calculate it again, this time using the exact probabilities from `dbinom()`. There is a 50% chance the coin is fair and a 50% chance the coin is biased.

**Steps**

1. Use the `dbinom()` function to calculate the exact probability of getting 11 heads out of 20 flips with a fair coin (50% chance of heads) and with a biased coin (75% chance of heads). Save them as `probability_fair` and `probability_biased`, respectively.
2. Use these to calculate the posterior probability that the coin is fair. This is the probability that you would get 11 from a fair coin, divided by the sum of the two probabilities.

```{r}
# Use dbinom to calculate the probability of 11/20 heads with fair or biased coin
probability_fair <- dbinom(11, 20, .5)
probability_biased <- dbinom(11, 20, .75)

# Calculate the posterior probability that the coin is fair
probability_fair / (probability_fair + probability_biased)
```

Awesome job! Do you think calculating or simulating the answer is easier? Which makes more sense?

## Updating for other outcomes

In the last exercise, you solved for the probability that the coin is fair if it results in 11 heads out of 20 flips, assuming that beforehand there was an equal chance of it being a fair coin or a biased coin. Recall that the code looked something like:

```{r}
probability_fair <- dbinom(11, 20, .5)
probability_biased <- dbinom(11, 20, .75)
probability_fair / (probability_fair + probability_biased)
```

Now you'll find, using the `dbinom()` approach, the posterior probability if there were two other outcomes.

**Steps**

1. Find the probability that a coin resulting in 14 heads out of 20 flips is fair.
2. Find the probability that a coin resulting in 18 heads out of 20 flips is fair.

```{r}
# Find the probability that a coin resulting in 14/20 is fair
dbinom(14, 20, .5) / (dbinom(14, 20, .5) + dbinom(14, 20, .75))

# Find the probability that a coin resulting in 18/20 is fair
dbinom(18, 20, .5) / (dbinom(18, 20, .5) + dbinom(18, 20, .75))
```

Wow! You're a Bayesian pro!

## More updating with priors

Suppose we see 16 heads out of 20 flips, which would normally be strong evidence that the coin is biased. However, suppose we had set a prior probability of a 99% chance that the coin is fair (50% chance of heads), and only a 1% chance that the coin is biased (75% chance of heads).

You'll solve this exercise by finding the exact answer with `dbinom()` and Bayes' theorem. Recall that Bayes' theorem looks like:

$$\\Pr(\\mbox{fair}|A)=\\frac{\\Pr(A|\\mbox{fair})\\Pr(\\mbox{fair})}{\\Pr(A|\\mbox{fair})\\Pr(\\mbox{fair})+\\Pr(A|\\mbox{biased})\\Pr(\\mbox{biased})}$$

**Steps**

1. Use `dbinom()` to calculate the probabilities that a fair coin and a biased coin would result in 16 heads out of 20 flips.
2. Use Bayes' theorem to find the posterior probability that the coin is fair, given that there is a **99% prior probability** that the coin is fair.

```{r}
# Use dbinom to find the probability of 16/20 from a fair or biased coin
probability_16_fair <- dbinom(16, 20, .5)
probability_16_biased <- dbinom(16, 20, .75)

# Use Bayes' theorem to find the posterior probability that the coin is fair
(probability_16_fair * .99) / (probability_16_fair * .99 + probability_16_biased * .01)
```

Amazing! It seems like your choice of prior can have a pretty big effect on the final answer.

# 4. Related distributions

So far we've been talking about the binomial distribution, but this is one of many probability distributions a random variable can take. In this chapter we'll introduce three more that are related to the binomial: the normal, the Poisson, and the geometric.

## The normal distribution

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. The normal distribution**

So far in this course, we've been exploring the binomial distribution, the result of flipping a coin multiple times and counting the number of heads. In this chapter we're going to introduce three other important probability distributions, and see how they each connect to the theme of flipping coins.You'll not only add these distributions to your statistical toolbox, but also see how the principles we've learned about working with random variables and simulation are important far beyond the binomial.

**2. Flipping 10 coins**

So far you've often looked at a binomial where each draw is made up of ten flips of a coin that has a 50% chance of heads. That would look like this distribution: a histogram where the most common value is 5.

**3. Flipping 1000 coins**

Now imagine that instead of flipping 10 or 20 fair coins in each draw, we flipped 1000. Now the distribution of the number of heads is centered around 500. Notice that the distribution is now taking on a sort of symmetrical "bell curve" shape.

**4. Flipping 1000 coins**

This is because when you draw from the binomial with a very large size (that is, many coins)- the result approximates a normal distribution. Approximations from one distribution to another are important in probability since they let you make connections between statistical tools.You'll sometimes hear a normal distribution referred to as a Gaussian distribution, or a bell curve. It's famous because many distributions in nature take up this shape, such as measurement errors in scientific experiments, and because its mathematical properties are well understood. Many of the courses on the DataCamp platform show how the normal distribution can be used in statistical inference.

**5. Normal distribution has mean and standard deviation**

While the binomial is defined in terms of parameters size and p-that is, the number of coins and their probability of heads-the normal is defined based on two other parameters: the mean and the standard deviation. Mathematically, we usually represent the mean as the Greek letter "mu", and standard deviation as the Greek letter sigma.In chapters 1 and 2 we introduced the concept of variance, the average squared distance from the mean. The standard deviation is the square root of the variance. While R defines the normal using the standard deviation, some other statisticians choose to define the normal distribution in terms of the variance. It's not that important which you choose, because if you know one-you know the other. You can square the standard deviation to get the variance, or take the square root of the variance to get the standard deviation.

**6. Normal approximation to the binomial**

Because the normal is defined in terms of the mean and variance, this makes it easy to find the normal approximation to a particular binomial distribution.For example, we could create a sample of 100,000 draws from the binomial distribution with size 1000 each and a probability of point-5. In Chapter 1 you learned how to compute the expected value and variance from these parameters. The expected value is the size, 1000, times the probability, point-5, and the variance is the size times the probability times 1 minus the probability, point-5. Since we want the standard deviation, we can take the square root of the variance.We can then simulate from the normal using these parameters. We do this with the rnorm function, rather than rbinom. The first argument is the number of draws, The second is the mean, the same as the expected value of the binomial, and the third is the desired standard deviation.

**7. Comparing histograms**

Now that we've simulated one hundred thousand draws from the binomial and from the corresponding normal, we'd like to compare them to see if the normal really is a good approximation.This course focuses on probability rather than visualization, so rather than have you build the graph yourself, we've provided a simple function: "compare_histograms", to which you simply provide the two simulated vectors. You'll notice the two histograms look very similar, confirming that this normal distribution is a good approximation to the binomial. You'll try some other comparisons in the exercises, including comparing the cumulative density between the two distributions.

**8. Let's practice!**



## Approximating a binomial to the normal

Suppose you flipped 1000 coins, each with a 20% chance of being heads. What would be the mean and variance of the binomial distribution?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ Mean 1000; variance 200<br>
> ⬜ Mean 200; variance 200<br>
> ✅ Mean 200; variance 160<br>
> ⬜ Mean 160; variance 160<br>

Great work, you got it!

## Simulating from the binomial and the normal

In this exercise you'll see for yourself whether the normal is a reasonable approximation to the binomial by simulating large samples from the binomial distribution and its normal approximation and comparing their histograms.

**Steps**

1. Generate 100,000 draws from the Binomial(1000, .2) distribution. Save this as `binom_sample`.
2. Generate 100,000 draws from the normal distribution that approximates this binomial distribution, using the `rnorm()` function. (Remember that `rnorm()` takes the mean and the standard deviation, which is the square root of the variance). Save this as `normal_sample`.
3. Compare the two distributions with the `compare_histograms()` function. (Remember that this takes two arguments: the first and second vectors to compare).

```{r}
compare_histograms <- function(variable1, variable2) {
                        x <- data.frame(value = variable1, variable = "Variable 1")
                        y <- data.frame(value = variable2, variable = "Variable 2")
                        ggplot(rbind(x, y), aes(value)) +
                          geom_histogram() +
                          facet_wrap(~ variable, nrow = 2)
                      }

# Draw a random sample of 100,000 from the Binomial(1000, .2) distribution
binom_sample <- rbinom(100000, 1000, .2)

# Draw a random sample of 100,000 from the normal approximation
normal_sample <- rnorm(100000, 200, sqrt(160))

# Compare the two distributions with the compare_histograms function
compare_histograms(binom_sample, normal_sample)
```

Great work! How similar do the two histograms look to you?

## Comparing the cumulative density of the binomial

If you flip 1000 coins that each have a 20% chance of being heads, what is the probability you would get 190 heads or fewer?

You'll get similar answers if you solve this with the binomial or its normal approximation. In this exercise, you'll solve it both ways, using both simulation and exact calculation.

**Steps**

1. Use the simulated `binom_sample` (provided) from the last exercise to estimate the probability of getting 190 or fewer heads.
2. Use the simulated `normal_sample` to estimate the probability of getting 190 or fewer heads.
3. Calculate the exact probability of the binomial being \n
4. Calculate the exact probability of the normal being \n

```{r}
# Simulations from the normal and binomial distributions
binom_sample <- rbinom(100000, 1000, .2)
normal_sample <- rnorm(100000, 200, sqrt(160))

# Use binom_sample to estimate the probability of <= 190 heads
mean(binom_sample <= 190)

# Use normal_sample to estimate the probability of <= 190 heads
mean(normal_sample <= 190)

# Calculate the probability of <= 190 heads with pbinom
pbinom(190, 1000, .2)

# Calculate the probability of <= 190 heads with pnorm
pnorm(190, 200, sqrt(160))
```

Great job! There are a lot of different ways to go about getting similar answers.

## Comparing the distributions of the normal and binomial for low n

When we flip a *lot* of coins, it looks like the normal distribution is a pretty close approximation. What about when we flip only 10 coins, each still having a 20% chance of coming up heads? Is the normal still a good approximation?

**Steps**

1. Generate 100,000 draws from the Binomial(10, .2) distribution. Save this as `binom_sample`.
2. Generate 100,000 draws from the normal distribution that approximates this binomial distribution, using the `rnorm()` function. Save this as `normal_sample`.
3. Compare the two distributions with the `compare_histograms()` function. (Remember that this takes two arguments: the two samples that are to be compared).

```{r}
# Draw a random sample of 100,000 from the Binomial(10, .2) distribution
binom_sample <- rbinom(100000, 10, .2)

# Draw a random sample of 100,000 from the normal approximation
normal_sample <- rnorm(100000, 2, sqrt(1.6))

# Compare the two distributions with the compare_histograms function
compare_histograms(binom_sample, normal_sample)
```

Good work! How do the sample size and the probability of success effect the accuracy of the normal approximation?

## The Poisson distribution

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. The Poisson distribution**

In the last chapter we tried flipping a very large number of coins, and we noticed that the resulting number of heads could be approximated by a normal distribution. In this lesson we'll introduce another distribution that's related to the binomial, the Poisson distribution.

**2. Flipping many coins, each with low probability**

Suppose that like the last lesson, we flip one thousand coins. But this time, every coin has only a one in one thousand probability of being heads. We're considering how often a rare event happens out of a large number of opportunities.We can simulate this with rbinom, by setting the second parameter to 1000 and the third to be 1 divided by 1000.The histogram would look like this. Notice that unlike the last exercises, this distribution doesn't look like a bell curve at all! For one thing, it's not symmetrical, because the number of heads can't be smaller than zero. This particular case of the binomial, where n is large and p is small, can be approximated by the Poisson distribution.

**3. Properties of the Poisson distribution**

The binomial distribution required two parameters to define it: a size, or number of flips, and a probability that each is heads.The Poisson distribution is simpler, because it's described by only one parameter: the mean, which for the Poisson we usually call by the greek letter "lambda".The mean of the binomial with 1000 flips and 1 / 1000 for each is simply 1. So to simulate the corresponding Poisson, we would use the rpois function, and after the argument 100,000 give it the parameter 1. We can see from the compare_histograms function that these are similar. That means for this distribution, we don't need the extra detail that it's out of 1000 coins, we just need the mean.One interesting fact about the Poisson distribution is that the variance is equal to the mean. That makes it convenient to work with, since you don't need to calculate the variance when you're simulating or estimating it.

**4. Poisson distribution**

The Poisson distribution can have any mean, as long as it's positive. It could have one very close to 0, like .1, in which case most of the outcomes will be 0 or 1. It could have a larger value like 10, in which case it will look a bit more symmetrical.Statisticians and scientists use the Poisson distribution when they're modeling rare events as counts, and when they don't care about the total in the way we would with the binomial distribution.For example, you could be running a bookstore, and modeling how many people walk in in each hour. You could be counting whales within a section of the ocean on one day, or counting cells under a microscope. In one sense each of these is technically a fraction of a total- a percentage of all the people, or whales, or cells in the world. But you wouldn't think of it that way: you don't care about the probability of seeing every whale in the world, you care about the number that you see.You'll explore a bit more about the Poisson distribution in these exercises.

**5. Let's practice!**



## Approximating a binomial with a Poisson

If you were drawing from a binomial with size = 1000 and p = .002, what would be the mean of the Poisson approximation?

> *Question*
> ---
> ???<br>
> <br>
> ⬜ 1<br>
> ✅ 2<br>
> ⬜ 4<br>
> ⬜ 20<br>

Great work!

## Simulating from a Poisson and a binomial

If we were flipping 100,000 coins that each have a .2% chance of coming up heads, you could use a Poisson(2) distribution to approximate it. Let's check that through simulation.

**Steps**

1. Generate 100,000 draws from the Binomial(1000, .002) distribution. Save it as `binom_sample`.
2. Generate 100,000 draws from the Poisson distribution that approximates this binomial distribution, using the `rpois()` function. Save it as `poisson_sample`.
3. Compare the two distributions with the `compare_histograms()` function. (Remember that this takes two arguments: the two samples that are to be compared).

```{r}
# Draw a random sample of 100,000 from the Binomial(1000, .002) distribution
binom_sample <- rbinom(100000, 1000, .002)

# Draw a random sample of 100,000 from the Poisson approximation
poisson_sample <- rpois(100000, 2)

# Compare the two distributions with the compare_histograms function
compare_histograms(binom_sample, poisson_sample)
```

Fantastic! It's interesting how the binomial distribution is related to so many others.

## Density of the Poisson distribution

In this exercise you'll find the probability that a Poisson random variable will be equal to zero by simulating and using the `dpois()` function, which gives an exact answer.

**Steps**

1. Simulate 100,000 draws from a Poisson distribution with a mean of 2.
2. Use this simulation to estimate the probability that a draw from this Poisson distribution will be 0.
3. Find the exact probability that a draw from a Poisson(2) distribution is zero, using the `dpois()` function.

```{r}
# Simulate 100,000 draws from Poisson(2)
poisson_sample <- rpois(100000, 2)

# Find the percentage of simulated values that are 0
mean(poisson_sample == 0)

# Use dpois to find the exact probability that a draw is 0
dpois(0, 2)
```

Great work! You're a real density pro by now!

## Sum of two Poisson variables

One of the useful properties of the Poisson distribution is that when you add multiple Poisson distributions together, the result is also a Poisson distribution.

Here you'll generate two random Poisson variables to test this.

**Steps**

1. Simulate 100,000 draws from the Poisson(1) distribution, saving them as `X`.
2. Simulate 100,000 draws separately from the Poisson(2) distribution, and save them as `Y`.
3. Add `X` and `Y` together to create a variable `Z`.
4. We expect `Z` to follow a Poisson(3) distribution. Use the `compare_histograms` function to compare `Z` to 100,000 draws from a Poisson(3) distribution.

```{r}
# Simulate 100,000 draws from Poisson(1)
X <- rpois(100000, 1)

# Simulate 100,000 draws from Poisson(2)
Y <- rpois(100000, 2)

# Add X and Y together to create Z
Z <- X + Y

# Use compare_histograms to compare Z to the Poisson(3)
compare_histograms(Z, rpois(100000, 3))
```

Awesome! It's convenient that two Poisson distributions sum to another Poisson distribution.

## The geometric distribution

Theory. Coming soon ...
<p class="dc-cookie-banner-text">                DataCamp and our partners use cookies and similar technologies to improve your learning experience, offer data science content relevant to your interests, improve the site and to show more relevant advertisements. You can change your mind at any time.            



**1. The geometric distribution**

Suppose this coin I'm holding has a 10% chance of heads. Now instead of flipping it a fixed number of times, I keep flipping it until the first time I see heads.How many tails do you think I'll get before the first time I get heads? I could get heads on the first try, so it could be zero, or I could be standing here flipping a coin all day. What can I expect?This random variable, where you're waiting for particular event with some probability, is called a geometric distribution, and it's the last one we'll explore in this course.

**2. Simulating waiting for heads**

One way to simulate waiting for a result of heads is to flip a large number of coins and see what the first case is. For example, with rbinom 100, 1, point-1 we can simulate flipping 100 coins, each with a 10% probability of heads, in sequence. In this case we're only showing the first 30, and here we can see that two of the first 30 coins were heads, but the rest were tails. If we don't want to count the coins manually, we can use the which function. which returns the indices that fit a particular condition. For example, with which flips equals equals 1, we can find out which coins in the sequence were heads.In this case, we see the first heads was the eighth. We're only interested in the first heads, so let's use bracket 1 end bracket to extract the first item. This code therefore gave us one draw of this random variable.

**3. Replicating simulations**

We could repeat this code to perform more draws from this distribution, and get a sense of their possible values. In one draw, the first heads could be the 28th flip. In another, it could be the 4th. In another case, the 11th.It's a hassle to keep writing the same line of code to perform multiple draws. So R makes it easy, with the replicate function. Replicate takes two arguments; the first is the number of replications to perform. The second is the line of code-you can just paste it directly in.The result is a set of ten outcomes-22, 12, 6, and so on: each one representing one draw where we kept flipping a coin and waiting for the first heads.This is the geometric distribution. It's useful for modeling situations where, for example, a machine has a 10% chance of breaking each day, and you want to know how long it will last before it breaks. As you see, such a machine might last for weeks, or it might break on one of the first days.

**4. Simulating with rgeom**

It's worth understanding how you can generate a distribution like this yourself with the replicate() function, since being flexible with simulations is important in probability. But in this case, R does provide a shortcut.Much as you've seen the rbinom, the rnorm, and the rpois functions for simulating draws from the binomial, normal, and Poisson, you can use the rgeom function to simulate draws from the geometric distribution. You give it the number of draws, such as one hundred thousand, and the probability of the event you're waiting for, in this case point-1.Notice that the distribution is steadily decreasing in density; every possible value being less likely than the previous one. The most likely value is therefore 0; meaning, in this case, that there were no tails before the first heads.However, what's the expected value, or the mean, of this distribution? We can estimate it with mean on this simulation, and see that it's about 9. That is, when each coin has a 10% chance of heads, the first heads will, on average, be the tenth coin.The general rule is that the expected value is 1 divided by the probability, minus 1. The minus one comes from the fact that R defines the geometric distribution as the number of tails before the first heads, if we defined it as the first heads, it would be simply 1/p.This means, for example, if each coin had a 50% chance of heads, the expected value would be 1-just one tails, on average you'd see one tails before the first heads. In your exercises, you'll use the geometric distribution to model useful situations like waiting for a machine to break.

**5. Let's practice!**



## Waiting for first coin flip

You'll start by simulating a series of coin flips, and "waiting" for the first heads.

**Steps**

1. Simulate 100 instances of flipping a single coin with a 20% chance of heads, and save it as the variable `flips`. (Thus, `flips` should be a vector of length 100).
2. Use `which()` to find the first case where a coin resulted in heads.

```{r}
# Simulate 100 instances of flipping a 20% coin
flips <- rbinom(100, 1, .2)

# Use which to find the first case of 1 ("heads")
which(flips == 1)[1]
```

Wow! What situations can you think of that are good examples of a geometric distribution?

## Using replicate() for simulation

Use the `replicate()` function to simulate 100,000 trials of waiting for the first heads after flipping coins with 20% chance of heads. Plot a histogram of this simulation by calling `qplot()`.

**Steps**

1. Use `replicate()` to simulate 100,000 geometric trials. Copy and paste the expression given to you as your second argument to `replicate()`.
2. Plot a histogram by calling `qplot()` on the `replications`.

```{r}
# Existing code for finding the first instance of heads
which(rbinom(100, 1, 0.2) == 1)[1]

# Replicate this 100,000 times using replicate()
replications <- replicate(100000, which(rbinom(100, 1, 0.2) == 1)[1])

# Histogram the replications with qplot
qplot(replications)
```

Great work! How does the density of the geometric distribution compare to the other ones you've seen?

## Simulating from the geometric distribution

In this exercise you'll compare your `replications` with the output of `rgeom()`.

**Steps**

1. Use the function `rgeom()` to simulate 100,000 draws from a geometric distributions with probability .2. Save this as `geom_sample`.
2. Compare `replications` and `geom_sample` with the `compare_histograms()` function.

```{r}
# Replications from the last exercise
replications <- replicate(100000, which(rbinom(100, 1, .2) == 1)[1])

# Generate 100,000 draws from the corresponding geometric distribution
geom_sample <- rgeom(100000, .2)

# Compare the two distributions with compare_histograms
compare_histograms(replications, geom_sample)
```

Fantastic simulating! And the `rgeom()` function is definitely easier to type than using `rbinom()` to simulate a geometric distribution.

## Probability of a machine lasting X days

A new machine arrives in a factory. This type of machine is very unreliable: every day, it has a 10% chance of breaking permanently. How long would you expect it to last?

Notice that this is described by the cumulative distribution of the geometric distribution, and therefore the `pgeom()` function. `pgeom(X, .1)` would describe the probability that there are X working days before the day it breaks (that is, that it breaks on day X + 1).

**Steps**

1. Use `pgeom()` to find the probability that the machine breaks on the 5th day or earlier.
2. Use `pgeom()` to find the probability that the machine is *still working* by the end of the 20th day.

```{r}
# Find the probability the machine breaks on 5th day or earlier
pgeom(4, .1)

# Find the probability the machine is still working on 20th day
1 - pgeom(19, .1)
```

Great work! Problems like this are one of the reasons the geometric distribution is so useful.

## Graphing the probability that a machine still works

If you were a supervisor at the factory with the unreliable machine, you might want to understand how likely the machine is to keep working over time. In this exercise, you'll plot the probability that the machine is still working across the first 30 days.

**Steps**

1. Calculate a vector of probabilities of whether the machine is *still working* on each day from day 1 to 30, and save it as `still_working`. You can do this with a single call to `pgeom()` by passing in a vector of numbers as the first argument. The machine has a 10% chance of breaking each day.
2. Run the command `qplot(still_working)` to graph the probability of the machine still working on each of the first 30 days, with the day on the x-axis and the probability on the y-axis.

```{r}
# Calculate the probability of machine working on day 1-30
still_working <- 1 - pgeom(0:29, .1)

# Plot the probability for days 1 to 30
qplot(1:30, still_working)
```

Great work! You've learned all the basics you need to start understanding the distributions underlying statistical models!

