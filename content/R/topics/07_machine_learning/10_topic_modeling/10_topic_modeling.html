<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joschka Schwarz">

<title>Joschka Schwarz - Topic Modeling in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" rel="next">
<link href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" rel="prev">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

<link href="../../../../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../../../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>


</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Joschka Schwarz</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-science" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-science">    
        <li>
    <a class="dropdown-item" href="../../../../../content/R/index.html">
 <span class="dropdown-text">R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/python/index.html">
 <span class="dropdown-text">Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/sql/index.html">
 <span class="dropdown-text">SQL</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../../slides/index.html">
 <span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../resumes/index.html">
 <span class="menu-text">Resumes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jwarz/"><i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/j-schwarz"><i class="bi bi-linkedin" role="img" aria-label="Quarto LinkedIn">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Topic Modeling in R</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <b><i>1 Programming Basics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/01_programming_beginner/01_programming_beginner.html" class="sidebar-item-text sidebar-link">1.1: Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/02_programming_intermediate/02_programming_intermediate.html" class="sidebar-item-text sidebar-link">1.2: Intermediate R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/03_programming_tidyverse/03_programming_tidyverse.html" class="sidebar-item-text sidebar-link">1.3: Introduction to the tidyverse</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>2 Importing Data</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/01_importing_data_beginner/01_importing_data_beginner.html" class="sidebar-item-text sidebar-link">2.1: Introduction to Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/02_importing_data_intermediate/02_importing_data_intermediate.html" class="sidebar-item-text sidebar-link">2.2: Intermediate Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/03_working_with_web_data/03_working_with_web_data.html" class="sidebar-item-text sidebar-link">2.3: Working with web data</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>3 Data Wrangling</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/01_data_manipulation_with_dplyr/01_data_manipulation_with_dplyr.html" class="sidebar-item-text sidebar-link">3.1: Data Manipulation with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/02_joining_data_with_dplyr/02_joining_data_with_dplyr.html" class="sidebar-item-text sidebar-link">3.2: Joining data with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/exploratory_data_analysis_in_r/exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.3: Exploratory Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/case_study_exploratory_data_analysis_in_r/case_study_exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.4: Case Study: EDA</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/cleaning_data_in_r/cleaning_data_in_r.html" class="sidebar-item-text sidebar-link">3.5: Cleaning Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/data_manipulation_with_datatable/data_manipulation_with_datatable.html" class="sidebar-item-text sidebar-link">3.6: Data Manipulation with data.table</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/joining_data_with_datatable/joining_data_with_datatable.html" class="sidebar-item-text sidebar-link">3.7: Joining Data with data.table</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>4 Data Visualization</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/introduction_to_data_visualization_with_ggplot2/ggplot2_intro.html" class="sidebar-item-text sidebar-link">4.1: Introduction to Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/data_visualization_with_ggplot2_intermediate/ggplot2_intermediate.html" class="sidebar-item-text sidebar-link">4.2: Intermediate Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>5 Statistics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/01_statistics_beginner/01_statistics_beginner.html" class="sidebar-item-text sidebar-link">5.1: Introduction to Statistics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/02_fundamentals_of_bayesian_data_analysis/02_fundamentals_of_bayesian_data_analysis.html" class="sidebar-item-text sidebar-link">5.2: Fundamentals of Bayesian Data Analysis</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>6 Regression</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/06_regression/01_regression_beginner/01_regression_beginner.html" class="sidebar-item-text sidebar-link">6.1: Introduction to Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/06_regression/02_regression_intermediate/02_regression_intermediate.html" class="sidebar-item-text sidebar-link">6.2: Intermediate Regression</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>7 Machine Learning</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/01_supervised_learning_classification/01_supervised_learning_classification.html" class="sidebar-item-text sidebar-link">7.1: Supervised Learning: Classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="sidebar-item-text sidebar-link">7.2: Supervised Learning: Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/03_unsupervised_learning/03_unsupervised_learning.html" class="sidebar-item-text sidebar-link">7.3: Unsupervised Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">7.4: Machine Learning in the tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/05_cluster_analysis/05_cluster_analysis.html" class="sidebar-item-text sidebar-link">7.5: Cluster Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/06_machine_learning_with_caret/06_machine_learning_with_caret.html" class="sidebar-item-text sidebar-link">7.6: Machine Learning with caret</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/07_modeling_with_tidymodels/07_modeling_with_tidymodels.html" class="sidebar-item-text sidebar-link">7.7: Modeling with tidymodels</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/08_machine_learning_with_tree-based_models/08_machine_learning_with_tree-based_models.html" class="sidebar-item-text sidebar-link">7.8: Machine Learning with tree-based Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" class="sidebar-item-text sidebar-link">7.9: Support Vector Machines</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" class="sidebar-item-text sidebar-link active">7.10: Topic Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="sidebar-item-text sidebar-link">7.11: Hyperparameter Tuning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" class="sidebar-item-text sidebar-link">7.12: Bayesian Regression Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" class="sidebar-item-text sidebar-link">7.13: Introduction to Spark</a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">R Manuals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/test.html" class="sidebar-item-text sidebar-link">1: An Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/02-content.html" class="sidebar-item-text sidebar-link">2: R Data Import/Export</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/03-content.html" class="sidebar-item-text sidebar-link">3: R Installation and Administration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/04-content.html" class="sidebar-item-text sidebar-link">4: Writing R Extensions</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/05-content.html" class="sidebar-item-text sidebar-link">5: R Language Definition</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/06-content.html" class="sidebar-item-text sidebar-link">6: R Internals</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul class="collapse">
  <li><a href="#info-for-rstan-rstanarm" id="toc-info-for-rstan-rstanarm" class="nav-link active" data-scroll-target="#info-for-rstan-rstanarm"><span class="toc-section-number">0.1</span>  INFO FOR RSTAN / RSTANARM</a></li>
  <li><a href="#quick-introduction-to-the-workflow" id="toc-quick-introduction-to-the-workflow" class="nav-link" data-scroll-target="#quick-introduction-to-the-workflow"><span class="toc-section-number">1</span>  1. Quick introduction to the workflow</a>
  <ul class="collapse">
  <li><a href="#why-learn-topic-modeling" id="toc-why-learn-topic-modeling" class="nav-link" data-scroll-target="#why-learn-topic-modeling"><span class="toc-section-number">1.1</span>  Why learn topic modeling</a></li>
  <li><a href="#topics-as-word-contexts" id="toc-topics-as-word-contexts" class="nav-link" data-scroll-target="#topics-as-word-contexts"><span class="toc-section-number">1.2</span>  Topics as word contexts</a></li>
  <li><a href="#topic-prevalence" id="toc-topic-prevalence" class="nav-link" data-scroll-target="#topic-prevalence"><span class="toc-section-number">1.4</span>  Topic prevalence</a></li>
  <li><a href="#probabilities-of-words-belonging-to-topics" id="toc-probabilities-of-words-belonging-to-topics" class="nav-link" data-scroll-target="#probabilities-of-words-belonging-to-topics"><span class="toc-section-number">1.6</span>  Probabilities of words belonging to topics</a></li>
  <li><a href="#counting-words" id="toc-counting-words" class="nav-link" data-scroll-target="#counting-words"><span class="toc-section-number">1.7</span>  Counting words</a></li>
  <li><a href="#removal-of-punctuation-marks" id="toc-removal-of-punctuation-marks" class="nav-link" data-scroll-target="#removal-of-punctuation-marks"><span class="toc-section-number">1.8</span>  Removal of punctuation marks</a></li>
  <li><a href="#word-frequencies" id="toc-word-frequencies" class="nav-link" data-scroll-target="#word-frequencies"><span class="toc-section-number">1.10</span>  Word frequencies</a></li>
  <li><a href="#our-first-lda-model" id="toc-our-first-lda-model" class="nav-link" data-scroll-target="#our-first-lda-model"><span class="toc-section-number">1.11</span>  Our first LDA model</a></li>
  <li><a href="#displaying-frequencies-with-ggplot" id="toc-displaying-frequencies-with-ggplot" class="nav-link" data-scroll-target="#displaying-frequencies-with-ggplot"><span class="toc-section-number">1.12</span>  Displaying frequencies with ggplot</a></li>
  <li><a href="#simple-lda-model" id="toc-simple-lda-model" class="nav-link" data-scroll-target="#simple-lda-model"><span class="toc-section-number">1.13</span>  Simple LDA model</a></li>
  </ul></li>
  <li><a href="#wordclouds-stopwords-and-control-arguments" id="toc-wordclouds-stopwords-and-control-arguments" class="nav-link" data-scroll-target="#wordclouds-stopwords-and-control-arguments"><span class="toc-section-number">2</span>  2. Wordclouds, stopwords, and control arguments</a>
  <ul class="collapse">
  <li><a href="#random-nature-of-lda-algorithm" id="toc-random-nature-of-lda-algorithm" class="nav-link" data-scroll-target="#random-nature-of-lda-algorithm"><span class="toc-section-number">2.1</span>  Random nature of LDA algorithm</a></li>
  <li><a href="#probabilities-of-words-in-topics" id="toc-probabilities-of-words-in-topics" class="nav-link" data-scroll-target="#probabilities-of-words-in-topics"><span class="toc-section-number">2.2</span>  Probabilities of words in topics</a></li>
  <li><a href="#effect-of-argument-alpha" id="toc-effect-of-argument-alpha" class="nav-link" data-scroll-target="#effect-of-argument-alpha"><span class="toc-section-number">2.3</span>  Effect of argument alpha</a></li>
  <li><a href="#manipulating-the-vocabulary" id="toc-manipulating-the-vocabulary" class="nav-link" data-scroll-target="#manipulating-the-vocabulary"><span class="toc-section-number">2.5</span>  Manipulating the vocabulary</a></li>
  <li><a href="#removing-stopwords" id="toc-removing-stopwords" class="nav-link" data-scroll-target="#removing-stopwords"><span class="toc-section-number">2.6</span>  Removing stopwords</a></li>
  <li><a href="#keeping-the-needed-words" id="toc-keeping-the-needed-words" class="nav-link" data-scroll-target="#keeping-the-needed-words"><span class="toc-section-number">2.7</span>  Keeping the needed words</a></li>
  <li><a href="#word-clouds" id="toc-word-clouds" class="nav-link" data-scroll-target="#word-clouds"><span class="toc-section-number">2.8</span>  Word clouds</a></li>
  <li><a href="#wordcloud-of-term-frequency" id="toc-wordcloud-of-term-frequency" class="nav-link" data-scroll-target="#wordcloud-of-term-frequency"><span class="toc-section-number">2.9</span>  Wordcloud of term frequency</a></li>
  <li><a href="#history-of-the-byzantine-empire" id="toc-history-of-the-byzantine-empire" class="nav-link" data-scroll-target="#history-of-the-byzantine-empire"><span class="toc-section-number">2.10</span>  History of the Byzantine Empire</a></li>
  <li><a href="#lda-model-fitting---first-iteration" id="toc-lda-model-fitting---first-iteration" class="nav-link" data-scroll-target="#lda-model-fitting---first-iteration"><span class="toc-section-number">2.11</span>  LDA model fitting - first iteration</a></li>
  <li><a href="#capturing-the-actions---dtm-with-verbs" id="toc-capturing-the-actions---dtm-with-verbs" class="nav-link" data-scroll-target="#capturing-the-actions---dtm-with-verbs"><span class="toc-section-number">2.12</span>  Capturing the actions - dtm with verbs</a></li>
  <li><a href="#making-a-chart" id="toc-making-a-chart" class="nav-link" data-scroll-target="#making-a-chart"><span class="toc-section-number">2.13</span>  Making a chart</a></li>
  <li><a href="#use-wordclouds" id="toc-use-wordclouds" class="nav-link" data-scroll-target="#use-wordclouds"><span class="toc-section-number">2.14</span>  Use wordclouds</a></li>
  </ul></li>
  <li><a href="#named-entity-recognition-as-unsupervised-classification" id="toc-named-entity-recognition-as-unsupervised-classification" class="nav-link" data-scroll-target="#named-entity-recognition-as-unsupervised-classification"><span class="toc-section-number">3</span>  3. Named entity recognition as unsupervised classification</a>
  <ul class="collapse">
  <li><a href="#using-topic-models-as-classifiers" id="toc-using-topic-models-as-classifiers" class="nav-link" data-scroll-target="#using-topic-models-as-classifiers"><span class="toc-section-number">3.1</span>  Using topic models as classifiers</a></li>
  <li><a href="#same-k-different-alpha" id="toc-same-k-different-alpha" class="nav-link" data-scroll-target="#same-k-different-alpha"><span class="toc-section-number">3.2</span>  Same k, different alpha</a></li>
  <li><a href="#probabilities-of-words-in-topics-1" id="toc-probabilities-of-words-in-topics-1" class="nav-link" data-scroll-target="#probabilities-of-words-in-topics-1"><span class="toc-section-number">3.4</span>  Probabilities of words in topics</a></li>
  <li><a href="#from-word-windows-to-dtm" id="toc-from-word-windows-to-dtm" class="nav-link" data-scroll-target="#from-word-windows-to-dtm"><span class="toc-section-number">3.5</span>  From word windows to dtm</a></li>
  <li><a href="#regex-patterns-for-entity-matching" id="toc-regex-patterns-for-entity-matching" class="nav-link" data-scroll-target="#regex-patterns-for-entity-matching"><span class="toc-section-number">3.6</span>  Regex patterns for entity matching</a></li>
  <li><a href="#making-a-corpus" id="toc-making-a-corpus" class="nav-link" data-scroll-target="#making-a-corpus"><span class="toc-section-number">3.8</span>  Making a corpus</a></li>
  <li><a href="#from-dtm-to-topic-model" id="toc-from-dtm-to-topic-model" class="nav-link" data-scroll-target="#from-dtm-to-topic-model"><span class="toc-section-number">3.9</span>  From dtm to topic model</a></li>
  <li><a href="#corpus-alignment-and-classification" id="toc-corpus-alignment-and-classification" class="nav-link" data-scroll-target="#corpus-alignment-and-classification"><span class="toc-section-number">3.10</span>  Corpus alignment and classification</a></li>
  <li><a href="#train-a-topic-model" id="toc-train-a-topic-model" class="nav-link" data-scroll-target="#train-a-topic-model"><span class="toc-section-number">3.11</span>  Train a topic model</a></li>
  <li><a href="#align-corpus" id="toc-align-corpus" class="nav-link" data-scroll-target="#align-corpus"><span class="toc-section-number">3.12</span>  Align corpus</a></li>
  <li><a href="#classify-test-data" id="toc-classify-test-data" class="nav-link" data-scroll-target="#classify-test-data"><span class="toc-section-number">3.13</span>  Classify test data</a></li>
  <li><a href="#explore-the-results" id="toc-explore-the-results" class="nav-link" data-scroll-target="#explore-the-results"><span class="toc-section-number">3.14</span>  Explore the results</a></li>
  </ul></li>
  <li><a href="#how-many-topics-is-enough" id="toc-how-many-topics-is-enough" class="nav-link" data-scroll-target="#how-many-topics-is-enough"><span class="toc-section-number">4</span>  4. How many topics is enough?</a>
  <ul class="collapse">
  <li><a href="#finding-the-best-number-of-topics" id="toc-finding-the-best-number-of-topics" class="nav-link" data-scroll-target="#finding-the-best-number-of-topics"><span class="toc-section-number">4.1</span>  Finding the best number of topics</a></li>
  <li><a href="#preparing-the-dtm" id="toc-preparing-the-dtm" class="nav-link" data-scroll-target="#preparing-the-dtm"><span class="toc-section-number">4.2</span>  Preparing the dtm</a></li>
  <li><a href="#filtering-by-word-frequency" id="toc-filtering-by-word-frequency" class="nav-link" data-scroll-target="#filtering-by-word-frequency"><span class="toc-section-number">4.3</span>  Filtering by word frequency</a></li>
  <li><a href="#fitting-one-model" id="toc-fitting-one-model" class="nav-link" data-scroll-target="#fitting-one-model"><span class="toc-section-number">4.4</span>  Fitting one model</a></li>
  <li><a href="#using-perplexity-to-find-the-best-k" id="toc-using-perplexity-to-find-the-best-k" class="nav-link" data-scroll-target="#using-perplexity-to-find-the-best-k"><span class="toc-section-number">4.5</span>  Using perplexity to find the best k</a></li>
  <li><a href="#topic-models-fitted-to-novels" id="toc-topic-models-fitted-to-novels" class="nav-link" data-scroll-target="#topic-models-fitted-to-novels"><span class="toc-section-number">4.7</span>  Topic models fitted to novels</a></li>
  <li><a href="#generating-chunk-numbers" id="toc-generating-chunk-numbers" class="nav-link" data-scroll-target="#generating-chunk-numbers"><span class="toc-section-number">4.8</span>  Generating chunk numbers</a></li>
  <li><a href="#inner-join-and-cast-dtm" id="toc-inner-join-and-cast-dtm" class="nav-link" data-scroll-target="#inner-join-and-cast-dtm"><span class="toc-section-number">4.9</span>  Inner join and cast dtm</a></li>
  <li><a href="#finding-the-best-value-for-k" id="toc-finding-the-best-value-for-k" class="nav-link" data-scroll-target="#finding-the-best-value-for-k"><span class="toc-section-number">4.10</span>  Finding the best value for k</a></li>
  <li><a href="#locking-topics-by-using-seed-words" id="toc-locking-topics-by-using-seed-words" class="nav-link" data-scroll-target="#locking-topics-by-using-seed-words"><span class="toc-section-number">4.12</span>  Locking topics by using seed words</a></li>
  <li><a href="#topics-without-seedwords" id="toc-topics-without-seedwords" class="nav-link" data-scroll-target="#topics-without-seedwords"><span class="toc-section-number">4.13</span>  Topics without seedwords</a></li>
  <li><a href="#topics-with-seedwords" id="toc-topics-with-seedwords" class="nav-link" data-scroll-target="#topics-with-seedwords"><span class="toc-section-number">4.14</span>  Topics with seedwords</a></li>
  <li><a href="#final-words-and-more-things-to-learn" id="toc-final-words-and-more-things-to-learn" class="nav-link" data-scroll-target="#final-words-and-more-things-to-learn"><span class="toc-section-number">4.15</span>  Final words (and more things to learn)</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jwarz/jwarz.github.io/edit/main/content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jwarz/jwarz.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Topic Modeling in R</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joschka Schwarz </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="info-for-rstan-rstanarm" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="info-for-rstan-rstanarm"><span class="header-section-number">0.1</span> INFO FOR RSTAN / RSTANARM</h2>
<p>https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started Sys.setenv(DOWNLOAD_STATIC_LIBV8 = 1) # only necessary for Linux without the nodejs library / headers install.packages(“rstan”, repos = “https://cloud.r-project.org/”, dependencies = TRUE)</p>
<p>Mac - Configuring C++ Toolchain</p>
<hr>
<p>This course introduces students to the areas involved in topic modeling: preparation of corpus, fitting of topic models using Latent Dirichlet Allocation algorithm (in package topicmodels), and visualizing the results using ggplot2 and wordclouds.</p>
</section>
<section id="quick-introduction-to-the-workflow" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 1. Quick introduction to the workflow</h1>
<p>This chapter introduces the workflow used in topic modeling: preparation of a document-term matrix, model fitting, and visualization of results with ggplot2.</p>
<section id="why-learn-topic-modeling" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="why-learn-topic-modeling"><span class="header-section-number">1.1</span> Why learn topic modeling</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Why learn topic modeling</strong></p>
<p>Welcome to the Topic Modeling with R. In this video you will learn about topic models: what inputs they take, what output they produce, and how they can be useful to your work.</p>
<p><strong>2. What are topic models</strong></p>
<p>In common language, we understand a topic as a very short (one or two words) summary of a conversation or a text.For example, if someone says that the topic of a conversation was weather, we expect that the words used were rain, storm, snow, winds, ice.The idea is, then, that a topic is a label for a collection of words that often occur together.Topic modeling is a process of finding collections of words that best represent a set of unknown topics.</p>
<p><strong>3. Rise of popularity</strong></p>
<p>Topic models give a way to quickly make a judgment about contents of a collection of documents. The joke is that topic models are a tool for those who do not like to read.Because topics are quantified, topic modeling opened the door for new applications, like tracking the prevalence of a topic through time, tracking similarity of documents, or fitting regression models to estimate causal effects.Topic models are used as a foundation for more technical applications like text segmentation, or classification.</p>
<p><strong>4. Topic models - descriptive side</strong></p>
<p>There are many algorithms that produce topic models. Our course will focus on Latent Dirichlet Allocation.For its input, LDA takes a document-term matrix. A document-term matrix is a bag-of-words representation of a collection of documents: it records frequencies of word occurrence, but ignores word order.LDA returns two matrices: one contains prevalence of topics in the documents, the other - probability of words belonging to topics.Let’s take a look at a quick example.</p>
<p><strong>5. Illustration</strong></p>
<p>We have a tiny collection of documents. (The technical term for such collection is <strong>corpus</strong>.)Each document consists of a single sentence.These sentences are related to two topics: restaurants and loans.</p>
<p><strong>6. Illustration</strong></p>
<p>The corpus is converted into a document-term matrix. We use only a subset of words: bank, loans, pay, new, opened, restaurant.Notice that the words “loans” and “pay” occur in document 5 that has both topics present in it.</p>
<p><strong>7. Illustration</strong></p>
<p>Function LDA() will fit a topic model to a collection of documents represented by document-term matrix.LDA is a supervised clustering algorithm: we need to specify the number of clusters we seek from the start.The result is two tables. The first table shows probabilities of words belonging to topics. For instance, probability of word ‘opened’ belonging to topic 2 is 36%.The second table shows probabilities of documents belonging to topics. Document 3 has 20% probability of belonging to topic 1, and 80% - of belonging to topic 2.</p>
<p><strong>8. Topic modeling - the other parts</strong></p>
<p>There are quite a few technical decisions involved in a project that uses topic modeling.First, matrices as output are fine only for small sets of data. For larger projects, we will have to use charts.Second, there are choices related to which words to keep for analysis. The earlier example used only 7 words out of the vocabulary of 34. Some approaches emphasize the use of nouns, others - the use of verbs.Then, a document can be constructed in multiple ways. Some scholars work with a single novel, like Moby Dick, and use 500 or 1000 words as a single document. In analysis of Twitter data, a single tweet is often treated as a document.Finally, there are several control parameters that strongly affect the results. To be fully competent with topic modeling, you must have an idea what these parameters control.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>Now let’s start doing things.</p>
</section>
<section id="topics-as-word-contexts" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="topics-as-word-contexts"><span class="header-section-number">1.2</span> Topics as word contexts</h2>
<p>We have a topic defined by the following terms: <strong>site, settlement, evidence, inhabit, region, period, earliest, ancient, reconstruct</strong>.</p>
<blockquote class="blockquote">
<h2 id="question" data-number="1.3" class="anchored"><span class="header-section-number">1.3</span> <em>Question</em></h2>
<p>Which concept is reflected in this topic? Pick one.<br> <br> ✅ Archeology<br> ⬜ Physics<br> ⬜ Chemistry<br></p>
</blockquote>
<p>The answer is <em>archeology</em>. You probably relied on the associations in your mind; these associations are just non-quantified word co-occurrences.</p>
</section>
<section id="topic-prevalence" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="topic-prevalence"><span class="header-section-number">1.4</span> Topic prevalence</h2>
<p>Matrix <code>document_topics</code> contains results of a topic model fitted on the same corpus, but this time the document-term matrix included all words.</p>
<blockquote class="blockquote">
<h2 id="question-1" data-number="1.5" class="anchored"><span class="header-section-number">1.5</span> <em>Question</em></h2>
<p>Print out the matrix and answer the question, which topic, 1 or 2, does the document 5 is more likely to belong?<br> <br> ⬜ Topic 1 has a higher proportion than topic 2.<br> ✅ Topic 2 has a higher proportion than topic 1.<br> ⬜ Both topics have the same proportion.<br></p>
</blockquote>
</section>
<section id="probabilities-of-words-belonging-to-topics" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="probabilities-of-words-belonging-to-topics"><span class="header-section-number">1.6</span> Probabilities of words belonging to topics</h2>
<p>Your session has a matrix <code>word_topics</code> which contains the probabilities of words belonging to the topics. The matrix has two rows (for two topics) and 34 columns (for each term). The columns are named, with terms serving as column name.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Display the column names of the <code>word_topics</code> matrix.</li>
<li>Display the probability of word <em>street</em> belonging to topic 1.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>word_topics <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/word_topics.rds"</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># Display the column names</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="fu">colnames</span>(word_topics)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;  [1] "agreed"     "bad"        "bank"       "due"        "fines"     
#&gt;  [6] "loans"      "pay"        "the"        "to"         "are"       
#&gt; [11] "face"       "if"         "late"       "off"        "will"      
#&gt; [16] "you"        "your"       "a"          "downtown"   "in"        
#&gt; [21] "new"        "opened"     "restaurant" "is"         "just"      
#&gt; [26] "on"         "street"     "that"       "there"      "warwick"   
#&gt; [31] "for"        "how"        "need"       "want"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Display the probability</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>word_topics[<span class="dv">1</span>, <span class="st">"street"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.03741497</code></pre>
</div>
</div>
</section>
<section id="counting-words" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="counting-words"><span class="header-section-number">1.7</span> Counting words</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Counting words</strong></p>
<p>In order to fit a topic model, we must prepare a document-term matrix that will contain counts of word occurrences in documents. In this lesson we will cover how to do it using packages tidytext and dplyr.</p>
<p><strong>2. Splitting text</strong></p>
<p>In text processing, the process of splitting a text is referred to as tokenization. In our case, we will be splitting text into words, but in general tokens can be a sequence of characters, or a sequence of words.Package tidytext has a function unnest_tokens() that performs tokenization.The function takes a column from a table, splits it into words and, by default, it will drop the column with text. It will also convert the output to lower case.</p>
<p><strong>3. Example of using unnest_tokens</strong></p>
<p>We have a data frame named “book”. It has two columns: chapter and text.We call unnest_tokens(), instructing that the column with tokens should be named “word” and that column “text” should be dropped.We get back a table in which each word is in its own row.</p>
<p><strong>4. Counting words</strong></p>
<p>We will use function count() from package dplyr to obtain frequencies of words within chapters.This function, essentially, groups the rows by chapter and word, and returns the number of rows in each group. This correspond to the number of times a specific word occurs in a specific chapter.The result is a table with one row per each combination of chapter and word. For example, the word “is” occurs twice in chapter 1.</p>
<p><strong>5. Getting the top words 1</strong></p>
<p>Once we have the counts, we often will want to examine the top words, for example, the top 10.This can be done by grouping the rows by chapter, sorting the rows within each group in order of descending counts, and then realizing that the rank of a word is equal to its row number.Most frequent word will be in row 1, second most frequent - in row 2, and so on. dplyr has a function row_number() that returns the row number. All we need to do is filter on the condition that row number is less than a threshold value.</p>
<p><strong>6. Getting the top words 2</strong></p>
<p>This is an example of getting top two words from each chapter. We use arrange() to sort the rows within each group, and wrap the count values n into a call to desc() to sort in descending order.Then we filter, inside each group, to keep only rows whose number is less than 3.Note that for chapter 2, the second word we got is “comes”. Its count is 1 and we know there were other words with that count. What we got, then, is also determined by the alphabetical order.</p>
<p><strong>7. Casting counts into a document-term matrix</strong></p>
<p>You may be familiar with function cast() that is used to transform a table from one format into another.We need to transform the table with counts into a document-term matrix, dtm for short. In dtm, rows correspond to documents, and columns to words, terms. In our case, each sentence will be a document.Package tidytext has function cast_dtm() that transforms a table in tidy format into a document-term matrix.It accepts a table with counts, and needs to know which column corresponds to a document ID, which - to the word, and which - to the value of the count.</p>
<p><strong>8. Example of using cast_dtm()</strong></p>
<p>Here is an example.You should recognize most of this script. Everything until cast_dtm() is the code that creates a tidy table with word counts.We add the call to cast_dtm() and get back a document-term matrix. It is stored in a special format, as a so-called sparse matrix.We can examine its contents by converting it to a regular matrix using function as.matrix().</p>
<p><strong>9. Let’s practice!</strong></p>
<p>It’s time to practice!</p>
</section>
<section id="removal-of-punctuation-marks" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="removal-of-punctuation-marks"><span class="header-section-number">1.8</span> Removal of punctuation marks</h2>
<p>Sometimes you have to work with hyphenated words like ‘ill-fated’. The documentation for <code>unnest_tokens</code> says that the tokenizer function will remove all punctuation marks. Your job is to verify this.</p>
<blockquote class="blockquote">
<h2 id="question-2" data-number="1.9" class="anchored"><span class="header-section-number">1.9</span> <em>Question</em></h2>
<p>You are given a data frame <code>d</code> with a single column <code>text</code> and two rows. Apply the <code>unnest_tokens</code> to table <code>d</code> and pick the answer that best describes its behavior.<br> <br> ⬜ <code>unnest_tokens</code> removes all punctuation marks.<br> ⬜ <code>unnest_tokens</code> keeps the punctuation marks.<br> ✅ It depends on where the punctuation mark is.<br></p>
</blockquote>
<p>Yes. If a punctuation mark is followed by a space, it is removed.</p>
</section>
<section id="word-frequencies" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="word-frequencies"><span class="header-section-number">1.10</span> Word frequencies</h2>
<p>You are given a table <code>chapters</code> with two columns: <code>chapter</code> and <code>text</code> and two rows. They contain chapters 1 and 2 from the book, <em>The Byzantine Empire</em>, by Charles Oman, published in 1902. Find the frequency of the word ‘after’ for each chapter.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Specify the input column for the <code>unnest_tokens()</code>.</li>
<li>Use <code>count</code> with arguments <code>chapter</code> and <code>word</code> to obtain word frequencies.</li>
<li>Use <code>filter</code> with a condition testing equality of values in column <code>word</code> to “after”.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Prepare data</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">library</span>(<span class="st">"gutenbergr"</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="fu">library</span>(<span class="st">"stringr"</span>)</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="fu">library</span>(<span class="st">"dplyr"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:stats':
#&gt; 
#&gt;     filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:base':
#&gt; 
#&gt;     intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="fu">library</span>(<span class="st">"tidyr"</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co"># tbe_book &lt;- gutenberg_download(37756)</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>tbe_book_lines <span class="ot">&lt;-</span> <span class="fu">gutenberg_works</span>(title <span class="sc">|&gt;</span> <span class="fu">str_detect</span>(<span class="st">"The Byzantine Empire"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>  </span>
<span id="cb9-6"><a href="#cb9-6"></a>  <span class="co"># Add title</span></span>
<span id="cb9-7"><a href="#cb9-7"></a>  <span class="fu">gutenberg_download</span>(<span class="at">meta_fields =</span> <span class="st">"title"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-8"><a href="#cb9-8"></a>  <span class="co"># Remove unnecessary whitespaces</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>  <span class="fu">mutate</span>(<span class="at">text =</span> text <span class="sc">|&gt;</span> <span class="fu">str_trim</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb9-10"><a href="#cb9-10"></a>  <span class="co"># Remove empty lines and "Illustrations"</span></span>
<span id="cb9-11"><a href="#cb9-11"></a>  <span class="fu">filter</span>(<span class="sc">!</span>(text <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">""</span>, <span class="st">"[Illustration]"</span>))) <span class="sc">|&gt;</span> </span>
<span id="cb9-12"><a href="#cb9-12"></a>  <span class="co"># Add indicator for chapter titles</span></span>
<span id="cb9-13"><a href="#cb9-13"></a>  <span class="fu">mutate</span>(<span class="at">is_chapter_title =</span> text <span class="sc">|&gt;</span> <span class="fu">str_detect</span>(<span class="st">"^(I|V|X)+</span><span class="sc">\\</span><span class="st">. </span><span class="sc">\\</span><span class="st">b[A-Z]+</span><span class="sc">\\</span><span class="st">b"</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb9-14"><a href="#cb9-14"></a>  </span>
<span id="cb9-15"><a href="#cb9-15"></a>  <span class="co"># Add chapter number</span></span>
<span id="cb9-16"><a href="#cb9-16"></a>  <span class="fu">mutate</span>(<span class="at">chapter =</span> <span class="fu">cumsum</span>(is_chapter_title)) <span class="sc">|&gt;</span> </span>
<span id="cb9-17"><a href="#cb9-17"></a>  <span class="fu">mutate</span>(<span class="at">chapter =</span> <span class="fu">ifelse</span>(<span class="fu">cumany</span>(text <span class="sc">==</span> <span class="st">"Finis."</span>),<span class="dv">0</span>,chapter)) <span class="sc">|&gt;</span> <span class="co"># Set Chapter == 0 After Finis</span></span>
<span id="cb9-18"><a href="#cb9-18"></a>  </span>
<span id="cb9-19"><a href="#cb9-19"></a>  <span class="co"># Add chapter titles</span></span>
<span id="cb9-20"><a href="#cb9-20"></a>  <span class="fu">mutate</span>(<span class="at">chapter_title =</span> <span class="fu">ifelse</span>(is_chapter_title, text, <span class="cn">NA_character_</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb9-21"><a href="#cb9-21"></a>  <span class="fu">fill</span>(chapter_title) <span class="sc">|&gt;</span> <span class="co"># impute</span></span>
<span id="cb9-22"><a href="#cb9-22"></a>  <span class="fu">filter</span>(<span class="sc">!</span>is_chapter_title, chapter <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-23"><a href="#cb9-23"></a>  <span class="fu">select</span>(<span class="sc">-</span>is_chapter_title)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Using mirror http://aleph.gutenberg.org</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>tbe_book_chapters <span class="ot">&lt;-</span> tbe_book_lines <span class="sc">|&gt;</span> </span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a>              <span class="co"># Collapse lines</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>              <span class="fu">group_by</span>(gutenberg_id, title, chapter, chapter_title) <span class="sc">|&gt;</span> </span>
<span id="cb12-5"><a href="#cb12-5"></a>              <span class="fu">summarise</span>(<span class="at">text =</span> text <span class="sc">|&gt;</span> <span class="fu">str_c</span>(<span class="at">collapse=</span> <span class="st">" "</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; `summarise()` has grouped output by 'gutenberg_id', 'title', 'chapter'. You can
#&gt; override using the `.groups` argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>chapters <span class="ot">&lt;-</span> tbe_book_chapters <span class="sc">|&gt;</span> <span class="fu">filter</span>(chapter <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Load package</span></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># Specify the input column</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>word_freq <span class="ot">&lt;-</span> chapters <span class="sc">%&gt;%</span> </span>
<span id="cb15-6"><a href="#cb15-6"></a>  <span class="fu">unnest_tokens</span>(<span class="at">output=</span>word, </span>
<span id="cb15-7"><a href="#cb15-7"></a>                <span class="at">input=</span>text, </span>
<span id="cb15-8"><a href="#cb15-8"></a>                <span class="at">token=</span><span class="st">"words"</span>, </span>
<span id="cb15-9"><a href="#cb15-9"></a>                <span class="at">format=</span><span class="st">"text"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-10"><a href="#cb15-10"></a>  <span class="co"># Obtain word frequencies</span></span>
<span id="cb15-11"><a href="#cb15-11"></a>  <span class="fu">count</span>(chapter, word) </span>
<span id="cb15-12"><a href="#cb15-12"></a></span>
<span id="cb15-13"><a href="#cb15-13"></a><span class="co"># Test equality</span></span>
<span id="cb15-14"><a href="#cb15-14"></a>word_freq <span class="sc">%&gt;%</span> <span class="fu">filter</span>(word <span class="sc">==</span> <span class="st">"after"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["gutenberg_id"],"name":[1],"type":["int"],"align":["right"]},{"label":["title"],"name":[2],"type":["chr"],"align":["left"]},{"label":["chapter"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["word"],"name":[4],"type":["chr"],"align":["left"]},{"label":["n"],"name":[5],"type":["int"],"align":["right"]}],"data":[{"1":"37756","2":"The Byzantine Empire\\r\\nThird Edition","3":"1","4":"after","5":"11"},{"1":"37756","2":"The Byzantine Empire\\r\\nThird Edition","3":"2","4":"after","5":"4"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</section>
<section id="our-first-lda-model" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="our-first-lda-model"><span class="header-section-number">1.11</span> Our first LDA model</h2>
<p>In order to fit a topic model to a corpus of text, we need to provide a document-term matrix (dtm) as an input to function <code>LDA</code>. Now that you’ve seen how to do that, you can fit your very first topic model.</p>
<p>You are given access to a table <code>corpus</code> that you have seen in lesson 1. Column <code>text</code> contains the sentences, column <code>id</code> - document id.</p>
<p>Fill in the blanks to run the code that will display proportion of topics in each document.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>In the call to <code>unnest_tokens</code> specify that the input column is <code>text</code> and the output column is <code>word</code>.</li>
<li>In the call to <code>cast_dtm</code>, specify that the token is in column <code>word</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># tm package is necessary</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="co"># Load packages</span></span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb16-4"><a href="#cb16-4"></a></span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="co"># Read data</span></span>
<span id="cb16-6"><a href="#cb16-6"></a>corpus1 <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/corpus1.rds"</span>)</span>
<span id="cb16-7"><a href="#cb16-7"></a></span>
<span id="cb16-8"><a href="#cb16-8"></a>dtm1 <span class="ot">&lt;-</span> corpus1 <span class="sc">%&gt;%</span> </span>
<span id="cb16-9"><a href="#cb16-9"></a>    <span class="co"># Specify the input column</span></span>
<span id="cb16-10"><a href="#cb16-10"></a>    <span class="fu">unnest_tokens</span>(<span class="at">input=</span>text, <span class="at">output=</span>word, <span class="at">drop=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb16-11"><a href="#cb16-11"></a>    <span class="fu">count</span>(id, word) <span class="sc">%&gt;%</span> </span>
<span id="cb16-12"><a href="#cb16-12"></a>    <span class="co"># Specify the token</span></span>
<span id="cb16-13"><a href="#cb16-13"></a>    tidytext<span class="sc">::</span><span class="fu">cast_dtm</span>(<span class="at">document=</span>id, <span class="at">term=</span>word, <span class="at">value=</span>n)</span>
<span id="cb16-14"><a href="#cb16-14"></a></span>
<span id="cb16-15"><a href="#cb16-15"></a>mod1_1 <span class="ot">=</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm1, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>, <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">delta=</span><span class="fl">0.1</span>, <span class="at">seed=</span><span class="dv">10005</span>))</span>
<span id="cb16-16"><a href="#cb16-16"></a></span>
<span id="cb16-17"><a href="#cb16-17"></a><span class="fu">posterior</span>(mod1_1)<span class="sc">$</span>topics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;             1         2
#&gt; d_1 0.2307692 0.7692308
#&gt; d_2 0.1666667 0.8333333
#&gt; d_3 0.8750000 0.1250000
#&gt; d_4 0.8461538 0.1538462
#&gt; d_5 0.2777778 0.7222222</code></pre>
</div>
</div>
<p>Well done. The call to <code>posterior(mod)$topics</code> returns the probabilities of topics.</p>
</section>
<section id="displaying-frequencies-with-ggplot" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="displaying-frequencies-with-ggplot"><span class="header-section-number">1.12</span> Displaying frequencies with ggplot</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Displaying results with ggplot</strong></p>
<p>In this chapter we will review how to display results of topic modeling using ggplot.</p>
<p><strong>2. Frequencies and probabilities</strong></p>
<p>As we work with LDA topic modeling, we will be interested in displaying two kinds of data.First, there are word counts in documents - we obtain them before we fit a topic model.Then, there are probabilities of topics in documents and words in topics. These are obtained after fitting a topic model.Fortunately, ggplot can do it all.ggplot requires the data to be in tidy format. Fortunately, word counts are already in tidy format, and LDA results can be converted into a tidy format using function tidy() from package tidytext.</p>
<p><strong>3. From LDA model to tidy table</strong></p>
<p>When we fit a topic model, we call function LDA() and it returns an object. Among many things, this object contains two matrices: beta and gamma. beta contains logarithms of probabilities of words belonging to topics, and gamma - probabilities of documents belonging to topics.Notice how in the output, the dimension of beta is 2 by 34: two topics and 34 words. The dimension of gamma is 5 by 2: five documents and two topics.</p>
<p><strong>4. Using function tidy</strong></p>
<p>Function tidy() takes an LDA model object and returns a tidy table with a specified matrix.</p>
<p><strong>5. Stacked columns chart</strong></p>
<p>The geometry column layer in ggplot will produce a column chart.By default, the columns will be stacked.In the call to ggplot(), the aesthetics specifies that values for axis x will come from column “document”, for axis y - from column “gamma”.</p>
<p><strong>6. Dodged columns</strong></p>
<p>Dodged, or side-by-side, columns are better for telling which column is taller. To make a chart with dodged columns, we need to add position_dodge() argument to the call of geom_col()The example shows probabilities of words. The data comes from matrix “beta” contained in LDA model. The word is contained in column “term”.</p>
<p><strong>7. Rotated labels</strong></p>
<p>When the number of labels on x axis gets large, the labels will overlap and obscure each other.The overlap can be reduced if we rotate the labels, for instance, by 90 degrees.This can be achieved using the specification for axis.text.x element in the call to theme().Notice that in the script in the slide, we first convert topic into a factor variable, and then use it to guide the color fill.</p>
<p><strong>8. Let’s practice!</strong></p>
<p>Let’s do a few examples.</p>
</section>
<section id="simple-lda-model" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="simple-lda-model"><span class="header-section-number">1.13</span> Simple LDA model</h2>
<p>In this exercise you will work through all steps involved in making a topic model analysis. For simplicity, we will use the tiny corpus of five sentences/documents that you have seen earlier.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>With the model object in hand, you can extract matrices <code>beta</code> and <code>gamma</code> for word and document probabilities, respectively. Retrieve the probabilities of word <code>will</code> belonging to topics 1 and 2. The column containing words will be named <code>term</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Retrieve the probabilities of word `will` belonging to topics 1 and 2</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="fu">tidy</span>(mod1_1, <span class="at">matrix=</span><span class="st">"beta"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>  <span class="fu">filter</span>(term <span class="sc">==</span> <span class="st">"will"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["topic"],"name":[1],"type":["int"],"align":["right"]},{"label":["term"],"name":[2],"type":["chr"],"align":["left"]},{"label":["beta"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"will","3":"0.003649635"},{"1":"2","2":"will","3":"0.078680203"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<ol start="2" type="1">
<li><p>Make a stacked column chart showing the probabilities of documents belonging to topics.</p>
<ul>
<li><code>tidy</code> will return a table with columns <code>document</code> for document id, <code>topic</code> for topic number, and <code>gamma</code> for the value of probability.</li>
<li>Retrieve matrix <code>gamma</code>, use <code>document</code> for <code>x</code>, <code>gamma</code> for <code>y</code> in the aesthetics, and <code>topic</code> as <code>fill</code> in the <code>geom_col</code> verb.</li>
</ul></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># Load package</span></span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a><span class="co"># Make a stacked column chart showing the probabilities of documents belonging to topics</span></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="fu">tidy</span>(mod1_1, <span class="at">matrix=</span><span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb19-6"><a href="#cb19-6"></a>  <span class="fu">mutate</span>(<span class="at">topic =</span> <span class="fu">as.factor</span>(topic)) <span class="sc">%&gt;%</span> </span>
<span id="cb19-7"><a href="#cb19-7"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>gamma)) <span class="sc">+</span> </span>
<span id="cb19-8"><a href="#cb19-8"></a>  <span class="fu">geom_col</span>(<span class="fu">aes</span>(<span class="at">fill=</span>topic))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-6-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Great job. You have covered all steps, from a text dataframe to a chart with results.</p>
</section>
</section>
<section id="wordclouds-stopwords-and-control-arguments" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 2. Wordclouds, stopwords, and control arguments</h1>
<p>This chapter explains how to use join functions to remove or keep words in the document-term matrix, how to make wordcloud charts, and how to use some of the many control arguments.</p>
<section id="random-nature-of-lda-algorithm" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="random-nature-of-lda-algorithm"><span class="header-section-number">2.1</span> Random nature of LDA algorithm</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Linking words to topics</strong></p>
<p>In this lesson you will get a glimpse into the way LDA algorithm searches for a topic model.</p>
<p><strong>2. LDA and random numbers</strong></p>
<p>When we call LDA() function we specify the document-term matrix “dtm”, number of clusters “k”, and some control and initialization parameters. Let’s talk about what they do.LDA uses random search through the space of parameters to find the best match between probabilities and data.This match is measured by log-likelihood, which is a measure of how plausible the model’s parameters are given the data. We want the model with the highest log-likelihood.</p>
<p><strong>3. Random search</strong></p>
<p>It uses “Gibbs sampling” - a randomized search algorithm, which is a type of Monte Carlo Markov Chain algorithms. This is why we specify “method” equals “Gibbs” in the input.The algorithm searches among possible combinations of probabilities. For example, 0.5 and 0.5 vs 0.8 and 0.2, for two topics in documents.These combinations are influenced by the values of control arguments “alpha” and “delta”.</p>
<p><strong>4. Random search - controlling the iterations</strong></p>
<p>Even though we say “randomized”, in reality the numbers in R are pseudo-random. They are generated by a special function that takes a seed value and produces new values by iterating over the seed.The seed can be specified to ensure reproducibility of results between runs of LDA().The number of steps in the search process is controlled by the “iter” argument. Increasing the number of iterations increases the chances of finding the best model, but it also makes the model fitting take longer.By default, the algorithm will perform 2000 iterations.</p>
<p><strong>5. Effect of seed value</strong></p>
<p>Here is an illustration of the effect of “seed” value.We have the same corpus of five sentencesWe run the same code, but the seed value is different.Topic proportions are roughly similar, but the numbering of topics has flipped.Documents that had high prevalence of topic 1 now have high prevalence of topic 2.Without special precautions, topic numbering is arbitrary.</p>
<p><strong>6. Handling intermediate results</strong></p>
<p>Package “topicmodels” uses a piece of C code that was originally written by the group of David Blei, the scholar who pioneered the use of LDA for topic modeling. Because of this, some control arguments perform in non-intuitive way.One argument like that is “thin”. It specifies the frequency with which an LDA model is saved.Specifying “thin” equals 1 makes the code return the topic model after every iteration, and return the one with the best log-likelihood. This is a trade-off: the code will run slower, but we will get to keep the best value.</p>
<p><strong>7. Most probable words in topics</strong></p>
<p>How can we find out the most probable words?Word probabilities are returned in matrix “beta”.One approach is to use function tidy() that you have seen before. tidy() would retrieve the matrix in tidy format and then we could use the power of dplyrFor example, to get the top five wordswe’d call tidy, then group by topic, arrange by probabilities “beta”, filter by row number.</p>
<p><strong>8. Using tidy() to get most probable words</strong></p>
<p>Here is what the script could look like.Filtering condition “row_number() &lt;= 3” is used to keep only the top 3 words.In the end, we get a tidy table that contains topics, terms, and probabilities.</p>
<p><strong>9. Using function terms()</strong></p>
<p>Package <code>topicmodels</code> has function terms() that returns the most probable words. It can return the top “k” words, or we can provide a probability threshold.For example, here we request top 5 words for every topic.The result is a table with a column per topicBecause this function does not return values of probabilities, the top k result can be confusing. The function will simply go down the list of words, even though their probabilities may be very small.The “threshold” option is useful when we are interested in knowing words that have high probability of belonging to a topic.Here we request words whose probability in a topic is above 0.05. Notice how topic 2 has fewer words whose probability is above the threshold than topic 1.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>You’ve seen the demos, and now it’s time to practice.</p>
</section>
<section id="probabilities-of-words-in-topics" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="probabilities-of-words-in-topics"><span class="header-section-number">2.2</span> Probabilities of words in topics</h2>
<p>You will now practice retrieving information about probabilities of words in topics.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>You are given a document-term matrix <code>dtm</code> constructed from the same corpus of five sentences, but using only seven words. Using your knowledge that a document-term matrix has the terms as its column names, display the terms of the <code>dtm</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># Read data</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="co"># dtm &lt;- readRDS("data/dtm1.rds")</span></span>
<span id="cb20-3"><a href="#cb20-3"></a></span>
<span id="cb20-4"><a href="#cb20-4"></a>cols <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"bank"</span>,<span class="st">"fines"</span>,<span class="st">"loans"</span>,<span class="st">"pay"</span>,<span class="st">"new"</span>,<span class="st">"opened"</span>,<span class="st">"restaurant"</span>)</span>
<span id="cb20-5"><a href="#cb20-5"></a></span>
<span id="cb20-6"><a href="#cb20-6"></a>dtm1_mat <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dtm1)[,cols]</span>
<span id="cb20-7"><a href="#cb20-7"></a></span>
<span id="cb20-8"><a href="#cb20-8"></a><span class="co"># Display column names</span></span>
<span id="cb20-9"><a href="#cb20-9"></a><span class="fu">colnames</span>(dtm1_mat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] "bank"       "fines"      "loans"      "pay"        "new"       
#&gt; [6] "opened"     "restaurant"</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Fit an LDA topic model for two topics. Argument <code>x</code> should be the document-term matrix, number of clusters <code>k</code> should be 2, <code>method</code> should be <code>Gibbs</code>. Keep the control argument unchanged.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># Fit an LDA model for 2 topics using Gibbs sampling</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>mod1_2 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm1_mat, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>, </span>
<span id="cb22-3"><a href="#cb22-3"></a>           <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">seed=</span><span class="dv">10005</span>, <span class="at">thin=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Use <code>dplyr</code> to display the probability of term “opened” in topic 2. You will be retrieving matrix <code>beta</code> from LDA object <code>mod</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Convert matrix beta into tidy format and filter on topic number and term</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="fu">tidy</span>(mod1_2, <span class="at">matrix=</span><span class="st">"beta"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="#cb23-3"></a>  <span class="fu">filter</span>(topic<span class="sc">==</span><span class="dv">2</span>, term<span class="sc">==</span><span class="st">"opened"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["topic"],"name":[1],"type":["int"],"align":["right"]},{"label":["term"],"name":[2],"type":["chr"],"align":["left"]},{"label":["beta"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"2","2":"opened","3":"0.01030928"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>Very good.</p>
</section>
<section id="effect-of-argument-alpha" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="effect-of-argument-alpha"><span class="header-section-number">2.3</span> Effect of argument alpha</h2>
<p>In this exercise you will compare how the quality of model’s fit to data varies with argument <code>alpha</code></p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>You have a document-term matrix <code>dtm</code> containing word frequencies for the corpus of 5 sentences with the vocabulary of 7(???) words.</li>
<li>Fit LDA topic model for 2 topics. Keep the arguments <code>seed</code> and <code>alpha</code> unchanged.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Fit LDA topic model using Gibbs sampling for 2 topics</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>mod1_3 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm1, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb24-3"><a href="#cb24-3"></a>           <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">seed=</span><span class="dv">10005</span>, <span class="at">thin=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Display the probabilities of topics in documents. Use function <code>tidy</code> to retrieve matrix <code>gamma</code> from the LDA model object.</li>
<li><code>pivot_wider</code> / <code>spread</code> will cast it into a table with two columns.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># Display the probabilities of topics in documents side by side</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="co"># tidy(mod1, "gamma") %&gt;% spread(topic, gamma)</span></span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="fu">tidy</span>(mod1_3, <span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> topic, <span class="at">values_from =</span> gamma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["document"],"name":[1],"type":["chr"],"align":["left"]},{"label":["1"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["2"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"d_1","2":"0.1538462","3":"0.84615385"},{"1":"d_2","2":"0.2777778","3":"0.72222222"},{"1":"d_3","2":"0.8750000","3":"0.12500000"},{"1":"d_4","2":"0.9230769","3":"0.07692308"},{"1":"d_5","2":"0.5000000","3":"0.50000000"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<ol start="5" type="1">
<li>Rerun the code, but this time set <code>alpha</code> equal to 25. Display the probabilities of topics in documents.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Fit LDA topic model with a different alpha</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>mod1_4 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm1, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb26-3"><a href="#cb26-3"></a>           <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">25</span>, <span class="at">seed=</span><span class="dv">10005</span>, <span class="at">thin=</span><span class="dv">1</span>))</span>
<span id="cb26-4"><a href="#cb26-4"></a></span>
<span id="cb26-5"><a href="#cb26-5"></a><span class="co"># Display the probabilities of topics in documents side by side</span></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="fu">tidy</span>(mod1_4, <span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> topic, <span class="at">values_from =</span> gamma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["document"],"name":[1],"type":["chr"],"align":["left"]},{"label":["1"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["2"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"d_1","2":"0.4754098","3":"0.5245902"},{"1":"d_2","2":"0.5303030","3":"0.4696970"},{"1":"d_3","2":"0.4821429","3":"0.5178571"},{"1":"d_4","2":"0.5081967","3":"0.4918033"},{"1":"d_5","2":"0.5000000","3":"0.5000000"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-3" data-number="2.4" class="anchored"><span class="header-section-number">2.4</span> <em>Question</em></h2>
<p>Knowing that our corpus contained documents related to two topics, which model, <code>mod1</code> or <code>mod2</code> returned more realistic results?<br> <br> ✅ Model <code>mod1</code> returned more realistic values of topic proportions.<br> ⬜ Model <code>mod2</code> returned more realistic values of topic proportions.<br> ⬜ Topic proportions in <code>mod1</code> and <code>mod2</code> are similar.<br></p>
</blockquote>
<p>Yes. We knew that the documents were dominated by one topic, and <code>mod1</code> captured that. This is an illustration of why argument <code>alpha</code> is important.</p>
</section>
<section id="manipulating-the-vocabulary" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="manipulating-the-vocabulary"><span class="header-section-number">2.5</span> Manipulating the vocabulary</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Manipulating the vocabulary</strong></p>
<p>In this lesson you will learn how to control what words will be included into the document-term matrix, dtm for short.</p>
<p><strong>2. Possible operations</strong></p>
<p>As we transform a corpus of documents into a document-term matrix, we will find ourselves in two types of situations.In one, we know what words we don’t want to see in the dtm.In the other, we know which words we do want to include.These situations are similar, and the choice depends on which set of words is smaller and is easier to specify: the stop words or the needed words.</p>
<p><strong>3. Removing stopwords</strong></p>
<p>Stopwords are words that are considered as noise in text and are removed. A good example of this are indefinite and definite articles <strong>a</strong> and <strong>the</strong>.Stopwords often obscure word associations in topics: they are the most frequent words and show up at the top of the frequency tables, pushing the important words out of sight.The previous lesson had an example showing five most probable words in two topics. Words “the”, “you”, and “to” were in the output and they did not contribute to understanding what the topics were about.</p>
<p><strong>4. Using anti_join()</strong></p>
<p>An inner join of two tables merges the tables using a key: a column that is present in both tables and which indicates which rows are a match. Only the rows with matching key values make it into the output.Opposite to that, anti_join() drops the rows that have matching key values. This is perfect for situations when we remove some rows based on a criterion.Here you are seeing an example where we have a very simple table with word counts. Words “we” and “went” are stopwords. As inner_join(), anti_join() would use columns with the same name as the key. When the names do not match, we need to specify the association using the “by” argument.After we perform anti_join(), the stop words are removed.</p>
<p><strong>5. Keeping the needed words in</strong></p>
<p>There are also situations when it’s easier to specify which words we’d like to keep, rather than having a list of words we’d like to drop.In such situations we can use inner_join()Some scholars who use topic models to analyze novels prefer to keep only nouns.As you will see in the big exercise in this chapter, we will focus our attention on verbs in a history text.Application of inner_join() should be familiar to anybody who worked with dplyr. We only need to specify the correspondence between columns that are used as keys. This is done with argument “by”.In the example, we chose to keep two words: “fishing” and “slept” - and remove everything else.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>Now that you’ve seen the examples, let’s practice.</p>
</section>
<section id="removing-stopwords" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="removing-stopwords"><span class="header-section-number">2.6</span> Removing stopwords</h2>
<p>It takes only one new line of code to remove the stopwords. Fill in the function names to make the code work.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>The <code>anti_join</code> must come in after <code>unnest_tokens</code> but before <code>count</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Create the document-term matrix with stop words removed</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>dtm2 <span class="ot">&lt;-</span> corpus1 <span class="sc">%&gt;%</span></span>
<span id="cb27-3"><a href="#cb27-3"></a>  <span class="fu">unnest_tokens</span>(<span class="at">output=</span>word, <span class="at">input=</span>text) <span class="sc">%&gt;%</span></span>
<span id="cb27-4"><a href="#cb27-4"></a>  <span class="fu">anti_join</span>(stop_words) <span class="sc">%&gt;%</span> </span>
<span id="cb27-5"><a href="#cb27-5"></a>  <span class="fu">count</span>(id, word) <span class="sc">%&gt;%</span></span>
<span id="cb27-6"><a href="#cb27-6"></a>  <span class="fu">cast_dtm</span>(<span class="at">document=</span>id, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Joining, by = "word"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># Display the matrix</span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="fu">as.matrix</span>(dtm2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      Terms
#&gt; Docs  agreed bad bank due fines loans pay late downtown restaurant street
#&gt;   d_1      1   1    1   1     1     1   1    0        0          0      0
#&gt;   d_2      0   0    1   0     1     1   1    1        0          0      0
#&gt;   d_3      0   0    0   0     0     0   0    0        1          1      0
#&gt;   d_4      0   0    0   0     0     0   0    0        0          1      1
#&gt;   d_5      0   0    0   0     0     1   1    0        0          1      0
#&gt;      Terms
#&gt; Docs  warwick
#&gt;   d_1       0
#&gt;   d_2       0
#&gt;   d_3       0
#&gt;   d_4       1
#&gt;   d_5       0</code></pre>
</div>
</div>
<p>Very good. dplyr makes things so easy, right?</p>
</section>
<section id="keeping-the-needed-words" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="keeping-the-needed-words"><span class="header-section-number">2.7</span> Keeping the needed words</h2>
<p>You are given the table with documents <code>corpus</code> and the table <code>dictionary</code> with one column - <code>word</code>, - containing the words we want to keep in the document-term matrix. Use <code>inner_join</code> to create a document-term matrix with the needed words.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Perform inner join on the table <code>dictionary</code>. The column names match, so you do not need to use the <code>by</code> argument.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># assign dictionary (from above)</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>dictionary <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">word =</span> cols)</span>
<span id="cb31-3"><a href="#cb31-3"></a></span>
<span id="cb31-4"><a href="#cb31-4"></a><span class="co"># Perform inner_join with the dictionary table</span></span>
<span id="cb31-5"><a href="#cb31-5"></a>dtm3 <span class="ot">&lt;-</span> corpus1 <span class="sc">%&gt;%</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>  <span class="fu">unnest_tokens</span>(<span class="at">output=</span>word, <span class="at">input=</span>text) <span class="sc">%&gt;%</span></span>
<span id="cb31-7"><a href="#cb31-7"></a>  <span class="fu">inner_join</span>(dictionary) <span class="sc">%&gt;%</span> </span>
<span id="cb31-8"><a href="#cb31-8"></a>  <span class="fu">count</span>(id, word) <span class="sc">%&gt;%</span></span>
<span id="cb31-9"><a href="#cb31-9"></a>  <span class="fu">cast_dtm</span>(<span class="at">document=</span>id, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Joining, by = "word"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># Display the contents of dtm</span></span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="fu">as.matrix</span>(dtm3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      Terms
#&gt; Docs  bank fines loans pay new opened restaurant
#&gt;   d_1    1     1     1   1   0      0          0
#&gt;   d_2    1     1     1   1   0      0          0
#&gt;   d_3    0     0     0   0   1      1          1
#&gt;   d_4    0     0     0   0   1      1          1
#&gt;   d_5    0     0     1   1   0      1          1</code></pre>
</div>
</div>
<p>Well done. An interesting detail: notice how the dtm contains two square blocks, suggesting two topics.</p>
</section>
<section id="word-clouds" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="word-clouds"><span class="header-section-number">2.8</span> Word clouds</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Word clouds</strong></p>
<p>In this lesson you will examine wordclouds. They are less precise than barplots, but offer a quick impressionistic look at the topics.</p>
<p><strong>2. Word clouds</strong></p>
<p>Earlier we used ggplot to make stacked bar charts showing probabilities of words in topics. Bar plots are nice, but when the number of words increases, the plots become hard to read.Instead, we can use wordcloud charts that are less precise but give a good impressionistic feeling for word frequencies and importance.Function wordcloud() requires only a vector of words and a vector of word frequencies.There is no need to sort words by frequency in order to keep only a few - wordcloud() will do that on its own.We will use package “wordcloud” to draw the charts</p>
<p><strong>3. Top 20 words</strong></p>
<p>Here is an example of making a wordcloud that will show the top 20 words.We start by counting the frequencies of words in the whole corpusThen we make a call to function wordcloud().The number of words is controlled by the argument “max.words”. We set it to 20.It is also possible to specify the range of frequencies, minimum and maximum, to be displayed. The default minimum frequency is 3 and we need to override this value.We get a very concise code.</p>
<p><strong>4. Top 20 words</strong></p>
<p>Here is the plot. Words with higher frequencies are displayed in larger font. We are using the default settings for the minimum and maximum font sizes.</p>
<p><strong>5. Adding color and rotations</strong></p>
<p>Black and white wordclouds are okay, but we can do more.To make the chart visually pleasing, we can provide a vector of colors. wordcloud() will cycle through it. There is no specific rule on how the colors will be applied.We can also specify the percentage of words that should be rotated. The default is 0.1 (ten percent).All it takes is to add two more arguments to a call to wordcloud(). I chose “DarkOrange”, “CornflowerBlue” and “DarkRed” as the colors.</p>
<p><strong>6. Word cloud with color</strong></p>
<p>And here is the output.</p>
<p><strong>7. Wordclouds with results of LDA</strong></p>
<p>With a little adjustment, we can use wordclouds to display results of LDA(). The challenge here is that wordcloud() expects integer numbers as frequencies.LDA(), however, returns probabilities, which are floating point numbers less than 1.Because wordcloud() rescales the word size, the solution is to multiply the probabilities by some large number and then truncate the fractional parts.Here we fit a topic model with 2 topics.Now, we extract the matrix with word probabilities using function tidy() and then use mutate() to create a new column “n”: we multiply values of beta by ten thousand and discard the fractional part. Finally, we use filter() to keep only rows with data on word probabilities in topic 1.After this, we are back on familiar ground and can call wordcloud().</p>
<p><strong>8. Top 20 words of topic 1</strong></p>
<p>Here is the result: top twenty words of the first topic, with size proportionate to probability of word in topic.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>Let’s practice our new skills.</p>
</section>
<section id="wordcloud-of-term-frequency" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="wordcloud-of-term-frequency"><span class="header-section-number">2.9</span> Wordcloud of term frequency</h2>
<p>You are given table <code>corpus</code> containing the “toy” corpus with five sentences/documents. You will practice modifying the word cloud to make it more interesting.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Using table <code>corpus</code>, generate the table with counts of words in the whole corpus. Save the result to variable <code>word_frequencies</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># Generate the counts of words in the corpus</span></span>
<span id="cb35-2"><a href="#cb35-2"></a>word_frequencies <span class="ot">&lt;-</span> corpus1 <span class="sc">%&gt;%</span> </span>
<span id="cb35-3"><a href="#cb35-3"></a>  <span class="fu">unnest_tokens</span>(<span class="at">input=</span>text, <span class="at">output=</span>word) <span class="sc">%&gt;%</span></span>
<span id="cb35-4"><a href="#cb35-4"></a>  <span class="fu">count</span>(word)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Create a wordcloud showing top 10 words, with the threshold of minimal word frequency set to 1.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="co"># Load package</span></span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="fu">library</span>(wordcloud)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Lade nötiges Paket: RColorBrewer</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># Create a wordcloud</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="fu">wordcloud</span>(<span class="at">words=</span>word_frequencies<span class="sc">$</span>word, </span>
<span id="cb38-3"><a href="#cb38-3"></a>          <span class="at">freq=</span>word_frequencies<span class="sc">$</span>n,</span>
<span id="cb38-4"><a href="#cb38-4"></a>          <span class="at">min.freq=</span><span class="dv">1</span>,</span>
<span id="cb38-5"><a href="#cb38-5"></a>          <span class="at">max.words=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-16-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Let’s add colors to the wordcloud. Modify the code by setting the argument <code>colors</code> equal to a vector with two values: <code>DarkOrange</code> and <code>Blue</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># Create a wordcloud</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="fu">wordcloud</span>(<span class="at">words=</span>word_frequencies<span class="sc">$</span>word, </span>
<span id="cb39-3"><a href="#cb39-3"></a>          <span class="at">freq=</span>word_frequencies<span class="sc">$</span>n,</span>
<span id="cb39-4"><a href="#cb39-4"></a>          <span class="at">min.freq=</span><span class="dv">1</span>,</span>
<span id="cb39-5"><a href="#cb39-5"></a>          <span class="at">max.words=</span><span class="dv">10</span>,</span>
<span id="cb39-6"><a href="#cb39-6"></a>          <span class="at">colors=</span><span class="fu">c</span>(<span class="st">"DarkOrange"</span>, <span class="st">"Blue"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-17-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="4" type="1">
<li>By default, words are drawn in random order and color assignment is also random. This can be modified by specifying two arguments: <code>random.order</code> and <code>random.color</code>.</li>
<li>Add these arguments to the call of <code>wordcloud</code> and set both of them to <code>FALSE</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Create a wordcloud</span></span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="fu">wordcloud</span>(<span class="at">words=</span>word_frequencies<span class="sc">$</span>word, </span>
<span id="cb40-3"><a href="#cb40-3"></a>          <span class="at">freq=</span>word_frequencies<span class="sc">$</span>n,</span>
<span id="cb40-4"><a href="#cb40-4"></a>          <span class="at">min.freq=</span><span class="dv">1</span>,</span>
<span id="cb40-5"><a href="#cb40-5"></a>          <span class="at">max.words=</span><span class="dv">10</span>,</span>
<span id="cb40-6"><a href="#cb40-6"></a>          <span class="at">colors=</span><span class="fu">c</span>(<span class="st">"DarkOrange"</span>, <span class="st">"Blue"</span>),</span>
<span id="cb40-7"><a href="#cb40-7"></a>          <span class="at">random.order=</span><span class="cn">FALSE</span>,</span>
<span id="cb40-8"><a href="#cb40-8"></a>          <span class="at">random.color=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-18-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Great! If you wanted to take it further, you can use package <code>rcolorBrewer</code> to obtain nice looking color palettes to pass as an argument in <code>wordcloud</code></p>
</section>
<section id="history-of-the-byzantine-empire" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="history-of-the-byzantine-empire"><span class="header-section-number">2.10</span> History of the Byzantine Empire</h2>
<p>Theory. Coming soon …</p>
</section>
<section id="lda-model-fitting---first-iteration" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="lda-model-fitting---first-iteration"><span class="header-section-number">2.11</span> LDA model fitting - first iteration</h2>
<p>This exercise covers the steps from making a document-term matrix to fitting a topic model and examining the terms in topics.</p>
<p>You are given a table <code>history</code> with two columns: <code>chapter</code> for chapter number, and <code>text</code> for chapter text.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a document-term matrix containing counts of words in chapters. Use <code>anti_join</code> to exclude stopwords. Save the result into variable <code>dtm</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># Assign history</span></span>
<span id="cb41-2"><a href="#cb41-2"></a><span class="co"># load("data/history_2.RData")</span></span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="co"># history &lt;- byzantium_clean</span></span>
<span id="cb41-4"><a href="#cb41-4"></a>history <span class="ot">&lt;-</span> tbe_book_chapters</span>
<span id="cb41-5"><a href="#cb41-5"></a></span>
<span id="cb41-6"><a href="#cb41-6"></a><span class="co"># Construct a document-term matrix</span></span>
<span id="cb41-7"><a href="#cb41-7"></a>dtm4 <span class="ot">&lt;-</span> history <span class="sc">%&gt;%</span> </span>
<span id="cb41-8"><a href="#cb41-8"></a>      <span class="fu">unnest_tokens</span>(<span class="at">input=</span>text, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb41-9"><a href="#cb41-9"></a>    <span class="fu">anti_join</span>(stop_words) <span class="sc">%&gt;%</span> </span>
<span id="cb41-10"><a href="#cb41-10"></a>    <span class="fu">count</span>(chapter, word) <span class="sc">%&gt;%</span> </span>
<span id="cb41-11"><a href="#cb41-11"></a>    <span class="fu">cast_dtm</span>(<span class="at">document=</span>chapter, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Joining, by = "word"</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Using the document-term matrix you just built, fit an LDA topic model for four topics. (We will cover how to find the best number of topics in chapter 4.) Use Gibbs method. Do not modify the control list.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="co"># Insert the missing arguments</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm4, <span class="at">k=</span><span class="dv">4</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>, </span>
<span id="cb43-3"><a href="#cb43-3"></a>           <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">seed=</span><span class="dv">10005</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Display the top 15 words of each topic using function <code>terms</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># Display top 15 words of each topic</span></span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="fu">terms</span>(mod4, <span class="at">k=</span><span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;       Topic 1          Topic 2          Topic 3       Topic 4      
#&gt;  [1,] "roman"          "empire"         "empire"      "greek"      
#&gt;  [2,] "war"            "constantinople" "emperor"     "city"       
#&gt;  [3,] "army"           "alexius"        "east"        "letter"     
#&gt;  [4,] "italy"          "emperor"        "throne"      "constantine"
#&gt;  [5,] "king"           "asia"           "reign"       "world"      
#&gt;  [6,] "empire"         "john"           "leo"         "byzantium"  
#&gt;  [7,] "justinian"      "brother"        "byzantine"   "century"    
#&gt;  [8,] "goths"          "son"            "constantine" "church"     
#&gt;  [9,] "heraclius"      "land"           "death"       "palace"     
#&gt; [10,] "danube"         "turks"          "army"        "rome"       
#&gt; [11,] "capital"        "byzantine"      "died"        "single"     
#&gt; [12,] "day"            "city"           "michael"     "empire"     
#&gt; [13,] "constantinople" "war"            "age"         "history"    
#&gt; [14,] "west"           "thrace"         "son"         "military"   
#&gt; [15,] "belisarius"     "arms"           "time"        "roman"</code></pre>
</div>
</div>
<p>Well done. The words were converted into lowercase, but we can pick out some names and action verbs.</p>
</section>
<section id="capturing-the-actions---dtm-with-verbs" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="capturing-the-actions---dtm-with-verbs"><span class="header-section-number">2.12</span> Capturing the actions - dtm with verbs</h2>
<p>In this exercise you will construct the dtm that will consist entirely of verbs, and then re-run the LDA algorithm.</p>
<p>You are given the dataframe <code>verbs</code> containing present and past tense forms of English verbs.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Modify the old code so that instead of removing stopwords it will return a dtm that contains only the past tense verbs. You will need to join on columns <code>word</code> and <code>past</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1"></a><span class="co"># Load data</span></span>
<span id="cb46-2"><a href="#cb46-2"></a>verbs <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/verbs.rds"</span>)</span>
<span id="cb46-3"><a href="#cb46-3"></a></span>
<span id="cb46-4"><a href="#cb46-4"></a><span class="co"># Display the structure of the verbs dataframe</span></span>
<span id="cb46-5"><a href="#cb46-5"></a><span class="fu">str</span>(verbs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; tibble [7,522 × 2] (S3: tbl_df/tbl/data.frame)
#&gt;  $ present: chr [1:7522] "abandon" "abase" "abash" "abate" ...
#&gt;  $ past   : chr [1:7522] "abandoned" "abased" "abashed" "abated" ...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1"></a><span class="co"># Construct a document-term matrix</span></span>
<span id="cb48-2"><a href="#cb48-2"></a>dtm5 <span class="ot">&lt;-</span> history <span class="sc">%&gt;%</span> </span>
<span id="cb48-3"><a href="#cb48-3"></a>    <span class="fu">unnest_tokens</span>(<span class="at">input=</span>text, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb48-4"><a href="#cb48-4"></a>    <span class="fu">inner_join</span>(verbs, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"word"</span><span class="ot">=</span><span class="st">"past"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb48-5"><a href="#cb48-5"></a>    <span class="fu">count</span>(chapter, word) <span class="sc">%&gt;%</span> </span>
<span id="cb48-6"><a href="#cb48-6"></a>    <span class="fu">cast_dtm</span>(<span class="at">document=</span>chapter, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Fit the Gibbs-sampling LDA topic model with four topics. Do not modify the <code>control</code> argument.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a><span class="co"># Fit LDA for four topics</span></span>
<span id="cb49-2"><a href="#cb49-2"></a>mod5 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm5, <span class="at">k=</span><span class="dv">4</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb49-3"><a href="#cb49-3"></a>          <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">seed=</span><span class="dv">10005</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Use function <code>terms</code> to display top 25 words from each topic</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a><span class="co"># Display top 25 words from each topic</span></span>
<span id="cb50-2"><a href="#cb50-2"></a><span class="fu">terms</span>(mod5, <span class="at">k=</span><span class="dv">25</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;       Topic 1     Topic 2        Topic 3       Topic 4     
#&gt;  [1,] "was"       "began"        "kept"        "came"      
#&gt;  [2,] "had"       "cut"          "raised"      "fell"      
#&gt;  [3,] "made"      "dwelt"        "stood"       "sent"      
#&gt;  [4,] "took"      "covered"      "placed"      "appeared"  
#&gt;  [5,] "found"     "knew"         "marked"      "laid"      
#&gt;  [6,] "did"       "continued"    "built"       "reigned"   
#&gt;  [7,] "died"      "did"          "devoted"     "marched"   
#&gt;  [8,] "left"      "displayed"    "turned"      "said"      
#&gt;  [9,] "destined"  "proved"       "headed"      "received"  
#&gt; [10,] "won"       "chose"        "survived"    "refused"   
#&gt; [11,] "lost"      "granted"      "celebrated"  "fled"      
#&gt; [12,] "led"       "burst"        "gave"        "slew"      
#&gt; [13,] "brought"   "strove"       "provoked"    "began"     
#&gt; [14,] "called"    "troubled"     "described"   "routed"    
#&gt; [15,] "formed"    "lasted"       "lived"       "threw"     
#&gt; [16,] "succeeded" "protracted"   "wrote"       "met"       
#&gt; [17,] "conquered" "checked"      "educated"    "cut"       
#&gt; [18,] "followed"  "kept"         "established" "served"    
#&gt; [19,] "forced"    "stormed"      "ground"      "married"   
#&gt; [20,] "held"      "fell"         "bound"       "got"       
#&gt; [21,] "seized"    "loved"        "supported"   "proclaimed"
#&gt; [22,] "broke"     "spoke"        "developed"   "recovered" 
#&gt; [23,] "became"    "accomplished" "reorganized" "ruled"     
#&gt; [24,] "put"       "contrived"    "converted"   "scattered" 
#&gt; [25,] "named"     "preserved"    "produced"    "defeated"</code></pre>
</div>
</div>
<p>Well done. Verbs may be better at capturing multi-century topics rather than names of individuals who did not live that long.</p>
</section>
<section id="making-a-chart" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="making-a-chart"><span class="header-section-number">2.13</span> Making a chart</h2>
<p>Variable <code>mod5</code> contains the LDA model that you fitted in the previous exercise. All necessary libraries have already been loaded for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Make a stacked column chart showing proportions of topics in documents/chapters.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1"></a><span class="co"># Extract matrix gamma and plot it</span></span>
<span id="cb52-2"><a href="#cb52-2"></a><span class="fu">tidy</span>(mod5, <span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb52-3"><a href="#cb52-3"></a>    <span class="fu">mutate</span>(<span class="at">document=</span><span class="fu">as.numeric</span>(document)) <span class="sc">%&gt;%</span> </span>
<span id="cb52-4"><a href="#cb52-4"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>gamma)) <span class="sc">+</span> </span>
<span id="cb52-5"><a href="#cb52-5"></a>    <span class="fu">geom_col</span>(<span class="fu">aes</span>(<span class="at">fill=</span><span class="fu">factor</span>(topic)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-25-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li><p>To remind yourself what the topics were about, display the words with probability above 0.0075 in each topic.</p>
<ul>
<li>Use function <code>terms</code> with argument <code>threshold</code>.</li>
</ul></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a><span class="co"># Display the words whose probability is above the threshold</span></span>
<span id="cb53-2"><a href="#cb53-2"></a><span class="fu">terms</span>(mod5, <span class="at">threshold=</span><span class="fl">0.0075</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; $`Topic 1`
#&gt;  [1] "destined" "did"      "found"    "had"      "left"     "made"    
#&gt;  [7] "took"     "was"      "won"      "died"    
#&gt; 
#&gt; $`Topic 2`
#&gt;  [1] "began"        "continued"    "cut"          "did"          "dwelt"       
#&gt;  [6] "fell"         "loved"        "protracted"   "proved"       "chose"       
#&gt; [11] "covered"      "burst"        "checked"      "granted"      "kept"        
#&gt; [16] "lasted"       "strove"       "troubled"     "spoke"        "accomplished"
#&gt; [21] "contrived"    "displayed"    "knew"         "preserved"    "endeavoured" 
#&gt; [26] "caused"       "stormed"     
#&gt; 
#&gt; $`Topic 3`
#&gt;  [1] "built"       "devoted"     "established" "gave"        "ground"     
#&gt;  [6] "marked"      "placed"      "stood"       "turned"      "bound"      
#&gt; [11] "celebrated"  "described"   "headed"      "lived"       "raised"     
#&gt; [16] "supported"   "survived"    "developed"   "kept"        "reorganized"
#&gt; [21] "converted"   "provoked"    "wrote"       "produced"    "wounded"    
#&gt; [26] "educated"    "immured"    
#&gt; 
#&gt; $`Topic 4`
#&gt;  [1] "appeared" "began"    "came"     "cut"      "fell"     "laid"    
#&gt;  [7] "received" "refused"  "reigned"  "said"     "served"   "threw"   
#&gt; [13] "marched"  "sent"     "fled"     "married"  "slew"     "met"     
#&gt; [19] "routed"   "got"</code></pre>
</div>
</div>
<p>Stacked bars make comparison of topic proportions difficult. It is easier to compare values when they are shown in line plots.</p>
<ol start="3" type="1">
<li>You are given a code that will generate a line plot. Specify new axes labels: set x label to <code>Chapter</code> and y label to <code>Topic probability</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1"></a><span class="co"># Extract matrix gamma and plot it</span></span>
<span id="cb55-2"><a href="#cb55-2"></a><span class="fu">tidy</span>(mod5, <span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb55-3"><a href="#cb55-3"></a>    <span class="fu">mutate</span>(<span class="at">document=</span><span class="fu">as.numeric</span>(document)) <span class="sc">%&gt;%</span> </span>
<span id="cb55-4"><a href="#cb55-4"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>gamma)) <span class="sc">+</span> </span>
<span id="cb55-5"><a href="#cb55-5"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color=</span><span class="fu">factor</span>(topic))) <span class="sc">+</span> </span>
<span id="cb55-6"><a href="#cb55-6"></a>    <span class="fu">labs</span>(<span class="at">x=</span><span class="st">"Chapter"</span>, <span class="at">y=</span><span class="st">"Topic probability"</span>) <span class="sc">+</span></span>
<span id="cb55-7"><a href="#cb55-7"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values=</span><span class="fu">brewer.pal</span>(<span class="at">n=</span><span class="dv">4</span>, <span class="st">"Set1"</span>), <span class="at">name=</span><span class="st">"Topic"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-27-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Great job. You can trace the history of the Byzantine Empire through action-oriented topics.</p>
</section>
<section id="use-wordclouds" class="level2" data-number="2.14">
<h2 data-number="2.14" class="anchored" data-anchor-id="use-wordclouds"><span class="header-section-number">2.14</span> Use wordclouds</h2>
<p>The flaw of function <code>terms()</code> is that it does not display the absolute value of the word probability. By comparison, wordclouds can convey that information through font size. In this exercise you will make wordclouds for topics found in the text on Byzantine Empire.</p>
<p>You are given the object with LDA model <code>mod</code>. You’re going to complete the script to draw four wordlcouds, one for each topic. You will be able to cycle through them in the output window.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Generate a table of word frequencies for each topic.</li>
<li>Display the word cloud. You need to pass the terms to the <code>word</code>argument and the frequencies to the <code>freq</code> argument. The for-loop will do one chart per topic.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a><span class="co"># Display wordclouds one at a time</span></span>
<span id="cb56-2"><a href="#cb56-2"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>) {</span>
<span id="cb56-3"><a href="#cb56-3"></a>  <span class="co"># Generate a table with word frequences for topic j</span></span>
<span id="cb56-4"><a href="#cb56-4"></a>  word_frequencies <span class="ot">&lt;-</span> <span class="fu">tidy</span>(mod5, <span class="at">matrix=</span><span class="st">"beta"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb56-5"><a href="#cb56-5"></a>    <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">trunc</span>(beta <span class="sc">*</span> <span class="dv">10000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb56-6"><a href="#cb56-6"></a>    <span class="fu">filter</span>(topic <span class="sc">==</span> j)</span>
<span id="cb56-7"><a href="#cb56-7"></a></span>
<span id="cb56-8"><a href="#cb56-8"></a>  <span class="co"># Display word cloud</span></span>
<span id="cb56-9"><a href="#cb56-9"></a>  <span class="fu">wordcloud</span>(<span class="at">words =</span> word_frequencies<span class="sc">$</span>term, </span>
<span id="cb56-10"><a href="#cb56-10"></a>            <span class="at">freq =</span> word_frequencies<span class="sc">$</span>n,</span>
<span id="cb56-11"><a href="#cb56-11"></a>            <span class="at">max.words =</span> <span class="dv">20</span>,</span>
<span id="cb56-12"><a href="#cb56-12"></a>            <span class="at">scale =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="fl">0.5</span>),</span>
<span id="cb56-13"><a href="#cb56-13"></a>            <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"DarkOrange"</span>, <span class="st">"CornflowerBlue"</span>, <span class="st">"DarkRed"</span>), </span>
<span id="cb56-14"><a href="#cb56-14"></a>            <span class="at">rot.per =</span> <span class="fl">0.3</span>)</span>
<span id="cb56-15"><a href="#cb56-15"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-28-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-28-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-28-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-28-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-28-3.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-28-4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-28-4.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Wonderful. You can navigate across topics by clicking the ‘Previous Plot’ and ‘Next Plot’ buttons to see other charts.</p>
</section>
</section>
<section id="named-entity-recognition-as-unsupervised-classification" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 3. Named entity recognition as unsupervised classification</h1>
<p>This chapter goes into detail on how LDA topic models can be used as classifiers. It covers the importance of the Dirichlet shape parameter alpha, construction of word contexts for named entities using regex, and technical issues like corpus alignment and held-out data.</p>
<section id="using-topic-models-as-classifiers" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="using-topic-models-as-classifiers"><span class="header-section-number">3.1</span> Using topic models as classifiers</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Using topic models as classifiers</strong></p>
<p>In this lesson you will learn how LDA can be used as a classifier, and what role control parameters alpha and delta play.</p>
<p><strong>2. Topic models as soft classifiers</strong></p>
<p>Although many applications of topic models involve topic discovery - finding out what topics are present in a body of documents - we can also apply topic modeling for document classification. In that case, we do not need to worry about finding the best number of topics. Instead, we need to tune the model so it works best as a classifier.In this chapter we will examine Named Entity Recognition - a task arising in processing text. An entity is a word, or several words, referring to a proper noun - a person, a place, or a corporation, if you are working with business documents.For example, in the sentence “Washington crossed the Delaware”, the reference is to George Washington, a person. In the phrase, “They did a road trip across Washington”, the reference is to a place, Washington state.The context in which the entity appears gives us clues on the entity’s meaning. We can perform topic modeling of the entity context words and use the result as the classification.</p>
<p><strong>3. Effect of control parameter alpha</strong></p>
<p>With the number of topics k known, the control parameters alpha and delta begin to matter.Back in chapter 2 we had a small corpus consisting of five sentences. We knew that there were only two topics: restaurants and loans. First two sentences were about loans, sentences 3 and 4 were about restaurants, and sentence 5 had words related to both topics.Back then you did a short exercise which illustrated the effect of alpha - when alpha was set to 25, each document was equally likely to belong to topic 1 and topic 2 - the split was 50/50.Now we can examine in more detail why this has happened.</p>
<p><strong>4. How LDA fits a model</strong></p>
<p>In order to understand where alpha matters, we need to talk a little bit about how the LDA algorithm fits a model.The key analogy here is with a bag of M&amp;M candy. The candy come in a small number of preset colors. Frequency of colors is fixed at the factory that filled the bag.Imagine that you reach in and grab ten pieces of candy. The probability that you got 5 yellow, 2 brown, 2 blue, and 1 black pieces is modeled by something called multinomial distribution. For comparison, binomial distribution describes the flips of a coin where there are only two outcomes. Multinomial distribution has more than two outcomes.In LDA algorithm, topics correspond to colors, and there are actually two “bags” from which items are drawn. One bag corresponds to documents, another to words.</p>
<p><strong>5. The Dirichlet in LDA</strong></p>
<p>If the topics are color, and documents are candy, how does the LDA algorithm find the probabilities of topics that give the best fit?One approach is to search for each combination, but it becomes very difficult when the number of topics, documents and words goes up.Instead, the LDA algorithm relies on the Dirichlet distribution to produce these values.The Dirichlet distribution returns a set of positive numbers that are less than 1, and sum up to 1 - the properties we want in a probability distribution.Here is an example of five draws from a 3-dimensional Dirichlet distribution. Note how each row adds up to 1. Each row could serve as a guess for the probability of topics in documents.</p>
<p><strong>6. Dirichlet distribution</strong></p>
<p>Here is a chart with the density profile of a 3-dimensional Dirichlet distribution.The dimensions correspond to the corners of a triangle: (1,0,0), (0,1,0), and (0,0,1)Combinations of dimensions will serve as the multinomial distribution modeling probabilities of topics.The peaks in the corners indicate that the most likely outcomes will be where one topic heavily dominates over others in a document.</p>
<p><strong>7. alpha and the shape of Dirichlet distribution</strong></p>
<p>Now we can finally see the effect of alpha on the probabilities of topics in documents.When alpha is greater than 1, the distribution has most of its mass in the center. This means that the more likely combinations of probabilities will be equal fractions of 1. For instance, (0.33, 0.33, 0.33) if we are dealing with 3 topics.When alpha is less than 1, the mass will be concentrated in the corners, and the more likely combinations will be something like (0.8, 0.1, 0.1), heavily favoring one topic.Parameter delta has a similar role, but for the distribution that models probabilities of words belonging to topics.As you will see in the exercises, it matters a lot when we want to use LDA topic modeling as a classifier.</p>
<p><strong>8. Let’s practice!</strong></p>
<p>By now you have seen a lot of slides talking about effect of alpha. Let’s explore this with numbers.</p>
</section>
<section id="same-k-different-alpha" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="same-k-different-alpha"><span class="header-section-number">3.2</span> Same k, different alpha</h2>
<p>You are given a document-term matrix <code>dtm</code> describing the five-sentence corpus of two topics. You will re-run the LDA algorithm, changing the value of <code>alpha</code>, and compare the outcomes.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Experiment with fitting a topic model and clicking Run Code for the following values of <code>alpha</code>: 0.5, 1, 2, and NULL.</li>
<li>When you are done, click Submit Answer with <code>alpha</code> equal to <code>NULL</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1"></a><span class="co"># load data</span></span>
<span id="cb57-2"><a href="#cb57-2"></a>dtm6 <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/dtm6.rds"</span>)</span>
<span id="cb57-3"><a href="#cb57-3"></a></span>
<span id="cb57-4"><a href="#cb57-4"></a><span class="co"># Fit a topic model using LDA with Gibbs sampling</span></span>
<span id="cb57-5"><a href="#cb57-5"></a>mod6_1 <span class="ot">=</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm6, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>, </span>
<span id="cb57-6"><a href="#cb57-6"></a>          <span class="at">control=</span><span class="fu">list</span>(<span class="at">iter=</span><span class="dv">500</span>, <span class="at">thin=</span><span class="dv">1</span>,</span>
<span id="cb57-7"><a href="#cb57-7"></a>                      <span class="at">seed =</span> <span class="dv">12345</span>,</span>
<span id="cb57-8"><a href="#cb57-8"></a>                      <span class="at">alpha=</span><span class="cn">NULL</span>))</span>
<span id="cb57-9"><a href="#cb57-9"></a></span>
<span id="cb57-10"><a href="#cb57-10"></a><span class="co"># Display topic prevalance in documents as a table</span></span>
<span id="cb57-11"><a href="#cb57-11"></a><span class="fu">tidy</span>(mod6_1, <span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> <span class="fu">spread</span>(topic, gamma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["document"],"name":[1],"type":["chr"],"align":["left"]},{"label":["1"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["2"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1047736","2":"0.5370370","3":"0.4629630"},{"1":"109738","2":"0.6625767","3":"0.3374233"},{"1":"1107465","2":"0.4638554","3":"0.5361446"},{"1":"1125897","2":"0.2251656","3":"0.7748344"},{"1":"1131046","2":"0.5375494","3":"0.4624506"},{"1":"1151252","2":"0.4285714","3":"0.5714286"},{"1":"1220615","2":"0.6276596","3":"0.3723404"},{"1":"1244489","2":"0.3513514","3":"0.6486486"},{"1":"1311540","2":"0.6524823","3":"0.3475177"},{"1":"1321339","2":"0.2362637","3":"0.7637363"},{"1":"139715","2":"0.5079365","3":"0.4920635"},{"1":"1409434","2":"0.4184397","3":"0.5815603"},{"1":"1433541","2":"0.6440678","3":"0.3559322"},{"1":"1456314","2":"0.3299492","3":"0.6700508"},{"1":"1459536","2":"0.7410359","3":"0.2589641"},{"1":"1500850","2":"0.7512438","3":"0.2487562"},{"1":"1502170","2":"0.6776316","3":"0.3223684"},{"1":"1506130","2":"0.5279188","3":"0.4720812"},{"1":"1515738","2":"0.6586826","3":"0.3413174"},{"1":"1535764","2":"0.6347305","3":"0.3652695"},{"1":"1544254","2":"0.3333333","3":"0.6666667"},{"1":"1545793","2":"0.6284153","3":"0.3715847"},{"1":"1553861","2":"0.5870968","3":"0.4129032"},{"1":"1565708","2":"0.7900763","3":"0.2099237"},{"1":"1605060","2":"0.7245179","3":"0.2754821"},{"1":"1608501","2":"0.6029963","3":"0.3970037"},{"1":"1608709","2":"0.7299270","3":"0.2700730"},{"1":"1611839","2":"0.6647059","3":"0.3352941"},{"1":"1613951","2":"0.6666667","3":"0.3333333"},{"1":"1615050","2":"0.7211896","3":"0.2788104"},{"1":"1620222","2":"0.5074074","3":"0.4925926"},{"1":"1620526","2":"0.5875706","3":"0.4124294"},{"1":"1623479","2":"0.4121212","3":"0.5878788"},{"1":"1642391","2":"0.5673759","3":"0.4326241"},{"1":"1660700","2":"0.2489796","3":"0.7510204"},{"1":"1719690","2":"0.5291480","3":"0.4708520"},{"1":"1725544","2":"0.3645320","3":"0.6354680"},{"1":"1738598","2":"0.2488688","3":"0.7511312"},{"1":"1744828","2":"0.5036496","3":"0.4963504"},{"1":"1757872","2":"0.4274809","3":"0.5725191"},{"1":"1806630","2":"0.5607477","3":"0.4392523"},{"1":"1806915","2":"0.6456693","3":"0.3543307"},{"1":"1808606","2":"0.3000000","3":"0.7000000"},{"1":"1809769","2":"0.3705179","3":"0.6294821"},{"1":"1812472","2":"0.3177570","3":"0.6822430"},{"1":"1827457","2":"0.3607306","3":"0.6392694"},{"1":"1827906","2":"0.4784173","3":"0.5215827"},{"1":"1829905","2":"0.5904255","3":"0.4095745"},{"1":"1834539","2":"0.2118644","3":"0.7881356"},{"1":"1838456","2":"0.5377778","3":"0.4622222"},{"1":"1854440","2":"0.5833333","3":"0.4166667"},{"1":"213502","2":"0.3837209","3":"0.6162791"},{"1":"221062","2":"0.7828054","3":"0.2171946"},{"1":"231834","2":"0.2408163","3":"0.7591837"},{"1":"301476","2":"0.6140351","3":"0.3859649"},{"1":"310542","2":"0.6410256","3":"0.3589744"},{"1":"324966","2":"0.6129032","3":"0.3870968"},{"1":"327546","2":"0.5142857","3":"0.4857143"},{"1":"346906","2":"0.3148936","3":"0.6851064"},{"1":"443128","2":"0.2874251","3":"0.7125749"},{"1":"456118","2":"0.4081633","3":"0.5918367"},{"1":"456683","2":"0.3602941","3":"0.6397059"},{"1":"538663","2":"0.3825137","3":"0.6174863"},{"1":"550234","2":"0.5270270","3":"0.4729730"},{"1":"603248","2":"0.3296089","3":"0.6703911"},{"1":"607372","2":"0.4574468","3":"0.5425532"},{"1":"607532","2":"0.5759494","3":"0.4240506"},{"1":"635449","2":"0.4230769","3":"0.5769231"},{"1":"732448","2":"0.6692308","3":"0.3307692"},{"1":"743835","2":"0.3455882","3":"0.6544118"},{"1":"814391","2":"0.6045198","3":"0.3954802"},{"1":"8711615","2":"0.4489796","3":"0.5510204"},{"1":"8715892","2":"0.6106870","3":"0.3893130"},{"1":"8750598","2":"0.3780488","3":"0.6219512"},{"1":"8807291","2":"0.3636364","3":"0.6363636"},{"1":"8918276","2":"0.6830986","3":"0.3169014"},{"1":"8951927","2":"0.4747475","3":"0.5252525"},{"1":"9109765","2":"0.6642857","3":"0.3357143"},{"1":"9115706","2":"0.7272727","3":"0.2727273"},{"1":"9123212","2":"0.4347826","3":"0.5652174"},{"1":"9202352","2":"0.5888889","3":"0.4111111"},{"1":"9252957","2":"0.5106383","3":"0.4893617"},{"1":"93292","2":"0.3036649","3":"0.6963351"},{"1":"9424053","2":"0.5520833","3":"0.4479167"},{"1":"9452350","2":"0.4600000","3":"0.5400000"},{"1":"9600080","2":"0.6941176","3":"0.3058824"},{"1":"961860","2":"0.4625000","3":"0.5375000"},{"1":"9619021","2":"0.1949153","3":"0.8050847"},{"1":"9876565","2":"0.7596154","3":"0.2403846"},{"1":"9911295","2":"0.8093220","3":"0.1906780"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-4" data-number="3.3" class="anchored"><span class="header-section-number">3.3</span> <em>Question</em></h2>
<p>Which value of alpha leads to the 50/50 (plus minus 2 percent) split in topic prevalence in all documents?<br> <br> ✅ alpha=NULL<br> ⬜ alpha = 2<br> ⬜ alpha = 1<br> ⬜ alpha = 0.5<br></p>
</blockquote>
<p>Yes. When alpha is NULL, the package sets alpha = 50/k which in our case is 25. This favors topic proportions that are nearly equal to each other.</p>
</section>
<section id="probabilities-of-words-in-topics-1" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="probabilities-of-words-in-topics-1"><span class="header-section-number">3.4</span> Probabilities of words in topics</h2>
<p>Parameter alpha determines the values of probabilities that a document belongs to a topic. Parameter delta does the same for probability distribution of words over topics. By default, delta is set to 0.1. You will fit a model with a different delta and make a plot of results.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Fit the model for delta set to 0.1, create a tidy table containing probabilities <code>beta</code> for words from the <code>my_terms</code> vector and make a stacked column chart from the data.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1"></a><span class="co"># Fit the model for delta = 0.1</span></span>
<span id="cb58-2"><a href="#cb58-2"></a>mod1_5 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm1, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb58-3"><a href="#cb58-3"></a>         <span class="at">control=</span><span class="fu">list</span>(<span class="at">iter=</span><span class="dv">500</span>, <span class="at">seed=</span><span class="dv">12345</span>, <span class="at">alpha=</span><span class="dv">1</span>, <span class="at">delta=</span><span class="fl">0.1</span>))</span>
<span id="cb58-4"><a href="#cb58-4"></a></span>
<span id="cb58-5"><a href="#cb58-5"></a><span class="co"># Define which words we want to examine</span></span>
<span id="cb58-6"><a href="#cb58-6"></a>my_terms <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"loans"</span>, <span class="st">"bank"</span>, <span class="st">"opened"</span>, <span class="st">"pay"</span>, <span class="st">"restaurant"</span>, <span class="st">"you"</span>)</span>
<span id="cb58-7"><a href="#cb58-7"></a></span>
<span id="cb58-8"><a href="#cb58-8"></a><span class="co"># Make a tidy table</span></span>
<span id="cb58-9"><a href="#cb58-9"></a>t <span class="ot">&lt;-</span> <span class="fu">tidy</span>(mod1_5, <span class="st">"beta"</span>) <span class="sc">%&gt;%</span> <span class="fu">filter</span>(term <span class="sc">%in%</span> my_terms)</span>
<span id="cb58-10"><a href="#cb58-10"></a></span>
<span id="cb58-11"><a href="#cb58-11"></a><span class="co"># Make a stacked column chart of word probabilities</span></span>
<span id="cb58-12"><a href="#cb58-12"></a><span class="fu">ggplot</span>(t, <span class="fu">aes</span>(<span class="at">x=</span>term, <span class="at">y=</span>beta)) <span class="sc">+</span> <span class="fu">geom_col</span>(<span class="fu">aes</span>(<span class="at">fill=</span><span class="fu">factor</span>(topic))) <span class="sc">+</span></span>
<span id="cb58-13"><a href="#cb58-13"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x=</span><span class="fu">element_text</span>(<span class="at">angle=</span><span class="dv">90</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-30-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Fit the model for delta set to 0.5, create a tidy table containing probabilities <code>beta</code> for words from the <code>my_terms</code> vector and make a stacked column chart from the data.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1"></a><span class="co"># Fit the model for delta = 0.5</span></span>
<span id="cb59-2"><a href="#cb59-2"></a>mod1_6 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm1, <span class="at">k=</span><span class="dv">2</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb59-3"><a href="#cb59-3"></a>         <span class="at">control=</span><span class="fu">list</span>(<span class="at">iter=</span><span class="dv">500</span>, <span class="at">seed=</span><span class="dv">12345</span>, <span class="at">alpha=</span><span class="dv">1</span>, <span class="at">delta=</span><span class="fl">0.5</span>))</span>
<span id="cb59-4"><a href="#cb59-4"></a></span>
<span id="cb59-5"><a href="#cb59-5"></a><span class="co"># Define which words we want to examine</span></span>
<span id="cb59-6"><a href="#cb59-6"></a>my_terms <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"loans"</span>, <span class="st">"bank"</span>, <span class="st">"opened"</span>, <span class="st">"pay"</span>, <span class="st">"restaurant"</span>, <span class="st">"you"</span>)</span>
<span id="cb59-7"><a href="#cb59-7"></a></span>
<span id="cb59-8"><a href="#cb59-8"></a><span class="co"># Make a tidy table</span></span>
<span id="cb59-9"><a href="#cb59-9"></a>t <span class="ot">&lt;-</span> <span class="fu">tidy</span>(mod1_6, <span class="st">"beta"</span>) <span class="sc">%&gt;%</span> <span class="fu">filter</span>(term <span class="sc">%in%</span> my_terms)</span>
<span id="cb59-10"><a href="#cb59-10"></a></span>
<span id="cb59-11"><a href="#cb59-11"></a><span class="co"># Make a stacked column chart</span></span>
<span id="cb59-12"><a href="#cb59-12"></a><span class="fu">ggplot</span>(t, <span class="fu">aes</span>(<span class="at">x=</span>term, <span class="at">y=</span>beta)) <span class="sc">+</span> <span class="fu">geom_col</span>(<span class="fu">aes</span>(<span class="at">fill=</span><span class="fu">factor</span>(topic))) <span class="sc">+</span></span>
<span id="cb59-13"><a href="#cb59-13"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x=</span><span class="fu">element_text</span>(<span class="at">angle=</span><span class="dv">90</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-31-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Good. The probabilities of words are more even in the second chart, when delta was set to 0.5. Modifying delta lets you control how exclusive the topic should be.</p>
</section>
<section id="from-word-windows-to-dtm" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="from-word-windows-to-dtm"><span class="header-section-number">3.5</span> From word windows to dtm</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. From word windows to dtm</strong></p>
<p>A context of an entity is represented by a word window. In this lesson you will learn how to extract them and transform them into a dtm.</p>
<p><strong>2. Word window</strong></p>
<p>An entity is a word that refers to a personal noun. We usually can tell personal nouns because they are capitalized.A word window is a sequence of words with the entity word in the middle. It is similar to the context, because it shows the words that occur together with the entity.For example, a phrase “attention of Megara was turned” is a window for entity Megara, with two words on the left and two on the right.We can differentiate the words by adding a suffix to tell where the word was originally located: first on the left (L1), or second on the right (R2).</p>
<p><strong>3. A document for every entity</strong></p>
<p>Once we have the word windows for entities, we can combine them, making one document per entity. We can use function paste() inside of summarise() for this purpose.Here is an example of the document for entity <code>Anastasius</code>. It consists of four words which came from two word windows.You can tell that the entity is a person: usually we do not say that a geographic entity “has bequeathed” anything.</p>
<p><strong>4. Finding entities with regular expressions &amp; stringr</strong></p>
<p>How would we find entities? Because an entity is a capitalized word, we can detect it using regular expressions: we will look for a word that has one uppercase letter followed by one or more lowercase letters.We can do that with the basic regex functions: gregexpr() and regmatches().Let’s look at the pattern used in the code example. In the simplest form, a regular expression pattern consists of declarations of character classes and quantifiers.The class can be specified as a range of characters. The range is based on the ASCII codes: “a” has smaller code than “z”, and “0” has smaller code than “9”.Quantifiers specify how many times a character (or a sequence of them) may occur. There is a lower bound “n”, and upper bound “m”.There are some shortcuts: the question mark stands for “occurs zero or one time”, plus sign means “occurs one or more times”, asterisk - “occurs zero or more times”.Our pattern was one upper-case letter followed by one or more lower-case letters.</p>
<p><strong>5. Regular expressions with groups</strong></p>
<p>In regular expressions, parentheses serve to group some patterns together. As a quick aside, parentheses can also serve to capture the groups - store their contents for later use.As an example, take a look at the new pattern for an entity. Compared to the previous one, it will match entities that have <code>St.</code> in them.It includes a group - characters S, t, dot, whitespace, followed by a quantifier.The quantifier is the question mark, meaning that the group can occur zero or one times. In other words, this group is optional. If it occurs, it will be included, and if it does not, the pattern will still match.</p>
<p><strong>6. Using capture groups to add a suffix</strong></p>
<p>And now we can talk about capture groups. Characters inside parentheses can be referenced later on in the pipeline of operations.The most widespread use is in substitution, as in this example. Function gsub() replaces a pattern throughout the whole text string.The pattern contains two capture groups, each one being a lowercase word.The caret symbol is an anchor that specifies position in the string - the start. Dollar sign is the anchor for end of string.The string with backslashes and a number is a reference to a capture group. The search pattern in gsub() says “match two lower case words, one in each group”. The substitution pattern says “insert contents of group 1 followed by underscore L1”, and “insert contents of group 2 followed by underscore L2”The result is that the words on the left now have suffixes and later on we will known which side of the entity they came from.</p>
<p><strong>7. Let’s practice</strong></p>
<p>By now you refreshed upon the basics of regular expressions. Let’s practice and put them to use.</p>
</section>
<section id="regex-patterns-for-entity-matching" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="regex-patterns-for-entity-matching"><span class="header-section-number">3.6</span> Regex patterns for entity matching</h2>
<p>Vector <code>text</code> contains text of chapters of The Byzantine Empire by Charles Oman. You will experiment with the regex patterns for entity matching.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>You are given a pattern that will match a capitalized word and two lowercase words before and after. Find how many times this pattern matched.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1"></a><span class="co"># Create text</span></span>
<span id="cb60-2"><a href="#cb60-2"></a>text <span class="ot">&lt;-</span> tbe_book_chapters<span class="sc">$</span>text</span>
<span id="cb60-3"><a href="#cb60-3"></a></span>
<span id="cb60-4"><a href="#cb60-4"></a><span class="co"># Regex pattern for an entity and word context</span></span>
<span id="cb60-5"><a href="#cb60-5"></a>p1 <span class="ot">&lt;-</span> <span class="st">"( [a-z]+){2}( (St[.] )?[A-Z][a-z]+)+( [a-z]+){2}"</span></span>
<span id="cb60-6"><a href="#cb60-6"></a></span>
<span id="cb60-7"><a href="#cb60-7"></a><span class="co"># Obtain the regex match object from gregexpr</span></span>
<span id="cb60-8"><a href="#cb60-8"></a>m <span class="ot">&lt;-</span> <span class="fu">gregexpr</span>(p1, text)</span>
<span id="cb60-9"><a href="#cb60-9"></a></span>
<span id="cb60-10"><a href="#cb60-10"></a><span class="co"># Get the matches and flatten the list</span></span>
<span id="cb60-11"><a href="#cb60-11"></a>v <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">regmatches</span>(text, m))</span>
<span id="cb60-12"><a href="#cb60-12"></a></span>
<span id="cb60-13"><a href="#cb60-13"></a><span class="co"># Find the number of elements in the vector</span></span>
<span id="cb60-14"><a href="#cb60-14"></a><span class="fu">length</span>(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 1505</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>The vertical bar <code>|</code> in regex means logical OR. You now have a modified pattern, <code>p2</code>, that has a nested group <code>( (of|the) [A-Z][a-z]+)?</code>. It matches entities like ‘Alexander the Great’ or ‘Darius of Persia’.</li>
<li>Find how many entities you match now.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1"></a><span class="co"># Regex pattern for an entity and word context</span></span>
<span id="cb62-2"><a href="#cb62-2"></a>p2 <span class="ot">&lt;-</span> <span class="st">"( [a-z]+){2}( (St[.] )?[A-Z][a-z]+( (of|the) [A-Z][a-z]+)?)+( [a-z]+){2}"</span></span>
<span id="cb62-3"><a href="#cb62-3"></a></span>
<span id="cb62-4"><a href="#cb62-4"></a><span class="co"># Obtain the regex match object from gregexpr</span></span>
<span id="cb62-5"><a href="#cb62-5"></a>m <span class="ot">&lt;-</span> <span class="fu">gregexpr</span>(p2, text)</span>
<span id="cb62-6"><a href="#cb62-6"></a></span>
<span id="cb62-7"><a href="#cb62-7"></a><span class="co"># Get the matches and flatten the list</span></span>
<span id="cb62-8"><a href="#cb62-8"></a>v <span class="ot">=</span> <span class="fu">unlist</span>(<span class="fu">regmatches</span>(text, m))</span>
<span id="cb62-9"><a href="#cb62-9"></a></span>
<span id="cb62-10"><a href="#cb62-10"></a><span class="co"># Find the number of elenents in the vector</span></span>
<span id="cb62-11"><a href="#cb62-11"></a><span class="fu">length</span>(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 1533</code></pre>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-5" data-number="3.7" class="anchored"><span class="header-section-number">3.7</span> <em>Question</em></h2>
<p>Which pattern, <code>p1</code> or <code>p2</code>, returned more matches?<br> <br> ⬜ <code>p1</code> returned more matches.<br> ✅ <code>p2</code> returned more matches.<br> ⬜ They both returned the same number of matches.<br></p>
</blockquote>
<p>Yes! By making the pattern more general, we were able to match more entities: 1533 vs.&nbsp;1505</p>
</section>
<section id="making-a-corpus" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="making-a-corpus"><span class="header-section-number">3.8</span> Making a corpus</h2>
<p>You are given the pattern <code>entity_pattern</code> for the named entity. Vector <code>v</code> contains strings with named entities and two words to the left and to the right of the entity. You are going to make a table containing entity and its context as two columns.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Print out the contents of the pattern string <code>entity_pattern</code>.</li>
<li>Function <code>gsub()</code> can be used to cut out strings by replacing them with zero-length strings, e.g.&nbsp;<code>gsub('[0-9]', "", "Year of 1203 CE")</code> will return <code>Year of  CE</code>. Use this trick to remove the named entity from text. This will produce the entity’s context.</li>
<li>Save the result into variable <code>v2</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1"></a><span class="co"># Print out contents of the `entity_pattern`</span></span>
<span id="cb64-2"><a href="#cb64-2"></a>entity_pattern <span class="ot">&lt;-</span> <span class="st">"( (St[.] )?[A-Z][a-z]+( (of|the) [A-Z][a-z]+)?)+"</span></span>
<span id="cb64-3"><a href="#cb64-3"></a></span>
<span id="cb64-4"><a href="#cb64-4"></a><span class="co"># Remove the named entity from text</span></span>
<span id="cb64-5"><a href="#cb64-5"></a>v2 <span class="ot">&lt;-</span> <span class="fu">gsub</span>(entity_pattern, <span class="st">""</span>, v)</span>
<span id="cb64-6"><a href="#cb64-6"></a></span>
<span id="cb64-7"><a href="#cb64-7"></a><span class="co"># Display the head of v2</span></span>
<span id="cb64-8"><a href="#cb64-8"></a><span class="fu">head</span>(v2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] " into the shore of"       " settlers were of the"   
#&gt; [3] " cities of in the"        " to the to plant"        
#&gt; [5] " attention of was turned" " of the and the"</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Regex capture groups can be used to add suffixes to lowercase. You are given a pattern <code>p</code> that will add suffixes <code>L1</code> and <code>L2</code>. Modify <code>p</code> so that <code>gsub()</code> would also add suffixes <code>R1</code> and <code>R2</code> to words occurring on the right side of the context.</li>
<li>Add backreferences to capture groups 3 and 4 and add suffixes ‘R1’ and ‘R2’ respectively.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1"></a><span class="co"># Remove the named entity</span></span>
<span id="cb66-2"><a href="#cb66-2"></a>v2 <span class="ot">&lt;-</span> <span class="fu">gsub</span>(entity_pattern, <span class="st">""</span>, v)</span>
<span id="cb66-3"><a href="#cb66-3"></a></span>
<span id="cb66-4"><a href="#cb66-4"></a><span class="co"># Pattern for inserting suffixes</span></span>
<span id="cb66-5"><a href="#cb66-5"></a>p <span class="ot">&lt;-</span> <span class="st">"</span><span class="sc">\\</span><span class="st">1_L1 </span><span class="sc">\\</span><span class="st">2_L2 </span><span class="sc">\\</span><span class="st">3_R1 </span><span class="sc">\\</span><span class="st">4_R2"</span></span>
<span id="cb66-6"><a href="#cb66-6"></a></span>
<span id="cb66-7"><a href="#cb66-7"></a><span class="co"># Add suffixes to words</span></span>
<span id="cb66-8"><a href="#cb66-8"></a>context <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">"([a-z]+) ([a-z]+) ([a-z]+) ([a-z]+)"</span>, p, v2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="6" type="1">
<li><p>By now you have vector <code>v2</code> which contains the new “documents” - context words of named entities. You have figured out how to add suffixes to the context words using <code>gsub()</code>. Now, two last steps toward making a corpus: converting the context strings into a data frame, and assigning named entity string as document ids.</p>
<ul>
<li>Generate a regular expression match object by using <code>gregexpr()</code> First argument is the pattern, second argument is text. Store the result in variable <code>re_match</code>.</li>
<li>Extract named entities and make a data frame named <code>corpus</code> with columns <code>doc_id</code> and <code>text</code> to make a document-term matrix. Function <code>regmatches()</code> will return a list with matched strings, which you can flatten by using <code>unlist()</code>.</li>
</ul></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1"></a><span class="co"># Extract named entity and use it as document ID</span></span>
<span id="cb67-2"><a href="#cb67-2"></a>re_match <span class="ot">&lt;-</span>  <span class="fu">gregexpr</span>(entity_pattern, v)</span>
<span id="cb67-3"><a href="#cb67-3"></a>doc_id   <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">regmatches</span>(v, re_match))</span>
<span id="cb67-4"><a href="#cb67-4"></a></span>
<span id="cb67-5"><a href="#cb67-5"></a><span class="co"># Make a data frame with columns doc_id and text</span></span>
<span id="cb67-6"><a href="#cb67-6"></a>corpus2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">doc_id =</span> doc_id, <span class="at">text =</span> context, <span class="at">stringsAsFactors =</span> F)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Very good! Regex functions can be tricky but you figured it out.</p>
</section>
<section id="from-dtm-to-topic-model" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="from-dtm-to-topic-model"><span class="header-section-number">3.9</span> From dtm to topic model</h2>
<p>You are given data frame <code>corpus</code>. Each row corresponds to one occurrence of a named entity. Column <code>doc_id</code> contains the entity, <code>text</code> - the context words with suffixes. You will build a document-term matrix and will fit a topic model.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>We need to combine text from multiple occurrences of the same entity into one document. Using <code>dplyr</code>, for each entity (<code>doc_id</code>) generate a summary variable <code>doc</code> that will contain combined <code>text</code> strings. Save the result into table <code>corpus3</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1"></a><span class="co"># Summarize the text to produce a document for each doc_id</span></span>
<span id="cb68-2"><a href="#cb68-2"></a>corpus3 <span class="ot">&lt;-</span> corpus2 <span class="sc">%&gt;%</span> </span>
<span id="cb68-3"><a href="#cb68-3"></a>            <span class="fu">group_by</span>(doc_id) <span class="sc">%&gt;%</span> </span>
<span id="cb68-4"><a href="#cb68-4"></a>            <span class="fu">summarize</span>(<span class="at">doc =</span> <span class="fu">paste</span>(text, <span class="at">collapse =</span> <span class="st">" "</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Now that we have dataframe <code>corpus3</code>, we are on familiar grounds. Create a document-term matrix and save it into variable <code>dtm7</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1"></a><span class="co"># Make a document-term matrix</span></span>
<span id="cb69-2"><a href="#cb69-2"></a>dtm7 <span class="ot">&lt;-</span> corpus3 <span class="sc">%&gt;%</span> </span>
<span id="cb69-3"><a href="#cb69-3"></a>        <span class="fu">unnest_tokens</span>(<span class="at">input =</span> doc, <span class="at">output =</span> word) <span class="sc">%&gt;%</span> </span>
<span id="cb69-4"><a href="#cb69-4"></a>        <span class="fu">count</span>(doc_id, word) <span class="sc">%&gt;%</span> </span>
<span id="cb69-5"><a href="#cb69-5"></a>        <span class="fu">cast_dtm</span>(<span class="at">document =</span> doc_id, <span class="at">term =</span> word, <span class="at">value =</span> n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Fit a Latent Dirichlet allocation topic model with <code>k=3</code>. Keep the <code>control</code> argument as is.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1"></a><span class="co"># Fit an LDA model for 3 topics</span></span>
<span id="cb70-2"><a href="#cb70-2"></a>mod7 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x =</span> dtm7, <span class="at">k =</span> <span class="dv">3</span>, <span class="at">method =</span> <span class="st">"Gibbs"</span>, </span>
<span id="cb70-3"><a href="#cb70-3"></a>          <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">seed =</span> <span class="dv">12345</span>, <span class="at">iter =</span> <span class="dv">1000</span>, <span class="at">thin =</span> <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Using function <code>tidy</code>, extract matrix <code>gamma</code> with probabilities of topics in documents, and convert it to a wide format using <code>spread</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1"></a><span class="co"># Create a table with probabilities of topics in documents</span></span>
<span id="cb71-2"><a href="#cb71-2"></a>topics <span class="ot">&lt;-</span> <span class="fu">tidy</span>(mod7, <span class="at">matrix=</span><span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb71-3"><a href="#cb71-3"></a>            <span class="fu">spread</span>(topic, gamma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent. Later on we will use this model in a classifier.</p>
</section>
<section id="corpus-alignment-and-classification" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="corpus-alignment-and-classification"><span class="header-section-number">3.10</span> Corpus alignment and classification</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Corpus alignment and classification</strong></p>
<p>In this lesson you will learn how to use a pre-trained model as a classifier on new data, and how this step requires aligning the vocabularies of the new input and the model.</p>
<p><strong>2. Unsupervised classification</strong></p>
<p>In the previous lesson, we fitted a topic model for k=3. It is useful for telling the meaning of a named entity.For example, the code below retrieves the probabilities of three topics for five documents. Three of them - Alboin, Amorium, and Cappadocian - are unknown to me.Probabilities of topics suggest that Alboin is similar to Alexander - it is a person, while Amorium and Cappadocian are related to geographic names.</p>
<p><strong>3. Using pre-trained model</strong></p>
<p>Ideally, we would like to use a pre-trained model to find probabilities of topics for entities that were not seen before.This can be done using function posterior() in package topicmodels.This function accepts a model and new data, and returns posterior probabilities of words in topics and topics in documents.There is a new problem we would face in this: corpus alignment.Internally, LDA algorithm iterates over items which are known by their index number. Thus, two dtm’s with different terms but same number of columns would appear the same to the LDA. It is our responsibility to make sure that the document-term matrix of the new data is consistent with the vocabulary of the model.</p>
<p><strong>4. Corpus alignment</strong></p>
<p>Corpus alignment involves making sure that the document-term matrix for the new data includes only the words that were present in the corpus when the topic model was fitted.An easy way to extract model’s vocabulary is to use tidy() with argument matrix=“beta” - it will return probabilities of words in topics. We really need only the column “term”.We can accomplish corpus alignment by doing a right-join between the new document’s table of counts and the table with model’s vocabulary. Right-join drops the rows of the left-side table if they have no match, and keeps all rows of the right-side table.One side effect of doing the right-join is that we will have to deal with the NA values which will appear in the column with counts and document ids.</p>
<p><strong>5. Handling NA values</strong></p>
<p>Right-join will leave us with NA values in two columns: the word counts, and the doc ID. We could easily handle the word counts with an ifelse() function, changing them to 0.Handling document names is a bit trickier. If we do nothing, then, when we perform cast_dtm(), we will end up with a new document. Its name will be “NA”.So, instead, we will take the first “good” document id and use it to replace NAs. It will not distort the prediction because the word counts in these rows are zeros, anyways.</p>
<p><strong>6. Held-out data</strong></p>
<p>If you took any classes on machine learning, you are familiar with a discussion about over-fitting and that a classifier must be tested on a different set of records than the ones used for training.In topic models, there is a similar concept of held-out data. There are two variations of the approach, though.In one, we would remove whole documents from testing. This is similar to setting aside a dataset for testing in machine learning.In the other approach, a certain percentage of terms (for example, 50%) are removed from some documents, and these documents are used as the test set. The quality of the fit is estimated through “held-out likelihood”.In this course, for the sake of simplicity, we will withhold a small set of full documents and examine how they got classified.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Let’s practice.</p>
</section>
<section id="train-a-topic-model" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="train-a-topic-model"><span class="header-section-number">3.11</span> Train a topic model</h2>
<p>You are given a table <code>corpus3</code>: column <code>doc_id</code> contains the named entity, column <code>doc</code> contains context words of entities. You will take a random sample of documents, construct a training dataset and use it to make a topic model.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Take a sample of 20 random integers in the range from 1 to <code>nrow(corpus2)</code> and assign it to variable <code>r</code>. These will be the testing rows.</li>
<li>Pass a subset of the dtm, with the testing rows excluded, as an argument to <code>unnest_tokens</code>, to create a dtm with training data.</li>
<li>Fit an LDA topic model for <code>k=3</code> on the training data.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1"></a><span class="co"># Set random seed for reproducability</span></span>
<span id="cb72-2"><a href="#cb72-2"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>, <span class="at">sample.kind=</span><span class="st">"Rounding"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in set.seed(12345, sample.kind = "Rounding"): non-uniform 'Rounding'
#&gt; sampler used</code></pre>
</div>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1"></a><span class="co"># Take a sample of 20 random integers, without replacement</span></span>
<span id="cb74-2"><a href="#cb74-2"></a>r <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="at">n=</span><span class="fu">nrow</span>(corpus3), <span class="at">size=</span><span class="dv">20</span>, <span class="at">replace=</span><span class="cn">FALSE</span>)</span>
<span id="cb74-3"><a href="#cb74-3"></a></span>
<span id="cb74-4"><a href="#cb74-4"></a><span class="co"># Generate a document-term matrix</span></span>
<span id="cb74-5"><a href="#cb74-5"></a>train_dtm <span class="ot">&lt;-</span> corpus3[<span class="sc">-</span>r, ] <span class="sc">%&gt;%</span> </span>
<span id="cb74-6"><a href="#cb74-6"></a>                <span class="fu">unnest_tokens</span>(<span class="at">input=</span>doc, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb74-7"><a href="#cb74-7"></a>                <span class="fu">count</span>(doc_id, word) <span class="sc">%&gt;%</span> </span>
<span id="cb74-8"><a href="#cb74-8"></a>                <span class="fu">cast_dtm</span>(<span class="at">document=</span>doc_id, <span class="at">term=</span>word, <span class="at">value=</span>n)</span>
<span id="cb74-9"><a href="#cb74-9"></a></span>
<span id="cb74-10"><a href="#cb74-10"></a><span class="co"># Fit an LDA topic model for k=3</span></span>
<span id="cb74-11"><a href="#cb74-11"></a>train_mod <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>train_dtm, <span class="at">k=</span><span class="dv">3</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb74-12"><a href="#cb74-12"></a>                <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">seed=</span><span class="dv">10001</span>,</span>
<span id="cb74-13"><a href="#cb74-13"></a>                             <span class="at">iter=</span><span class="dv">1000</span>, <span class="at">thin=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Very good! We just trained our classifier.</p>
</section>
<section id="align-corpus" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="align-corpus"><span class="header-section-number">3.12</span> Align corpus</h2>
<p>You have LDA model object <code>train_mod</code> and table <code>corpus3</code> with initial data. You will need to align the corpus of the test records and make a document-term matrix for testing.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Rerun <code>sample.int</code> with <code>set.seed</code> to reproduce the row indices for testing rows.</li>
<li>Extract vocabulary of the training model using <code>tidy</code></li>
<li>Create a table of counts, making sure that you keep only the rows with words that were present in the training data</li>
<li>Generate a document-term matrix with the testing data</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1"></a><span class="co"># Get the test row indices</span></span>
<span id="cb75-2"><a href="#cb75-2"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>, <span class="at">sample.kind=</span><span class="st">"Rounding"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in set.seed(12345, sample.kind = "Rounding"): non-uniform 'Rounding'
#&gt; sampler used</code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1"></a>r <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="at">n=</span><span class="fu">nrow</span>(corpus3), <span class="at">size=</span><span class="dv">20</span>, <span class="at">replace=</span><span class="cn">FALSE</span>)</span>
<span id="cb77-2"><a href="#cb77-2"></a></span>
<span id="cb77-3"><a href="#cb77-3"></a><span class="co"># Extract the vocabulary of the training model</span></span>
<span id="cb77-4"><a href="#cb77-4"></a>model_vocab <span class="ot">&lt;-</span> <span class="fu">tidy</span>(train_mod, <span class="at">matrix=</span><span class="st">"beta"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb77-5"><a href="#cb77-5"></a>  <span class="fu">select</span>(term) <span class="sc">%&gt;%</span> <span class="fu">distinct</span>()</span>
<span id="cb77-6"><a href="#cb77-6"></a></span>
<span id="cb77-7"><a href="#cb77-7"></a><span class="co"># Create a table of counts with aligned vocabularies</span></span>
<span id="cb77-8"><a href="#cb77-8"></a>test_table <span class="ot">&lt;-</span> corpus3[r, ] <span class="sc">%&gt;%</span> <span class="fu">unnest_tokens</span>(<span class="at">input=</span>doc, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb77-9"><a href="#cb77-9"></a>  <span class="fu">count</span>(doc_id, word) <span class="sc">%&gt;%</span></span>
<span id="cb77-10"><a href="#cb77-10"></a>  <span class="fu">right_join</span>(model_vocab, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"word"</span><span class="ot">=</span><span class="st">"term"</span>))</span>
<span id="cb77-11"><a href="#cb77-11"></a></span>
<span id="cb77-12"><a href="#cb77-12"></a><span class="co"># Prepare a document-term matrix</span></span>
<span id="cb77-13"><a href="#cb77-13"></a>test_dtm <span class="ot">&lt;-</span> test_table <span class="sc">%&gt;%</span> </span>
<span id="cb77-14"><a href="#cb77-14"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(doc_id)) <span class="sc">%&gt;%</span> </span>
<span id="cb77-15"><a href="#cb77-15"></a>  <span class="fu">mutate</span>(<span class="at">doc_id =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(doc_id), <span class="fu">first</span>(doc_id), doc_id),</span>
<span id="cb77-16"><a href="#cb77-16"></a>         <span class="at">n =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(n), <span class="dv">0</span>, n)) <span class="sc">%&gt;%</span> </span>
<span id="cb77-17"><a href="#cb77-17"></a>  <span class="fu">cast_dtm</span>(<span class="at">document=</span>doc_id, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent! This exercise packed a lot of steps, but you did them all.</p>
</section>
<section id="classify-test-data" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="classify-test-data"><span class="header-section-number">3.13</span> Classify test data</h2>
<p>You have a data object <code>train_mod</code> with an LDA model, and a document-term matrix <code>test_dtm</code> with data for the test cases. Now you can see how well (or how poorly) our classifier performs.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Obtain posterior probabilities for test documents using function <code>posterior()</code>.</li>
<li>Probabilities of topics in documents are contained in the element <code>$topics</code> inside the <code>result</code> list. Display the matrix with topic probabilities.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1"></a><span class="co"># Obtain posterior probabilities for test documents</span></span>
<span id="cb78-2"><a href="#cb78-2"></a>results <span class="ot">&lt;-</span> <span class="fu">posterior</span>(<span class="at">object=</span>train_mod, <span class="at">newdata=</span>test_dtm)</span>
<span id="cb78-3"><a href="#cb78-3"></a></span>
<span id="cb78-4"><a href="#cb78-4"></a><span class="co"># Display the matrix with topic probabilities</span></span>
<span id="cb78-5"><a href="#cb78-5"></a>results<span class="sc">$</span>topics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                                      1          2         3
#&gt;  Visigothic                  0.5714286 0.14285714 0.2857143
#&gt;  Theodorics                  0.2000000 0.50000000 0.3000000
#&gt;  Stephen Lecapenus           0.3333333 0.33333333 0.3333333
#&gt;  St. Louis                   0.4000000 0.20000000 0.4000000
#&gt;  Po                          0.1428571 0.28571429 0.5714286
#&gt;  Patriarch of Constantinople 0.2857143 0.14285714 0.5714286
#&gt;  Parthian                    0.1428571 0.57142857 0.2857143
#&gt;  Paris                       0.4000000 0.40000000 0.2000000
#&gt;  John Cantacuzenus           0.5000000 0.37500000 0.1250000
#&gt;  Hunnish                     0.1666667 0.66666667 0.1666667
#&gt;  Hun                         0.2857143 0.14285714 0.5714286
#&gt;  Germans                     0.2857143 0.42857143 0.2857143
#&gt;  Gaul                        0.4285714 0.14285714 0.4285714
#&gt;  Gainas                      0.7777778 0.03703704 0.1851852
#&gt;  Empress                     0.1428571 0.42857143 0.4285714
#&gt;  Central Italy               0.2857143 0.14285714 0.5714286
#&gt;  Cappadocian                 0.1666667 0.66666667 0.1666667
#&gt;  Caesar                      0.4000000 0.20000000 0.4000000
#&gt;  Amalphi                     0.2857143 0.14285714 0.5714286
#&gt;  Abu Obeida                  0.2000000 0.40000000 0.4000000</code></pre>
</div>
</div>
<p>Nice! The named entities are used as document IDs, which makes reading the table easy.</p>
</section>
<section id="explore-the-results" class="level2" data-number="3.14">
<h2 data-number="3.14" class="anchored" data-anchor-id="explore-the-results"><span class="header-section-number">3.14</span> Explore the results</h2>
<p>Print out the table <code>results$topics</code> and note the values for the rows where the document name is ” Amalphi”. (The space before the entity is the result of the pattern we used.) Look up the articles in Wikipedia for Amalphi. Choose the option below that describes the results:</p>
<blockquote class="blockquote">
<h2 id="question-6" data-number="3.15" class="anchored"><span class="header-section-number">3.15</span> <em>Question</em></h2>
<p>???<br> <br> ✅ Amalphi is a town and topic 1 corresponds to geographic names.<br> ⬜ Amalphi was a king and topic 1 corresponds to personal names.<br> ⬜ Amalphi is a town and topic 2 corresponds to geographic names.<br></p>
</blockquote>
<p>Yes! There are some exceptions, but topic 1 has the highest probability of being in document ” Amalphi”.</p>
</section>
</section>
<section id="how-many-topics-is-enough" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 4. How many topics is enough?</h1>
<p>This chapter explains the basic methods used in the search for the optimal number of topics. It also covers how to use a single document as a source of data, and how topic numbering can be controlled using seed words.</p>
<section id="finding-the-best-number-of-topics" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="finding-the-best-number-of-topics"><span class="header-section-number">4.1</span> Finding the best number of topics</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Finding the best number of topics</strong></p>
<p>In this chapter you will learn how to determine <code>k</code> - the number of topics that must be provided to the LDA algorithm</p>
<p><strong>2. Approaches</strong></p>
<p>In the examples so far we did not explore what the value of k - the number of topics in a model - should be. In the case of restaurants and loans it was obvious that k should be 2, but in the case of Byzantine Empire it was far from clear. Now we can get a look into how we determine the value of k.In general, there are two approaches. One involves fitting a model, inspecting the words that were assigned to topics, and deciding if the topics make sense. This is known as topic coherence.&lt;/br&gt;If we see words that normally should not occur together, we conclude that coherence is low. For example, if we expect the topic to be about archeology and we see the top words to be site, settlement, excavation, and popsicle, then we know that something went wrong. A popsicle usually does not occur in texts on archeology. &lt;/br&gt;Another approach involves looking at quantitative measures of fit: log-likelihood and perplexity. &lt;/br&gt;Log likelihood is a measure of how plausible the model parameters are given the data&lt;/br&gt;and perplexity is a measure of “surprise” when the model is given new data&lt;/br&gt;</p>
<p><strong>3. Log-likelihood</strong></p>
<p>Likelihood is a measure of how plausible model parameters are given the data.&lt;/br&gt;Calculating the likelihood involves multiplying probabilities of individual outcomes. By taking a logarithm, multiplication is transformed into summation, which is easier to implement, and hence we work with log-likelihood.&lt;/br&gt;All probabilities are values that are less than 1. When x is less than 1, logarithm of x will be less than zero. Log-likelihood is a negative number.&lt;/br&gt;Gibbs sampling is used to find the parameters which would have the highest log-likelihood. Note that the comparison is not by absolute magnitudes, but by position on the numbers scale: a log-likelihood of minus one hundred is better than a log-likelihood value of minus one hundred and five.&lt;/br&gt;Function <code>logLik</code> in <code>topicmodels</code> returns the log-likelihood of an LDA model.&lt;/br&gt;</p>
<p><strong>4. Log-likelihood</strong></p>
<p>This chart shows an example of how LDA searches for the model with the best log-likelihood. There is an initial period, called “burn-in”, where the algorithm is producing suboptimal values, followed by the more or less steady exploration of parameter space.&lt;/br&gt;</p>
<p><strong>5. Perplexity</strong></p>
<p>Perplexity is a mesuare of model’s surprise at the data&lt;/br&gt;It is a positive number&lt;/br&gt;and the less surprise the better. That is, we prefer a model whose perplexity is smaller &lt;/br&gt;Function <code>perplexity</code> in package <code>topicmodels</code> will return the surprise of the model when presented newdata.&lt;/br&gt;</p>
<p><strong>6. Finding the best k</strong></p>
<p>In order to find the best value for k, we fit the model for the varying number of clusters and record the results.&lt;/br&gt;Then we plot the log-likelihood and perplexity &lt;/br&gt;and look for ‘bends’ in the curves which are either the optimum or after which the improvements are not as dramatic.&lt;/br&gt;If you have done k-means clustering, you are familiar with this approach. This is similar to making the “elbow plot” and looking for the value where the curve bends. &lt;/br&gt;In this code example we are using a for-loop to iterate over the values of k&lt;/br&gt;</p>
<p><strong>7. Plot of perplexity</strong></p>
<p>Like in k-means clustering, the trivial solution to the best fit is when the number of clusters is equal the number of documents.However, we are searching for a balance between the smaller number of clusters and the quality of the fit.”k equal six” is a local minimum, which suggests it could be a reasonable choice.</p>
<p><strong>8. Time costs</strong></p>
<p>Searching for the best <code>k</code> can take a lot of time. &lt;/br&gt;The running time is determined by several factors: the number of documents, number of terms, and number of iterations. &lt;/br.The good news is that model fitting can be resumed: we can the model, save it, and then use the result as the starting point later on.&lt;/br&gt;Function <code>LDA</code> accepts a topic model as an object for initialization.&lt;/br&gt;Here is a code example showing how we run the LDA for one thousand iterations, save the model, then use it as the starting point and run the model for two hundred iterations more.&lt;/br&gt;.</p>
<p><strong>9. Practice dataset</strong></p>
<p>In this lesson we will use a simple dataset for practice. It is a corpus of 90 documents&lt;/br&gt;These documents are abstracts of projects approved for funding by the US National Science Foundation&lt;/br&gt;The 90 documents are a sample from the search for four keywords: mathematics, physics, chemistry, and marine biology.&lt;/br&gt;Here is a snippet from one of the documents&lt;/br&gt;</p>
<p><strong>10. Let’s practice</strong></p>
<p>We got the theory. Now it’s time to practice.</p>
</section>
<section id="preparing-the-dtm" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="preparing-the-dtm"><span class="header-section-number">4.2</span> Preparing the dtm</h2>
<p>You are given a dataframe <code>df</code> containing 90 abstracts of NSF awards. Its three columns are <code>Abstract</code>, <code>AwardNumber</code>, and <code>field</code>. Your task is to construct a document-term matrix, with stop words being filtered out. Use <code>AwardNumber</code> as the document ID.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Split the Abstract column into tokens.</li>
<li>Remove stopwords.</li>
<li>Count the number of occurrences.</li>
<li>Create a document term matrix.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1"></a><span class="co"># Load data</span></span>
<span id="cb80-2"><a href="#cb80-2"></a>df <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/df.rds"</span>)</span>
<span id="cb80-3"><a href="#cb80-3"></a></span>
<span id="cb80-4"><a href="#cb80-4"></a><span class="co"># Split the Abstract column into tokens</span></span>
<span id="cb80-5"><a href="#cb80-5"></a>dtm8 <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb80-6"><a href="#cb80-6"></a>  </span>
<span id="cb80-7"><a href="#cb80-7"></a>   <span class="co"># Fix missing spaces after dots and colons (except for decimal places in numbers)</span></span>
<span id="cb80-8"><a href="#cb80-8"></a>   <span class="fu">mutate</span>(<span class="at">Abstract =</span> Abstract <span class="sc">|&gt;</span> <span class="fu">str_replace_all</span>(<span class="st">"(?&lt;!</span><span class="sc">\\</span><span class="st">d)[:</span><span class="sc">\\</span><span class="st">.](?!</span><span class="sc">\\</span><span class="st">d)"</span>, <span class="st">" "</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb80-9"><a href="#cb80-9"></a>  </span>
<span id="cb80-10"><a href="#cb80-10"></a>   <span class="fu">unnest_tokens</span>(<span class="at">input=</span>Abstract, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb80-11"><a href="#cb80-11"></a>   <span class="co"># Remove stopwords</span></span>
<span id="cb80-12"><a href="#cb80-12"></a>   <span class="fu">anti_join</span>(stop_words) <span class="sc">%&gt;%</span> </span>
<span id="cb80-13"><a href="#cb80-13"></a>   <span class="co"># Count the number of occurrences</span></span>
<span id="cb80-14"><a href="#cb80-14"></a>   <span class="fu">count</span>(AwardNumber, word) <span class="sc">%&gt;%</span> </span>
<span id="cb80-15"><a href="#cb80-15"></a>   <span class="co"># Create a document term matrix</span></span>
<span id="cb80-16"><a href="#cb80-16"></a>   <span class="fu">cast_dtm</span>(<span class="at">document=</span>AwardNumber, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Joining, by = "word"</code></pre>
</div>
</div>
<p>Well done! Let’s keep going.</p>
</section>
<section id="filtering-by-word-frequency" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="filtering-by-word-frequency"><span class="header-section-number">4.3</span> Filtering by word frequency</h2>
<p>The small size of our corpus poses a problem: some terms will occur only once and are not useful for inferring the topics. In this exercise your task is to remove the words whose corpus-wide frequency is less than 10. This will require grouping by words and then adding up per-document frequencies.</p>
<p>Unnesting tokens and removing stopwords using <code>anti_join()</code> has already been done for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Count occurrences within documents/awards.</li>
<li>Group the data using <code>word</code> as the grouping variable.</li>
<li>Filter using a nested call to <code>sum(n)</code> for corpus-wide frequency that is 10 or higher.</li>
<li>Ungroup the data and create a document-term matrix.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1"></a><span class="fu">library</span>(SnowballC)</span>
<span id="cb82-2"><a href="#cb82-2"></a><span class="co"># rems &lt;- c( "context", "dai", "ecologi", "ph","qepa", "size")</span></span>
<span id="cb82-3"><a href="#cb82-3"></a></span>
<span id="cb82-4"><a href="#cb82-4"></a>dtm9 <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb82-5"><a href="#cb82-5"></a>  </span>
<span id="cb82-6"><a href="#cb82-6"></a>  <span class="co"># Fix missing spaces after dots and colons (except for decimal places in numbers)</span></span>
<span id="cb82-7"><a href="#cb82-7"></a>   <span class="fu">mutate</span>(<span class="at">Abstract =</span> Abstract <span class="sc">|&gt;</span> <span class="fu">str_replace_all</span>(<span class="st">"(?&lt;!</span><span class="sc">\\</span><span class="st">d)[:</span><span class="sc">\\</span><span class="st">.](?!</span><span class="sc">\\</span><span class="st">d)"</span>, <span class="st">" "</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb82-8"><a href="#cb82-8"></a>  </span>
<span id="cb82-9"><a href="#cb82-9"></a>   <span class="fu">unnest_tokens</span>(<span class="at">input=</span>Abstract, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb82-10"><a href="#cb82-10"></a>   <span class="fu">anti_join</span>(stop_words) <span class="sc">%&gt;%</span> </span>
<span id="cb82-11"><a href="#cb82-11"></a>  </span>
<span id="cb82-12"><a href="#cb82-12"></a>  <span class="fu">mutate</span>(<span class="at">word =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb82-13"><a href="#cb82-13"></a>  </span>
<span id="cb82-14"><a href="#cb82-14"></a>   <span class="co"># Count occurences within documents</span></span>
<span id="cb82-15"><a href="#cb82-15"></a>   <span class="fu">count</span>(AwardNumber, word) <span class="sc">%&gt;%</span></span>
<span id="cb82-16"><a href="#cb82-16"></a>  </span>
<span id="cb82-17"><a href="#cb82-17"></a>  <span class="co"># filter(!(word %in% rems)) |&gt; </span></span>
<span id="cb82-18"><a href="#cb82-18"></a>  </span>
<span id="cb82-19"><a href="#cb82-19"></a>   <span class="co"># Group the data</span></span>
<span id="cb82-20"><a href="#cb82-20"></a>   <span class="fu">group_by</span>(word) <span class="sc">%&gt;%</span> </span>
<span id="cb82-21"><a href="#cb82-21"></a>   <span class="co"># Filter for corpus wide frequency</span></span>
<span id="cb82-22"><a href="#cb82-22"></a>   <span class="fu">filter</span>(<span class="fu">sum</span>(n) <span class="sc">&gt;=</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb82-23"><a href="#cb82-23"></a>   <span class="co"># Ungroup the data andreate a document term matrix</span></span>
<span id="cb82-24"><a href="#cb82-24"></a>   <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb82-25"><a href="#cb82-25"></a>   <span class="fu">cast_dtm</span>(<span class="at">document=</span>AwardNumber, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Joining, by = "word"</code></pre>
</div>
</div>
<p>Excellent. Now we move on to model fitting.</p>
</section>
<section id="fitting-one-model" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="fitting-one-model"><span class="header-section-number">4.4</span> Fitting one model</h2>
<p>With the document-term matrix in hand(load), your task now is to fit a topic model and examine its log-likelihood and perplexity.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a LDA model. Set <code>k=3</code> and <code>method="Gibbs"</code>. Do not modify the <code>control</code> argument.</li>
<li>Retrieve the log-likelihood of the model.</li>
<li>Find perplexity for the dataset.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1"></a>dtm10 <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/dtm10.rds"</span>)</span>
<span id="cb84-2"><a href="#cb84-2"></a></span>
<span id="cb84-3"><a href="#cb84-3"></a><span class="co"># Create a LDA model</span></span>
<span id="cb84-4"><a href="#cb84-4"></a>mod10_1 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm10, <span class="at">method=</span><span class="st">"Gibbs"</span>, <span class="at">k=</span><span class="dv">3</span>, </span>
<span id="cb84-5"><a href="#cb84-5"></a>          <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="fl">0.5</span>, <span class="at">seed=</span><span class="dv">1234</span>, <span class="at">iter=</span><span class="dv">500</span>, <span class="at">thin=</span><span class="dv">1</span>))</span>
<span id="cb84-6"><a href="#cb84-6"></a>          </span>
<span id="cb84-7"><a href="#cb84-7"></a><span class="co"># Retrieve log-likelihood</span></span>
<span id="cb84-8"><a href="#cb84-8"></a><span class="fu">logLik</span>(mod10_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 'log Lik.' -64691.15 (df=1434)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1"></a><span class="co"># Find perplexity</span></span>
<span id="cb86-2"><a href="#cb86-2"></a><span class="fu">perplexity</span>(<span class="at">object=</span>mod10_1, <span class="at">newdata=</span>dtm10)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 291.3196</code></pre>
</div>
</div>
<p>Very good. Just a reminder: log-likelihood is always a negative number, and perplexity - a positive.</p>
</section>
<section id="using-perplexity-to-find-the-best-k" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="using-perplexity-to-find-the-best-k"><span class="header-section-number">4.5</span> Using perplexity to find the best k</h2>
<p>To save you time, you are given a list ‘models’ that contains LDA models fitted for values of <code>k</code> from 2 to 10. You will examine the current quality of fit, let LDA do more iterations on the models, and compare the outcomes.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Generate a plot of perplexity vs.&nbsp;k similar to the one you’ve seen in the lesson.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1"></a><span class="co"># load data</span></span>
<span id="cb88-2"><a href="#cb88-2"></a>models <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/models.rds"</span>)</span>
<span id="cb88-3"><a href="#cb88-3"></a></span>
<span id="cb88-4"><a href="#cb88-4"></a><span class="co"># Display names of elements in the list</span></span>
<span id="cb88-5"><a href="#cb88-5"></a><span class="fu">names</span>(models[[<span class="dv">1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] "k"              "model"          "log_likelihood" "perplexity"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1"></a><span class="co"># Retrieve the values of k and perplexity, and plot perplexity vs k</span></span>
<span id="cb90-2"><a href="#cb90-2"></a>x <span class="ot">&lt;-</span> <span class="fu">sapply</span>(models, <span class="st">'[['</span>, <span class="st">'k'</span>)</span>
<span id="cb90-3"><a href="#cb90-3"></a>y <span class="ot">&lt;-</span> <span class="fu">sapply</span>(models, <span class="st">'[['</span>, <span class="st">'perplexity'</span>)</span>
<span id="cb90-4"><a href="#cb90-4"></a><span class="fu">plot</span>(x, y, <span class="at">xlab=</span><span class="st">"number of clusters, k"</span>, <span class="at">ylab=</span><span class="st">"perplexity score"</span>, <span class="at">type=</span><span class="st">"o"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-47-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-47-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Run each model from the <code>models</code> list for an additional 100 iterations. Record the new perplexity scores. An LDA model can be retrieved using <code>models[[i]]$model</code> reference.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1"></a><span class="co"># Record the new perplexity scores</span></span>
<span id="cb91-2"><a href="#cb91-2"></a>new_perplexity_score <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(models))</span>
<span id="cb91-3"><a href="#cb91-3"></a></span>
<span id="cb91-4"><a href="#cb91-4"></a><span class="co"># Run each model for 100 iterations</span></span>
<span id="cb91-5"><a href="#cb91-5"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(models)) {</span>
<span id="cb91-6"><a href="#cb91-6"></a>  mod10_2 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(<span class="at">x=</span>dtm10, <span class="at">model=</span>models[[i]]<span class="sc">$</span>model,</span>
<span id="cb91-7"><a href="#cb91-7"></a>             <span class="at">control=</span><span class="fu">list</span>(<span class="at">iter=</span><span class="dv">100</span>, <span class="at">seed=</span><span class="dv">12345</span>, <span class="at">thin=</span><span class="dv">1</span>))</span>
<span id="cb91-8"><a href="#cb91-8"></a>  new_perplexity_score[i] <span class="ot">&lt;-</span> <span class="fu">perplexity</span>(<span class="at">object=</span>mod10_2, <span class="at">newdata=</span>dtm10)</span>
<span id="cb91-9"><a href="#cb91-9"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Generate a plot of new perplexity scores.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1"></a><span class="co"># Specify the possible values of k and build the plot</span></span>
<span id="cb92-2"><a href="#cb92-2"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb92-3"><a href="#cb92-3"></a><span class="fu">plot</span>(<span class="at">x=</span>k, <span class="at">y=</span>new_perplexity_score, <span class="at">xlab=</span><span class="st">"number of clusters, k"</span>, </span>
<span id="cb92-4"><a href="#cb92-4"></a>     <span class="at">ylab=</span><span class="st">"perplexity score"</span>, <span class="at">type=</span><span class="st">"o"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10_topic_modeling_files/figure-html/unnamed-chunk-49-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="10_topic_modeling_files/figure-html/unnamed-chunk-49-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-7" data-number="4.6" class="anchored"><span class="header-section-number">4.6</span> <em>Question</em></h2>
<p>Did running the models for additional 100 iterations change the preferred number of topics?<br> <br> ⬜ Yes. The new preferred value of <code>k</code> is 4.<br> ✅ No.&nbsp;The preferred value is the same, <code>k=6</code>.<br></p>
</blockquote>
<p>Yes. The local minimum did not move.</p>
</section>
<section id="topic-models-fitted-to-novels" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="topic-models-fitted-to-novels"><span class="header-section-number">4.7</span> Topic models fitted to novels</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Topic model fitted on one document</strong></p>
<p>In this chapter you will learn how to prepare documents for topic modeling when your data is one long text.</p>
<p><strong>2. Analyzing one (long) novel</strong></p>
<p>So far we had chapters of a book act as individual documents, and we also built documents from the word context of a named entity.Often, especially in digital humanities, you will see applications where a topic model is fitted to one very large textual work, for instance, Moby Dick by Herman Melville.You can see an example of such kind of analysis at the JSTOR Labs Text Analyzer.Chapters are too coarse for such kind of analysis. Instead, the text is split into chunks that are long enough to capture a scene in the plot, but short enough to generate a large number of documents.We will use the chunks of one thousand words, which corresponds to about three pages of text in a novel.</p>
<p><strong>3. Text chunks as chapters</strong></p>
<p>When we worked with chapters, we had a variable containing the chapter number. We used it to group the words and generate their counts. Each chapter would correspond to a row in the document-term matrix.With text chunks, we are on our own.A convenient way to generate the surrogate chapter numbers is to sequentially number the words and then use integer division.Here are two examples showing how integer division works.</p>
<p><strong>4. Generating the document number</strong></p>
<p>We can generate the document numbers with minor modification to the code.After we unnest the tokens,we get a table that contains one word per row. We add new column <code>word_index</code> which contains the row number,and then do integer division.For convenience, we add 1, otherwise the first nine hundred ninety nine words will have chapter number zero.</p>
<p><strong>5. Craft vs.&nbsp;science</strong></p>
<p>The choice of a chunk size is very much a matter of experience rather than exact mathematical formulas.The length of the chunk may depend on the writing style of an author whose work we are analyzing.There is more than one solution to this problem:One is to try different chunk sizes.Another is to incorporate additional information about chapter boundaries, to make sure that a text chunk does not span the boundary.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>For practice, we are going back to the text of The Byzantine Empire. Let’s see what we find.</p>
</section>
<section id="generating-chunk-numbers" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="generating-chunk-numbers"><span class="header-section-number">4.8</span> Generating chunk numbers</h2>
<p>You are given a table <code>history</code> with two columns: <code>chapter</code> for chapter number, and <code>text</code> for chapter text. Assuming a text chunk size of 1000 words, create a new column <code>document_number</code> containing the sequential number of a chunk.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Unnest the tokens.</li>
<li>Assign table row number as the word index number.</li>
<li>Do integer division by 1000 and assign the result to a new column <code>document_number</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1"></a>t <span class="ot">&lt;-</span> history <span class="sc">%&gt;%</span> </span>
<span id="cb93-2"><a href="#cb93-2"></a>        <span class="co"># Unnest the tokens</span></span>
<span id="cb93-3"><a href="#cb93-3"></a>        <span class="fu">unnest_tokens</span>(<span class="at">input=</span>text, <span class="at">output=</span>word) <span class="sc">%&gt;%</span> </span>
<span id="cb93-4"><a href="#cb93-4"></a>        <span class="co"># Create a word index column</span></span>
<span id="cb93-5"><a href="#cb93-5"></a>        <span class="fu">mutate</span>(<span class="at">word_index =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">n</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb93-6"><a href="#cb93-6"></a>        <span class="co"># Create a document number column</span></span>
<span id="cb93-7"><a href="#cb93-7"></a>        <span class="fu">mutate</span>(<span class="at">document_number =</span> word_index <span class="sc">%/%</span> <span class="dv">1000</span> <span class="sc">+</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great. Everything is correct.</p>
</section>
<section id="inner-join-and-cast-dtm" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="inner-join-and-cast-dtm"><span class="header-section-number">4.9</span> Inner join and cast dtm</h2>
<p>You have the table <code>t</code> that you created in the previous exercise. It has columns <code>word</code> and <code>document_number</code>. You also have the table <code>verbs</code> with columns <code>present</code> and <code>past</code> containing present tense and past tense forms of verbs.</p>
<p>Like you did in the second chapter of the course, join both tables to keep only the past tense verbs, and then generate the word counts and create the document-term matrix.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Perform <code>inner_join()</code> using columns <code>word</code> and <code>past</code> as the keys.</li>
<li>Count <code>word</code> using <code>document_number</code> as a grouping variable.</li>
<li>Cast the table into a document-term matrix.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1"></a>dtm11 <span class="ot">&lt;-</span> t <span class="sc">%&gt;%</span> </span>
<span id="cb94-2"><a href="#cb94-2"></a>    <span class="co"># Join verbs on "word" and "past"</span></span>
<span id="cb94-3"><a href="#cb94-3"></a>    <span class="fu">inner_join</span>(verbs, <span class="at">by=</span><span class="fu">c</span>(<span class="st">"word"</span><span class="ot">=</span><span class="st">"past"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb94-4"><a href="#cb94-4"></a>    <span class="co"># Count word</span></span>
<span id="cb94-5"><a href="#cb94-5"></a>    <span class="fu">count</span>(document_number, word) <span class="sc">%&gt;%</span> </span>
<span id="cb94-6"><a href="#cb94-6"></a>    <span class="co"># Create a document-term matrix</span></span>
<span id="cb94-7"><a href="#cb94-7"></a>    <span class="fu">cast_dtm</span>(<span class="at">document=</span>document_number, <span class="at">term=</span>word, <span class="at">value=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Very good!</p>
</section>
<section id="finding-the-best-value-for-k" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="finding-the-best-value-for-k"><span class="header-section-number">4.10</span> Finding the best value for k</h2>
<p>You are given object <code>dtm</code> with the document-term matrix you generated in the previous exercise. You also have a user-defined function <code>p(dtm=___, k=___)</code> that will fit an LDA topic model on matrix <code>dtm</code> for the number of topics <code>k</code> and will return the perplexity score of the model. Here is an example of calling the function for k=3: <code>p(dtm=dtm, k=3)</code>.</p>
<p>Run the function for values of k equal to 5, 6, 7, 8, 9, and 10. Take note of the perplexity values that you receive.</p>
<blockquote class="blockquote">
<h2 id="question-8" data-number="4.11" class="anchored"><span class="header-section-number">4.11</span> <em>Question</em></h2>
<p>Based on perplexity scores, is <code>k=9</code> better than <code>k=5</code>?<br> <br> ⬜ The perplexity scores are equal so both choices are good.<br> ⬜ <code>k=5</code> gives a better (lower) perplexity score than <code>k=9</code>.<br> ✅ <code>k=9</code> has lower perplexity and is a better choice.<br></p>
</blockquote>
<p>Yes, model with <code>k=9</code> has lower perplexity and is better. In chapter 2 we used k=4 as a guess, and you can see that we were very off.</p>
</section>
<section id="locking-topics-by-using-seed-words" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="locking-topics-by-using-seed-words"><span class="header-section-number">4.12</span> Locking topics by using seed words</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Using seed words for initialization</strong></p>
<p><strong>2. Seed for random numbers</strong></p>
<p>So far, every time we called the LDA() function, we included a “seed”argument in the control list. For example, we would have “seed=12345”.This is done to make sure that the result that you obtain is the same as the result that I got when I was preparing the code.As you heard before, LDA with Gibbs sampling is a type of a probabilistic algorithm. It fits the topics by finding the best values for probabilities of words in topics, and of topics in documents. It incorporates an element of randomness into the decisions which neighborhoods of values to explore.One downside of this randomness is that topic number may change between runs. What used to be topic 1 - persons, - in our NER classifier, may no longer be when we run the algorithm tomorrow. Specifying random seed ensures that we end up with the same topic numbering.</p>
<p><strong>3. Seed words</strong></p>
<p>When we specify a seed for random number generation, we essentially specify the full chain of random numbers that LDA will receive. This may mean that we exclude some solutions. Conveniently, the implementation of Gibbs method in the topicmodels package includes an option of initialization by seed words.The benefit of this option is that we get to “lock” topic numbers without specifying random seed.To use it, we need to provide the weights for seed words for topics.The “seedwords” argument requires a matrix as an input.Each row in the matrix corresponds to a topic, and each column - to a term.The algorithm will normalize the weight values, to make sure that they sum up to 1.</p>
<p><strong>4. Example</strong></p>
<p>Here is an example. Consider the tiny dataset that we used before. We had five sentences, which we treated as documents. The total vocabulary size was 34 words. Dimensions of the document-term matrix were 5 rows by 34 columns.A seedwords matrix would need to have two rows, because we want to fit the model for two topics, and the same number of columns as there are tokens - thirty four.In the code, we assign 1 to column “restaurant” in row 1, and to column “loans” in row 2.</p>
<p><strong>5. Example, continued</strong></p>
<p>To see the effect of seedwords, we first fit the topic model only with the numeric seed.The algorithm converges on “loans” becoming topic 1, and “restaurants” - topic 2.When we provide the seedwords argument,the topics get flipped. This happened even though we specified the same numeric seed.</p>
<p><strong>6. Uses</strong></p>
<p>Seedwords are useful for two reasons.First, they make operations with pre-trained models more convenient.As you saw, when you extract the results using tidy() function, there are no symbolic names assigned to topics. Finding the best model involves multiple runs of the algorithm, even for the same value of k.Seedwords give us the ability to know in advance which topic number corresponds to what words.Second, seedwords help with the speed of convergence.The algorithm finds the probabilities of words through semi-random exploration. By providing the starting point, we reduce the number of iterations needed to locate the point in the space of parameters that matches our expectations.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Let’s try in practice what we just saw.</p>
</section>
<section id="topics-without-seedwords" class="level2" data-number="4.13">
<h2 data-number="4.13" class="anchored" data-anchor-id="topics-without-seedwords"><span class="header-section-number">4.13</span> Topics without seedwords</h2>
<p>You are given LDA model <code>mod</code> fitted on the corpus of named entities, for <code>k=3</code>. Each document corresponds to one named entity. Your task will be to determine which topic corresponds to names, and which - to geographic entities.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Using <code>tidy</code>, convert matrix <code>gamma</code> to tidy table.</li>
<li>Convert the table from tidy to wide format.</li>
<li>Display the rows in which column <code>document</code> matches entities ” Adrianople”, ” Emperor Heraclius”, ” Daniel”, ” Africa”, and ” African”.</li>
</ol>
<p><strong>create mod</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1"></a><span class="co"># Store the names of documents in a vector</span></span>
<span id="cb95-2"><a href="#cb95-2"></a>required_documents <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">" Africa"</span>, <span class="st">" Emperor Heraclius"</span>, </span>
<span id="cb95-3"><a href="#cb95-3"></a>                       <span class="st">" Adrianople"</span>, <span class="st">" Daniel"</span>, <span class="st">" African"</span>)</span>
<span id="cb95-4"><a href="#cb95-4"></a></span>
<span id="cb95-5"><a href="#cb95-5"></a><span class="co"># Convert table into wide format</span></span>
<span id="cb95-6"><a href="#cb95-6"></a><span class="fu">tidy</span>(mod, <span class="at">matrix=</span><span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb95-7"><a href="#cb95-7"></a>   <span class="fu">spread</span>(<span class="at">key=</span>topic, <span class="at">value=</span>gamma) <span class="sc">%&gt;%</span> </span>
<span id="cb95-8"><a href="#cb95-8"></a>   <span class="co"># Keep only the rows with document names matching the required documents</span></span>
<span id="cb95-9"><a href="#cb95-9"></a>   <span class="fu">filter</span>(document <span class="sc">%in%</span> required_documents)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Good. Notice that Daniel and Emperor Heraclius have higher values in column 1.</p>
</section>
<section id="topics-with-seedwords" class="level2" data-number="4.14">
<h2 data-number="4.14" class="anchored" data-anchor-id="topics-with-seedwords"><span class="header-section-number">4.14</span> Topics with seedwords</h2>
<p>You are given a document-term matrix <code>dtm</code> for the named entities. Your task is to create a seedwords matrix, initialize it so that topic 1 would correspond to persons, topic 2 - to places, fit the model, and examine the topic probabilities for documents. As a reminder, the terms in <code>dtm</code> are the context words with suffix indicating position, e.g.&nbsp;“to_l2”.</p>
<p>An empty matrix <code>seedword</code> has been created for you, with number of rows equal to the number of topics <code>k</code> and number of columns equal to the number of terms in <code>dtm</code>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Set the column names of the matrix equal to column names of <code>dtm</code>.</li>
<li>Set the weight for “defeated_l2” in topic 1 equal to 1, same for “across_l2” in topic 2.</li>
<li>Fit the topic model using seedwords for <code>k=3</code>.</li>
<li>Display the topic probabilities for documents ” Daniel”, ” Adrianople”, and ” African”.</li>
</ol>
<p><strong>create dtm</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1"></a><span class="co"># Set up the column names</span></span>
<span id="cb96-2"><a href="#cb96-2"></a><span class="fu">colnames</span>(seedwords) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(dtm)</span>
<span id="cb96-3"><a href="#cb96-3"></a></span>
<span id="cb96-4"><a href="#cb96-4"></a><span class="co"># Set the weights</span></span>
<span id="cb96-5"><a href="#cb96-5"></a>seedwords[<span class="dv">1</span>, <span class="st">"defeated_l2"</span>] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb96-6"><a href="#cb96-6"></a>seedwords[<span class="dv">2</span>, <span class="st">"across_l2"</span>] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb96-7"><a href="#cb96-7"></a></span>
<span id="cb96-8"><a href="#cb96-8"></a><span class="co"># Fit the topic model</span></span>
<span id="cb96-9"><a href="#cb96-9"></a>mod <span class="ot">&lt;-</span> <span class="fu">LDA</span>(dtm, <span class="at">k=</span><span class="dv">3</span>, <span class="at">method=</span><span class="st">"Gibbs"</span>,</span>
<span id="cb96-10"><a href="#cb96-10"></a>         <span class="at">seedwords=</span>seedwords,</span>
<span id="cb96-11"><a href="#cb96-11"></a>         <span class="at">control=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="dv">1</span>, <span class="at">iter=</span><span class="dv">500</span>, <span class="at">seed=</span><span class="dv">1234</span>))</span>
<span id="cb96-12"><a href="#cb96-12"></a></span>
<span id="cb96-13"><a href="#cb96-13"></a><span class="co"># Examine topic assignment in the fitted model</span></span>
<span id="cb96-14"><a href="#cb96-14"></a><span class="fu">tidy</span>(mod, <span class="st">"gamma"</span>) <span class="sc">%&gt;%</span> <span class="fu">spread</span>(topic, gamma) <span class="sc">%&gt;%</span> </span>
<span id="cb96-15"><a href="#cb96-15"></a>\<span class="fu">tfilter</span>(document <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">" Daniel"</span>, <span class="st">" Adrianople"</span>, <span class="st">" African"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great! By setting up seed words, we forced the model to switch topics. Topic 1 is no longer the most probable for document ’ Daniel’</p>
</section>
<section id="final-words-and-more-things-to-learn" class="level2" data-number="4.15">
<h2 data-number="4.15" class="anchored" data-anchor-id="final-words-and-more-things-to-learn"><span class="header-section-number">4.15</span> Final words (and more things to learn)</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Final words (and more things to learn)</strong></p>
<p>Congratulations! You made it to the end. In case you are wondering, here are a few final words and some ideas on what you could do next.</p>
<p><strong>2. Not just words</strong></p>
<p>Even though we used text as data in this course, topic models are not just about words. In its core, LDA topic modeling is a clustering algorithm.It produces “soft clustering” - in contrast to “hard” clustering where we get a firm assignment of an item to a cluster, LDA topic model returns a probability.An important restriction is that LDA uses counts data - its inputs must be integer numbers.This method can be used if, for instance, you are doing customer segmentation and have counts of how many events a customer attended, or what stores they visited.With minor transformations, topic modeling can be used on non-integer data, like representing shipping routes using coordinates from ship logs.</p>
<p><strong>3. Structured topic models - STM</strong></p>
<p>Latent Dirichlet Allocation is not the only method to fit a topic model. Another method is VEM - variational expectation-maximization.It can be applied in situations when topic proportions are correlated with each other.In such cases the proportions follow a multivariate normal distribution,A prominent package implementing these methods is “stm” - short for Structured Topic Models, created by Margaret Roberts, Brandon Stewart, Brandon Lingley, and Kenneth Benoit It can estimate a regression model with topic proportions as the dependent variables and some other attributes of documents as the independent variables.To name a few of its other features, “stm” automatically does corpus alignment, which we had to code by hand. It implements held-out data as omitted words within the document.It is compatible with the package topicmodels with respect to the input data, and it can also use the result of LDA topic modeling as the starting point for its own model fitting.</p>
<p><strong>4. Deep learning and word embeddings</strong></p>
<p>There is one interesting connection between what we did in chapter 3 and word embeddings - a method that emerged within deep learning community. The word embeddings are also known as word-to-vector models.Word-to-vector models are generated by training a neural network to predict words that occur adjacent to the target word, within a small distance from it. Plus-minus n, with n typically equal to 2 or 4.The number of words can be up to a million, but in the end each word is represented by a numeric vector of much smaller dimensions: 25, 50, a hundred, or 300 depending on a model.If you feel that this resembles the word windows that we constructed in chapter 3, you are right.There are some distinctions, though.First, word-to-vector models use very large corpora for training. It is not uncommon to see models that used over a billion words, often scraped from web documents, as input.Second, word-to-vector models do not make accommodations for multi-word entities.These models take a long time to train.You can use package “wordVectors” created by Ben Schmidt to experiment.</p>
<p><strong>5. Go out and play!</strong></p>
<p>If this was a video in the middle of a chapter, this slide would have said “Let’s go and practice”. But we are done! And so instead, the final message to you is “Go out and play!” Have fun with the new skills that you learned. Bye bye!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">7.9: Support Vector Machines</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="pagination-link">
        <span class="nav-page-text">7.11: Hyperparameter Tuning</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Content 2022 by <a href="https://www.startupengineer.io/authors/schwarz/">Joschka Schwarz</a> <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with and <a href="https://quarto.org/">Quarto</a><br> <a href="https://www.github.com/jwarz/jwarz.github.io">View the source at GitHub</a></div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","openEffect":"zoom","closeEffect":"zoom","selector":".lightbox","loop":true});</script>



<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>