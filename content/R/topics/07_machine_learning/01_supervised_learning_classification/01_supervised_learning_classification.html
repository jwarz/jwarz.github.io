<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Joschka Schwarz">
<title>Joschka Schwarz - Supervised Learning in R: Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" rel="next">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<link href="../../../../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../../../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>
</head>
<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg navbar-dark "><div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Joschka Schwarz</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
<li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-science" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-science">
<li>
    <a class="dropdown-item" href="../../../../../content/R/index.html">
 <span class="dropdown-text">R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/python/index.html">
 <span class="dropdown-text">Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/sql/index.html">
 <span class="dropdown-text">SQL</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../../slides/index.html">
 <span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../resumes/index.html">
 <span class="menu-text">Resumes</span></a>
  </li>  
</ul>
<ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jwarz/"><i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/j-schwarz"><i class="bi bi-linkedin" role="img" aria-label="Quarto LinkedIn">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
<div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Supervised Learning in R: Classification</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto"><div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <b><i>1 Programming Basics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/01_programming_beginner/01_programming_beginner.html" class="sidebar-item-text sidebar-link">1.1: Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/02_programming_intermediate/02_programming_intermediate.html" class="sidebar-item-text sidebar-link">1.2: Intermediate R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/03_programming_tidyverse/03_programming_tidyverse.html" class="sidebar-item-text sidebar-link">1.3: Introduction to the tidyverse</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>2 Importing Data</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/01_importing_data_beginner/01_importing_data_beginner.html" class="sidebar-item-text sidebar-link">2.1: Introduction to Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/02_importing_data_intermediate/02_importing_data_intermediate.html" class="sidebar-item-text sidebar-link">2.2: Intermediate Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/03_working_with_web_data/03_working_with_web_data.html" class="sidebar-item-text sidebar-link">2.3: Working with web data</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>3 Data Wrangling</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/01_data_manipulation_with_dplyr/01_data_manipulation_with_dplyr.html" class="sidebar-item-text sidebar-link">3.1: Data Manipulation with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/02_joining_data_with_dplyr/02_joining_data_with_dplyr.html" class="sidebar-item-text sidebar-link">3.2: Joining data with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/exploratory_data_analysis_in_r/exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.3: Exploratory Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/case_study_exploratory_data_analysis_in_r/case_study_exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.4: Case Study: EDA</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/cleaning_data_in_r/cleaning_data_in_r.html" class="sidebar-item-text sidebar-link">3.5: Cleaning Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/data_manipulation_with_datatable/data_manipulation_with_datatable.html" class="sidebar-item-text sidebar-link">3.6: Data Manipulation with data.table</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/joining_data_with_datatable/joining_data_with_datatable.html" class="sidebar-item-text sidebar-link">3.7: Joining Data with data.table</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>4 Data Visualization</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/introduction_to_data_visualization_with_ggplot2/ggplot2_intro.html" class="sidebar-item-text sidebar-link">4.1: Introduction to Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/data_visualization_with_ggplot2_intermediate/ggplot2_intermediate.html" class="sidebar-item-text sidebar-link">4.2: Intermediate Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>5 Statistics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/01_statistics_beginner/01_statistics_beginner.html" class="sidebar-item-text sidebar-link">5.1: Introduction to Statistics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/02_foundations_of_probability_in_r/02_foundations_of_probability_in_r.html" class="sidebar-item-text sidebar-link">5.2: Foundations of Probability</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/03_regression_beginner/03_regression_beginner.html" class="sidebar-item-text sidebar-link">5.3: Introduction to Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/04_regression_intermediate/04_regression_intermediate.html" class="sidebar-item-text sidebar-link">5.4: Intermediate Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/06_modeling_with_data_in_the_tidyverse/06_modeling_with_data_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">5.5: Modeling with Data in the Tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/09_experimental_design/09_experimental_design.html" class="sidebar-item-text sidebar-link">5.6: Experimental Design</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/10_ab_testing_in_r/10_ab_testing_in_r.html" class="sidebar-item-text sidebar-link">5.7: A/B Testing</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/19_fundamentals_of_bayesian_data_analysis/19_fundamentals_of_bayesian_data_analysis.html" class="sidebar-item-text sidebar-link">5.8: Fundamentals of Bayesian Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/21_factor_analysis_in_r/21_factor_analysis_in_r.html" class="sidebar-item-text sidebar-link">5.9: Factor Analysis</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>7 Machine Learning</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/01_supervised_learning_classification/01_supervised_learning_classification.html" class="sidebar-item-text sidebar-link active">7.1: Supervised Learning: Classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="sidebar-item-text sidebar-link">7.2: Supervised Learning: Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/03_unsupervised_learning/03_unsupervised_learning.html" class="sidebar-item-text sidebar-link">7.3: Unsupervised Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">7.4: Machine Learning in the tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/05_cluster_analysis/05_cluster_analysis.html" class="sidebar-item-text sidebar-link">7.5: Cluster Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/06_machine_learning_with_caret/06_machine_learning_with_caret.html" class="sidebar-item-text sidebar-link">7.6: Machine Learning with caret</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/07_modeling_with_tidymodels/07_modeling_with_tidymodels.html" class="sidebar-item-text sidebar-link">7.7: Modeling with tidymodels</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/08_machine_learning_with_tree-based_models/08_machine_learning_with_tree-based_models.html" class="sidebar-item-text sidebar-link">7.8: Machine Learning with tree-based Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" class="sidebar-item-text sidebar-link">7.9: Support Vector Machines</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" class="sidebar-item-text sidebar-link">7.10: Topic Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="sidebar-item-text sidebar-link">7.11: Hyperparameter Tuning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" class="sidebar-item-text sidebar-link">7.12: Bayesian Regression Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" class="sidebar-item-text sidebar-link">7.13: Introduction to Spark</a>
  </div>
</li>
      </ul>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">R Manuals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/test.html" class="sidebar-item-text sidebar-link">1: An Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/02-content.html" class="sidebar-item-text sidebar-link">2: R Data Import/Export</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/03-content.html" class="sidebar-item-text sidebar-link">3: R Installation and Administration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/04-content.html" class="sidebar-item-text sidebar-link">4: Writing R Extensions</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/05-content.html" class="sidebar-item-text sidebar-link">5: R Language Definition</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/06-content.html" class="sidebar-item-text sidebar-link">6: R Internals</a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Contents</h2>
   
  <ul class="collapse">
<li>
<a href="#chapter-1-k-nearest-neighbors-knn" id="toc-chapter-1-k-nearest-neighbors-knn" class="nav-link active" data-scroll-target="#chapter-1-k-nearest-neighbors-knn"><span class="toc-section-number">1</span>  Chapter 1: k-Nearest Neighbors (kNN)</a>
  <ul class="collapse">
<li><a href="#classification-with-nearest-neighbors" id="toc-classification-with-nearest-neighbors" class="nav-link" data-scroll-target="#classification-with-nearest-neighbors"><span class="toc-section-number">1.1</span>  Classification with Nearest Neighbors</a></li>
  <li><a href="#recognizing-a-road-sign-with-knn" id="toc-recognizing-a-road-sign-with-knn" class="nav-link" data-scroll-target="#recognizing-a-road-sign-with-knn"><span class="toc-section-number">1.2</span>  Recognizing a road sign with kNN</a></li>
  <li><a href="#thinking-like-knn" id="toc-thinking-like-knn" class="nav-link" data-scroll-target="#thinking-like-knn"><span class="toc-section-number">1.3</span>  Thinking like kNN</a></li>
  <li><a href="#exploring-the-traffic-sign-dataset" id="toc-exploring-the-traffic-sign-dataset" class="nav-link" data-scroll-target="#exploring-the-traffic-sign-dataset"><span class="toc-section-number">1.5</span>  Exploring the traffic sign dataset</a></li>
  <li><a href="#classifying-a-collection-of-road-signs" id="toc-classifying-a-collection-of-road-signs" class="nav-link" data-scroll-target="#classifying-a-collection-of-road-signs"><span class="toc-section-number">1.6</span>  Classifying a collection of road signs</a></li>
  <li><a href="#what-about-the-k-in-knn" id="toc-what-about-the-k-in-knn" class="nav-link" data-scroll-target="#what-about-the-k-in-knn"><span class="toc-section-number">1.7</span>  What about the ‘k’ in kNN?</a></li>
  <li><a href="#understanding-the-impact-of-k" id="toc-understanding-the-impact-of-k" class="nav-link" data-scroll-target="#understanding-the-impact-of-k"><span class="toc-section-number">1.8</span>  Understanding the impact of ‘k’</a></li>
  <li><a href="#testing-other-k-values" id="toc-testing-other-k-values" class="nav-link" data-scroll-target="#testing-other-k-values"><span class="toc-section-number">1.10</span>  Testing other ‘k’ values</a></li>
  <li><a href="#seeing-how-the-neighbors-voted" id="toc-seeing-how-the-neighbors-voted" class="nav-link" data-scroll-target="#seeing-how-the-neighbors-voted"><span class="toc-section-number">1.11</span>  Seeing how the neighbors voted</a></li>
  <li><a href="#data-preparation-for-knn" id="toc-data-preparation-for-knn" class="nav-link" data-scroll-target="#data-preparation-for-knn"><span class="toc-section-number">1.12</span>  Data preparation for kNN</a></li>
  <li><a href="#why-normalize-data" id="toc-why-normalize-data" class="nav-link" data-scroll-target="#why-normalize-data"><span class="toc-section-number">1.13</span>  Why normalize data?</a></li>
  </ul>
</li>
  <li>
<a href="#chapter-2-naive-bayes" id="toc-chapter-2-naive-bayes" class="nav-link" data-scroll-target="#chapter-2-naive-bayes"><span class="toc-section-number">2</span>  2. Chapter 2: Naive Bayes</a>
  <ul class="collapse">
<li><a href="#understanding-bayesian-methods" id="toc-understanding-bayesian-methods" class="nav-link" data-scroll-target="#understanding-bayesian-methods"><span class="toc-section-number">2.1</span>  Understanding Bayesian methods</a></li>
  <li><a href="#computing-probabilities" id="toc-computing-probabilities" class="nav-link" data-scroll-target="#computing-probabilities"><span class="toc-section-number">2.2</span>  Computing probabilities</a></li>
  <li><a href="#understanding-dependent-events" id="toc-understanding-dependent-events" class="nav-link" data-scroll-target="#understanding-dependent-events"><span class="toc-section-number">2.3</span>  Understanding dependent events</a></li>
  <li><a href="#a-simple-naive-bayes-location-model" id="toc-a-simple-naive-bayes-location-model" class="nav-link" data-scroll-target="#a-simple-naive-bayes-location-model"><span class="toc-section-number">2.5</span>  A simple Naive Bayes location model</a></li>
  <li><a href="#examining-raw-probabilities" id="toc-examining-raw-probabilities" class="nav-link" data-scroll-target="#examining-raw-probabilities"><span class="toc-section-number">2.6</span>  Examining “raw” probabilities</a></li>
  <li><a href="#understanding-independence" id="toc-understanding-independence" class="nav-link" data-scroll-target="#understanding-independence"><span class="toc-section-number">2.7</span>  Understanding independence</a></li>
  <li><a href="#understanding-nbs-naivety" id="toc-understanding-nbs-naivety" class="nav-link" data-scroll-target="#understanding-nbs-naivety"><span class="toc-section-number">2.9</span>  Understanding NB’s “naivety”</a></li>
  <li><a href="#who-are-you-calling-naive" id="toc-who-are-you-calling-naive" class="nav-link" data-scroll-target="#who-are-you-calling-naive"><span class="toc-section-number">2.10</span>  Who are you calling naive?</a></li>
  <li><a href="#a-more-sophisticated-location-model" id="toc-a-more-sophisticated-location-model" class="nav-link" data-scroll-target="#a-more-sophisticated-location-model"><span class="toc-section-number">2.12</span>  A more sophisticated location model</a></li>
  <li><a href="#preparing-for-unforeseen-circumstances" id="toc-preparing-for-unforeseen-circumstances" class="nav-link" data-scroll-target="#preparing-for-unforeseen-circumstances"><span class="toc-section-number">2.13</span>  Preparing for unforeseen circumstances</a></li>
  <li><a href="#understanding-the-laplace-correction" id="toc-understanding-the-laplace-correction" class="nav-link" data-scroll-target="#understanding-the-laplace-correction"><span class="toc-section-number">2.14</span>  Understanding the Laplace correction</a></li>
  <li><a href="#applying-naive-bayes-to-other-problems" id="toc-applying-naive-bayes-to-other-problems" class="nav-link" data-scroll-target="#applying-naive-bayes-to-other-problems"><span class="toc-section-number">2.16</span>  Applying Naive Bayes to other problems</a></li>
  <li><a href="#handling-numeric-predictors" id="toc-handling-numeric-predictors" class="nav-link" data-scroll-target="#handling-numeric-predictors"><span class="toc-section-number">2.17</span>  Handling numeric predictors</a></li>
  </ul>
</li>
  <li>
<a href="#chapter-3-logistic-regression" id="toc-chapter-3-logistic-regression" class="nav-link" data-scroll-target="#chapter-3-logistic-regression"><span class="toc-section-number">3</span>  3. Chapter 3: Logistic Regression</a>
  <ul class="collapse">
<li><a href="#making-binary-predictions-with-regression" id="toc-making-binary-predictions-with-regression" class="nav-link" data-scroll-target="#making-binary-predictions-with-regression"><span class="toc-section-number">3.1</span>  Making binary predictions with regression</a></li>
  <li><a href="#building-simple-logistic-regression-models" id="toc-building-simple-logistic-regression-models" class="nav-link" data-scroll-target="#building-simple-logistic-regression-models"><span class="toc-section-number">3.2</span>  Building simple logistic regression models</a></li>
  <li><a href="#making-a-binary-prediction" id="toc-making-a-binary-prediction" class="nav-link" data-scroll-target="#making-a-binary-prediction"><span class="toc-section-number">3.3</span>  Making a binary prediction</a></li>
  <li><a href="#the-limitations-of-accuracy" id="toc-the-limitations-of-accuracy" class="nav-link" data-scroll-target="#the-limitations-of-accuracy"><span class="toc-section-number">3.4</span>  The limitations of accuracy</a></li>
  <li><a href="#model-performance-tradeoffs" id="toc-model-performance-tradeoffs" class="nav-link" data-scroll-target="#model-performance-tradeoffs"><span class="toc-section-number">3.6</span>  Model performance tradeoffs</a></li>
  <li><a href="#calculating-roc-curves-and-auc" id="toc-calculating-roc-curves-and-auc" class="nav-link" data-scroll-target="#calculating-roc-curves-and-auc"><span class="toc-section-number">3.7</span>  Calculating ROC Curves and AUC</a></li>
  <li><a href="#comparing-roc-curves" id="toc-comparing-roc-curves" class="nav-link" data-scroll-target="#comparing-roc-curves"><span class="toc-section-number">3.8</span>  Comparing ROC curves</a></li>
  <li><a href="#dummy-variables-missing-data-and-interactions" id="toc-dummy-variables-missing-data-and-interactions" class="nav-link" data-scroll-target="#dummy-variables-missing-data-and-interactions"><span class="toc-section-number">3.10</span>  Dummy variables, missing data, and interactions</a></li>
  <li><a href="#coding-categorical-features" id="toc-coding-categorical-features" class="nav-link" data-scroll-target="#coding-categorical-features"><span class="toc-section-number">3.11</span>  Coding categorical features</a></li>
  <li><a href="#handling-missing-data" id="toc-handling-missing-data" class="nav-link" data-scroll-target="#handling-missing-data"><span class="toc-section-number">3.12</span>  Handling missing data</a></li>
  <li><a href="#understanding-missing-value-indicators" id="toc-understanding-missing-value-indicators" class="nav-link" data-scroll-target="#understanding-missing-value-indicators"><span class="toc-section-number">3.13</span>  Understanding missing value indicators</a></li>
  <li><a href="#building-a-more-sophisticated-model" id="toc-building-a-more-sophisticated-model" class="nav-link" data-scroll-target="#building-a-more-sophisticated-model"><span class="toc-section-number">3.15</span>  Building a more sophisticated model</a></li>
  <li><a href="#automatic-feature-selection" id="toc-automatic-feature-selection" class="nav-link" data-scroll-target="#automatic-feature-selection"><span class="toc-section-number">3.16</span>  Automatic feature selection</a></li>
  <li><a href="#the-dangers-of-stepwise-regression" id="toc-the-dangers-of-stepwise-regression" class="nav-link" data-scroll-target="#the-dangers-of-stepwise-regression"><span class="toc-section-number">3.17</span>  The dangers of stepwise regression</a></li>
  <li><a href="#building-a-stepwise-regression-model" id="toc-building-a-stepwise-regression-model" class="nav-link" data-scroll-target="#building-a-stepwise-regression-model"><span class="toc-section-number">3.19</span>  Building a stepwise regression model</a></li>
  </ul>
</li>
  <li>
<a href="#chapter-4-classification-trees" id="toc-chapter-4-classification-trees" class="nav-link" data-scroll-target="#chapter-4-classification-trees"><span class="toc-section-number">4</span>  4. Chapter 4: Classification Trees</a>
  <ul class="collapse">
<li><a href="#making-decisions-with-trees" id="toc-making-decisions-with-trees" class="nav-link" data-scroll-target="#making-decisions-with-trees"><span class="toc-section-number">4.1</span>  Making decisions with trees</a></li>
  <li><a href="#building-a-simple-decision-tree" id="toc-building-a-simple-decision-tree" class="nav-link" data-scroll-target="#building-a-simple-decision-tree"><span class="toc-section-number">4.2</span>  Building a simple decision tree</a></li>
  <li><a href="#visualizing-classification-trees" id="toc-visualizing-classification-trees" class="nav-link" data-scroll-target="#visualizing-classification-trees"><span class="toc-section-number">4.3</span>  Visualizing classification trees</a></li>
  <li><a href="#understanding-the-trees-decisions" id="toc-understanding-the-trees-decisions" class="nav-link" data-scroll-target="#understanding-the-trees-decisions"><span class="toc-section-number">4.4</span>  Understanding the tree’s decisions</a></li>
  <li><a href="#growing-larger-classification-trees" id="toc-growing-larger-classification-trees" class="nav-link" data-scroll-target="#growing-larger-classification-trees"><span class="toc-section-number">4.6</span>  Growing larger classification trees</a></li>
  <li><a href="#why-do-some-branches-split" id="toc-why-do-some-branches-split" class="nav-link" data-scroll-target="#why-do-some-branches-split"><span class="toc-section-number">4.7</span>  Why do some branches split?</a></li>
  <li><a href="#creating-random-test-datasets" id="toc-creating-random-test-datasets" class="nav-link" data-scroll-target="#creating-random-test-datasets"><span class="toc-section-number">4.9</span>  Creating random test datasets</a></li>
  <li><a href="#building-and-evaluating-a-larger-tree" id="toc-building-and-evaluating-a-larger-tree" class="nav-link" data-scroll-target="#building-and-evaluating-a-larger-tree"><span class="toc-section-number">4.10</span>  Building and evaluating a larger tree</a></li>
  <li><a href="#conducting-a-fair-performance-evaluation" id="toc-conducting-a-fair-performance-evaluation" class="nav-link" data-scroll-target="#conducting-a-fair-performance-evaluation"><span class="toc-section-number">4.11</span>  Conducting a fair performance evaluation</a></li>
  <li><a href="#tending-to-classification-trees" id="toc-tending-to-classification-trees" class="nav-link" data-scroll-target="#tending-to-classification-trees"><span class="toc-section-number">4.13</span>  Tending to classification trees</a></li>
  <li><a href="#preventing-overgrown-trees" id="toc-preventing-overgrown-trees" class="nav-link" data-scroll-target="#preventing-overgrown-trees"><span class="toc-section-number">4.14</span>  Preventing overgrown trees</a></li>
  <li><a href="#creating-a-nicely-pruned-tree" id="toc-creating-a-nicely-pruned-tree" class="nav-link" data-scroll-target="#creating-a-nicely-pruned-tree"><span class="toc-section-number">4.15</span>  Creating a nicely pruned tree</a></li>
  <li><a href="#why-do-trees-benefit-from-pruning" id="toc-why-do-trees-benefit-from-pruning" class="nav-link" data-scroll-target="#why-do-trees-benefit-from-pruning"><span class="toc-section-number">4.16</span>  Why do trees benefit from pruning?</a></li>
  <li><a href="#seeing-the-forest-from-the-trees" id="toc-seeing-the-forest-from-the-trees" class="nav-link" data-scroll-target="#seeing-the-forest-from-the-trees"><span class="toc-section-number">4.18</span>  Seeing the forest from the trees</a></li>
  <li><a href="#understanding-random-forests" id="toc-understanding-random-forests" class="nav-link" data-scroll-target="#understanding-random-forests"><span class="toc-section-number">4.19</span>  Understanding random forests</a></li>
  <li><a href="#building-a-random-forest-model" id="toc-building-a-random-forest-model" class="nav-link" data-scroll-target="#building-a-random-forest-model"><span class="toc-section-number">4.21</span>  Building a random forest model</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jwarz/jwarz.github.io/edit/main/content/R/topics/07_machine_learning/01_supervised_learning_classification/01_supervised_learning_classification.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jwarz/jwarz.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title d-none d-lg-block">Supervised Learning in R: Classification</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joschka Schwarz </p>
          </div>
  </div>
    
  
    
  </div>
  

</header><p>This beginner-level introduction to machine learning covers four of the most common classification algorithms. You will come away with a basic understanding of how each algorithm approaches a learning task, as well as learn the R functions needed to apply these tools to your own work.</p>
<section id="chapter-1-k-nearest-neighbors-knn" class="level1" data-number="1"><h1 data-number="1">
<span class="header-section-number">1</span> Chapter 1: k-Nearest Neighbors (kNN)</h1>
<p>As the kNN algorithm literally “learns by example” it is a case in point for starting to understand supervised machine learning. This chapter will introduce classification while working through the application of kNN to self-driving vehicle road sign recognition.</p>
<section id="classification-with-nearest-neighbors" class="level2" data-number="1.1"><h2 data-number="1.1" class="anchored" data-anchor-id="classification-with-nearest-neighbors">
<span class="header-section-number">1.1</span> Classification with Nearest Neighbors</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Classification with nearest neighbors</strong></p>
<p>Hi! My name is Brett Lantz and I’m a data scientist at the University of Michigan and the author of the book “Machine Learning with R.” Machine learning utilizes computers to turn data into insight and action.This course focuses on a subset of machine learning. The sub-domain called supervised learning focuses on training a machine to learn from prior examples.When the concept to be learned is a set of categories, the task is called classification. From identifying diseases, predicting the weather, or detecting whether an image contains a cat, classification tasks are diverse yet common.In this course, you’ll learn classification methods while exploring four real-world applications. Let’s get started!</p>
<p><strong>2. Classification tasks for driverless cars</strong></p>
<p>If your experiences on the road are anything like mine, self-driving cars can’t get here soon enough! It’s easy to imagine aspects of autonomous driving that involve classification; for example, when a vehicle’s camera observes an object, it must classify the object before it can react.Though the algorithms that govern autonomous cars are sophisticated, we can simulate aspects of their behavior. In this example, we’ll suppose the vehicle can see but not distinguish the roadway signs. Your job will be to use machine learning to classify the sign’s type.</p>
<p><strong>3. Understanding Nearest Neighbors</strong></p>
<p>To start training a self-driving car, you might supervise it by demonstrating the desired behavior as it observes each type of sign. You stop at intersections, yield to pedestrians, and change speed as needed.After some time under your instruction, the vehicle has built a database that records the sign as well as the target behavior. The image here illustrates this dataset.I suspect you already see some similarities, the machine can too! A nearest neighbor classifier takes advantage of the fact that signs that look alike should be similar to, or “nearby” other signs of the same type. For example, if the car observes a sign that seems similar to those in the group of stop signs, the car will probably need to stop.</p>
<p><strong>4. Measuring similarity with distance</strong></p>
<p>So how does a nearest neighbor learner decide whether two signs are similar? It does so by literally measuring the distance between them.That’s not to say that it measures the distance between signs in physical space, a stop sign in New York is the same as a stop sign in Los Angeles, but instead, it imagines the properties of the signs as coordinates in what is called a feature space.Consider, for instance, the sign’s color. By imagining the color as a 3-dimensional feature space measuring levels of red, green, and blue, signs of similar color are located naturally close to one another.Once the feature space has been constructed in this way, you can measure distance using a formula like those you may have seen in a geometry class. Many nearest neighbor learners use the Euclidean distance formula here, which measures the straight-line distance between two points. If the formula is confusing, don’t worry; R will compute it for you.</p>
<p><strong>5. Applying nearest neighbors in R</strong></p>
<p>An algorithm called k-Nearest Neighbors, or kNN, uses the principle of nearest neighbors to classify unlabeled examples. We’ll get into the specifics later, but for now it suffices to know that, by default, R’s knn function searches a dataset for the historic observation most similar to the newly-observed one.The knn function is part of the class package, and requires three parameters: first, the set of training data; second, the test data to be classified; and third, the labels for the training data.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>The test car is ready at the track. Can you help it drive away?</p>
</section><section id="recognizing-a-road-sign-with-knn" class="level2" data-number="1.2"><h2 data-number="1.2" class="anchored" data-anchor-id="recognizing-a-road-sign-with-knn">
<span class="header-section-number">1.2</span> Recognizing a road sign with kNN</h2>
<p>After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.</p>
<p>As it begins to drive away, its camera captures the following image:</p>
<p><img src="https://assets.datacamp.com/production/course_2906/datasets/knn_stop_99.gif" alt="Stop Sign"></p>
<p>Can you apply a kNN classifier to help the car recognize this sign?</p>
<p>The dataset <code>signs</code> is loaded in your workspace along with the dataframe <code>next_sign</code>, which holds the observation you want to classify.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Load the <code>class</code> package.</li>
<li>Create a vector of sign labels to use with kNN by extracting the column <code>sign_type</code> from <code>signs</code>.</li>
<li>Identify the <code>next_sign</code> using the <code>knn()</code> function.the <code>train</code> argument equal to the <code>signs</code> data frame <em>without</em> the first column.the <code>test</code> argument equal to the data frame <code>next_sign</code>.the vector of labels you created as the <code>cl</code> argument.</li>
<li>Set the <code>train</code> argument equal to the <code>signs</code> data frame <em>without</em> the first column.</li>
<li>Set the <code>test</code> argument equal to the data frame <code>next_sign</code>.</li>
<li>Use the vector of labels you created as the <code>cl</code> argument.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load the 'class' package</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(class)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">library</span>(dplyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:stats':
#&gt; 
#&gt;     filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:base':
#&gt; 
#&gt;     intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Load &amp; create data</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>signs_all  <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="st">"data/knn_traffic_signs.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 206 Columns: 51</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr  (2): sample, sign_type
#&gt; dbl (49): id, r1, g1, b1, r2, g2, b2, r3, g3, b3, r4, g4, b4, r5, g5, b5, r6...
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>signs      <span class="ot">&lt;-</span> signs_all <span class="sc">|&gt;</span> <span class="fu">filter</span>(sample <span class="sc">==</span> <span class="st">"train"</span>)   <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>))</span>
<span id="cb8-2"><a href="#cb8-2"></a>next_sign  <span class="ot">&lt;-</span> signs_all <span class="sc">|&gt;</span> <span class="fu">filter</span>(sample <span class="sc">==</span> <span class="st">"example"</span>) <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>))</span>
<span id="cb8-3"><a href="#cb8-3"></a>signs_test <span class="ot">&lt;-</span> signs_all <span class="sc">|&gt;</span> <span class="fu">filter</span>(sample <span class="sc">==</span> <span class="st">"test"</span>)    <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>))</span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="co"># Create a vector of labels</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>sign_types <span class="ot">&lt;-</span> signs<span class="sc">$</span>sign_type</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># Classify the next sign observed</span></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="fu">knn</span>(<span class="at">train =</span> signs[<span class="sc">-</span><span class="dv">1</span>], <span class="at">test =</span> next_sign, <span class="at">cl =</span> sign_types)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] stop
#&gt; Levels: pedestrian speed stop</code></pre>
</div>
</div>
<p>Awesome! You’ve trained your first nearest neighbor classifier!</p>
</section><section id="thinking-like-knn" class="level2" data-number="1.3"><h2 data-number="1.3" class="anchored" data-anchor-id="thinking-like-knn">
<span class="header-section-number">1.3</span> Thinking like kNN</h2>
<p>With your help, the test car successfully identified the sign and stopped safely at the intersection.</p>
<blockquote class="blockquote">
<h2 id="question" data-number="1.4" class="anchored">
<span class="header-section-number">1.4</span> <em>Question</em>
</h2>
<p>How did the <code>knn()</code> function correctly classify the stop sign?<br><br> ⬜ It learned that stop signs are red<br> ✅ The sign was in some way similar to another stop sign<br> ⬜ Stop signs have eight sides<br> ⬜ The other types of signs were less likely<br></p>
</blockquote>
<p>Correct! kNN isn’t really learning anything; it simply looks for the most similar example.</p>
</section><section id="exploring-the-traffic-sign-dataset" class="level2" data-number="1.5"><h2 data-number="1.5" class="anchored" data-anchor-id="exploring-the-traffic-sign-dataset">
<span class="header-section-number">1.5</span> Exploring the traffic sign dataset</h2>
<p>To better understand how the <code>knn()</code> function was able to classify the stop sign, it may help to examine the training dataset it used.</p>
<p>Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.</p>
<p><img src="https://assets.datacamp.com/production/course_2906/datasets/knn_sign_data.png" alt="Stop Sign Data Encoding"></p>
<p>The result is a dataset that records the <code>sign_type</code> as well as 16 x 3 = 48 color properties of each sign.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code> function to examine the <code>signs</code> dataset.</li>
<li>Use <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> to count the number of observations of each sign type by passing it the column containing the labels.</li>
<li>Run the provided <code><a href="https://rdrr.io/r/stats/aggregate.html">aggregate()</a></code> command to see whether the average red level might vary by sign type.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Examine the structure of the signs dataset</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="fu">str</span>(signs[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>])</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co">#&gt; tibble [146 × 5] (S3: tbl_df/tbl/data.frame)</span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co">#&gt;  $ sign_type: chr [1:146] "pedestrian" "pedestrian" "pedestrian" "pedestrian" ...</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">#&gt;  $ r1       : num [1:146] 155 142 57 22 169 75 136 149 13 123 ...</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co">#&gt;  $ g1       : num [1:146] 228 217 54 35 179 67 149 225 34 124 ...</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co">#&gt;  $ b1       : num [1:146] 251 242 50 41 170 60 157 241 28 107 ...</span></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co">#&gt;  $ r2       : num [1:146] 135 166 187 171 231 131 200 34 5 83 ...</span></span>
<span id="cb10-9"><a href="#cb10-9"></a></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="co"># Count the number of signs of each type</span></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="fu">table</span>(signs<span class="sc">$</span>sign_type)</span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="co">#&gt; </span></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="co">#&gt; pedestrian      speed       stop </span></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="co">#&gt;         46         49         51</span></span>
<span id="cb10-15"><a href="#cb10-15"></a></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="co"># Check r10's average red level by sign type</span></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="fu">aggregate</span>(r10 <span class="sc">~</span> sign_type, <span class="at">data =</span> signs, mean)</span>
<span id="cb10-18"><a href="#cb10-18"></a><span class="co">#&gt;    sign_type       r10</span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="co">#&gt; 1 pedestrian 113.71739</span></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="co">#&gt; 2      speed  80.63265</span></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="co">#&gt; 3       stop 132.39216</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! As you might have expected, stop signs tend to have a higher average red value. This is how kNN identifies similar signs.</p>
</section><section id="classifying-a-collection-of-road-signs" class="level2" data-number="1.6"><h2 data-number="1.6" class="anchored" data-anchor-id="classifying-a-collection-of-road-signs">
<span class="header-section-number">1.6</span> Classifying a collection of road signs</h2>
<p>Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.</p>
<p>The test course includes 59 additional road signs divided into three types:</p>
<p><img src="https://assets.datacamp.com/production/course_2906/datasets/knn_stop_28.gif" alt="Stop Sign"><img src="https://assets.datacamp.com/production/course_2906/datasets/knn_speed_55.gif" alt="Speed Limit Sign"><img src="https://assets.datacamp.com/production/course_2906/datasets/knn_peds_47.gif" alt="Pedestrian Sign"></p>
<p>At the conclusion of the trial, you are asked to measure the car’s overall performance at recognizing these signs.</p>
<p>The <code>class</code> package and the dataset <code>signs</code> are already loaded in your workspace. So is the dataframe <code>test_signs</code>, which holds a set of observations you’ll test your model on.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>
<p>Classify the <code>test_signs</code> data using <code>knn()</code>.<code>train</code> equal to the observations in <code>signs</code> <em>without</em> labels.<code>test_signs</code> for the <code>test</code> argument, again without labels.the <code>cl</code> argument, use the vector of labels provided for you.</p>
<ul>
<li>Set <code>train</code> equal to the observations in <code>signs</code> <em>without</em> labels.</li>
<li>Use <code>test_signs</code> for the <code>test</code> argument, again without labels.</li>
<li>For the <code>cl</code> argument, use the vector of labels provided for you.</li>
</ul>
</li>
</ol>
<p>2.. Use <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> to explore the classifier’s performance at identifying the three sign types (the confusion matrix).the vector <code>signs_actual</code> by extracting the labels from <code>test_signs</code>.the vector of predictions and the vector of actual signs to <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> to cross tabulate them.</p>
<pre><code>* Create the vector `signs_actual` by extracting the labels from `test_signs`.
* Pass the vector of predictions and the vector of actual signs to `table()` to cross tabulate them.</code></pre>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Use kNN to identify the test road signs</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>sign_types <span class="ot">&lt;-</span> signs<span class="sc">$</span>sign_type</span>
<span id="cb12-3"><a href="#cb12-3"></a>signs_pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> signs[<span class="sc">-</span><span class="dv">1</span>], <span class="at">test =</span> signs_test[<span class="sc">-</span><span class="dv">1</span>], <span class="at">cl =</span> sign_types)</span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co"># Create a confusion matrix of the predicted versus actual values</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>signs_actual <span class="ot">&lt;-</span> signs_test<span class="sc">$</span>sign_type</span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="fu">table</span>(signs_pred, signs_actual)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;             signs_actual
#&gt; signs_pred   pedestrian speed stop
#&gt;   pedestrian         19     2    0
#&gt;   speed               0    17    0
#&gt;   stop                0     2   19</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compute the overall accuracy of the kNN learner using the <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># Compute the accuracy</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="fu">mean</span>(signs_pred <span class="sc">==</span> signs_actual)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.9322034</code></pre>
</div>
</div>
<p>Fantastic! That self-driving car is really coming along! The confusion matrix lets you look for patterns in the classifier’s errors.</p>
</section><section id="what-about-the-k-in-knn" class="level2" data-number="1.7"><h2 data-number="1.7" class="anchored" data-anchor-id="what-about-the-k-in-knn">
<span class="header-section-number">1.7</span> What about the ‘k’ in kNN?</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. What about the ‘k’ in kNN?</strong></p>
<p>You may be wondering why kNN is called ‘k’ Nearest Neighbors, what exactly is ‘k’? The letter k is a variable that specifies the number of neighbors to consider when making the classification. You can imagine it as determining the size of the neighborhoods. Until now, we’ve ignored k, and thus R has used the default value of ‘1’. This means that only the single nearest, most similar, neighbor was used to classify the unlabeled example. While this seems OK on the surface, let’s work through an example to see why the value of k may have a substantial impact on the performance of our classifier.</p>
<p><strong>2. Choosing ‘k’ neighbors</strong></p>
<p>Suppose our vehicle observed the sign at the center of the image here. Its five nearest neighbors are depicted. The single nearest neighbor is a speed limit sign, which shares a very similar background color. Unfortunately, in this case, a kNN classifier with k set to one would make an incorrect classification. Slightly further away are the second, third, and fourth nearest neighbors, which are all pedestrian crossing signs. Suppose we set k to three. What would happen? The three nearest neighbors, a speed limit sign and two pedestrian crossing signs, would take a vote. The category with the majority of nearest neighbors, in this case the pedestrian crossing sign, is the winner. Increasing k to five allows the five nearest neighbors to vote. The pedestrian crossing sign still wins with a margin of 3-to-2. Note that in the case of a tie, the winner is typically decided at random.</p>
<p><strong>3. Bigger ‘k’ is not always better</strong></p>
<p>In the previous example, setting k to a higher value resulted in a correct prediction. But it is not always the case that bigger is better. A small k creates very small neighborhoods; the classifier is able to discover very subtle patterns. As this image illustrates, you might imagine it as being able to distinguish between groups even when their boundary is somewhat “fuzzy.” On the other hand, sometimes a “fuzzy” boundary is not a true pattern, but rather due to some other factor that adds randomness into the data. This is called noise. Setting k larger, as this image shows, ignores some potentially-noisy points in an effort to discover a broader, more general pattern.</p>
<p><strong>4. Choosing ‘k’</strong></p>
<p>So, how should you set k? Unfortunately, there is no universal rule. In practice, the optimal value depends on the complexity of the pattern to be learned, as well as the impact of noisy data. Some suggest a rule of thumb starting with k equal to the square root of the number of observations in the training data. For example, if the car had observed 100 previous road signs, you might set k to 10. An even better approach is to test several different values of k and compare the performance on data it has not seen before.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>In the next coding exercise, you’ll have an opportunity to see the impact of k on the vehicle’s ability to correctly classify signs.</p>
</section><section id="understanding-the-impact-of-k" class="level2" data-number="1.8"><h2 data-number="1.8" class="anchored" data-anchor-id="understanding-the-impact-of-k">
<span class="header-section-number">1.8</span> Understanding the impact of ‘k’</h2>
<p>There is a complex relationship between k and classification accuracy. Bigger is not always better.</p>
<blockquote class="blockquote">
<h2 id="question-1" data-number="1.9" class="anchored">
<span class="header-section-number">1.9</span> <em>Question</em>
</h2>
<p>Which of these is a valid reason for keeping k as small as possible (but no smaller)?<br><br> ⬜ A smaller k requires less processing power<br> ⬜ A smaller k reduces the impact of noisy data<br> ⬜ A smaller k minimizes the chance of a tie vote<br> ✅ A smaller k may utilize more subtle patterns<br></p>
</blockquote>
<p>Yes! With smaller neighborhoods, kNN can identify more subtle patterns in the data.</p>
</section><section id="testing-other-k-values" class="level2" data-number="1.10"><h2 data-number="1.10" class="anchored" data-anchor-id="testing-other-k-values">
<span class="header-section-number">1.10</span> Testing other ‘k’ values</h2>
<p>By default, the <code>knn()</code> function in the <code>class</code> package uses only the single nearest neighbor.</p>
<p>Setting a <code>k</code> parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.</p>
<p>Compare <code>k</code> values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.</p>
<p>The <code>class</code> package is already loaded in your workspace along with the datasets <code>signs</code>, <code>signs_test</code>, and <code>sign_types</code>. The object <code>signs_actual</code> holds the true values of the signs.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Compute the accuracy of the default <code>k = 1</code> model using the given code, then find the accuracy of the model using <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> to compare <code>signs_actual</code> and the model’s predictions.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Compute the accuracy of the baseline model (default k = 1)</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>k_1 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> signs[<span class="sc">-</span><span class="dv">1</span>], <span class="at">test =</span> signs_test[<span class="sc">-</span><span class="dv">1</span>], <span class="at">cl =</span> sign_types)</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="fu">mean</span>(signs_actual <span class="sc">==</span> k_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.9322034</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Modify the <code>knn()</code> function call by setting <code>k = 7</code> and again find accuracy value.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Modify the above to set k = 7</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>k_7 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> signs[<span class="sc">-</span><span class="dv">1</span>], <span class="at">test =</span> signs_test[<span class="sc">-</span><span class="dv">1</span>], <span class="at">cl =</span> sign_types, <span class="at">k =</span> <span class="dv">7</span>)</span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="fu">mean</span>(signs_actual <span class="sc">==</span> k_7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.9661017</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Revise the code once more by setting <code>k = 15</code>, plus find the accuracy value one more time.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># Set k = 15 and compare to the above</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>k_15 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> signs[<span class="sc">-</span><span class="dv">1</span>], <span class="at">test =</span> signs_test[<span class="sc">-</span><span class="dv">1</span>], <span class="at">cl =</span> sign_types, <span class="at">k =</span> <span class="dv">15</span>)</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="fu">mean</span>(signs_actual <span class="sc">==</span> k_15)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.8983051</code></pre>
</div>
</div>
<p>You’re a kNN pro! Which value of k gave the highest accuracy?</p>
</section><section id="seeing-how-the-neighbors-voted" class="level2" data-number="1.11"><h2 data-number="1.11" class="anchored" data-anchor-id="seeing-how-the-neighbors-voted">
<span class="header-section-number">1.11</span> Seeing how the neighbors voted</h2>
<p>When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.</p>
<p>For example, knowing more about the voters’ confidence in the classification could allow an autonomous vehicle to use caution in the case there is <em>any chance at all</em> that a stop sign is ahead.</p>
<p>In this exercise, you will learn how to obtain the voting results from the <code>knn()</code> function.</p>
<p>The <code>class</code> package has already been loaded in your workspace along with the datasets <code>signs</code>, <code>sign_types</code>, and <code>signs_test</code>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Build a kNN model with the <code>prob = TRUE</code> parameter to compute the vote proportions. Set <code>k = 7</code>.</li>
<li>Use the <code><a href="https://rdrr.io/r/base/attr.html">attr()</a></code> function to obtain the vote proportions for the predicted class. These are stored in the attribute <code>"prob"</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># Use the prob parameter to get the proportion of votes for the winning class</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>sign_pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> signs[<span class="sc">-</span><span class="dv">1</span>], <span class="at">test =</span> signs_test[<span class="sc">-</span><span class="dv">1</span>], <span class="at">cl =</span> sign_types, <span class="at">k =</span> <span class="dv">7</span>, <span class="at">prob =</span> <span class="cn">TRUE</span>)</span>
<span id="cb22-3"><a href="#cb22-3"></a></span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="co"># Get the "prob" attribute from the predicted classes</span></span>
<span id="cb22-5"><a href="#cb22-5"></a>sign_prob <span class="ot">&lt;-</span> <span class="fu">attr</span>(sign_pred, <span class="st">"prob"</span>)</span>
<span id="cb22-6"><a href="#cb22-6"></a></span>
<span id="cb22-7"><a href="#cb22-7"></a><span class="co"># Examine the first several predictions</span></span>
<span id="cb22-8"><a href="#cb22-8"></a><span class="fu">head</span>(sign_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] pedestrian pedestrian pedestrian stop       pedestrian pedestrian
#&gt; Levels: pedestrian speed stop</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Examine the first several vote outcomes and percentages using the <code><a href="https://rdrr.io/r/utils/head.html">head()</a></code> function to see how the confidence varies from sign to sign.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Examine the proportion of votes for the winning class</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="fu">head</span>(sign_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.5714286 0.5714286 0.8571429 0.5714286 0.8571429 0.5714286</code></pre>
</div>
</div>
<p>Wow! Awesome job! Now you can get an idea of how certain your kNN learner is about its classifications.</p>
</section><section id="data-preparation-for-knn" class="level2" data-number="1.12"><h2 data-number="1.12" class="anchored" data-anchor-id="data-preparation-for-knn">
<span class="header-section-number">1.12</span> Data preparation for kNN</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Data preparation for kNN</strong></p>
<p>You’ve now seen the kNN algorithm in action while simulating aspects of a self-driving vehicle. You’ve gained an understanding of the impact of k on the algorithm’s performance, and know how to examine the neighbors’ votes to better understand which predictions are closer to unanimous.But before applying kNN to your own projects, you’ll need to know one more thing: how to prepare your data for nearest neighbors.</p>
<p><strong>2. kNN assumes numeric data</strong></p>
<p>As noted previously, nearest neighbor learners use distance functions to identify the most similar, or nearest examples. Many common distance functions assume that your data are in numeric format, as it is difficult to define the distance between categories.For example, there’s no obvious way to define the distance between “red” and “yellow”; consequently, the traffic sign dataset represented these using numeric color intensities. But suppose that you have a property that cannot be measured numerically, such as whether a road sign is a rectangle, diamond, or octagon. A common solution uses 1/0 indicators to represent these categories. This is called dummy coding.A binary “dummy” variable is created for each category except one. This variable is set to ‘1’ if the category applies and ‘0’ otherwise. The category that is left out can be easily deduced, if the stop sign is not a rectangle or a diamond, then it must be an octagon.Dummy coded data can be used directly in a distance function; two rectangle signs, both having values of ‘1’, will be found to be closer together than a rectangle and a diamond.</p>
<p><strong>3. kNN benefits from normalized data</strong></p>
<p>It is also important to be aware that when calculating distance, each feature of the input data should be measured with the same range of values.This was true for the traffic sign data; each color component ranged from a minimum of zero to a maximum of 255.However, suppose that we added the 1/0 dummy variables for sign shapes into the distance calculation. Two different shapes may differ by at most one unit, but two different colors may differ by as much as 255 units!Such a different scale allows the features with a wider range to have more influence over the distance calculation, as this figure illustrates. Here, the topmost speed limit sign is closer to the pedestrian sign than it is to its correct neighbors, simply because the range of blue values is wider than the 0-to-1 range of shape values.Compressing the blue axis so that it also follows a 0-to-1 range corrects this issue, and the speed limit sign is now closer to its true neighborhood.</p>
<p><strong>4. Normalizing data in R</strong></p>
<p>R does not have a built-in function in R to rescale data to a given range, so you’ll need to create one yourself. The code here defines a function called normalize which can be used to perform min-max normalization. This rescales a vector x such that it its minimum value is zero and its maximum value is one. It does this by subtracting the minimum value from each value of x and dividing by the range of x values.After applying this function to r-one, one of the color vectors, we can use the summary function to see that the new minimum and maximum values are 0 and 1 respectively. Calculating the same summary statistics for the unnormalized data shows a minimum of 3 and a maximum of 251.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>I hope you enjoyed your time simulating the training of an autonomous car. After a brief test of what you’ve learned about normalization, you’ll get started on another interesting classification task.</p>
</section><section id="why-normalize-data" class="level2" data-number="1.13"><h2 data-number="1.13" class="anchored" data-anchor-id="why-normalize-data">
<span class="header-section-number">1.13</span> Why normalize data?</h2>
<p>Before applying kNN to a classification task, it is common practice to rescale the data using a technique like <strong>min-max normalization</strong>.</p>
<blockquote class="blockquote">
<h2 id="question-2" data-number="1.14" class="anchored">
<span class="header-section-number">1.14</span> <em>Question</em>
</h2>
<p>What is the purpose of this step?<br><br> ✅ To ensure all data elements may contribute equal shares to distance.<br> ⬜ To help the kNN algorithm converge on a solution faster.<br> ⬜ To convert all of the data elements to numbers.<br> ⬜ To redistribute the data as a normal bell curve.<br></p>
</blockquote>
<p>Yes! Rescaling reduces the influence of extreme values on kNN’s distance function.</p>
</section></section><section id="chapter-2-naive-bayes" class="level1" data-number="2"><h1 data-number="2">
<span class="header-section-number">2</span> 2. Chapter 2: Naive Bayes</h1>
<p>Naive Bayes uses principles from the field of statistics to make predictions. This chapter will introduce the basics of Bayesian methods while exploring how to apply these techniques to iPhone-like destination suggestions.</p>
<section id="understanding-bayesian-methods" class="level2" data-number="2.1"><h2 data-number="2.1" class="anchored" data-anchor-id="understanding-bayesian-methods">
<span class="header-section-number">2.1</span> Understanding Bayesian methods</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Understanding Bayesian methods</strong></p>
<p>Some smartphones and apps predict the user’s destination to offer routes and traffic estimates without the user even asking. If you didn’t know about machine learning, the phone’s ability to predict the future this way might seem a bit like magic!The phone obviously keeps a record of the user’s past locations. It then uses this data to forecast the user’s most probable future location, much like a meteorologist estimates the precipitation probability in a weather report.A branch of statistics called Bayesian methods apply the work of 18th century statistician Thomas Bayes, who proposed rules for estimating probabilities in light of historic data. By applying these methods to my own location tracking data, you will learn how probability estimates can forecast action. Let’s see where the data finds me!</p>
<p><strong>2. Estimating probability</strong></p>
<p>This map shows the number of times my phone recorded my position at four different locations. Based on this data, my phone can predict that at any given time my most probable location is at work, because I was there 57-point-5 percent of the time: 23 of the past 40 times it checked.This illustrates how the probability of an event is estimated from historic data; it the number of times the event happened, divided by the number of times it could have happened.But even though I am at work a lot, the phone should not predict I am there all the time. Instead, it should incorporate additional data like time of day to better tailor its predictions to the situation. This requires an understanding of how to combine information from several events into a single probability estimate.</p>
<p><strong>3. Joint probability and independent events</strong></p>
<p>When events occur together, they have a joint probability. Their intersection can be depicted using a Venn diagram like those shown here. These show that there is a much greater probability that I am at work in the afternoon than the evening; the overlap is much greater for work and afternoon.The joint probability of two events is computed by finding the proportion of observations they occurred together.Sometimes one event does not influence the probability of another. These are said to be independent events. For example, my location is unrelated to most other users’ locations. Knowing where they are does not provide information about where I might be. This notion of independent events will be important later on.However, many of the other data elements my phone collects, such as the time and date, are VERY predictive of where I may be. When one event is predictive of another, they are called dependent.</p>
<p><strong>4. Conditional probability and dependent events</strong></p>
<p>These are the basis of prediction with Bayesian methods.Conditional probability expresses exactly how one event depends on another. The formula shows that the probability of event A given B is equal to their joint probability divided by the probability of B.We can use this to compute the 4% probability that I am at work, given the knowledge that it is evening. In comparison, there is an 80% chance I am at work in the afternoon.</p>
<p><strong>5. Making predictions with Naive Bayes</strong></p>
<p>The algorithm known as “Naive Bayes” applies Bayesian methods to estimate the conditional probability of an outcome.The naivebayes package provides a function to build this model. Because the location depends on the time of day, it is specified as location-tilde-time-of-day; this form is called the R formula interface, which relates the outcome to be predicted to its predictors.The corresponding predict() function computes conditional probabilities to predict a future location based on the future conditions.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>In the next exercises, you’ll apply what you’ve learned to a real dataset that tracked my location over time.</p>
</section><section id="computing-probabilities" class="level2" data-number="2.2"><h2 data-number="2.2" class="anchored" data-anchor-id="computing-probabilities">
<span class="header-section-number">2.2</span> Computing probabilities</h2>
<p>The <code>where9am</code> data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his <code>location</code> at 9am each day as well as whether the <code>daytype</code> was a weekend or weekday.</p>
<p>Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.</p>
<p><img src="https://render.githubusercontent.com/render/math?math=%7B%5Ccolor%7Bred%7DP(A%7CB)=%5Cfrac%7BP(A%20and%20B)%7D%7BP(B)%7D%7D"></p>
<p>Calculations like these are the basis of the Naive Bayes destination prediction model you’ll develop in later exercises.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Find P(office) using <code><a href="https://rdrr.io/r/base/nrow.html">nrow()</a></code> and <code><a href="https://rdrr.io/r/base/subset.html">subset()</a></code> to count rows in the dataset and save the result as <code>p_A</code>.</li>
<li>Find P(weekday), using <code><a href="https://rdrr.io/r/base/nrow.html">nrow()</a></code> and <code><a href="https://rdrr.io/r/base/subset.html">subset()</a></code> again, and save the result as <code>p_B</code>.</li>
<li>Use <code><a href="https://rdrr.io/r/base/nrow.html">nrow()</a></code> and <code><a href="https://rdrr.io/r/base/subset.html">subset()</a></code> a final time to find P(office and weekday). Save the result as <code>p_AB</code>.</li>
<li>Compute P(office | weekday) and save the result as <code>p_A_given_B</code>.</li>
<li>Print the value of <code>p_A_given_B</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Load package</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="fu">library</span>(readr)</span>
<span id="cb26-3"><a href="#cb26-3"></a></span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="co"># Load data</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>locations <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/locations.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 2184 Columns: 7
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr (4): weekday, daytype, hourtype, location
#&gt; dbl (3): month, day, hour
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a>where9am  <span class="ot">&lt;-</span> locations <span class="sc">|&gt;</span> </span>
<span id="cb28-2"><a href="#cb28-2"></a>                <span class="fu">filter</span>(hour <span class="sc">==</span> <span class="dv">9</span>) <span class="sc">|&gt;</span> </span>
<span id="cb28-3"><a href="#cb28-3"></a>                <span class="fu">select</span>(daytype, location) <span class="sc">|&gt;</span> </span>
<span id="cb28-4"><a href="#cb28-4"></a>                <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">everything</span>(), as.factor))</span>
<span id="cb28-5"><a href="#cb28-5"></a></span>
<span id="cb28-6"><a href="#cb28-6"></a><span class="co"># Compute P(A) </span></span>
<span id="cb28-7"><a href="#cb28-7"></a>p_A <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(where9am, location <span class="sc">==</span> <span class="st">"office"</span>)) <span class="sc">/</span> <span class="fu">nrow</span>(where9am)</span>
<span id="cb28-8"><a href="#cb28-8"></a></span>
<span id="cb28-9"><a href="#cb28-9"></a><span class="co"># Compute P(B)</span></span>
<span id="cb28-10"><a href="#cb28-10"></a>p_B <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(where9am, daytype <span class="sc">==</span> <span class="st">"weekday"</span>)) <span class="sc">/</span> <span class="fu">nrow</span>(where9am)</span>
<span id="cb28-11"><a href="#cb28-11"></a></span>
<span id="cb28-12"><a href="#cb28-12"></a><span class="co"># Compute the observed P(A and B)</span></span>
<span id="cb28-13"><a href="#cb28-13"></a>p_AB <span class="ot">&lt;-</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(where9am, location <span class="sc">==</span> <span class="st">"office"</span> <span class="sc">&amp;</span> daytype <span class="sc">==</span> <span class="st">"weekday"</span>)) <span class="sc">/</span> <span class="fu">nrow</span>(where9am)</span>
<span id="cb28-14"><a href="#cb28-14"></a></span>
<span id="cb28-15"><a href="#cb28-15"></a><span class="co"># Compute P(A | B) and print its value</span></span>
<span id="cb28-16"><a href="#cb28-16"></a>p_A_given_B <span class="ot">&lt;-</span> p_AB <span class="sc">/</span> p_B</span>
<span id="cb28-17"><a href="#cb28-17"></a>p_A_given_B</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.6</code></pre>
</div>
</div>
<p>Great work! In a lot of cases, calculating probabilities is as simple as counting.</p>
</section><section id="understanding-dependent-events" class="level2" data-number="2.3"><h2 data-number="2.3" class="anchored" data-anchor-id="understanding-dependent-events">
<span class="header-section-number">2.3</span> Understanding dependent events</h2>
<blockquote class="blockquote">
<h2 id="question-3" data-number="2.4" class="anchored">
<span class="header-section-number">2.4</span> <em>Question</em>
</h2>
<p>In the previous exercise, you found that there is a 60% chance Brett is in the office at 9am given that it is a weekday. On the other hand, if Brett is never in the office on a weekend, which of the following is/are true?<br><br> ⬜ P(office and weekend) = 0.<br> ⬜ P(office | weekend) = 0.<br> ⬜ Brett’s location is dependent on the day of the week.<br> ✅ All of the above.<br></p>
</blockquote>
<p>Correct! Because the events do not overlap, knowing that one occurred tells you much about the status of the other.</p>
</section><section id="a-simple-naive-bayes-location-model" class="level2" data-number="2.5"><h2 data-number="2.5" class="anchored" data-anchor-id="a-simple-naive-bayes-location-model">
<span class="header-section-number">2.5</span> A simple Naive Bayes location model</h2>
<p>The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.</p>
<p>To see this finding in action, use the <code>where9am</code> data frame to build a Naive Bayes model on the same data.</p>
<p>You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?</p>
<p>The data frame <code>where9am</code> is available in your workspace. This dataset contains information about Brett’s location at 9am on different days.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Load the <code>naivebayes</code> package.</li>
<li>Use <code>naive_bayes()</code> with a formula like <code>y ~ x</code> to build a model of <code>location</code> as a function of <code>daytype</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Load the naivebayes package</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="fu">library</span>(naivebayes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; naivebayes 0.9.7 loaded</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># Build the location prediction model</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>locmodel <span class="ot">&lt;-</span> <span class="fu">naive_bayes</span>(location <span class="sc">~</span> daytype, <span class="at">data =</span> where9am)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: naive_bayes(): Feature daytype - zero probabilities are present.
#&gt; Consider Laplace smoothing.</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Forecast the Thursday 9am location using <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> with the <code>thursday9am</code> object as the <code>newdata</code> argument.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a>thursday9am <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">daytype =</span> <span class="fu">factor</span>(<span class="st">"weekday"</span>, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"weekday"</span>, <span class="st">"weekend"</span>)))</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="co"># Predict Thursday's 9am location</span></span>
<span id="cb34-4"><a href="#cb34-4"></a><span class="fu">predict</span>(locmodel, thursday9am)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] office
#&gt; Levels: appointment campus home office</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Do the same for predicting the <code>saturday9am</code> location.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a>saturday9am <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">daytype =</span> <span class="fu">factor</span>(<span class="st">"weekend"</span>, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"weekday"</span>, <span class="st">"weekend"</span>)))</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="co"># Predict Saturdays's 9am location</span></span>
<span id="cb36-4"><a href="#cb36-4"></a><span class="fu">predict</span>(locmodel, saturday9am)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] home
#&gt; Levels: appointment campus home office</code></pre>
</div>
</div>
<p>Awesome job! Not surprisingly, Brett is most likely at the office at 9am on a Thursday, but at home at the same time on a Saturday!</p>
</section><section id="examining-raw-probabilities" class="level2" data-number="2.6"><h2 data-number="2.6" class="anchored" data-anchor-id="examining-raw-probabilities">
<span class="header-section-number">2.6</span> Examining “raw” probabilities</h2>
<p>The <code>naivebayes</code> package offers several ways to peek inside a Naive Bayes model.</p>
<p>Typing the name of the model object provides the <em>a priori</em> (overall) and conditional probabilities of each of the model’s predictors. If one were so inclined, you might use these for calculating <em>posterior</em> (predicted) probabilities by hand.</p>
<p>Alternatively, R will compute the posterior probabilities for you if the <code>type = "prob"</code> parameter is supplied to the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function.</p>
<p>Using these methods, examine how the model’s predicted 9am location probability varies from day-to-day. The model <code>locmodel</code> that you fit in the previous exercise is in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Print the <code>locmodel</code> object to the console to view the computed <em>a priori</em> and conditional probabilities.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># The 'naivebayes' package is loaded into the workspace</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="co"># and the Naive Bayes 'locmodel' has been built</span></span>
<span id="cb38-3"><a href="#cb38-3"></a></span>
<span id="cb38-4"><a href="#cb38-4"></a><span class="co"># Examine the location prediction model</span></span>
<span id="cb38-5"><a href="#cb38-5"></a>locmodel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; ================================== Naive Bayes ================================== 
#&gt;  
#&gt;  Call: 
#&gt; naive_bayes.formula(formula = location ~ daytype, data = where9am)
#&gt; 
#&gt; --------------------------------------------------------------------------------- 
#&gt;  
#&gt; Laplace smoothing: 0
#&gt; 
#&gt; --------------------------------------------------------------------------------- 
#&gt;  
#&gt;  A priori probabilities: 
#&gt; 
#&gt; appointment      campus        home      office 
#&gt;  0.01098901  0.10989011  0.45054945  0.42857143 
#&gt; 
#&gt; --------------------------------------------------------------------------------- 
#&gt;  
#&gt;  Tables: 
#&gt; 
#&gt; --------------------------------------------------------------------------------- 
#&gt;  ::: daytype (Bernoulli) 
#&gt; --------------------------------------------------------------------------------- 
#&gt;          
#&gt; daytype   appointment    campus      home    office
#&gt;   weekday   1.0000000 1.0000000 0.3658537 1.0000000
#&gt;   weekend   0.0000000 0.0000000 0.6341463 0.0000000
#&gt; 
#&gt; ---------------------------------------------------------------------------------</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function similarly to the previous exercise, but with <code>type = "prob"</code> to see the predicted probabilities for Thursday at 9am.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Obtain the predicted probabilities for Thursday at 9am</span></span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="fu">predict</span>(locmodel, thursday9am, <span class="at">type =</span> <span class="st">"prob"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      appointment    campus      home office
#&gt; [1,]  0.01538462 0.1538462 0.2307692    0.6</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare these to the predicted probabilities for Saturday at 9am.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># Obtain the predicted probabilities for Saturday at 9am</span></span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="fu">predict</span>(locmodel, saturday9am, <span class="at">type =</span> <span class="st">"prob"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;       appointment       campus      home      office
#&gt; [1,] 3.838772e-05 0.0003838772 0.9980806 0.001497121</code></pre>
</div>
</div>
<p>Fantastic! Did you notice the predicted probability of Brett being at the office on a Saturday is zero?</p>
</section><section id="understanding-independence" class="level2" data-number="2.7"><h2 data-number="2.7" class="anchored" data-anchor-id="understanding-independence">
<span class="header-section-number">2.7</span> Understanding independence</h2>
<blockquote class="blockquote">
<h2 id="question-4" data-number="2.8" class="anchored">
<span class="header-section-number">2.8</span> <em>Question</em>
</h2>
<p>Understanding the idea of event independence will become important as you learn more about how “naive” Bayes got its name. Which of the following is true about independent events?<br><br> ⬜ The events cannot occur at the same time.<br> ⬜ A Venn diagram will always show no intersection.<br> ✅ Knowing the outcome of one event does not help predict the other.<br> ⬜ At least one of the events is completely random.<br></p>
</blockquote>
<p>Yes! One event is independent of another if knowing one doesn’t give you information about how likely the other is. For example, knowing if it’s raining in New York doesn’t help you predict the weather in San Francisco. The weather events in the two cities are independent of each other.</p>
</section><section id="understanding-nbs-naivety" class="level2" data-number="2.9"><h2 data-number="2.9" class="anchored" data-anchor-id="understanding-nbs-naivety">
<span class="header-section-number">2.9</span> Understanding NB’s “naivety”</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Understanding NB’s “naivety”</strong></p>
<p>In the previous exercises, you built a simple Naive Bayes model that used historic location data to predict my future location. To build a more sophisticated model, you might add additional data points to help inform the estimated probability of my location.But until now, we’ve only considered conditional probability when a single event predicts another. Adding more predictors complicates matters and is the reason why this method is called “naive.” Keep listening to find out why.</p>
<p><strong>2. The challenge of multiple predictors</strong></p>
<p>With a single predictor, conditional probability is based on the overlap between the two events, as the Venn diagram here illustrates.When we start adding more events, the Venn diagram can start to look a bit messy. Here it is with three events, imagine it with dozens or more! And, as confusing as this looks to us, for a number of reasons it also becomes more inefficient for a computer to calculate the overlap.</p>
<p><strong>3. A “naive” simplification</strong></p>
<p>Instead, the Naive Bayes algorithm uses a shortcut to approximate the conditional probability we hope to compute.Rather than treating the problem as the intersection of all of the related events, the algorithm makes a so-called “naive” assumption about the data. Specifically, it assumes that the events are independent.When events are independent, the joint probability can be computed by multiplying the individual probabilities. Therefore, under the naive assumption, the algorithm does not need to observe all of the possible intersections in the full Venn diagram. Instead, it simply multiplies the probabilities from a series of much simpler intersections.Researchers have found that although the naive assumption is rarely true in practice, the Naive Bayes model still performs admirably on many real-world tasks. So there’s little need to worry about a potential downside.</p>
<p><strong>4. An “infrequent” problem</strong></p>
<p>There is one other potential issue to be aware of when building a Naive Bayes model. Suppose you have a set of predictors, chained together under the naive assumption. Suppose further that one of those events has never been observed previously in combination with the outcome. For instance, I may never have gone into work on a weekend. I may do this someday in the future, I just haven’t done so before.In this case, the Venn diagram for work and weekend has no overlap; the joint probability of these two events is zero. And whenever zero is multiplied in a chain, the entire sequence becomes zero. For this reason, the weekend event seems to have “veto” power over the entire prediction. No matter how overwhelming the rest of the evidence, any predicted probability of work on a weekend will always be zero.</p>
<p><strong>5. The Laplace correction</strong></p>
<p>The solution to this problem involves adding a small number, usually ‘1’, to each event and outcome combination to eliminate this veto power. This is called the Laplace correction or Laplace estimator. After adding this correction, each Venn diagram now has at least a small bit of overlap; there is no longer any joint probability of zero. As a result, there will be at least some predicted probability for every future outcome even if it has never been seen before.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>The Naive Bayes function you’ve used so far will let you set the Laplace parameter; you’ll see its impact in the coming exercises as you build a more sophisticated location model.</p>
</section><section id="who-are-you-calling-naive" class="level2" data-number="2.10"><h2 data-number="2.10" class="anchored" data-anchor-id="who-are-you-calling-naive">
<span class="header-section-number">2.10</span> Who are you calling naive?</h2>
<p>The Naive Bayes algorithm got its name because it makes a “naive” assumption about event independence.</p>
<blockquote class="blockquote">
<h2 id="question-5" data-number="2.11" class="anchored">
<span class="header-section-number">2.11</span> <em>Question</em>
</h2>
<p>What is the purpose of making this assumption?<br><br> ⬜ Independent events can never have a joint probability of zero.<br> ✅ The joint probability calculation is simpler for independent events.<br> ⬜ Conditional probability is undefined for dependent events.<br> ⬜ Dependent events cannot be used to make predictions.<br></p>
</blockquote>
<p>Yes! The joint probability of independent events can be computed much more simply by multiplying their individual probabilities.</p>
</section><section id="a-more-sophisticated-location-model" class="level2" data-number="2.12"><h2 data-number="2.12" class="anchored" data-anchor-id="a-more-sophisticated-location-model">
<span class="header-section-number">2.12</span> A more sophisticated location model</h2>
<p>The <code>locations</code> dataset records Brett’s location every hour for 13 weeks. Each hour, the tracking information includes the <code>daytype</code> (weekend or weekday) as well as the <code>hourtype</code> (morning, afternoon, evening, or night).</p>
<p>Using this data, build a more sophisticated model to see how Brett’s predicted location not only varies by the day of week but also by the time of day. The dataset <code>locations</code> is already loaded in your workspace.</p>
<p>You can specify additional independent variables in your formula using the <code>+</code> sign (e.g.&nbsp;<code>y ~ x + b</code>).</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the R formula interface to build a model where location depends on both <code>daytype</code> and <code>hourtype</code>. Recall that the function <code>naive_bayes()</code> takes 2 arguments: <code>formula</code> and <code>data</code>.</li>
<li>Predict Brett’s location on a weekday afternoon using the dataframe <code>weekday_afternoon</code> and the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># Build a NB model of location</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>locmodel <span class="ot">&lt;-</span> <span class="fu">naive_bayes</span>(location <span class="sc">~</span> daytype <span class="sc">+</span> hourtype, <span class="at">data =</span> locations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: naive_bayes(): Feature daytype - zero probabilities are present.
#&gt; Consider Laplace smoothing.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: naive_bayes(): Feature hourtype - zero probabilities are present.
#&gt; Consider Laplace smoothing.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a><span class="co"># Data</span></span>
<span id="cb47-2"><a href="#cb47-2"></a>weekday_afternoon <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">daytype  =</span> <span class="fu">factor</span>(<span class="st">"weekday"</span>,   <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"weekday"</span>, <span class="st">"weekend"</span>)),</span>
<span id="cb47-3"><a href="#cb47-3"></a>                            <span class="at">hourtype =</span> <span class="fu">factor</span>(<span class="st">"afternoon"</span>, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"afternoon"</span>, <span class="st">"evening"</span>, <span class="st">"morning"</span>, <span class="st">"night"</span>)),</span>
<span id="cb47-4"><a href="#cb47-4"></a>                            <span class="at">location =</span> <span class="fu">factor</span>(<span class="st">"office"</span>,    <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"appointment"</span>, <span class="st">"campus"</span>, <span class="st">"home"</span>, <span class="st">"office"</span>, <span class="st">"restaurant"</span>, <span class="st">"store"</span>, <span class="st">"theater"</span>)))</span>
<span id="cb47-5"><a href="#cb47-5"></a></span>
<span id="cb47-6"><a href="#cb47-6"></a><span class="co"># Predict Brett's location on a weekday afternoon</span></span>
<span id="cb47-7"><a href="#cb47-7"></a><span class="fu">predict</span>(locmodel, weekday_afternoon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: predict.naive_bayes(): more features in the newdata are provided as
#&gt; there are probability tables in the object. Calculation is performed based on
#&gt; features to be found in the tables.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] office
#&gt; Levels: appointment campus home office restaurant store theater</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Do the same for a <code>weekday_evening</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a><span class="co"># Data</span></span>
<span id="cb50-2"><a href="#cb50-2"></a>weekday_evening <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">daytype  =</span> <span class="fu">factor</span>(<span class="st">"weekday"</span>, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"weekday"</span>, <span class="st">"weekend"</span>)),</span>
<span id="cb50-3"><a href="#cb50-3"></a>                          <span class="at">hourtype =</span> <span class="fu">factor</span>(<span class="st">"evening"</span>, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"afternoon"</span>, <span class="st">"evening"</span>, <span class="st">"morning"</span>, <span class="st">"night"</span>)),</span>
<span id="cb50-4"><a href="#cb50-4"></a>                          <span class="at">location =</span> <span class="fu">factor</span>(<span class="st">"home"</span>,    <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"appointment"</span>, <span class="st">"campus"</span>, <span class="st">"home"</span>, <span class="st">"office"</span>, <span class="st">"restaurant"</span>, <span class="st">"store"</span>, <span class="st">"theater"</span>)))</span>
<span id="cb50-5"><a href="#cb50-5"></a></span>
<span id="cb50-6"><a href="#cb50-6"></a><span class="co"># Predict Brett's location on a weekday evening</span></span>
<span id="cb50-7"><a href="#cb50-7"></a><span class="fu">predict</span>(locmodel, weekday_evening)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: predict.naive_bayes(): more features in the newdata are provided as
#&gt; there are probability tables in the object. Calculation is performed based on
#&gt; features to be found in the tables.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] home
#&gt; Levels: appointment campus home office restaurant store theater</code></pre>
</div>
</div>
<p>Great job! Your Naive Bayes model forecasts that Brett will be at the office on a weekday afternoon and at home in the evening.</p>
</section><section id="preparing-for-unforeseen-circumstances" class="level2" data-number="2.13"><h2 data-number="2.13" class="anchored" data-anchor-id="preparing-for-unforeseen-circumstances">
<span class="header-section-number">2.13</span> Preparing for unforeseen circumstances</h2>
<p>While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of P(office and weekend) = 0.</p>
<p>Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the Laplace correction will allow a small chance for these types of unforeseen circumstances.</p>
<p>The model <code>locmodel</code> is already in your workspace, along with the dataframe <code>weekend_afternoon</code>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the <code>locmodel</code> to output predicted probabilities for a weekend afternoon by using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function. Remember to set the <code>type</code> argument.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a><span class="co"># The 'naivebayes' package is loaded into the workspace already</span></span>
<span id="cb53-2"><a href="#cb53-2"></a><span class="co"># The Naive Bayes location model (locmodel) has already been built</span></span>
<span id="cb53-3"><a href="#cb53-3"></a></span>
<span id="cb53-4"><a href="#cb53-4"></a><span class="co"># Data</span></span>
<span id="cb53-5"><a href="#cb53-5"></a>weekend_afternoon <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">daytype  =</span> <span class="fu">factor</span>(<span class="st">"weekend"</span>,   <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"weekday"</span>, <span class="st">"weekend"</span>)),</span>
<span id="cb53-6"><a href="#cb53-6"></a>                            <span class="at">hourtype =</span> <span class="fu">factor</span>(<span class="st">"afternoon"</span>, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"afternoon"</span>, <span class="st">"evening"</span>, <span class="st">"morning"</span>, <span class="st">"night"</span>)),</span>
<span id="cb53-7"><a href="#cb53-7"></a>                            <span class="at">location =</span> <span class="fu">factor</span>(<span class="st">"home"</span>,      <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"appointment"</span>, <span class="st">"campus"</span>, <span class="st">"home"</span>, <span class="st">"office"</span>, <span class="st">"restaurant"</span>, <span class="st">"store"</span>, <span class="st">"theater"</span>)))</span>
<span id="cb53-8"><a href="#cb53-8"></a></span>
<span id="cb53-9"><a href="#cb53-9"></a><span class="co"># Observe the predicted probabilities for a weekend afternoon</span></span>
<span id="cb53-10"><a href="#cb53-10"></a><span class="fu">predict</span>(locmodel, weekend_afternoon, <span class="at">type =</span> <span class="st">"prob"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: predict.naive_bayes(): more features in the newdata are provided as
#&gt; there are probability tables in the object. Calculation is performed based on
#&gt; features to be found in the tables.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      appointment       campus      home      office restaurant      store
#&gt; [1,]  0.02462883 0.0004802622 0.8439145 0.003349521  0.1111338 0.01641922
#&gt;          theater
#&gt; [1,] 7.38865e-05</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Create a new naive Bayes model with the Laplace smoothing parameter set to <code>1</code>. You can do this by setting the <code>laplace</code> argument in your call to <code>naive_bayes()</code>. Save this as <code>locmodel2</code>.</li>
<li>See how the new predicted probabilities compare by using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function on your new model.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a><span class="co"># Build a new model using the Laplace correction</span></span>
<span id="cb56-2"><a href="#cb56-2"></a>locmodel2 <span class="ot">&lt;-</span> <span class="fu">naive_bayes</span>(location <span class="sc">~</span> daytype <span class="sc">+</span> hourtype, <span class="at">data =</span> locations, <span class="at">laplace =</span> <span class="dv">1</span>)</span>
<span id="cb56-3"><a href="#cb56-3"></a></span>
<span id="cb56-4"><a href="#cb56-4"></a><span class="co"># Observe the new predicted probabilities for a weekend afternoon</span></span>
<span id="cb56-5"><a href="#cb56-5"></a><span class="fu">predict</span>(locmodel2, weekend_afternoon, <span class="at">type =</span> <span class="st">"prob"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: predict.naive_bayes(): more features in the newdata are provided as
#&gt; there are probability tables in the object. Calculation is performed based on
#&gt; features to be found in the tables.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      appointment      campus      home      office restaurant      store
#&gt; [1,]  0.02013872 0.006187715 0.8308154 0.007929249  0.1098743 0.01871085
#&gt;          theater
#&gt; [1,] 0.006343697</code></pre>
</div>
</div>
<p>Fantastic work! Adding the Laplace correction allows for the small chance that Brett might go to the office on the weekend in the future.</p>
</section><section id="understanding-the-laplace-correction" class="level2" data-number="2.14"><h2 data-number="2.14" class="anchored" data-anchor-id="understanding-the-laplace-correction">
<span class="header-section-number">2.14</span> Understanding the Laplace correction</h2>
<blockquote class="blockquote">
<h2 id="question-6" data-number="2.15" class="anchored">
<span class="header-section-number">2.15</span> <em>Question</em>
</h2>
<p>By default, the <code>naive_bayes()</code> function in the <code>naivebayes</code> package does not use the Laplace correction. What is the risk of leaving this parameter unset?<br><br> ✅ Some potential outcomes may be predicted to be impossible.<br> ⬜ The algorithm may have a divide by zero error.<br> ⬜ Naive Bayes will ignore features with zero values.<br> ⬜ The model may not estimate probabilities for some cases.<br></p>
</blockquote>
<p>Correct! The small probability added to every outcome ensures that they are all possible even if never previously observed.</p>
</section><section id="applying-naive-bayes-to-other-problems" class="level2" data-number="2.16"><h2 data-number="2.16" class="anchored" data-anchor-id="applying-naive-bayes-to-other-problems">
<span class="header-section-number">2.16</span> Applying Naive Bayes to other problems</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Applying Naive Bayes to other problems</strong></p>
<p>Smartphone destination suggestions are a very specific application of Naive Bayes. But the algorithm can also be used for many other types of problems. Naive Bayes tends to work well on problems where the information from multiple attributes needs to be considered simultaneously and evaluated as a whole. The process is somewhat analogous to how a medical doctor might evaluate symptoms and test results to make a final diagnosis. Historically, Naive Bayes has also been frequently used for classifying text data, like identifying whether or not an email is spam. In this video, I’ll present some of the challenges you may encounter when applying the algorithm to other classification tasks.</p>
<p><strong>2. How Naive Bayes uses data</strong></p>
<p>Consider the fact that Naive Bayes makes predictions by computing conditional probabilities of events and outcomes. Beginning with a tabular dataset, it builds frequency tables that count the number of times each event overlaps with the outcome of interest. The probabilities are then multiplied, naively, in a chain of all the events. A consequence of this approach is that each of the predictors used in Naive Bayes typically comprises a set of categories. Numeric properties, like age or time-of-day, are difficult for Naive Bayes to use as-is without knowing more about the properties of the data. Similarly, unstructured text data also defies categorization. Thus, it is generally necessary to prepare these types of data before using them with Naive Bayes.</p>
<p><strong>3. Binning numeric data for Naive Bayes</strong></p>
<p>A technique called binning is a simple method for creating categories from numeric data. The idea is to divide a range of numbers into a series of sets called “bins.” For instance, you might divide a numbers into bins based on percentiles by creating a category for the bottom 25%, the next 25%, and so on. Perhaps a better approach is to group ranges of values into meaningful bins. For instance, you might group times into categories like afternoon and evening, and temperature readings into values like hot, warm, and cold. You can use R’s data preparation functions to recode data this way.</p>
<p><strong>4. Preparing text data for Naive Bayes</strong></p>
<p>Text documents are considered unstructured data because they do not conform to the typical table or spreadsheet format of most datasets. A common process for adding structure to text data uses a model called bag-of-words. The bag-of-words model does not consider word order, grammar, or semantics. It simply creates an event for each word that appears in a particular collection of text documents. For example, the bag-of-words for this document on Naive Bayes would include events for words like “naive” and “bayes” and “understanding.” In spreadsheet form, this results in a wide table where the rows are documents and the columns are words that may appear in the documents. Each spreadsheet cell indicates whether or not the word appeared in that document. When the Naive Bayes algorithm is applied to the bag of words, it can estimate the probability of the outcome given the evidence provided by the words in the text. For instance, a document with the words “viagra” and “prescription” is more likely to be spam than a document with the words “naive” and “bayes.” Naive Bayes models trained with bag of words can be very effective text classifiers.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>You can learn more about this in DataCamp’s course on text mining, which will teach you how to apply R’s tm package to build datasets you can use with Naive Bayes.</p>
</section><section id="handling-numeric-predictors" class="level2" data-number="2.17"><h2 data-number="2.17" class="anchored" data-anchor-id="handling-numeric-predictors">
<span class="header-section-number">2.17</span> Handling numeric predictors</h2>
<p>Numeric data is often <strong>binned</strong> before it is used with Naive Bayes. Which of these is not an example of binning?</p>
<blockquote class="blockquote">
<h2 id="question-7" data-number="2.18" class="anchored">
<span class="header-section-number">2.18</span> <em>Question</em>
</h2>
<p>???<br><br> ⬜ age values recoded as ‘child’ or ‘adult’ categories<br> ⬜ geographic coordinates recoded into geographic regions (West, East, etc.)<br> ⬜ test scores divided into four groups by percentile<br> ✅ income values standardized to follow a normal bell curve<br></p>
</blockquote>
<p>Right! Transforming income values into a bell curve doesn’t create a set of categories.</p>
</section></section><section id="chapter-3-logistic-regression" class="level1" data-number="3"><h1 data-number="3">
<span class="header-section-number">3</span> 3. Chapter 3: Logistic Regression</h1>
<p>Logistic regression involves fitting a curve to numeric data to make predictions about binary events. Arguably one of the most widely used machine learning methods, this chapter will provide an overview of the technique while illustrating how to apply it to fundraising data.</p>
<section id="making-binary-predictions-with-regression" class="level2" data-number="3.1"><h2 data-number="3.1" class="anchored" data-anchor-id="making-binary-predictions-with-regression">
<span class="header-section-number">3.1</span> Making binary predictions with regression</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Making binary predictions with regression</strong></p>
<p>If you’ve spent any time at all studying data science, you are likely to have encountered regression analysis, which is a branch of statistics interested in modeling numeric relationships within data.Regression methods are perhaps the single most common form of machine learning. The technique can be adapted to virtually any type of problem in any domain. In this video, you’ll see how regression methods can be used to classify a binary outcome.Later, you’ll use what you learn to predict whether or not someone will donate to charity a topic directly related to my own work as a fund-raising data scientist.</p>
<p><strong>2. Introducing linear regression</strong></p>
<p>In its most basic form, regression involves predicting an outcome y using one or more predictors, labeled as x variables. The y variable is known as the dependent variable, as it seems to depend upon the x’s.Suppose you have a numeric y which you plot versus a numeric x term, resulting in the figure seen here. The y might reflect something like income or life expectancy, while the x-axis could represent age or education.Linear regression involves fitting the straight line to this data that best captures the relationship between the x and y terms.</p>
<p><strong>3. Regression for binary classification</strong></p>
<p>Suppose you have a binary y outcome instead something that can take only ‘1’ or ‘0’ values, like “donate” or “not donate.” Constructing a plot of y versus x, the points fall in two flat rows rather than spread along the diagonal.You can still apply a straight line to the data, but this doesn’t seem to fit very well. Additionally, for some values of x, the model will predict values less than 0 or greater than 1. This is obviously not ideal.</p>
<p><strong>4. Introducing logistic regression</strong></p>
<p>Now imagine the same binary outcome, but rather than trying to model it with a straight line, we use a curve instead. This is the idea behind logistic regression.A type of S-shaped curve called a logistic function has the property that for any input value of x, the output is always between 0 and 1 just like a probability.The greater this probability, the more likely the outcome is to be the one labeled ‘1’.</p>
<p><strong>5. Making predictions with logistic regression</strong></p>
<p>In R, logistic regression uses the glm function with the syntax as shown here. First, the terms y, x1, x2, and x3 specify the dependent and independent variables that will go into your model. This is called the R formula interface, and is a way to define the model’s form.You will replace the y and x terms with the outcome and predictors needed for your analysis.The family parameter specifies the type of model you are building, because GLM can be be used to do many different types of regression. In this case, family = “binomial” tells R to perform logistic regression.Once the model has been built, it can be used to estimate probabilities. Supplying the type = “response” parameter to the predict function produces the predicted probabilities which are easier to interpret than the default log-odds values.To make predictions, the probabilities must be converted into the outcome of interest using an ifelse step. This if/else predicts ‘1’ if the predicted probability is greater than 50%, and ‘0’ otherwise. Sometimes you may need to set this threshold higher or lower to make the model more or less aggressive.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>Though this may seem confusing at first, you’ll have an opportunity to practice building logistic regression models and making predictions in the coming exercises.</p>
</section><section id="building-simple-logistic-regression-models" class="level2" data-number="3.2"><h2 data-number="3.2" class="anchored" data-anchor-id="building-simple-logistic-regression-models">
<span class="header-section-number">3.2</span> Building simple logistic regression models</h2>
<p>The <code>donors</code> dataset contains 93,462 examples of people mailed in a fundraising solicitation for paralyzed military veterans. The <code>donated</code> column is <code>1</code> if the person made a donation in response to the mailing and <code>0</code> otherwise. This binary outcome will be the <em>dependent</em> variable for the logistic regression model.</p>
<p>The remaining columns are features of the prospective donors that may influence their donation behavior. These are the model’s <em>independent variables</em>.</p>
<p>When building a regression model, it is often helpful to form a hypothesis about which independent variables will be predictive of the dependent variable. The <code>bad_address</code> column, which is set to <code>1</code> for an invalid mailing address and <code>0</code> otherwise, seems like it might reduce the chances of a donation. Similarly, one might suspect that religious interest (<code>interest_religion</code>) and interest in veterans affairs (<code>interest_veterans</code>) would be associated with greater charitable giving.</p>
<p>In this exercise, you will use these three factors to create a simple model of donation behavior. The dataset <code>donors</code> is available in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Examine <code>donors</code> using the <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1"></a><span class="co"># Load data</span></span>
<span id="cb59-2"><a href="#cb59-2"></a>donors <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/donors.csv"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb59-3"><a href="#cb59-3"></a>              <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), as.integer),</span>
<span id="cb59-4"><a href="#cb59-4"></a>                     <span class="fu">across</span>(<span class="fu">where</span>(is.character), as.factor))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 93462 Columns: 13
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr  (3): recency, frequency, money
#&gt; dbl (10): donated, veteran, bad_address, age, has_children, wealth_rating, i...
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1"></a><span class="co"># Examine the dataset to identify potential independent variables</span></span>
<span id="cb61-2"><a href="#cb61-2"></a><span class="fu">str</span>(donors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; tibble [93,462 × 13] (S3: tbl_df/tbl/data.frame)
#&gt;  $ donated          : int [1:93462] 0 0 0 0 0 0 0 0 0 0 ...
#&gt;  $ veteran          : int [1:93462] 0 0 0 0 0 0 0 0 0 0 ...
#&gt;  $ bad_address      : int [1:93462] 0 0 0 0 0 0 0 0 0 0 ...
#&gt;  $ age              : int [1:93462] 60 46 NA 70 78 NA 38 NA NA 65 ...
#&gt;  $ has_children     : int [1:93462] 0 1 0 0 1 0 1 0 0 0 ...
#&gt;  $ wealth_rating    : int [1:93462] 0 3 1 2 1 0 2 3 1 0 ...
#&gt;  $ interest_veterans: int [1:93462] 0 0 0 0 0 0 0 0 0 0 ...
#&gt;  $ interest_religion: int [1:93462] 0 0 0 0 1 0 0 0 0 0 ...
#&gt;  $ pet_owner        : int [1:93462] 0 0 0 0 0 0 1 0 0 0 ...
#&gt;  $ catalog_shopper  : int [1:93462] 0 0 0 0 1 0 0 0 0 0 ...
#&gt;  $ recency          : Factor w/ 2 levels "CURRENT","LAPSED": 1 1 1 1 1 1 1 1 1 1 ...
#&gt;  $ frequency        : Factor w/ 2 levels "FREQUENT","INFREQUENT": 1 1 1 1 1 2 2 1 2 2 ...
#&gt;  $ money            : Factor w/ 2 levels "HIGH","MEDIUM": 2 1 2 2 2 2 2 2 2 2 ...</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Count the number of occurrences of each level of the <code>donated</code> variable using the <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1"></a><span class="co"># Explore the dependent variable</span></span>
<span id="cb63-2"><a href="#cb63-2"></a><span class="fu">table</span>(donors<span class="sc">$</span>donated)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt;     0     1 
#&gt; 88751  4711</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Fit a logistic regression model using the formula interface and the three independent variables described above. <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> with the formula as its first argument and the dataframe as the <code>data</code> argument.the result as <code>donation_model</code>.</li>
<li>Call <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> with the formula as its first argument and the dataframe as the <code>data</code> argument.</li>
<li>Save the result as <code>donation_model</code>.</li>
<li>Summarize the model object with <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1"></a><span class="co"># Build the donation model</span></span>
<span id="cb65-2"><a href="#cb65-2"></a>donation_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(donated <span class="sc">~</span> bad_address <span class="sc">+</span> interest_religion <span class="sc">+</span> interest_veterans, </span>
<span id="cb65-3"><a href="#cb65-3"></a>                      <span class="at">data =</span> donors, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb65-4"><a href="#cb65-4"></a></span>
<span id="cb65-5"><a href="#cb65-5"></a><span class="co"># Summarize the model results</span></span>
<span id="cb65-6"><a href="#cb65-6"></a><span class="fu">summary</span>(donation_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = donated ~ bad_address + interest_religion + interest_veterans, 
#&gt;     family = "binomial", data = donors)
#&gt; 
#&gt; Coefficients:
#&gt;                   Estimate Std. Error  z value Pr(&gt;|z|)    
#&gt; (Intercept)       -2.95139    0.01652 -178.664   &lt;2e-16 ***
#&gt; bad_address       -0.30780    0.14348   -2.145   0.0319 *  
#&gt; interest_religion  0.06724    0.05069    1.327   0.1847    
#&gt; interest_veterans  0.11009    0.04676    2.354   0.0186 *  
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 37330  on 93461  degrees of freedom
#&gt; Residual deviance: 37316  on 93458  degrees of freedom
#&gt; AIC: 37324
#&gt; 
#&gt; Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>Great work! With the model built, you can now use it to make predictions!</p>
</section><section id="making-a-binary-prediction" class="level2" data-number="3.3"><h2 data-number="3.3" class="anchored" data-anchor-id="making-a-binary-prediction">
<span class="header-section-number">3.3</span> Making a binary prediction</h2>
<p>In the previous exercise, you used the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function to build a logistic regression model of donor behavior. As with many of R’s machine learning methods, you can apply the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to the model object to forecast future behavior. By default, <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> outputs predictions in terms of <em>log odds</em> unless <code>type = "response"</code> is specified. This converts the log odds to <em>probabilities</em>.</p>
<p>Because a logistic regression model estimates the <em>probability</em> of the outcome, it is up to you to determine the threshold at which the probability implies action. One must balance the extremes of being too cautious versus being too aggressive. For example, if you were to solicit only the people with a 99% or greater donation probability, you may miss out on many people with lower estimated probabilities that still choose to donate. This balance is particularly important to consider for severely imbalanced outcomes, such as in this dataset where donations are relatively rare.</p>
<p>The dataset <code>donors</code> and the model <code>donation_model</code> are already loaded in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to estimate each person’s donation probability. Use the <code>type</code> argument to get probabilities. Assign the predictions to a new column called <code>donation_prob</code>.</li>
<li>Find the actual probability that an average person would donate by passing the <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> function the appropriate column of the <code>donors</code> dataframe.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1"></a><span class="co"># Estimate the donation probability</span></span>
<span id="cb67-2"><a href="#cb67-2"></a>donors<span class="sc">$</span>donation_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(donation_model, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb67-3"><a href="#cb67-3"></a></span>
<span id="cb67-4"><a href="#cb67-4"></a><span class="co"># Find the donation probability of the average prospect</span></span>
<span id="cb67-5"><a href="#cb67-5"></a><span class="fu">mean</span>(donors<span class="sc">$</span>donated)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.05040551</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Use <code><a href="https://rdrr.io/r/base/ifelse.html">ifelse()</a></code> to predict a donation if their predicted donation probability is greater than average. Assign the predictions to a new column called <code>donation_pred</code>.</li>
<li>Use the <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> function to calculate the model’s accuracy.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1"></a><span class="co"># Predict a donation if probability of donation is greater than average</span></span>
<span id="cb69-2"><a href="#cb69-2"></a>donors<span class="sc">$</span>donation_pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(donors<span class="sc">$</span>donation_prob <span class="sc">&gt;</span> <span class="fl">0.0504</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb69-3"><a href="#cb69-3"></a></span>
<span id="cb69-4"><a href="#cb69-4"></a><span class="co"># Calculate the model's accuracy</span></span>
<span id="cb69-5"><a href="#cb69-5"></a><span class="fu">mean</span>(donors<span class="sc">$</span>donated <span class="sc">==</span> donors<span class="sc">$</span>donation_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.794815</code></pre>
</div>
</div>
<p>Nice work! With an accuracy of nearly 80%, the model seems to be doing its job. But is it too good to be true?</p>
</section><section id="the-limitations-of-accuracy" class="level2" data-number="3.4"><h2 data-number="3.4" class="anchored" data-anchor-id="the-limitations-of-accuracy">
<span class="header-section-number">3.4</span> The limitations of accuracy</h2>
<p>In the previous exercise, you found that the logistic regression model made a correct prediction nearly 80% of the time. Despite this relatively high accuracy, the result is misleading due to the rarity of outcome being predicted.</p>
<blockquote class="blockquote">
<h2 id="question-8" data-number="3.5" class="anchored">
<span class="header-section-number">3.5</span> <em>Question</em>
</h2>
<p>What would the accuracy have been if a model had simply predicted “no donation” for each person?<br><br> ⬜ 80%<br> ⬜ 85%<br> ⬜ 90%<br> ✅ 95%<br></p>
</blockquote>
<p>Correct! With an accuracy of only 80%, the model is actually performing WORSE than if it were to predict non-donor for every record.</p>
</section><section id="model-performance-tradeoffs" class="level2" data-number="3.6"><h2 data-number="3.6" class="anchored" data-anchor-id="model-performance-tradeoffs">
<span class="header-section-number">3.6</span> Model performance tradeoffs</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Model performance tradeoffs</strong></p>
<p>As the previous exercise illustrated, rare events create challenges for classification models. When one outcome is very rare, predicting the opposite can result in a very high accuracy. This was the case in the donations dataset; because only about 5% of people were donors, predicting non-donation resulted in an overall accuracy of 95% but an accuracy of zero on the outcome that mattered the most: the donations.In cases like these, it may be necessary to to sacrifice a bit of overall accuracy in order to better target the outcome of interest.</p>
<p><strong>2. Understanding ROC curves</strong></p>
<p>A visualization called an ROC curve provides a way to better understand a model’s ability to distinguish between positive and negative predictions the outcome of interest versus all others.To understand how it works, imagine that you were working on a project where the positive outcome is ‘X’ and the negative outcome is ‘O’. The classifier is trying to distinguish between the two. If the classifier is poor, the X’s and O’s will remain very mixed as shown here.The ROC curve depicts the relationship between the percentage of positive examples as it relates to the percentage of the other outcomes. Here, because the X’s and O’s are even, the ROC curve makes a diagonal line showing that the proportion of interesting examples rises evenly with the proportion of negative examples.On the other hand, suppose we have a machine learning model that is able to sort the examples of interest so they appear at the front of the dataset. The outcomes might be arranged as shown here, with more X’s on the left than on the right.When the ROC curve is drawn for this arrangement, it is no longer on the diagonal because the model is able to identify several positive examples for each negative example it accidentally prioritized.The diagonal line is the baseline performance for a very poor model. The further another curve is away from this, the better it is performing. Conversely, a model that is very close to the diagonal line is not performing very well at all. To</p>
<p><strong>3. Area under the ROC curve</strong></p>
<p>quantify this performance, a measurement called AUC, or Area Under the Curve, is used. The AUC literally measures the area under the ROC curve.The baseline model that is no better than random chance has an AUC of 0-point-50, because it divides the 1-by-1 unit square perfectly in half.A perfect model has an AUC of 1-point-00, with a curve all the way in the upper-left of the square.Most real-world results are somewhere in between. Generally speaking, the closer the AUC is to 1-point-00, the better, but there are some cases where AUC can be misleading.</p>
<p><strong>4. Using AUC and ROC appropriately</strong></p>
<p>Curves of varying shapes can have the same AUC value. For this reason, it is important to look not only at the AUC but also how the shape of each curve indicates how a model is performing across the range of predictions.For example, one model may do extremely well at identifying a few easy cases at first but perform poorly on more difficult cases. Another model may do just the opposite. As this figure shows, both may end up with the same AUC.ROC curves are an important tool for comparing models and selecting the best model for your specific project needs. When used with a single model, it can help to visualize the tradeoff between true positives and false positives for the outcome of interest.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>In the next exercise, you’ll have the opportunity to plot an ROC curve for the donation data, to see how well the model is really performing.</p>
</section><section id="calculating-roc-curves-and-auc" class="level2" data-number="3.7"><h2 data-number="3.7" class="anchored" data-anchor-id="calculating-roc-curves-and-auc">
<span class="header-section-number">3.7</span> Calculating ROC Curves and AUC</h2>
<p>The previous exercises have demonstrated that accuracy is a very misleading measure of model performance on imbalanced datasets. Graphing the model’s performance better illustrates the tradeoff between a model that is overly agressive and one that is overly passive.</p>
<p>In this exercise you will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model of donations you built earlier.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Load the <code>pROC</code> package.</li>
<li>Create a ROC curve with <code>roc()</code> and the columns of actual and predicted donations. Store the result as <code>ROC</code>.</li>
<li>Use <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> to draw the <code>ROC</code> object. Specify <code>col = "blue"</code> to color the curve blue.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1"></a><span class="co"># Load the pROC package</span></span>
<span id="cb71-2"><a href="#cb71-2"></a><span class="fu">library</span>(pROC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Type 'citation("pROC")' for a citation.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'pROC'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:stats':
#&gt; 
#&gt;     cov, smooth, var</code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1"></a><span class="co"># Create a ROC curve</span></span>
<span id="cb75-2"><a href="#cb75-2"></a>ROC <span class="ot">&lt;-</span> <span class="fu">roc</span>(donors<span class="sc">$</span>donated, donors<span class="sc">$</span>donation_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Setting levels: control = 0, case = 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Setting direction: controls &lt; cases</code></pre>
</div>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1"></a><span class="co"># Plot the ROC curve</span></span>
<span id="cb78-2"><a href="#cb78-2"></a><span class="fu">plot</span>(ROC, <span class="at">col =</span> <span class="st">"blue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_supervised_learning_classification_files/figure-html/unnamed-chunk-27-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="01_supervised_learning_classification_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="4" type="1">
<li>Compute the area under the curve with <code>auc()</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1"></a><span class="co"># Calculate the area under the curve (AUC)</span></span>
<span id="cb79-2"><a href="#cb79-2"></a><span class="fu">auc</span>(ROC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Area under the curve: 0.5102</code></pre>
</div>
</div>
<p>Awesome job! Based on this visualization, the model isn’t doing much better than baseline— a model doing nothing but making predictions at random.</p>
</section><section id="comparing-roc-curves" class="level2" data-number="3.8"><h2 data-number="3.8" class="anchored" data-anchor-id="comparing-roc-curves">
<span class="header-section-number">3.8</span> Comparing ROC curves</h2>
<p>Which of the following ROC curves illustrates the best model?</p>
<p><img src="https://assets.datacamp.com/production/course_2906/datasets/lr_auc_compare.png" width="600" height="200" alt="3 ROC Curves"></p>
<blockquote class="blockquote">
<h2 id="question-9" data-number="3.9" class="anchored">
<span class="header-section-number">3.9</span> <em>Question</em>
</h2>
<p>???<br><br> ⬜ AUC 0.55<br> ⬜ AUC 0.59<br> ⬜ AUC 0.62<br> ✅ I need more information!<br></p>
</blockquote>
<p>Correct! When AUC values are very close, it’s important to know more about how the model will be used.</p>
</section><section id="dummy-variables-missing-data-and-interactions" class="level2" data-number="3.10"><h2 data-number="3.10" class="anchored" data-anchor-id="dummy-variables-missing-data-and-interactions">
<span class="header-section-number">3.10</span> Dummy variables, missing data, and interactions</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Dummy variables, missing data, and interactions</strong></p>
<p>All of the predictors used in a regression analysis must be numeric. This means that all categorical data must be represented as a number. Missing data also poses a problem, as the empty value cannot be used to make predictions.In this video, you will learn tips for preparing these types of data to be used in a logistic regression model. You will also learn about how to model the interactions among predictors an important step in building more powerful predictive models.</p>
<p><strong>2. Dummy coding categorical data</strong></p>
<p>In chapter 1, you learned about dummy coding, which creates a set of binary (one-zero) variables that represent each category except one that serves as the reference group.Dummy coding is the most common method for handling categorical data for logistic regression. The glm function will automatically dummy code any factor type variables used in the model. Simply apply the factor function to the data as in the example here.Keep in mind that you may run into a case where a categorical feature is represented with numbers, such as 1, 2, 3 for ‘hot’, ‘warm’, and ‘cold’. Even in the case, it may be advisable to convert this to a factor. This allows each category to have a unique impact on the outcome.</p>
<p><strong>3. Imputing missing data</strong></p>
<p>By default, the regression model will exclude any observation with missing values on its predictors. This may not be a big deal for small amounts of missing data, but can very quickly become a much larger problem.With categorical missing data, a missing value can be treated like any other category. You might construct categories for male, female, other, and missing.When a numeric value is missing, the solution is less clear. One potential solution uses a technique called imputation. This fills, or imputes, the missing value with a guess about what the value may be. A very simple strategy is called mean imputation, which as you might expect, imputes the average. Because records having missing data may differ systematically from those without, a binary 1/0 missing value indicator can be added to model the fact that a value was imputed. Sometimes, this becomes one of the model’s most important predictors!It is important to note that although this strategy is OK for simple predictive models, it is not appropriate for every regression application. More sophisticated forms of imputation use models to predict the missing data based on the non-missing values.</p>
<p><strong>4. Interaction effects</strong></p>
<p>An interaction effect considers the fact that two predictors, when combined, may have a different impact on an outcome than the sum of their separate individual impacts. Their combination may strengthen, weaken, or completely eliminate the impact of the individual predictors.For example, obesity and smoking are both known to be harmful to one’s health, but put together they may be even more harmful. Alternatively, two predictors may be harmful when applied separately, but when combined they neutralize, suppress, or nullify each other.Being able to model these combinations is important for creating the best predictive models. As illustrated here, the R formula interface uses the multiplication symbol to create an interaction between two predictors. The resulting model will include terms for each of the individual components as well as the combined effect.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>In the next series of exercises, you will apply dummy coding, missing value imputation, and interaction effects to build a stronger donation model.</p>
</section><section id="coding-categorical-features" class="level2" data-number="3.11"><h2 data-number="3.11" class="anchored" data-anchor-id="coding-categorical-features">
<span class="header-section-number">3.11</span> Coding categorical features</h2>
<p>Sometimes a dataset contains numeric values that represent a categorical feature.</p>
<p>In the <code>donors</code> dataset, <code>wealth_rating</code> uses numbers to indicate the donor’s wealth level:</p>
<ul>
<li>
<strong>0</strong> = Unknown</li>
<li>
<strong>1</strong> = Low</li>
<li>
<strong>2</strong> = Medium</li>
<li>
<strong>3</strong> = High</li>
</ul>
<p>This exercise illustrates how to prepare this type of categorical feature and examines its impact on a logistic regression model. The dataframe <code>donors</code> is loaded in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a factor <code>wealth_levels</code> from the numeric <code>wealth_rating</code> with labels as shown above by passing the <code><a href="https://rdrr.io/r/base/factor.html">factor()</a></code> function the column you want to convert, the individual levels, and the labels.</li>
<li>Use <code><a href="https://rdrr.io/r/stats/relevel.html">relevel()</a></code> to change the reference category to <code>Medium</code>. The first argument should be your new <code>factor</code> column.</li>
<li>Build a logistic regression model using the column <code>wealth_levels</code> to predict <code>donated</code> and display the result with <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1"></a><span class="co"># Convert the wealth rating to a factor</span></span>
<span id="cb81-2"><a href="#cb81-2"></a>donors<span class="sc">$</span>wealth_levels <span class="ot">&lt;-</span> <span class="fu">factor</span>(donors<span class="sc">$</span>wealth_rating, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Unknown"</span>, <span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span>))</span>
<span id="cb81-3"><a href="#cb81-3"></a></span>
<span id="cb81-4"><a href="#cb81-4"></a><span class="co"># Use relevel() to change reference category</span></span>
<span id="cb81-5"><a href="#cb81-5"></a>donors<span class="sc">$</span>wealth_levels <span class="ot">&lt;-</span> <span class="fu">relevel</span>(donors<span class="sc">$</span>wealth_levels, <span class="at">ref =</span> <span class="st">"Medium"</span>)</span>
<span id="cb81-6"><a href="#cb81-6"></a></span>
<span id="cb81-7"><a href="#cb81-7"></a><span class="co"># See how our factor coding impacts the model</span></span>
<span id="cb81-8"><a href="#cb81-8"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(donated <span class="sc">~</span> wealth_levels, <span class="at">data =</span> donors, <span class="at">family =</span> <span class="st">"binomial"</span>) </span>
<span id="cb81-9"><a href="#cb81-9"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = donated ~ wealth_levels, family = "binomial", data = donors)
#&gt; 
#&gt; Coefficients:
#&gt;                      Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)          -2.91894    0.03614 -80.772   &lt;2e-16 ***
#&gt; wealth_levelsUnknown -0.04373    0.04243  -1.031    0.303    
#&gt; wealth_levelsLow     -0.05245    0.05332  -0.984    0.325    
#&gt; wealth_levelsHigh     0.04804    0.04768   1.008    0.314    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 37330  on 93461  degrees of freedom
#&gt; Residual deviance: 37323  on 93458  degrees of freedom
#&gt; AIC: 37331
#&gt; 
#&gt; Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>Great job! What would the model output have looked like if this variable had been left as a numeric column?</p>
</section><section id="handling-missing-data" class="level2" data-number="3.12"><h2 data-number="3.12" class="anchored" data-anchor-id="handling-missing-data">
<span class="header-section-number">3.12</span> Handling missing data</h2>
<p>Some of the prospective donors have missing <code>age</code> data. Unfortunately, R will exclude any cases with <code>NA</code> values when building a regression model.</p>
<p>One workaround is to replace, or <strong>impute</strong>, the missing values with an estimated value. After doing so, you may also create a missing data indicator to model the possibility that cases with missing data are different in some way from those without.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> on <code>donors$age</code> to find the average age of prospects with non-missing data.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1"></a><span class="co"># Load data</span></span>
<span id="cb83-2"><a href="#cb83-2"></a>donors <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/donors.csv"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb83-3"><a href="#cb83-3"></a>              <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), as.integer),</span>
<span id="cb83-4"><a href="#cb83-4"></a>                     <span class="fu">across</span>(<span class="fu">where</span>(is.character), as.factor))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 93462 Columns: 13
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr  (3): recency, frequency, money
#&gt; dbl (10): donated, veteran, bad_address, age, has_children, wealth_rating, i...
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1"></a><span class="co"># Find the average age among non-missing values</span></span>
<span id="cb85-2"><a href="#cb85-2"></a><span class="fu">summary</span>(donors<span class="sc">$</span>age)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
#&gt;    1.00   48.00   62.00   61.65   75.00   98.00   22546</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Use <code><a href="https://rdrr.io/r/base/ifelse.html">ifelse()</a></code> and the test <code>is.na(donors$age)</code> to impute the average (rounded to 2 decimal places) for cases with missing <code>age</code>. Be sure to also ignore <code>NA</code>s.</li>
<li>Create a binary dummy variable named <code>missing_age</code> indicating the presence of missing data using another <code><a href="https://rdrr.io/r/base/ifelse.html">ifelse()</a></code> call and the same test.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1"></a><span class="co"># Impute missing age values with the mean age</span></span>
<span id="cb87-2"><a href="#cb87-2"></a>donors<span class="sc">$</span>imputed_age <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(donors<span class="sc">$</span>age), <span class="fu">round</span>(<span class="fu">mean</span>(donors<span class="sc">$</span>age, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="dv">2</span>), donors<span class="sc">$</span>age)</span>
<span id="cb87-3"><a href="#cb87-3"></a></span>
<span id="cb87-4"><a href="#cb87-4"></a><span class="co"># Create missing value indicator for age</span></span>
<span id="cb87-5"><a href="#cb87-5"></a>donors<span class="sc">$</span>missing_age <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(donors<span class="sc">$</span>age), <span class="dv">1</span>, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Super! This is one way to handle missing data, but be careful! Sometimes missing data has to be dealt with using more complicated methods.</p>
</section><section id="understanding-missing-value-indicators" class="level2" data-number="3.13"><h2 data-number="3.13" class="anchored" data-anchor-id="understanding-missing-value-indicators">
<span class="header-section-number">3.13</span> Understanding missing value indicators</h2>
<p>A missing value indicator provides a reminder that, before imputation, there was a missing value present on the record.</p>
<blockquote class="blockquote">
<h2 id="question-10" data-number="3.14" class="anchored">
<span class="header-section-number">3.14</span> <em>Question</em>
</h2>
<p>Why is it often useful to include this indicator as a predictor in the model?<br><br> ⬜ A missing value may represent a unique category by itself<br> ⬜ There may be an important difference between records with and without missing data<br> ⬜ Whatever caused the missing value may also be related to the outcome<br> ✅ All of the above<br></p>
</blockquote>
<p>Yes! Sometimes a missing value says a great deal about the record it appeared on!</p>
</section><section id="building-a-more-sophisticated-model" class="level2" data-number="3.15"><h2 data-number="3.15" class="anchored" data-anchor-id="building-a-more-sophisticated-model">
<span class="header-section-number">3.15</span> Building a more sophisticated model</h2>
<p>One of the best predictors of future giving is a history of recent, frequent, and large gifts. In marketing terms, this is known as R/F/M:</p>
<ul>
<li>Recency</li>
<li>Frequency</li>
<li>Money</li>
</ul>
<p>Donors that haven’t given both recently and frequently may be especially likely to give again; in other words, the <em>combined</em> impact of recency and frequency may be greater than the sum of the separate effects.</p>
<p>Because these predictors together have a greater impact on the dependent variable, their joint effect must be modeled as an interaction. The <code>donors</code> dataset has been loaded for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a logistic regression model of <code>donated</code> as a function of <code>money</code> plus the interaction of <code>recency</code> and <code>frequency</code>. Use <code>*</code> to add the interaction term.</li>
<li>Examine the model’s <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> to confirm the interaction effect was added.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1"></a><span class="co"># Build a recency, frequency, and money (RFM) model</span></span>
<span id="cb88-2"><a href="#cb88-2"></a>rfm_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(donated <span class="sc">~</span> recency <span class="sc">*</span> frequency <span class="sc">+</span> money, <span class="at">data =</span> donors, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb88-3"><a href="#cb88-3"></a></span>
<span id="cb88-4"><a href="#cb88-4"></a><span class="co"># Summarize the RFM model to see how the parameters were coded</span></span>
<span id="cb88-5"><a href="#cb88-5"></a><span class="fu">summary</span>(rfm_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = donated ~ recency * frequency + money, family = "binomial", 
#&gt;     data = donors)
#&gt; 
#&gt; Coefficients:
#&gt;                                   Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)                       -3.01142    0.04279 -70.375   &lt;2e-16 ***
#&gt; recencyLAPSED                     -0.86677    0.41434  -2.092   0.0364 *  
#&gt; frequencyINFREQUENT               -0.50148    0.03107 -16.143   &lt;2e-16 ***
#&gt; moneyMEDIUM                        0.36186    0.04300   8.415   &lt;2e-16 ***
#&gt; recencyLAPSED:frequencyINFREQUENT  1.01787    0.51713   1.968   0.0490 *  
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 37330  on 93461  degrees of freedom
#&gt; Residual deviance: 36938  on 93457  degrees of freedom
#&gt; AIC: 36948
#&gt; 
#&gt; Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Save the model’s predicted probabilities as <code>rfm_prob</code>. Use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function, and remember to set the <code>type</code> argument.</li>
<li>Plot a ROC curve by using the function <code>roc()</code>. Remember, this function takes the column of outcomes and the vector of predictions.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1"></a><span class="co"># Compute predicted probabilities for the RFM model</span></span>
<span id="cb90-2"><a href="#cb90-2"></a>rfm_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(rfm_model, <span class="at">data =</span> donors, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb90-3"><a href="#cb90-3"></a></span>
<span id="cb90-4"><a href="#cb90-4"></a><span class="co"># Plot the ROC curve for the new model</span></span>
<span id="cb90-5"><a href="#cb90-5"></a>ROC <span class="ot">&lt;-</span> <span class="fu">roc</span>(donors<span class="sc">$</span>donated, rfm_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Setting levels: control = 0, case = 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Setting direction: controls &lt; cases</code></pre>
</div>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1"></a><span class="fu">plot</span>(ROC, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_supervised_learning_classification_files/figure-html/unnamed-chunk-33-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="01_supervised_learning_classification_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="5" type="1">
<li>Compute the AUC for the new model with the function <code>auc()</code> and compare performance to the simpler model.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1"></a><span class="fu">auc</span>(ROC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Area under the curve: 0.5785</code></pre>
</div>
</div>
<p>Great work! Based on the ROC curve, you’ve confirmed that past giving patterns are certainly predictive of future giving.</p>
</section><section id="automatic-feature-selection" class="level2" data-number="3.16"><h2 data-number="3.16" class="anchored" data-anchor-id="automatic-feature-selection">
<span class="header-section-number">3.16</span> Automatic feature selection</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Automatic feature selection</strong></p>
<p>Unlike some machine learning methods, regression typically asks the human to specify the model’s predictors ahead of time. Thus, each of the donation models you’ve built so far required a little bit of fund-raising subject-matter expertise to identify the variables that may be predictive of donations.Sometimes you may not have this type of insight ahead of time. You may not know what all of the predictors mean, or you may have so many predictors there’s no easy way to sort through them all.A process called automatic feature selection can be used here, but as you’ll soon see, with this great power comes great responsibility to apply it carefully.</p>
<p><strong>2. Stepwise regression</strong></p>
<p>Stepwise regression involves building a regression model step by step, evaluating each predictor to see which ones add value to the final model. A procedure called backward deletion begins with a model containing all of the predictors. It then checks to see what happens when each one of the predictors is removed from the model. If removing a predictor does not substantially impact the model’s ability to predict the outcome, then it can be safely deleted. At each step, the predictor that impacts the model the least is removed– assuming, of course, it has minimal impact. This continues step-by-step until only important predictors remain.The same idea applied in the other direction is called forward selection. Beginning with a model containing no predictors, it examines each potential predictor to see which one, if any, offers the greatest improvement to the model’s predictive power. Predictors are added step-by-step until no new predictors add substantial value to the model.Keep in mind that although the figures here show the same final model for backward and forward stepwise, this is not always the case. It is possible that the two could come to completely different conclusions about the most important predictors.</p>
<p><strong>3. Stepwise regression caveats</strong></p>
<p>This is just one of the potential caveats of stepwise regression. Not only can backward and forward selection create completely different models, but neither is guaranteed to find the best possible model. Statisticians also raise concerns about the fact that a stepwise model violates some of the principles that allow a regression model to explain data as well as predict. Of course, if you only care about the PREDICTIVE power, this may not be a very big concern– the use of stepwise doesn’t mean the model’s predictions are worthless; it simply means that the model may over or understate the importance of some of the predictors.Perhaps most importantly, feature selection methods like stepwise regression allow the model to be built in the absence of theory or even common sense. This can result in a model that seems counterintuitive in the real world.It may be best to consider stepwise regression as just one tool for exploring potential models in the absence of another good starting point.</p>
<p><strong>4. Let’s practice!</strong></p>
<p>You’ll have a chance to see how to build a stepwise regression model during the next coding exercise.</p>
</section><section id="the-dangers-of-stepwise-regression" class="level2" data-number="3.17"><h2 data-number="3.17" class="anchored" data-anchor-id="the-dangers-of-stepwise-regression">
<span class="header-section-number">3.17</span> The dangers of stepwise regression</h2>
<blockquote class="blockquote">
<h2 id="question-11" data-number="3.18" class="anchored">
<span class="header-section-number">3.18</span> <em>Question</em>
</h2>
<p>In spite of its utility for feature selection, stepwise regression is not frequently used in disciplines outside of machine learning due to some important caveats. Which of these is NOT one of these concerns?<br><br> ⬜ It is not guaranteed to find the best possible model<br> ✅ A stepwise model’s predictions can not be trusted<br> ⬜ The stepwise regression procedure violates some statistical assumptions<br> ⬜ It can result in a model that makes little sense in the real world<br></p>
</blockquote>
<p>Correct! Though stepwise regression is frowned upon, it may still be useful for building predictive models in the absence of another starting place.</p>
</section><section id="building-a-stepwise-regression-model" class="level2" data-number="3.19"><h2 data-number="3.19" class="anchored" data-anchor-id="building-a-stepwise-regression-model">
<span class="header-section-number">3.19</span> Building a stepwise regression model</h2>
<p>In the absence of subject-matter expertise, <strong>stepwise regression</strong> can assist with the search for the most important predictors of the outcome of interest.</p>
<p>In this exercise, you will use a forward stepwise approach to add predictors to the model one-by-one until no additional benefit is seen. The <code>donors</code> dataset has been loaded for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the R formula interface with <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> to specify the base model with no predictors. Set the explanatory variable equal to <code>1</code>.</li>
<li>Use the R formula interface again with <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> to specify the model with all predictors.</li>
<li>Apply <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> to these models to perform forward stepwise regression. Set the first argument to <code>null_model</code> and set <code>direction = "forward"</code>. This might take a while (up to 10 or 15 seconds) as your computer has to fit quite a few different models to perform stepwise selection.</li>
<li>Create a vector of predicted probabilities using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function.</li>
<li>Plot the ROC curve with <code>roc()</code> and <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> and compute the AUC of the stepwise model with <code>auc()</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1"></a><span class="co"># Specify a null model with no predictors</span></span>
<span id="cb96-2"><a href="#cb96-2"></a>null_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(donated <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> donors, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb96-3"><a href="#cb96-3"></a></span>
<span id="cb96-4"><a href="#cb96-4"></a><span class="co"># Specify the full model using all of the potential predictors</span></span>
<span id="cb96-5"><a href="#cb96-5"></a>full_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(donated <span class="sc">~</span> ., <span class="at">data =</span> donors, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb96-6"><a href="#cb96-6"></a></span>
<span id="cb96-7"><a href="#cb96-7"></a><span class="co"># Use a forward stepwise algorithm to build a parsimonious model</span></span>
<span id="cb96-8"><a href="#cb96-8"></a>step_model <span class="ot">&lt;-</span> <span class="fu">step</span>(null_model, <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> null_model, <span class="at">upper =</span> full_model), <span class="at">direction =</span> <span class="st">"forward"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Start:  AIC=37332.13
#&gt; donated ~ 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :
#&gt; benutze 70916/93462 Zeilen aus einem kombinierten Fit</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     Df Deviance   AIC
#&gt; + frequency          1    28502 37122
#&gt; + money              1    28621 37241
#&gt; + wealth_rating      1    28705 37326
#&gt; + has_children       1    28705 37326
#&gt; + age                1    28707 37328
#&gt; + imputed_age        1    28707 37328
#&gt; + interest_veterans  1    28709 37330
#&gt; + catalog_shopper    1    28710 37330
#&gt; + pet_owner          1    28711 37331
#&gt; &lt;none&gt;                    28714 37332
#&gt; + interest_religion  1    28712 37333
#&gt; + recency            1    28713 37333
#&gt; + bad_address        1    28714 37334
#&gt; + veteran            1    28714 37334
#&gt; 
#&gt; Step:  AIC=37024.77
#&gt; donated ~ frequency</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :
#&gt; benutze 70916/93462 Zeilen aus einem kombinierten Fit</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     Df Deviance   AIC
#&gt; + money              1    28441 36966
#&gt; + wealth_rating      1    28493 37018
#&gt; + has_children       1    28494 37019
#&gt; + interest_veterans  1    28498 37023
#&gt; + catalog_shopper    1    28499 37024
#&gt; + age                1    28499 37024
#&gt; + imputed_age        1    28499 37024
#&gt; + pet_owner          1    28499 37024
#&gt; &lt;none&gt;                    28502 37025
#&gt; + interest_religion  1    28501 37026
#&gt; + recency            1    28501 37026
#&gt; + bad_address        1    28502 37026
#&gt; + veteran            1    28502 37027
#&gt; 
#&gt; Step:  AIC=36949.71
#&gt; donated ~ frequency + money</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :
#&gt; benutze 70916/93462 Zeilen aus einem kombinierten Fit</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     Df Deviance   AIC
#&gt; + wealth_rating      1    28431 36942
#&gt; + has_children       1    28432 36943
#&gt; + interest_veterans  1    28438 36948
#&gt; + catalog_shopper    1    28438 36949
#&gt; + age                1    28438 36949
#&gt; + imputed_age        1    28438 36949
#&gt; + pet_owner          1    28439 36949
#&gt; &lt;none&gt;                    28441 36950
#&gt; + interest_religion  1    28440 36951
#&gt; + recency            1    28440 36951
#&gt; + bad_address        1    28441 36951
#&gt; + veteran            1    28441 36952
#&gt; 
#&gt; Step:  AIC=36945.26
#&gt; donated ~ frequency + money + wealth_rating</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :
#&gt; benutze 70916/93462 Zeilen aus einem kombinierten Fit</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     Df Deviance   AIC
#&gt; + has_children       1    28421 36937
#&gt; + interest_veterans  1    28429 36945
#&gt; + catalog_shopper    1    28429 36945
#&gt; + age                1    28429 36945
#&gt; + imputed_age        1    28429 36945
#&gt; &lt;none&gt;                    28431 36945
#&gt; + pet_owner          1    28430 36945
#&gt; + interest_religion  1    28431 36947
#&gt; + recency            1    28431 36947
#&gt; + bad_address        1    28431 36947
#&gt; + veteran            1    28431 36947
#&gt; 
#&gt; Step:  AIC=36938.08
#&gt; donated ~ frequency + money + wealth_rating + has_children</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :
#&gt; benutze 70916/93462 Zeilen aus einem kombinierten Fit</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     Df Deviance   AIC
#&gt; + pet_owner          1    28418 36937
#&gt; + catalog_shopper    1    28418 36937
#&gt; + interest_veterans  1    28418 36937
#&gt; &lt;none&gt;                    28421 36938
#&gt; + interest_religion  1    28420 36939
#&gt; + recency            1    28421 36940
#&gt; + age                1    28421 36940
#&gt; + imputed_age        1    28421 36940
#&gt; + bad_address        1    28421 36940
#&gt; + veteran            1    28421 36940
#&gt; 
#&gt; Step:  AIC=36932.08
#&gt; donated ~ frequency + money + wealth_rating + has_children + 
#&gt;     pet_owner</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :
#&gt; benutze 70916/93462 Zeilen aus einem kombinierten Fit</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     Df Deviance   AIC
#&gt; &lt;none&gt;                    28418 36932
#&gt; + interest_veterans  1    28416 36932
#&gt; + catalog_shopper    1    28416 36932
#&gt; + age                1    28417 36933
#&gt; + imputed_age        1    28417 36933
#&gt; + recency            1    28417 36934
#&gt; + interest_religion  1    28417 36934
#&gt; + bad_address        1    28418 36934
#&gt; + veteran            1    28418 36934</code></pre>
</div>
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1"></a><span class="co"># Estimate the stepwise donation probability</span></span>
<span id="cb110-2"><a href="#cb110-2"></a>step_prob  <span class="ot">&lt;-</span> <span class="fu">predict</span>(step_model, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb110-3"><a href="#cb110-3"></a></span>
<span id="cb110-4"><a href="#cb110-4"></a><span class="co"># Plot the ROC of the stepwise model</span></span>
<span id="cb110-5"><a href="#cb110-5"></a>ROC <span class="ot">&lt;-</span> <span class="fu">roc</span>(donors<span class="sc">$</span>donated, step_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Setting levels: control = 0, case = 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Setting direction: controls &lt; cases</code></pre>
</div>
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1"></a><span class="fu">plot</span>(ROC, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_supervised_learning_classification_files/figure-html/unnamed-chunk-35-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="01_supervised_learning_classification_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1"></a><span class="fu">auc</span>(ROC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Area under the curve: 0.5855</code></pre>
</div>
</div>
<p>Fantastic work! Despite the caveats of stepwise regression, it seems to have resulted in a relatively strong model!</p>
</section></section><section id="chapter-4-classification-trees" class="level1" data-number="4"><h1 data-number="4">
<span class="header-section-number">4</span> 4. Chapter 4: Classification Trees</h1>
<p>Classification trees use flowchart-like structures to make decisions. Because humans can readily understand these tree structures, classification trees are useful when transparency is needed, such as in loan approval. We’ll use the Lending Club dataset to simulate this scenario.</p>
<section id="making-decisions-with-trees" class="level2" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="making-decisions-with-trees">
<span class="header-section-number">4.1</span> Making decisions with trees</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Making decisions with trees</strong></p>
<p>Sometimes a difficult or complex decision can be made simpler by breaking it down into a series of smaller decisions. If you’re considering whether to take a new job offer, you might define requirements for accepting the position. Does it offer a high enough salary? Does it have a long commute or require long hours? Does it provide free coffee?Classification trees, also known as decision trees, work much the same way. They are used to find a set of if/else conditions that are helpful for taking action. As you will see soon, because their decisions are easily understood without statistics, they can be useful for business strategy, especially in areas where transparency is needed, like loan application approval.</p>
<p><strong>2. A decision tree model</strong></p>
<p>Let’s start by considering the decision tree structure. As you might expect, it closely resembles real-world trees. The goal is to model the relationship between predictors and an outcome of interest.Beginning at the root node, data flows through if/else decision nodes that split the data according to its attributes.The branches indicate the potential choices, and the leaf nodes denote the final decisions. These are also known as terminal nodes because they terminate the decision making process.</p>
<p><strong>3. Decision trees for prediction</strong></p>
<p>To understand how the tree structure is built, let’s consider a business process like whether or not to provide someone a loan. After an applicant fills out a form with personal information like income, credit history, and loan purpose, the bank must quickly decide whether or not the individual is likely to repay the debt.Using historical applicant data and loan outcomes, a classification tree can be built to learn the criteria that were most predictive of future loan repayment.</p>
<p><strong>4. Divide-and-conquer</strong></p>
<p>Growing the decision tree uses a process called divide-and-conquer because it attempts to divide the dataset into partitions with similar values for the outcome of interest. For loan applications, it needs to separate the applicants who are likely to repay from those who are likely to default on the debt.Suppose the tree considers two aspects of each applicant: the credit score and the requested loan amount. This figure visualizes these characteristics in relation to whether the loan was repaid.To divide-and-conquer, the algorithm looks for an initial split that creates the two most homogeneous groups.</p>
<p><strong>5. Divide-and-conquer</strong></p>
<p>First, it splits into groups of “high” and “low” credit scores.</p>
<p><strong>6. Divide-and-conquer</strong></p>
<p>Then, it divides-and-conquers again with another split, creating groups for “high” and “low” requested loan amounts.</p>
<p><strong>7. The resulting tree</strong></p>
<p>Each one of these splits results in an if/else decision in the tree structure, as shown here. If the credit score is low, it predicts “loan default.” If the credit score is high and the loan value is large, it also predicts “default.” Otherwise, it predicts “repaid.”Obviously, a decision tree built on actual lending data is likely to be much more complex. But this illustrates the basic process of how such a tree might be built; you’ll learn more about how it works shortly. For now, let’s ignore the implementation details to focus on putting the algorithm to work.</p>
<p><strong>8. Building trees in R</strong></p>
<p>There are several packages that can be used to build classification trees in R. One of the most widely used is called rpart for recursive partitioning, a synonym for divide-and-conquer.Simply use the rpart function with the R formula interface to specify the outcome and predictors. The “class” parameter tells rpart to build a classification tree.And like the other machine learning methods you’ve seen before, the predict function obtains the predicted class values for the test dataset.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>In the next exercise, you’ll have a chance to apply what you’ve learned to actual Lending Club loan data.</p>
</section><section id="building-a-simple-decision-tree" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="building-a-simple-decision-tree">
<span class="header-section-number">4.2</span> Building a simple decision tree</h2>
<p>The <code>loans</code> dataset contains 11,312 randomly-selected people who applied for and later received loans from Lending Club, a US-based peer-to-peer lending company.</p>
<p>You will use a decision tree to try to learn patterns in the outcome of these loans (either repaid or default) based on the requested loan amount and credit score at the time of application.</p>
<p>Then, see how the tree’s predictions differ for an applicant with good credit versus one with bad credit.</p>
<p>The dataset <code>loans</code> is already in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li><p>Load the <code>rpart</code> package.</p></li>
<li>
<p>Fit a decision tree model with the function <code>rpart()</code>.the R formula that specifies <code>outcome</code> as a function of <code>loan_amount</code> and <code>credit_score</code> as the first argument. the <code>control</code> argument alone for now. (You’ll learn more about that later!)</p>
<ul>
<li>Supply the R formula that specifies <code>outcome</code> as a function of <code>loan_amount</code> and <code>credit_score</code> as the first argument.</li>
<li>Leave the <code>control</code> argument alone for now. (You’ll learn more about that later!)</li>
</ul>
</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1"></a><span class="co"># Load the rpart package</span></span>
<span id="cb116-2"><a href="#cb116-2"></a><span class="fu">library</span>(rpart)</span>
<span id="cb116-3"><a href="#cb116-3"></a><span class="fu">library</span>(forcats)</span>
<span id="cb116-4"><a href="#cb116-4"></a></span>
<span id="cb116-5"><a href="#cb116-5"></a><span class="co"># Load data</span></span>
<span id="cb116-6"><a href="#cb116-6"></a>loans <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/loans.csv"</span>, <span class="at">col_types =</span> <span class="fu">cols</span>(<span class="at">.default =</span> <span class="st">"f"</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb116-7"><a href="#cb116-7"></a>            <span class="fu">filter</span>(keep <span class="sc">==</span> <span class="st">"1"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb116-8"><a href="#cb116-8"></a>            <span class="fu">mutate</span>(<span class="at">outcome =</span> default <span class="sc">|&gt;</span> <span class="fu">fct_recode</span>(<span class="at">default =</span> <span class="st">"1"</span>, <span class="at">repaid =</span> <span class="st">"0"</span>) <span class="sc">|&gt;</span> <span class="fu">fct_rev</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb116-9"><a href="#cb116-9"></a>            <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(keep, rand, default))</span>
<span id="cb116-10"><a href="#cb116-10"></a></span>
<span id="cb116-11"><a href="#cb116-11"></a><span class="co"># Build a lending model predicting loan outcome versus loan amount and credit score</span></span>
<span id="cb116-12"><a href="#cb116-12"></a>loan_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(outcome <span class="sc">~</span> loan_amount <span class="sc">+</span> credit_score, <span class="at">data =</span> loans, <span class="at">method =</span> <span class="st">"class"</span>, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Use <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> with the resulting loan model to predict the outcome for the <code>good_credit</code> applicant. Use the <code>type</code> argument to predict the <code>"class"</code> of the outcome.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1"></a><span class="co"># Create good_credit applicant</span></span>
<span id="cb117-2"><a href="#cb117-2"></a>good_credit <span class="ot">&lt;-</span> loans <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="dv">8</span>)</span>
<span id="cb117-3"><a href="#cb117-3"></a></span>
<span id="cb117-4"><a href="#cb117-4"></a><span class="co"># Make a prediction for someone with good credit</span></span>
<span id="cb117-5"><a href="#cb117-5"></a><span class="fu">predict</span>(loan_model, good_credit, <span class="at">type =</span> <span class="st">"class"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      1 
#&gt; repaid 
#&gt; Levels: default repaid</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Do the same for the <code>bad_credit</code> applicant.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1"></a><span class="co"># Create bad_credit applicant</span></span>
<span id="cb119-2"><a href="#cb119-2"></a>bad_credit <span class="ot">&lt;-</span> loans <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="dv">3</span>)</span>
<span id="cb119-3"><a href="#cb119-3"></a></span>
<span id="cb119-4"><a href="#cb119-4"></a><span class="co"># Make a prediction for someone with bad credit</span></span>
<span id="cb119-5"><a href="#cb119-5"></a><span class="fu">predict</span>(loan_model, bad_credit, <span class="at">type =</span> <span class="st">"class"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;       1 
#&gt; default 
#&gt; Levels: default repaid</code></pre>
</div>
</div>
<p>Great job! Growing a decision tree is certainly faster than growing a real tree!</p>
</section><section id="visualizing-classification-trees" class="level2" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="visualizing-classification-trees">
<span class="header-section-number">4.3</span> Visualizing classification trees</h2>
<p>Due to government rules to prevent illegal discrimination, lenders are required to explain why a loan application was rejected.</p>
<p>The structure of classification trees can be depicted visually, which helps to understand how the tree makes its decisions. The model <code>loan_model</code> that you fit in the last exercise is in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Type <code>loan_model</code> to see a text representation of the classification tree.</li>
<li>Load the <code>rpart.plot</code> package.</li>
<li>Apply the <code>rpart.plot()</code> function to the loan model to visualize the tree.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1"></a><span class="co"># Examine the loan_model object</span></span>
<span id="cb121-2"><a href="#cb121-2"></a>loan_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; n= 11312 
#&gt; 
#&gt; node), split, n, loss, yval, (yprob)
#&gt;       * denotes terminal node
#&gt; 
#&gt;  1) root 11312 5654 repaid (0.4998232 0.5001768)  
#&gt;    2) credit_score=AVERAGE,LOW 9490 4437 default (0.5324552 0.4675448)  
#&gt;      4) credit_score=LOW 1667  631 default (0.6214757 0.3785243) *
#&gt;      5) credit_score=AVERAGE 7823 3806 default (0.5134859 0.4865141)  
#&gt;       10) loan_amount=HIGH 2472 1079 default (0.5635113 0.4364887) *
#&gt;       11) loan_amount=LOW,MEDIUM 5351 2624 repaid (0.4903756 0.5096244)  
#&gt;         22) loan_amount=LOW 1810  874 default (0.5171271 0.4828729) *
#&gt;         23) loan_amount=MEDIUM 3541 1688 repaid (0.4767015 0.5232985) *
#&gt;    3) credit_score=HIGH 1822  601 repaid (0.3298573 0.6701427) *</code></pre>
</div>
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1"></a><span class="co"># Load the rpart.plot package</span></span>
<span id="cb123-2"><a href="#cb123-2"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb123-3"><a href="#cb123-3"></a></span>
<span id="cb123-4"><a href="#cb123-4"></a><span class="co"># Plot the loan_model with default settings</span></span>
<span id="cb123-5"><a href="#cb123-5"></a><span class="fu">rpart.plot</span>(loan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_supervised_learning_classification_files/figure-html/unnamed-chunk-39-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="01_supervised_learning_classification_files/figure-html/unnamed-chunk-39-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="4" type="1">
<li>See how changing other plotting parameters impacts the visualization by running the supplied command.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1"></a><span class="co"># Plot the loan_model with customized settings</span></span>
<span id="cb124-2"><a href="#cb124-2"></a><span class="fu">rpart.plot</span>(loan_model, <span class="at">type =</span> <span class="dv">3</span>, <span class="at">box.palette =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">fallen.leaves =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_supervised_learning_classification_files/figure-html/unnamed-chunk-40-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="01_supervised_learning_classification_files/figure-html/unnamed-chunk-40-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Awesome! What do you think of the fancy visualization?</p>
</section><section id="understanding-the-trees-decisions" class="level2" data-number="4.4"><h2 data-number="4.4" class="anchored" data-anchor-id="understanding-the-trees-decisions">
<span class="header-section-number">4.4</span> Understanding the tree’s decisions</h2>
<p>The following image shows the structure of a classification tree predicting loan outcome from the applicant’s credit score and requested loan amount.</p>
<p><img src="https://assets.datacamp.com/production/course_2906/datasets/dtree_plot.png" alt="Decision Tree Plot"></p>
<p>Based on this tree structure, which of the following applicants would be predicted to repay the loan?</p>
<blockquote class="blockquote">
<h2 id="question-12" data-number="4.5" class="anchored">
<span class="header-section-number">4.5</span> <em>Question</em>
</h2>
<p>???<br><br> ⬜ Someone with an average credit score and a low requested loan amount.<br> ⬜ Someone with a low credit score and a medium requested loan amount.<br> ⬜ Someone with a high requested loan amount and average credit.<br> ✅ Someone with a low requested loan amount and high credit.<br></p>
</blockquote>
<p>Correct! Using the tree structure, you can clearly see how the tree makes its decisions.</p>
</section><section id="growing-larger-classification-trees" class="level2" data-number="4.6"><h2 data-number="4.6" class="anchored" data-anchor-id="growing-larger-classification-trees">
<span class="header-section-number">4.6</span> Growing larger classification trees</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Growing larger classification trees</strong></p>
<p>Starting from a seed, real-world trees need the proper combination of soil, water, air, and light to grow. Just like understanding these principles helps you become a better gardener, knowing a bit about the growing conditions of decision trees will help you produce more robust classification models.In this lesson, you’ll learn more about how trees grow, branch out, and sometimes even outgrow their environment.</p>
<p><strong>2. Choosing where to split</strong></p>
<p>Earlier, you learned that classification trees use divide-and-conquer to identify splits that create the most “pure”, or homogeneous, partitions.To see how this works in practice, let’s consider the tree being built with data on loan applicants’ credit and requested amount. For each of these predictors, the algorithm attempts a split on the feature values and then calculates the purity of the resulting partitions. The split that produces the purest partitions will be used first.Here, split A divides the data into partitions with high and low credit scores, while split B divides the data into large and small loan amounts. Split B results in one very homogeneous partition, but its other partition is very mixed. In comparison, split A results in two partitions that are both relatively pure.As a result, the tree will choose split A first. It then continues to divide-and-conquer further.</p>
<p><strong>3. Axis-parallel splits</strong></p>
<p>As the tree continues to grow, it creates smaller and more homogeneous partitions as shown here. You may have noticed, however, that there was an easier way to create a set of perfectly-pure partitions simply use a diagonal line to divide the outcomes.Unfortunately, a decision tree cannot discover this itself because a diagonal line requires a it to consider two features at once, which is not possible in the divide-and-conquer process.Instead, a decision tree always creates so-called axis-parallel splits. This limitation is a potential weakness of decision trees; they can be overly complex when modeling certain patterns in the data.</p>
<p><strong>4. The problem of overfitting</strong></p>
<p>Generally speaking, decision trees have the tendency to become very complex very quickly. A tree can happily divide-and-conquer until it classifies every example correctly, or until it runs out of feature values to split upon.When a tree has grown overly large and overly complex, it may experience the problem of overfitting. Rather than modeling the most important trends in the data, a tree that has been over-fitted tends to model the noise. It focuses on extremely subtle patterns that may not apply more generally.More-so than many other machine learning algorithms, classification trees have this tendency to overfit the dataset it is trained on.</p>
<p><strong>5. Evaluating model performance</strong></p>
<p>When a machine learning model has been over-fitted to its training dataset, you must take care not to over-estimate how well the model will perform in the future. Just because it perfectly classifies every training example correctly does not mean it will do so on unseen data.Thus, it is important to simulate unseen future data by constructing a test dataset that the algorithm cannot use when growing the tree. A simple method for constructing test sets involves holding out a small random portion of the full dataset. This is a fair estimate of the tree’s performance; if the tree performs much more poorly on the test set than the training set, it suggests the model may have been over-fitted.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>You’ll get a chance to construct random test sets in the next exercises. Good luck!</p>
</section><section id="why-do-some-branches-split" class="level2" data-number="4.7"><h2 data-number="4.7" class="anchored" data-anchor-id="why-do-some-branches-split">
<span class="header-section-number">4.7</span> Why do some branches split?</h2>
<p>A classification tree grows using a <strong>divide-and-conquer</strong> process. Each time the tree grows larger, it splits groups of data into smaller subgroups, creating new branches in the tree.</p>
<blockquote class="blockquote">
<h2 id="question-13" data-number="4.8" class="anchored">
<span class="header-section-number">4.8</span> <em>Question</em>
</h2>
<p>Given a dataset to divide-and-conquer, which groups would the algorithm prioritize to split first?<br><br> ⬜ The group with the largest number of examples.<br> ⬜ The group creating branches that improve the model’s prediction accuracy.<br> ✅ The group it can split to create the greatest improvement in subgroup homogeneity.<br> ⬜ The group that has not been split already.<br></p>
</blockquote>
<p>Correct! Divide-and-conquer always looks to create the split resulting in the greatest improvement to purity.</p>
</section><section id="creating-random-test-datasets" class="level2" data-number="4.9"><h2 data-number="4.9" class="anchored" data-anchor-id="creating-random-test-datasets">
<span class="header-section-number">4.9</span> Creating random test datasets</h2>
<p>Before building a more sophisticated lending model, it is important to hold out a portion of the loan data to simulate how well it will predict the outcomes of future loan applicants.</p>
<p>As depicted in the following image, you can use 75% of the observations for training and 25% for testing the model.</p>
<p><img src="https://assets.datacamp.com/production/course_2906/datasets/dtree_test_set.png" height="150"></p>
<p>The <code><a href="https://rdrr.io/r/base/sample.html">sample()</a></code> function can be used to generate a random sample of rows to include in the training set. Simply supply it the total number of observations and the number needed for training.</p>
<p>Use the resulting vector of row IDs to subset the loans into training and testing datasets. The dataset <code>loans</code> is loaded in your workspace.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Apply the <code><a href="https://rdrr.io/r/base/nrow.html">nrow()</a></code> function to determine how many observations are in the <code>loans</code> dataset, and the number needed for a 75% sample.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1"></a><span class="co"># Determine the number of rows for training</span></span>
<span id="cb125-2"><a href="#cb125-2"></a><span class="fu">nrow</span>(loans) <span class="sc">*</span> <span class="fl">0.75</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 8484</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Use the <code><a href="https://rdrr.io/r/base/sample.html">sample()</a></code> function to create an integer vector of row IDs for the 75% sample. The first argument of <code><a href="https://rdrr.io/r/base/sample.html">sample()</a></code> should be the number of rows in the data set, and the second is the number of rows you need in your training set.</li>
<li>Subset the <code>loans</code> data using the row IDs to create the training dataset. Save this as <code>loans_train</code>.</li>
<li>Subset <code>loans</code> again, but this time select all the rows that are <em>not</em> in <code>sample_rows</code>. Save this as <code>loans_test</code>
</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1"></a><span class="co"># Create a random sample of row IDs</span></span>
<span id="cb127-2"><a href="#cb127-2"></a>sample_rows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(loans), <span class="fu">nrow</span>(loans) <span class="sc">*</span> <span class="fl">0.75</span>)</span>
<span id="cb127-3"><a href="#cb127-3"></a></span>
<span id="cb127-4"><a href="#cb127-4"></a><span class="co"># Create the training dataset</span></span>
<span id="cb127-5"><a href="#cb127-5"></a>loans_train <span class="ot">&lt;-</span> loans[sample_rows, ]</span>
<span id="cb127-6"><a href="#cb127-6"></a></span>
<span id="cb127-7"><a href="#cb127-7"></a><span class="co"># Create the test dataset</span></span>
<span id="cb127-8"><a href="#cb127-8"></a>loans_test <span class="ot">&lt;-</span> loans[<span class="sc">-</span>sample_rows, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Amazing work! Creating a test set is an easy way to check your model’s performance.</p>
</section><section id="building-and-evaluating-a-larger-tree" class="level2" data-number="4.10"><h2 data-number="4.10" class="anchored" data-anchor-id="building-and-evaluating-a-larger-tree">
<span class="header-section-number">4.10</span> Building and evaluating a larger tree</h2>
<p>Previously, you created a simple decision tree that used the applicant’s credit score and requested loan amount to predict the loan outcome.</p>
<p>Lending Club has additional information about the applicants, such as home ownership status, length of employment, loan purpose, and past bankruptcies, that may be useful for making more accurate predictions.</p>
<p>Using all of the available applicant data, build a more sophisticated lending model using the random training dataset created previously. Then, use this model to make predictions on the testing dataset to estimate the performance of the model on future loan applications.</p>
<p>The <code>rpart</code> package is loaded into the workspace and the <code>loans_train</code> and <code>loans_test</code> datasets have been created.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>rpart()</code> to build a loan model using the training dataset and all of the available predictors. Again, leave the <code>control</code> argument alone.</li>
<li>Applying the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to the testing dataset, create a vector of predicted outcomes. Don’t forget the <code>type</code> argument.</li>
<li>Create a <code><a href="https://rdrr.io/r/base/table.html">table()</a></code> to compare the predicted values to the actual <code>outcome</code> values.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1"></a><span class="co"># Grow a tree using all of the available applicant data</span></span>
<span id="cb128-2"><a href="#cb128-2"></a>loan_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(outcome <span class="sc">~</span> ., <span class="at">data =</span> loans_train, <span class="at">method =</span> <span class="st">"class"</span>, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>))</span>
<span id="cb128-3"><a href="#cb128-3"></a></span>
<span id="cb128-4"><a href="#cb128-4"></a><span class="co"># Make predictions on the test dataset</span></span>
<span id="cb128-5"><a href="#cb128-5"></a>loans_test<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(loan_model, loans_test, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb128-6"><a href="#cb128-6"></a></span>
<span id="cb128-7"><a href="#cb128-7"></a><span class="co"># Examine the confusion matrix</span></span>
<span id="cb128-8"><a href="#cb128-8"></a><span class="fu">table</span>(loans_test<span class="sc">$</span>pred, loans_test<span class="sc">$</span>outcome)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;          
#&gt;           default repaid
#&gt;   default     822    605
#&gt;   repaid      601    800</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Compute the accuracy of the predictions using the <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1"></a><span class="co"># Compute the accuracy on the test dataset</span></span>
<span id="cb130-2"><a href="#cb130-2"></a><span class="fu">mean</span>(loans_test<span class="sc">$</span>pred <span class="sc">==</span> loans_test<span class="sc">$</span>outcome)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.5735502</code></pre>
</div>
</div>
<p>Awesome! How did adding more predictors change the model’s performance?</p>
</section><section id="conducting-a-fair-performance-evaluation" class="level2" data-number="4.11"><h2 data-number="4.11" class="anchored" data-anchor-id="conducting-a-fair-performance-evaluation">
<span class="header-section-number">4.11</span> Conducting a fair performance evaluation</h2>
<p>Holding out test data reduces the amount of data available for growing the decision tree. In spite of this, it is very important to evaluate decision trees on data it has not seen before.</p>
<p>Which of these is NOT true about the evaluation of decision tree performance?</p>
<blockquote class="blockquote">
<h2 id="question-14" data-number="4.12" class="anchored">
<span class="header-section-number">4.12</span> <em>Question</em>
</h2>
<p>???<br><br> ⬜ Decision trees sometimes overfit the training data.<br> ✅ The model’s accuracy is unaffected by the rarity of the outcome.<br> ⬜ Performance on the training dataset can overestimate performance on future data.<br> ⬜ Creating a test dataset simulates the model’s performance on unseen data.<br></p>
</blockquote>
<p>Right! Rare events cause problems for many machine learning approaches.</p>
</section><section id="tending-to-classification-trees" class="level2" data-number="4.13"><h2 data-number="4.13" class="anchored" data-anchor-id="tending-to-classification-trees">
<span class="header-section-number">4.13</span> Tending to classification trees</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Tending to classification trees</strong></p>
<p>In the previous video, you learned that decision trees have a tendency to grow overly large and complex very quickly. If this were to happen to trees in your yard, you’d be outside with clippers, looking to trim away some of the excess greenery. Grooming healthy classification trees likewise requires this kind of attention. In this lesson, you’ll learn about pruning strategies, which help ensure the trees are just right not too large and not too small.</p>
<p><strong>2. Pre-pruning</strong></p>
<p>One method of preventing a tree from becoming too large involves stopping the growing process early. This is known as pre-pruning.Perhaps the simplest approach to pre-pruning stops divide-and-conquer once the tree reaches a predefined size. The figure here shows a tree that has been stopped early because it reached a maximum depth of three levels. Another pre-pruning method requires a minimum number of observations at a node in order for a split to occur. For example, this figure stops the tree from growing any branch with fewer than 10 observations.Both of these pre-pruning strategies prevent the tree from growing too large. However, a tree stopped too early may fail to discover subtle or important patterns it might have discovered later.</p>
<p><strong>3. Post-pruning</strong></p>
<p>To address this concern, it is also possible to grow a very large tree, knowing that it will be overly complex, but then prune it back to reduce the size. This is known as post-pruning.In post-pruning, nodes and branches with only a minor impact on the tree’s overall accuracy are removed after the fact. This figure illustrates a tree that grew to four levels deep, but had a branch pruned away because its presence did not substantially improve the classification accuracy.The relationship between the tree’s complexity and the accuracy can be depicted visually as illustrated here. As the tree becomes increasingly complex, the model makes fewer errors. However, though the performance improves a lot at first, it then improves only slightly for the later increases in complexity. This trend provides insight into the optimal point at which to prune the tree; simply look for the point at which the curve flattens. The horizontal dotted line identifies the point at which the error rate becomes statistically similar to the most complex model. Typically, you should prune the tree at the complexity level that results in a classification error rate just under this line.</p>
<p><strong>4. Pre- and post-pruning with R</strong></p>
<p>The rpart decision tree package provides a function for creating this visualization, as well as performing pre- and post-pruning.Pre-pruning is performed when building the decision tree model. The rpart-dot-control function can be supplied with a maxdepth parameter that controls the maximum depth of the decision tree, or a minsplit parameter that dictates the minimum number of observations a branch must contain in order for the tree to be allowed to split. Then, simply supply the resulting control object to the rpart function when building the tree.Post-pruning is applied to a decision tree model that has been previously built. The plotcp function will generate a visualization of the error rate versus model complexity, which provides insight into the optimal cutpoint for pruning. When this value has been identified, it can be supplied to the prune function’s complexity parameter, cp, to create a simpler pruned tree.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>In the next several exercises, you will have a chance to apply both pre- and post-pruning methods to the Lending Club data to examine the impact on the tree complexity and test set accuracy. Let’s see what happens!</p>
</section><section id="preventing-overgrown-trees" class="level2" data-number="4.14"><h2 data-number="4.14" class="anchored" data-anchor-id="preventing-overgrown-trees">
<span class="header-section-number">4.14</span> Preventing overgrown trees</h2>
<p>The tree grown on the full set of applicant data grew to be extremely large and extremely complex, with hundreds of splits and leaf nodes containing only a handful of applicants. This tree would be almost impossible for a loan officer to interpret.</p>
<p>Using the <strong>pre-pruning</strong> methods for early stopping, you can prevent a tree from growing too large and complex. See how the <code>rpart</code> control options for maximum tree depth and minimum split count impact the resulting tree.</p>
<p><code>rpart</code> is loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>
<p>Use <code>rpart()</code> to build a loan model using the training dataset and all of the available predictors. the model <code>control</code>s using <code>rpart.control()</code> with parameters <code>cp</code> set to <code>0</code> and <code>maxdepth</code> set to <code>6</code>.</p>
<ul>
<li><p>Set the model <code>control</code>s using <code>rpart.control()</code> with parameters <code>cp</code> set to <code>0</code> and <code>maxdepth</code> set to <code>6</code>. 2.. See how the test set accuracy of the simpler model compares to the original accuracy of 58.3%.create a vector of predictions using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function.the predictions to the actual outcomes and use <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> to calculate the accuracy.</p></li>
<li><p>First create a vector of predictions using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function.</p></li>
<li><p>Compare the predictions to the actual outcomes and use <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> to calculate the accuracy.</p></li>
</ul>
</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1"></a><span class="co"># Grow a tree with maxdepth of 6</span></span>
<span id="cb132-2"><a href="#cb132-2"></a>loan_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(outcome <span class="sc">~</span> ., <span class="at">data =</span> loans_train, <span class="at">method =</span> <span class="st">"class"</span>, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>, <span class="at">maxdepth =</span> <span class="dv">6</span>))</span>
<span id="cb132-3"><a href="#cb132-3"></a></span>
<span id="cb132-4"><a href="#cb132-4"></a><span class="co"># Make a class prediction on the test set</span></span>
<span id="cb132-5"><a href="#cb132-5"></a>loans_test<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(loan_model, loans_test, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb132-6"><a href="#cb132-6"></a></span>
<span id="cb132-7"><a href="#cb132-7"></a><span class="co"># Compute the accuracy of the simpler tree</span></span>
<span id="cb132-8"><a href="#cb132-8"></a><span class="fu">mean</span>(loans_test<span class="sc">$</span>pred <span class="sc">==</span> loans_test<span class="sc">$</span>outcome)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.5739038</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>In the model controls, remove <code>maxdepth</code> and add a minimum split parameter, <code>minsplit</code>, set to <code>500</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1"></a><span class="co"># Swap maxdepth for a minimum split of 500 </span></span>
<span id="cb134-2"><a href="#cb134-2"></a>loan_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(outcome <span class="sc">~</span> ., <span class="at">data =</span> loans_train, <span class="at">method =</span> <span class="st">"class"</span>, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>, <span class="at">minsplit =</span> <span class="dv">500</span>))</span>
<span id="cb134-3"><a href="#cb134-3"></a></span>
<span id="cb134-4"><a href="#cb134-4"></a><span class="co"># Run this. How does the accuracy change?</span></span>
<span id="cb134-5"><a href="#cb134-5"></a>loans_test<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(loan_model, loans_test, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb134-6"><a href="#cb134-6"></a><span class="fu">mean</span>(loans_test<span class="sc">$</span>pred <span class="sc">==</span> loans_test<span class="sc">$</span>outcome)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.5958274</code></pre>
</div>
</div>
<p>Nice work! It may seem surprising, but creating a simpler decision tree may actually result in greater performance on the test dataset.</p>
</section><section id="creating-a-nicely-pruned-tree" class="level2" data-number="4.15"><h2 data-number="4.15" class="anchored" data-anchor-id="creating-a-nicely-pruned-tree">
<span class="header-section-number">4.15</span> Creating a nicely pruned tree</h2>
<p>Stopping a tree from growing all the way can lead it to ignore some aspects of the data or miss important trends it may have discovered later.</p>
<p>By using <strong>post-pruning</strong>, you can intentionally grow a large and complex tree then prune it to be smaller and more efficient later on.</p>
<p>In this exercise, you will have the opportunity to construct a visualization of the tree’s performance versus complexity, and use this information to prune the tree to an appropriate level.</p>
<p>The <code>rpart</code> package is loaded into the workspace, along with <code>loans_test</code> and <code>loans_train</code>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use all of the applicant variables and no pre-pruning to create an overly complex tree. Make sure to set <code>cp = 0</code> in <code>rpart.control()</code> to prevent pre-pruning.</li>
<li>Create a complexity plot by using <code>plotcp()</code> on the model.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1"></a><span class="co"># Grow an overly complex tree</span></span>
<span id="cb136-2"><a href="#cb136-2"></a>loan_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(outcome <span class="sc">~</span> ., <span class="at">data =</span> loans_train, <span class="at">method =</span> <span class="st">"class"</span>, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>))</span>
<span id="cb136-3"><a href="#cb136-3"></a></span>
<span id="cb136-4"><a href="#cb136-4"></a><span class="co"># Examine the complexity plot</span></span>
<span id="cb136-5"><a href="#cb136-5"></a><span class="fu">plotcp</span>(loan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_supervised_learning_classification_files/figure-html/unnamed-chunk-47-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="01_supervised_learning_classification_files/figure-html/unnamed-chunk-47-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Based on the complexity plot, prune the tree to a complexity of 0.0014 using the <code>prune()</code> function with the tree and the complexity parameter.</li>
<li>Compare the accuracy of the pruned tree to the original accuracy of 58.3%. To calculate the accuracy use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> and <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> functions.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1"></a><span class="co"># Prune the tree</span></span>
<span id="cb137-2"><a href="#cb137-2"></a>loan_model_pruned <span class="ot">&lt;-</span> <span class="fu">prune</span>(loan_model, <span class="at">cp =</span> <span class="fl">0.0014</span>)</span>
<span id="cb137-3"><a href="#cb137-3"></a></span>
<span id="cb137-4"><a href="#cb137-4"></a><span class="co"># Compute the accuracy of the pruned tree</span></span>
<span id="cb137-5"><a href="#cb137-5"></a>loans_test<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(loan_model_pruned, loans_test, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb137-6"><a href="#cb137-6"></a><span class="fu">mean</span>(loans_test<span class="sc">$</span>pred <span class="sc">==</span> loans_test<span class="sc">$</span>outcome)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.5873409</code></pre>
</div>
</div>
<p>Great job! As with pre-pruning, creating a simpler tree actually improved the performance of the tree on the test dataset.</p>
</section><section id="why-do-trees-benefit-from-pruning" class="level2" data-number="4.16"><h2 data-number="4.16" class="anchored" data-anchor-id="why-do-trees-benefit-from-pruning">
<span class="header-section-number">4.16</span> Why do trees benefit from pruning?</h2>
<p>Classification trees can grow indefinitely, until they are told to stop or run out of data to divide-and-conquer.</p>
<p>Just like trees in nature, classification trees that grow overly large can require pruning to reduce the excess growth. However, this generally results in a tree that classifies fewer training examples correctly.</p>
<blockquote class="blockquote">
<h2 id="question-15" data-number="4.17" class="anchored">
<span class="header-section-number">4.17</span> <em>Question</em>
</h2>
<p>Why, then, are pre-pruning and post-pruning almost always used?<br><br> ⬜ Simpler trees are easier to interpret<br> ⬜ Simpler trees using early stopping are faster to train<br> ⬜ Simpler trees may perform better on the testing data<br> ✅ All of the above<br></p>
</blockquote>
<p>Yes! There are many benefits to creating carefully pruned decision trees!</p>
</section><section id="seeing-the-forest-from-the-trees" class="level2" data-number="4.18"><h2 data-number="4.18" class="anchored" data-anchor-id="seeing-the-forest-from-the-trees">
<span class="header-section-number">4.18</span> Seeing the forest from the trees</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Seeing the forest from the trees</strong></p>
<p>Consider the ways that decision trees parallel trees in the natural environment: from a root node that grows into branches and leaf nodes that sometimes need pruning, you might think that by now we would have exhausted the tree metaphors.In fact, there is one more.Just as living trees can be grouped as a forest, a number of classification trees can be combined into a collection known as a decision tree forest. For reasons that you will soon see, these forests are among the most powerful machine learning classifiers, yet remain remarkably efficient and easy to use.</p>
<p><strong>2. Understanding random forests</strong></p>
<p>Because of their combine versatility and power, decision tree forests have become one of the most popular approaches for classification.This power does not come from a single tree that has grown large and complex, but rather from a collection of smaller, simpler trees that together reflect the data’s complexity. Each of the forest’s trees is diverse, and may reflect some subtle pattern in the outcome to be modeled.Generating this diversity is the key to building powerful decision tree forests. However, if you were to grow 100 trees on the same set of data, you’d have 100 times the same tree. Growing diverse trees requires the growing conditions to be varied from tree to tree.This is done by allocating each tree a random subset of data; one may receive a vastly different training set than another. The term random forests refers to a specific growing algorithm in which both the features and examples may differ from tree to tree.</p>
<p><strong>3. Making decisions as an ensemble</strong></p>
<p>It seems somewhat counterintuitive to think that a group of trees built on small, random subsets of the data could perform any better than a single really complex tree that had the benefit of learning the entire dataset.But the forest’s power is based on the same principles that govern successful team work in business or on the athletic field. In these cases, it is certainly advantageous to have team members that are extremely good at some tasks. However, these people typically have weaknesses in other areas. For this reason, it is even better for the team to have members with complementary skills. Even if none of the members is especially strong, good teamwork usually wins.Machine learning methods like random forests that apply this principle are called ensemble methods. All ensemble methods are based on the principle that weaker learners become stronger with teamwork. In a random forest, each tree is asked to make a prediction, and the group’s overall prediction is determined by a majority vote. Though each tree may reflect only a narrow portion of the data, the overall consensus is strengthened by these diverse perspectives.</p>
<p><strong>4. Random forests in R</strong></p>
<p>The R package randomForest implements the random forest algorithm. The function offers two parameters of note.The first, ntree, dictates the number of trees to include in the forest. Setting this sufficiently large will ensure good representation of the complete set of data. Don’t worry even with a large number of trees, the model typically runs relatively quickly as it uses only a portion of the full dataset.The second parameter, mtry, is the number of features selected at random for each tree. By default, it uses the square root of the total number of predictors. Generally, this parameter is OK to leave as is.As usual, the predict function uses the model to make predictions.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>By now I’m sure you’re excited to see a random forest in action. In the next exercises you’ll have a chance to grow one and compare its performance to the best decision tree.</p>
</section><section id="understanding-random-forests" class="level2" data-number="4.19"><h2 data-number="4.19" class="anchored" data-anchor-id="understanding-random-forests">
<span class="header-section-number">4.19</span> Understanding random forests</h2>
<p>Groups of classification trees can be combined into an <strong>ensemble</strong> that generates a single prediction by allowing the trees to “vote” on the outcome.</p>
<blockquote class="blockquote">
<h2 id="question-16" data-number="4.20" class="anchored">
<span class="header-section-number">4.20</span> <em>Question</em>
</h2>
<p>Why might someone think that this could result in more accurate predictions than a single tree?<br><br> ⬜ Each tree in the forest is larger and more complex than a typical single tree.<br> ⬜ Every tree in a random forest uses the complete set of predictors.<br> ✅ [The diversity among the trees may lead it to discover more subtle patterns.]<br> ⬜ The random forest is not affected by noisy data.<br></p>
</blockquote>
<p>Yes! The teamwork-based approach of the random forest may help it find important trends a single tree may miss.</p>
</section><section id="building-a-random-forest-model" class="level2" data-number="4.21"><h2 data-number="4.21" class="anchored" data-anchor-id="building-a-random-forest-model">
<span class="header-section-number">4.21</span> Building a random forest model</h2>
<p>In spite of the fact that a forest can contain hundreds of trees, growing a decision tree forest is perhaps even easier than creating a single highly-tuned tree.</p>
<p>Using the <code>randomForest</code> package, build a random forest and see how it compares to the single trees you built previously.</p>
<p>Keep in mind that due to the random nature of the forest, the results may vary slightly each time you create the forest.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Load the <code>randomForest</code> package.</li>
<li>Build a random forest model using all of the loan application variables. The <code>randomForest</code> function also uses the formula interface.</li>
<li>Compute the accuracy of the random forest model to compare to the original tree’s accuracy of 58.3% using <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> and <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1"></a><span class="co"># Load the randomForest package</span></span>
<span id="cb139-2"><a href="#cb139-2"></a><span class="fu">library</span>(randomForest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; randomForest 4.7-1.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Type rfNews() to see new features/changes/bug fixes.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'randomForest'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Das folgende Objekt ist maskiert 'package:dplyr':
#&gt; 
#&gt;     combine</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Das folgende Objekt ist maskiert 'package:ggplot2':
#&gt; 
#&gt;     margin</code></pre>
</div>
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1"></a><span class="co"># Build a random forest model</span></span>
<span id="cb145-2"><a href="#cb145-2"></a>loan_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(outcome <span class="sc">~</span> ., <span class="at">data =</span> loans_train)</span>
<span id="cb145-3"><a href="#cb145-3"></a></span>
<span id="cb145-4"><a href="#cb145-4"></a><span class="co"># Compute the accuracy of the random forest</span></span>
<span id="cb145-5"><a href="#cb145-5"></a>loans_test<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(loan_model, loans_test)</span>
<span id="cb145-6"><a href="#cb145-6"></a><span class="fu">mean</span>(loans_test<span class="sc">$</span>pred <span class="sc">==</span> loans_test<span class="sc">$</span>outcome)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.5922914</code></pre>
</div>
</div>
<p>Wow! Great job! Now you’re really a classification pro! Classification is only one of the problems you’ll have to tackle as a data scientist. Check out some other machine learning courses to learn more about supervised and unsupervised learning.</p>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="pagination-link">
        <span class="nav-page-text">7.2: Supervised Learning: Regression</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">Content 2022 by <a href="https://www.startupengineer.io/authors/schwarz/">Joschka Schwarz</a> <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with and <a href="https://quarto.org/">Quarto</a><br><a href="https://www.github.com/jwarz/jwarz.github.io">View the source at GitHub</a></div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","selector":".lightbox","loop":true,"openEffect":"zoom","descPosition":"bottom"});</script>


<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>