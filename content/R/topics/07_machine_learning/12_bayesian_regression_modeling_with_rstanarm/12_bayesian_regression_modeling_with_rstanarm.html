<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joschka Schwarz">

<title>Joschka Schwarz - Bayesian Regression Modeling with rstanarm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" rel="next">
<link href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" rel="prev">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

<link href="../../../../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../../../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>


</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Joschka Schwarz</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-science" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-science">    
        <li>
    <a class="dropdown-item" href="../../../../../content/R/index.html">
 <span class="dropdown-text">R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/python/index.html">
 <span class="dropdown-text">Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/sql/index.html">
 <span class="dropdown-text">SQL</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../../slides/index.html">
 <span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../resumes/index.html">
 <span class="menu-text">Resumes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jwarz/"><i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/j-schwarz"><i class="bi bi-linkedin" role="img" aria-label="Quarto LinkedIn">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Bayesian Regression Modeling with rstanarm</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <b><i>Programming Basics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/01_programming_beginner/01_programming_beginner.html" class="sidebar-item-text sidebar-link">1: Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/02_programming_intermediate/02_programming_intermediate.html" class="sidebar-item-text sidebar-link">2: Intermediate R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/03_programming_tidyverse/03_programming_tidyverse.html" class="sidebar-item-text sidebar-link">3: Introduction to the tidyverse</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>Importing Data</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/01_importing_data_beginner/01_importing_data_beginner.html" class="sidebar-item-text sidebar-link">4: Introduction to Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/02_importing_data_intermediate/02_importing_data_intermediate.html" class="sidebar-item-text sidebar-link">5: Intermediate Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/03_working_with_web_data/03_working_with_web_data.html" class="sidebar-item-text sidebar-link">6: Working with web data</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>Data Wrangling</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/exploratory_data_analysis_in_r/exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">7: Exploratory Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/case_study_exploratory_data_analysis_in_r/case_study_exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">8: Case Study: EDA</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/cleaning_data_in_r/cleaning_data_in_r.html" class="sidebar-item-text sidebar-link">9: Cleaning Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/data_manipulation_with_datatable/data_manipulation_with_datatable.html" class="sidebar-item-text sidebar-link">10: Data Manipulation with data.table</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/joining_data_with_datatable/joining_data_with_datatable.html" class="sidebar-item-text sidebar-link">11: Joining Data with data.table</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>Data Visualization</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/data_visualization_with_ggplot2_intermediate/ggplot2_intermediate.html" class="sidebar-item-text sidebar-link">12: Intermediate Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>Statistics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/01_statistics_beginner/01_statistics_beginner.html" class="sidebar-item-text sidebar-link">13: Introduction to Statistics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/02_fundamentals_of_bayesian_data_analysis/02_fundamentals_of_bayesian_data_analysis.html" class="sidebar-item-text sidebar-link">14: Fundamentals of Bayesian Data Analysis</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>Regression</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/06_regression/01_regression_beginner/01_regression_beginner.html" class="sidebar-item-text sidebar-link">15: Introduction to Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/06_regression/02_regression_intermediate/02_regression_intermediate.html" class="sidebar-item-text sidebar-link">16: Intermediate Regression</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>Machine Learning</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/01_supervised_learning_classification/01_supervised_learning_classification.html" class="sidebar-item-text sidebar-link">17: Supervised Learning: Classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="sidebar-item-text sidebar-link">18: Supervised Learning: Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/03_unsupervised_learning/03_unsupervised_learning.html" class="sidebar-item-text sidebar-link">19: Unsupervised Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">20: Machine Learning in the tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/05_cluster_analysis/05_cluster_analysis.html" class="sidebar-item-text sidebar-link">21: Cluster Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/06_machine_learning_with_caret/06_machine_learning_with_caret.html" class="sidebar-item-text sidebar-link">22: Cluster Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/07_modeling_with_tidymodels/07_modeling_with_tidymodels.html" class="sidebar-item-text sidebar-link">23: Modeling with tidymodels</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/08_machine_learning_with_tree-based_models/08_machine_learning_with_tree-based_models.html" class="sidebar-item-text sidebar-link">24: Machine Learning with tree-based Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" class="sidebar-item-text sidebar-link">25: Support Vector Machines</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" class="sidebar-item-text sidebar-link">26: Topic Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="sidebar-item-text sidebar-link">27: Hyperparameter Tuning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" class="sidebar-item-text sidebar-link active">28: Bayesian Regression Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" class="sidebar-item-text sidebar-link">29: Introduction to Spark</a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">R Manuals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/test.html" class="sidebar-item-text sidebar-link">1: An Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/02-content.html" class="sidebar-item-text sidebar-link">2: R Data Import/Export</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/03-content.html" class="sidebar-item-text sidebar-link">3: R Installation and Administration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/04-content.html" class="sidebar-item-text sidebar-link">4: Writing R Extensions</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/05-content.html" class="sidebar-item-text sidebar-link">5: R Language Definition</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/06-content.html" class="sidebar-item-text sidebar-link">6: R Internals</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul class="collapse">
  <li><a href="#introduction-to-bayesian-linear-models" id="toc-introduction-to-bayesian-linear-models" class="nav-link active" data-scroll-target="#introduction-to-bayesian-linear-models"><span class="toc-section-number">1</span>  1. Introduction to Bayesian Linear Models</a>
  <ul class="collapse">
  <li><a href="#non-bayesian-linear-regression" id="toc-non-bayesian-linear-regression" class="nav-link" data-scroll-target="#non-bayesian-linear-regression"><span class="toc-section-number">1.1</span>  Non-Bayesian Linear Regression</a></li>
  <li><a href="#exploring-the-data" id="toc-exploring-the-data" class="nav-link" data-scroll-target="#exploring-the-data"><span class="toc-section-number">1.2</span>  Exploring the data</a></li>
  <li><a href="#fitting-a-frequentist-linear-regression" id="toc-fitting-a-frequentist-linear-regression" class="nav-link" data-scroll-target="#fitting-a-frequentist-linear-regression"><span class="toc-section-number">1.3</span>  Fitting a frequentist linear regression</a></li>
  <li><a href="#bayesian-linear-regression" id="toc-bayesian-linear-regression" class="nav-link" data-scroll-target="#bayesian-linear-regression"><span class="toc-section-number">1.4</span>  Bayesian Linear Regression</a></li>
  <li><a href="#fitting-a-bayesian-linear-regression" id="toc-fitting-a-bayesian-linear-regression" class="nav-link" data-scroll-target="#fitting-a-bayesian-linear-regression"><span class="toc-section-number">1.5</span>  Fitting a Bayesian linear regression</a></li>
  <li><a href="#convergence-criteria" id="toc-convergence-criteria" class="nav-link" data-scroll-target="#convergence-criteria"><span class="toc-section-number">1.6</span>  Convergence criteria</a></li>
  <li><a href="#assessing-model-convergence" id="toc-assessing-model-convergence" class="nav-link" data-scroll-target="#assessing-model-convergence"><span class="toc-section-number">1.8</span>  Assessing model convergence</a></li>
  <li><a href="#comparing-frequentist-and-bayesian-methods" id="toc-comparing-frequentist-and-bayesian-methods" class="nav-link" data-scroll-target="#comparing-frequentist-and-bayesian-methods"><span class="toc-section-number">1.10</span>  Comparing frequentist and Bayesian methods</a></li>
  <li><a href="#difference-between-frequentists-and-bayesians" id="toc-difference-between-frequentists-and-bayesians" class="nav-link" data-scroll-target="#difference-between-frequentists-and-bayesians"><span class="toc-section-number">1.11</span>  Difference between frequentists and Bayesians</a></li>
  <li><a href="#creating-credible-intervals" id="toc-creating-credible-intervals" class="nav-link" data-scroll-target="#creating-credible-intervals"><span class="toc-section-number">1.13</span>  Creating credible intervals</a></li>
  </ul></li>
  <li><a href="#modifying-a-bayesian-model" id="toc-modifying-a-bayesian-model" class="nav-link" data-scroll-target="#modifying-a-bayesian-model"><span class="toc-section-number">2</span>  2. Modifying a Bayesian Model</a>
  <ul class="collapse">
  <li><a href="#whats-in-a-bayesian-model" id="toc-whats-in-a-bayesian-model" class="nav-link" data-scroll-target="#whats-in-a-bayesian-model"><span class="toc-section-number">2.1</span>  What’s in a Bayesian Model?</a></li>
  <li><a href="#altering-chains" id="toc-altering-chains" class="nav-link" data-scroll-target="#altering-chains"><span class="toc-section-number">2.2</span>  Altering chains</a></li>
  <li><a href="#do-i-have-enough-iterations" id="toc-do-i-have-enough-iterations" class="nav-link" data-scroll-target="#do-i-have-enough-iterations"><span class="toc-section-number">2.3</span>  Do I have enough iterations?</a></li>
  <li><a href="#prior-distributions" id="toc-prior-distributions" class="nav-link" data-scroll-target="#prior-distributions"><span class="toc-section-number">2.5</span>  Prior distributions</a></li>
  <li><a href="#determine-prior-distributions" id="toc-determine-prior-distributions" class="nav-link" data-scroll-target="#determine-prior-distributions"><span class="toc-section-number">2.6</span>  Determine Prior Distributions</a></li>
  <li><a href="#calculate-adjusted-scales" id="toc-calculate-adjusted-scales" class="nav-link" data-scroll-target="#calculate-adjusted-scales"><span class="toc-section-number">2.7</span>  Calculate Adjusted Scales</a></li>
  <li><a href="#unadjusted-priors" id="toc-unadjusted-priors" class="nav-link" data-scroll-target="#unadjusted-priors"><span class="toc-section-number">2.8</span>  Unadjusted Priors</a></li>
  <li><a href="#user-specified-priors" id="toc-user-specified-priors" class="nav-link" data-scroll-target="#user-specified-priors"><span class="toc-section-number">2.9</span>  User Specified Priors</a></li>
  <li><a href="#changing-priors" id="toc-changing-priors" class="nav-link" data-scroll-target="#changing-priors"><span class="toc-section-number">2.10</span>  Changing Priors</a></li>
  <li><a href="#specifying-informative-priors" id="toc-specifying-informative-priors" class="nav-link" data-scroll-target="#specifying-informative-priors"><span class="toc-section-number">2.11</span>  Specifying informative priors</a></li>
  <li><a href="#consequences-of-informative-priors" id="toc-consequences-of-informative-priors" class="nav-link" data-scroll-target="#consequences-of-informative-priors"><span class="toc-section-number">2.12</span>  Consequences of informative priors</a></li>
  <li><a href="#altering-the-estimation-process" id="toc-altering-the-estimation-process" class="nav-link" data-scroll-target="#altering-the-estimation-process"><span class="toc-section-number">2.14</span>  Altering the estimation process</a></li>
  <li><a href="#altering-the-estimation" id="toc-altering-the-estimation" class="nav-link" data-scroll-target="#altering-the-estimation"><span class="toc-section-number">2.15</span>  Altering the Estimation</a></li>
  </ul></li>
  <li><a href="#assessing-model-fit" id="toc-assessing-model-fit" class="nav-link" data-scroll-target="#assessing-model-fit"><span class="toc-section-number">3</span>  3. Assessing Model Fit</a>
  <ul class="collapse">
  <li><a href="#using-the-r-squared-statistic" id="toc-using-the-r-squared-statistic" class="nav-link" data-scroll-target="#using-the-r-squared-statistic"><span class="toc-section-number">3.1</span>  Using the R Squared statistic</a></li>
  <li><a href="#calculating-frequentist-r-squared" id="toc-calculating-frequentist-r-squared" class="nav-link" data-scroll-target="#calculating-frequentist-r-squared"><span class="toc-section-number">3.2</span>  Calculating Frequentist R-squared</a></li>
  <li><a href="#r-squared-for-a-bayesian-model" id="toc-r-squared-for-a-bayesian-model" class="nav-link" data-scroll-target="#r-squared-for-a-bayesian-model"><span class="toc-section-number">3.3</span>  R-squared for a Bayesian Model</a></li>
  <li><a href="#posterior-predictive-model-checks" id="toc-posterior-predictive-model-checks" class="nav-link" data-scroll-target="#posterior-predictive-model-checks"><span class="toc-section-number">3.4</span>  Posterior predictive model checks</a></li>
  <li><a href="#predicted-score-distributions" id="toc-predicted-score-distributions" class="nav-link" data-scroll-target="#predicted-score-distributions"><span class="toc-section-number">3.5</span>  Predicted score distributions</a></li>
  <li><a href="#distributions-for-a-single-observation" id="toc-distributions-for-a-single-observation" class="nav-link" data-scroll-target="#distributions-for-a-single-observation"><span class="toc-section-number">3.6</span>  Distributions for a single observation</a></li>
  <li><a href="#model-fit-with-posterior-predictive-model-checks" id="toc-model-fit-with-posterior-predictive-model-checks" class="nav-link" data-scroll-target="#model-fit-with-posterior-predictive-model-checks"><span class="toc-section-number">3.8</span>  Model fit with posterior predictive model checks</a></li>
  <li><a href="#r-squared-posterior" id="toc-r-squared-posterior" class="nav-link" data-scroll-target="#r-squared-posterior"><span class="toc-section-number">3.9</span>  R-squared Posterior</a></li>
  <li><a href="#posterior-predictive-testing" id="toc-posterior-predictive-testing" class="nav-link" data-scroll-target="#posterior-predictive-testing"><span class="toc-section-number">3.10</span>  Posterior Predictive Testing</a></li>
  <li><a href="#bayesian-model-comparisons" id="toc-bayesian-model-comparisons" class="nav-link" data-scroll-target="#bayesian-model-comparisons"><span class="toc-section-number">3.11</span>  Bayesian model comparisons</a></li>
  <li><a href="#calculating-the-loo-estimate" id="toc-calculating-the-loo-estimate" class="nav-link" data-scroll-target="#calculating-the-loo-estimate"><span class="toc-section-number">3.12</span>  Calculating the LOO estimate</a></li>
  <li><a href="#comparing-models" id="toc-comparing-models" class="nav-link" data-scroll-target="#comparing-models"><span class="toc-section-number">3.13</span>  Comparing models</a></li>
  </ul></li>
  <li><a href="#presenting-and-using-a-bayesian-regression" id="toc-presenting-and-using-a-bayesian-regression" class="nav-link" data-scroll-target="#presenting-and-using-a-bayesian-regression"><span class="toc-section-number">4</span>  4. Presenting and Using a Bayesian Regression</a>
  <ul class="collapse">
  <li><a href="#visualizing-a-bayesian-model" id="toc-visualizing-a-bayesian-model" class="nav-link" data-scroll-target="#visualizing-a-bayesian-model"><span class="toc-section-number">4.1</span>  Visualizing a Bayesian model</a></li>
  <li><a href="#plotting-a-bayesian-model" id="toc-plotting-a-bayesian-model" class="nav-link" data-scroll-target="#plotting-a-bayesian-model"><span class="toc-section-number">4.2</span>  Plotting a Bayesian model</a></li>
  <li><a href="#plotting-model-uncertainty" id="toc-plotting-model-uncertainty" class="nav-link" data-scroll-target="#plotting-model-uncertainty"><span class="toc-section-number">4.3</span>  Plotting Model Uncertainty</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions"><span class="toc-section-number">4.4</span>  Making predictions</a></li>
  <li><a href="#popularity-for-observed-songs" id="toc-popularity-for-observed-songs" class="nav-link" data-scroll-target="#popularity-for-observed-songs"><span class="toc-section-number">4.5</span>  Popularity for Observed Songs</a></li>
  <li><a href="#popularity-for-new-songs" id="toc-popularity-for-new-songs" class="nav-link" data-scroll-target="#popularity-for-new-songs"><span class="toc-section-number">4.6</span>  Popularity for New Songs</a></li>
  <li><a href="#visualizing-predictions" id="toc-visualizing-predictions" class="nav-link" data-scroll-target="#visualizing-predictions"><span class="toc-section-number">4.7</span>  Visualizing predictions</a></li>
  <li><a href="#format-prediction-posteriors" id="toc-format-prediction-posteriors" class="nav-link" data-scroll-target="#format-prediction-posteriors"><span class="toc-section-number">4.8</span>  Format prediction posteriors</a></li>
  <li><a href="#visualize-new-predictions" id="toc-visualize-new-predictions" class="nav-link" data-scroll-target="#visualize-new-predictions"><span class="toc-section-number">4.9</span>  Visualize New Predictions</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">4.10</span>  Conclusion</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jwarz/jwarz.github.io/edit/main/content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jwarz/jwarz.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Bayesian Regression Modeling with rstanarm</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joschka Schwarz </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p><strong>Short Description</strong></p>
<p>Learn how to leverage Bayesian estimation methods to make better inferences about linear regression models.</p>
<p><strong>Long Description</strong></p>
<p>Bayesian estimation offers a flexible alternative to modeling techniques where the inferences depend on p-values. In this course, you’ll learn how to estimate linear regression models using Bayesian methods and the rstanarm package. You’ll be introduced to prior distributions, posterior predictive model checking, and model comparisons within the Bayesian framework. You’ll also learn how to use your estimated model to make predictions for new data.</p>
<section id="introduction-to-bayesian-linear-models" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> 1. Introduction to Bayesian Linear Models</h1>
<p>A review of frequentist regression using lm(), an introduction to Bayesian regression using stan_glm(), and a comparison of the respective outputs.</p>
<section id="non-bayesian-linear-regression" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="non-bayesian-linear-regression"><span class="header-section-number">1.1</span> Non-Bayesian Linear Regression</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Welcome!</strong></p>
<p>Hello! My name is Jake Thompson. I’m a psychometrician at the University of Kansas, and I’ll be your instructor in this course. For this course I’m assuming that you are already familiar with linear regression and know the basics of Bayesian analysis.</p>
<p><strong>2. Overview</strong></p>
<p>Throughout this course we’ll learn how to estimate a Bayesian model, customize a model, evaluate a model and its predictive power, and finally how to present and use a Bayesian regression model.</p>
<p><strong>3. A review of frequentist regression</strong></p>
<p>Before we get into the Bayesian methods, we’ll first review linear regression using non-Bayesian, or frequentist, methods. This will provide helpful comparisons between the inferences we can make when using frequentist and Bayesian methods. For examples, I’ll be using the kidiq data from the rstanarm package, a package for Bayesian applied regression modeling which you’ll be introduced to in the next video and use throughout this course. This dataset includes scores of kids on an IQ test, along with the mother’s IQ, age, and whether or not she finished high school.</p>
<p><strong>4. A review of frequentist regression</strong></p>
<p>We can estimate a frequentist linear regression by using the lm function. For example, we can predict a child’s score from the mother’s IQ. We can then look at a summary of the model. This output should look familiar. We have information about the model’s residuals, coefficient estimates, and information about how our model is performing.</p>
<p><strong>5. Examing model coefficients</strong></p>
<p>If we only want information about the coefficients, we can use the tidy function from the broom package. This shows the estimate, standard error, test statistic and p-value for each coefficient in the model. Using a p-value cutoff of 0.05, we see that the mom’s IQ is a significant predictor of the child’s score on the IQ test.However, recall what the p-value really tells us. This only tells us the probability of observing data that give rise to a test statistic this large if the true value of the parameter were zero. This is the key problem with frequentist regression.</p>
<p><strong>6. Comparing Frequentist and Bayesian probabilities</strong></p>
<p>To illustrate, let’s calculate the probability of a woman having cancer, given a positive mammogram. We know that if a woman has cancer, they will have a positive mammogram 90% of the time. This is like the p-value, the probability of our data, given a null hypothesis. We also know that in the United States, 0.4% of women have breast cancer. This what we will later call our prior, or our belief about the parameter before looking at the data. From this we can calculate that the probability of a random woman getting a positive mammogram is 10%. So, given a positive mammogram, what are the chances that the woman has cancer? Only 3.6%! This is very different from the 90%, and illustrates the importance of making inferences about the parameter we are interested in (the probability of cancer), rather than the data (the probability of a positive mammogram).In this course, we’ll apply these Bayesian methods to regression to make better inferences about model parameters.</p>
<p><strong>7. Spotify data</strong></p>
<p>For the exercises throughout this course, we’ll be using data on Adele, Beyoncé, and Taylor Swift songs from the Spotify API. This data includes the name and artist of each song, the age of the song in days, the valence, or how positive or negative the song sounds, the tempo, or speed, of the song, the popularity according to Spotify, and the length of the song. Throughout this course, we’ll predict the popularity from the other variables.</p>
<p><strong>8. Let’s practice!</strong></p>
<p>Let’s start with a frequentist regression.</p>
</section>
<section id="exploring-the-data" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="exploring-the-data"><span class="header-section-number">1.2</span> Exploring the data</h2>
<p>Let’s get familiar with the Spotify data, <code>songs</code>, which is already loaded for you. Before we start developing models, it’s a good idea to take a peek at our data to make sure we know everything that is included.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Print the first 6 rows of the dataset.</li>
<li>Print the structure of the dataset.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load data</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(readr)</span>
<span id="cb1-3"><a href="#cb1-3"></a>songs <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/datacamp-spotify-data.csv"</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Print the first 6 rows</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="fu">head</span>(songs)</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># Print the structure</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="fu">str</span>(songs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great! The data contains information on the name and artist of each song, along with quantitative information related to the tempo, valence, popularity, age, and length. Now let’s use the data to estimate a regression model.</p>
</section>
<section id="fitting-a-frequentist-linear-regression" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="fitting-a-frequentist-linear-regression"><span class="header-section-number">1.3</span> Fitting a frequentist linear regression</h2>
<p>Practice creating a linear model using data on songs from Spotify. This will give us base line to compare our Bayesian model to. The <code>songs</code> dataset is already loaded for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a linear model, <code>lm_model</code>, that predicts song popularity from song age.</li>
<li>Print a summary of the linear model.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Create the model here</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>lm_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs)</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co"># Produce the summary</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="fu">summary</span>(lm_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Use the <strong>broom</strong> package to view only the coefficients.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="co"># Print a tidy summary of the coefficients</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="fu">tidy</span>(lm_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nice work! You’ve created a frequentist linear model and learned how to examine the output. In this model, the coefficient for <code>song_age</code> is -0.00585, which means we’d expected the popularity of a song to decrease by -0.00585 for each additional day.</p>
</section>
<section id="bayesian-linear-regression" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="bayesian-linear-regression"><span class="header-section-number">1.4</span> Bayesian Linear Regression</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Bayesian Linear Regression</strong></p>
<p>Now that we’ve reviewed the characteristics of a frequentist regression, let’s examine how to estimate that same regression using Bayesian methods.</p>
<p><strong>2. Why use Bayesian methods?</strong></p>
<p>As we learned in the last lesson, if we want to make inferences about the actual values of parameters, p-values and frequentist regression fails us. Bayesian estimation is one solution to this problem. With Bayesian methods the likelihood that used for frequentist regression, and what’s known as a prior, form a posterior distribution. The details of this process are beyond the scope of this course. The key point is that Bayesian methods sample from this posterior distribution, and we can then create summaries of the distributions to make parameter inferences. Using the summaries allows us to make inferences about what values parameters might take.</p>
<p><strong>3. The rstanarm package</strong></p>
<p>To estimate Bayesian regression models in this course, we’ll be using the rstanarm package. rstanarm is an interface to Stan, which is a programming language for Bayesian inference. The rstanarm package offers a high level interface with pre-written Stan scripts for common models, like linear regression.</p>
<p><strong>4. Using rstanarm</strong></p>
<p>We can load rstanarm using the normal library command that we use to load all packages. We can then estimate a linear regression using the stan_glm function. Here, we estimate the same model that we estimated using the lm function. Normally, when using the stan_glm function, we would see a great deal of output that looks like this. This output mainly provides progress updates. However, the models we’ll estimate in this course all estimate very quickly. Therefore, this output has been suppressed in the rest of the course.</p>
<p><strong>5. Examining an rstanarm model</strong></p>
<p>Just like a regression estimated with lm, we can look at a summary of a regression estimated with stan_glm. Using the summary function, we are presented with some information about the estimated model, the parameter estimates, and some model diagnostics.</p>
<p><strong>6. rstanarm summary: Estimates</strong></p>
<p>Notice the the parameter estimates no longer have test statistics and p-values as in the frequentist regression. This is because Bayesian estimation samples from the posterior distribution. This means that instead of a point estimate and a test statistic, we get a distribution of plausible values for the parameters, and the estimates section summarizes those distributions. Specifically we get the mean, standard deviation, and commonly used percentiles.We also see that there are parameters in the estimates section other than the (Intercept) and mom_iq coefficient that we entered into the model. Sigma represents the standard deviation of errors, mean_ppd is the mean of the posterior predictive distribution our our outcome variable, kid_iq. We’ll learn more about predictive distributions in Chapter 3. Finally, log-posterior is analogous to the likelihood of a frequentist regression. This represents the log of the combined posterior distributions. This will be used for model comparisons, which we’ll also explore in Chapter 3.</p>
<p><strong>7. rstanarm summary: Diagnostics</strong></p>
<p>In the diagnostics section, the most important statistic to pay attention to is the R-hat. Unlike in frequentist regression where there is always a solution using ordinary least squares, in Bayesian models we have to check to make sure the model converged. If a model is converged, then the parameter estimates are stable. Otherwise, our results will be unreliable. In Bayesian estimation, posterior distributions are sampled in groups, known as chains. By comparing the variance within chains to the variance across chains, we can measure the stability of our estimates. This is the R-hat statistic. In general, we want all R-hat values to be less than 1.1 in order to conclude the model has converged, as in this example.</p>
<p><strong>8. Let’s practice!</strong></p>
<p>Now you try estimating a Bayesian regression model.</p>
</section>
<section id="fitting-a-bayesian-linear-regression" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="fitting-a-bayesian-linear-regression"><span class="header-section-number">1.5</span> Fitting a Bayesian linear regression</h2>
<p>Practice fitting a Bayesian model. This is the same model we already estimated with frequentist methods, so we’ll be able to compare the parameter outputs later. The <code>songs</code> data is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a Bayesian linear model, <code>stan_model</code>, that predicts song popularity from song age</li>
<li>Print a summary of the Bayesian linear model</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Load package</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="fu">library</span>(rstanarm)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co"># Create the model here</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>stan_model <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs)</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co"># Produce the summary</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="fu">summary</span>(stan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#   The supplied model object seems to be outputted from the rstanarm package. Tidiers for mixed model output now live in the broom.mixed package.</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># Load package</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="fu">library</span>(broom.mixed)</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># Print a tidy summary of the coefficients</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="fu">tidy</span>(stan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Yes! You now know how to estimate a Bayesian regression model! Notice that the parameter estimates are very similar to those of the frequentist model. Bayesian estimation won’t usually have a large impact on your estimates, but will greatly influence the types of estimates you are able to make.</p>
</section>
<section id="convergence-criteria" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="convergence-criteria"><span class="header-section-number">1.6</span> Convergence criteria</h2>
<blockquote class="blockquote">
<h2 id="question" data-number="1.7" class="anchored"><span class="header-section-number">1.7</span> <em>Question</em></h2>
<p>What is the accepted threshold for Rhat to conclude the model has converged?<br> <br> ⬜ 1.0<br> ✅ 1.1<br> ⬜ 1.5<br> ⬜ 2<br></p>
</blockquote>
<p>That’s right! If all Rhat values are below 1.1, your model converged!</p>
</section>
<section id="assessing-model-convergence" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="assessing-model-convergence"><span class="header-section-number">1.8</span> Assessing model convergence</h2>
<p>Has the Bayesian regression model <code>stan_model</code> converged?</p>
<blockquote class="blockquote">
<h2 id="question-1" data-number="1.9" class="anchored"><span class="header-section-number">1.9</span> <em>Question</em></h2>
<p>???<br> <br> ✅ Yes!<br> ⬜ No way!<br> ⬜ We don’t have enough information.<br></p>
</blockquote>
<p>Correct! All of the Rhat values are less than 1.1.</p>
</section>
<section id="comparing-frequentist-and-bayesian-methods" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="comparing-frequentist-and-bayesian-methods"><span class="header-section-number">1.10</span> Comparing frequentist and Bayesian methods</h2>
<p>Theory. Coming soon …</p>
</section>
<section id="difference-between-frequentists-and-bayesians" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="difference-between-frequentists-and-bayesians"><span class="header-section-number">1.11</span> Difference between frequentists and Bayesians</h2>
<blockquote class="blockquote">
<h2 id="question-2" data-number="1.12" class="anchored"><span class="header-section-number">1.12</span> <em>Question</em></h2>
<p>What is the core difference between frequentists and Bayesians?<br> <br> ✅ Frequentists believe data is random, Bayesians assume parameters are random<br> ⬜ Frequentists believe data is fixed, Bayesians assume parameters are fixed<br> ⬜ There is no difference, just a matter of preference<br> ⬜ Bayesian estimation requires special data<br></p>
</blockquote>
<p>Yes! Frequentists assume data is random and parameters are fixed, whereas Bayesian assume the opposite.</p>
</section>
<section id="creating-credible-intervals" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="creating-credible-intervals"><span class="header-section-number">1.13</span> Creating credible intervals</h2>
<p>Practice creating credible intervals. Credible intervals allow us to make inferences about the probability of a parameter taking a given value. This is how we determine if a parameter is meaningful when estimated with Bayesian methods. The Bayesian model, <code>stan_model</code>, is already created for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create 90% credible intervals for the parameters in <code>stan_model</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Create the 90% credible intervals</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">posterior_interval</span>(stan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Create 95% credible intervals for the parameters in <code>stan_model</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Create the 95% credible intervals</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">posterior_interval</span>(stan_model, <span class="at">prob =</span> <span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Create 80% credible intervals for the parameters in <code>stan_model</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Create the 80% credible intervals</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">posterior_interval</span>(stan_model, <span class="at">prob =</span> <span class="fl">0.80</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great job! You’re well on your way to becoming a Bayesian! Here, we’ve learned how to create a credible interval for our parameters, and how to change how big of an interval we want. These intervals allow us to make inferences about the actual values of the parameters, unlike in frequentist regression.</p>
</section>
</section>
<section id="modifying-a-bayesian-model" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 2. Modifying a Bayesian Model</h1>
<p>Learn how to modify your Bayesian model including changing the number and length of chains, changing prior distributions, and adding predictors.</p>
<section id="whats-in-a-bayesian-model" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="whats-in-a-bayesian-model"><span class="header-section-number">2.1</span> What’s in a Bayesian Model?</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. What’s in a Bayesian Model?</strong></p>
<p>In the previous chapter we learned how to estimate a Bayesian linear regression. In this chapter we’re going learn how these models can be modified. Unlike in a frequentist regression, there are several ways we can modify the estimation process. This is important because these factors will ultimately impact your results. We’ll start by looking at the sampling of the posterior that creates the distribution summaries that we see in the output.</p>
<p><strong>2. Posterior distributions</strong></p>
<p>As we briefly talked about last chapter, the posterior distribution is sampled in groups, called chains. Each sample from within the chain is called an iteration. The chains begin at a random locations. As it samples, the chain moves toward the area where the combination of likelihood and prior indicates a high probability of the true parameter value residing. The more iterations in a chain, the larger the samples of the posterior distribution will be. This means that the summaries are directly impacted by the length of the chain, as larger samples allow for more robust estimates of those summary statistics.</p>
<p><strong>3. Sampling the posterior distribution</strong></p>
<p>Here is an example of this process. This is known as a trace plot. It shows the value of the parameter at each iteration for each chain. We can see that each chain starts in a different location, but they all converge on the same area. This is the convergence we talked about measuring with the R-hat parameter. Convergence is important because it ensures stable estimates. We can see that this model has not converged at the beginning, as the chains are in different places and not horizontal, meaning that the estimates are not stable.</p>
<p><strong>4. Sampling the Posterior Distribution</strong></p>
<p>Because the model has not converged at the beginning, we discard these iterations, leaving only the converged iterations to make up our final posterior distribution. Here, we can see that by only using the final 1,000 iterations, all of the chains are fully mixed. The iterations that are discarded are know as warm-up, or burn-in. By default, the rstanarm package estimates 4 chains. Each chain is 2,000 iterations long, and the first 1,000 are discarded for warm-up. For the exercises in this course, to cut down on estimation time, we’ve changed the default to 2 chains, each with 1,000 iterations, and the first 500 discarded for warm-up. In your own work, we recommend using the rstanarm defaults.</p>
<p><strong>5. Changing the number and length of chains</strong></p>
<p>We change this behavior in rstanarm by using the chains, iter, and warm-up arguments. Here we’ve specified that we want three chains, each with 1,000 iterations, and the first 500 should be discarded for warm-up. This means that our posterior distributions will be made of 1,500 total samples (500 from each chain).</p>
<p><strong>6. Changing the number and length of chains</strong></p>
<p>Indeed, when we look at the summary of the model, under Model Info, we see that the sample is 1500.</p>
<p><strong>7. Too short chains</strong></p>
<p>However, we have to be careful about making the number or length of the chains to short. Using our same example from earlier, if we had instead requested only 500 iterations, and discarded the first 250, our posterior distribution wouldn’t be converged, because the chains haven’t mixed.</p>
<p><strong>8. How many iterations?</strong></p>
<p>Because of this, the number of iterations is a balancing act. Fewer iterations means the model estimates faster, but too few iterations may keep the model from converging. The number of iterations needed is different for each model, so it’s important to pay attention to our R-hat values. If you estimate a model and it hasn’t converged, increasing the number of iterations and the length of the warm-up is a good place to start.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>Now let’s try some examples.</p>
</section>
<section id="altering-chains" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="altering-chains"><span class="header-section-number">2.2</span> Altering chains</h2>
<p>Let’s practice changing the number and length of chains so that we can get a posterior distribution of different sizes. By changing the size of the posterior, we can change the number of samples used for the posterior summaries, and impact the estimation time. The <code>songs</code> data is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>For all models, predict <code>popularity</code> from the <code>song_age</code>.</li>
<li>Estimate a model with 3 chains, each 1000 iterations long, with the first 500 discarded</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># 3 chains, 1000 iterations, 500 warmup</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>model_3chains <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb9-3"><a href="#cb9-3"></a>    <span class="at">chains =</span> <span class="dv">3</span>, <span class="at">iter =</span> <span class="dv">1000</span>, <span class="at">warmup =</span> <span class="dv">500</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="co"># Print a summary of model_3chains</span></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="fu">summary</span>(model_3chains)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Estimate a model with 2 chains, each 100 iterations long, discarding the first 50</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># 2 chains, 100 iterations, 50 warmup</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>model_2chains <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb10-3"><a href="#cb10-3"></a>    <span class="at">chains =</span> <span class="dv">2</span>, <span class="at">iter =</span> <span class="dv">100</span>, <span class="at">warmup =</span> <span class="dv">50</span>)</span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co"># Print a summary of model_2chains</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="fu">summary</span>(model_2chains)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Correct! Now you can alter the size of the sample of your poterior distribution! Be careful when making your chains shorter though. Notice the warning messages that we received for <code>model_2chains</code>. These are an indication that we didn’t draw enough samples from the posterior distribution to get good estimates of the parameters.</p>
</section>
<section id="do-i-have-enough-iterations" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="do-i-have-enough-iterations"><span class="header-section-number">2.3</span> Do I have enough iterations?</h2>
<p>The <code>model_2chains</code> object from the last exercise is load in the environment.</p>
<blockquote class="blockquote">
<h2 id="question-3" data-number="2.4" class="anchored"><span class="header-section-number">2.4</span> <em>Question</em></h2>
<p>Has the model converged?<br> <br> ⬜ Yeah!<br> ✅ Not a chance!<br> ⬜ We can’t tell.<br></p>
</blockquote>
<p>Correct! The Rhat values are above 1.1. (depends on the seed)</p>
</section>
<section id="prior-distributions" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="prior-distributions"><span class="header-section-number">2.5</span> Prior distributions</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Prior distributions</strong></p>
<p>Now that we’ve talked about how to control the size of our posterior distribution samples, we can talk about one of the components of the posterior distribution that we’ve only mentioned in passing so far: prior distributions.</p>
<p><strong>2. What’s a prior distribution?</strong></p>
<p>Prior distributions reflect our prior beliefs about the values of parameters. This information gets combined with the likelihood of the data to create the posterior distribution.</p>
<p><strong>3. Visualizing prior distributions</strong></p>
<p>Here is an example of how prior distributions can affect the resulting posterior distribution. The likelihood of the of the data, indicated by the dashed purple line, stays the same across all three panels. Notice how, as the teal line showing the priors gets more narrow or informative, the posterior distribution begins to shift closer to the prior, and away from the likelihood. In general, priors get more informative when their distributions are narrow, or we have less data. We can think about the prior like an additional data point. If we have a sample of five, a sixth data point can be really influential. If we have a sample of 5000, one extra data point won’t have much of an effect.Because priors have the potential to be influential, it is almost always a good idea to use non-informative or weakly informative priors, unless you have a good reason for believe that your parameters come from the informative distribution specified by the prior.</p>
<p><strong>4. Prior distributions in rstanarm</strong></p>
<p>We can view the prior distributions for each of our parameters in an rstanarm model by using the <code>prior_summary</code> function. By default the intercept gets a normally distributed prior with a mean of 0 an standard deviation of 10, and other coefficients get a normally distributed prior with a mean of 0 and standard deviation of 2.5. Auxiliary is the error standard deviation. This uses an exponentially distributed prior with a rate of 1. However, notice that there are also adjusted scales listed. This is because rstanarm recognizes that these defaults may not be appropriate for every dataset. Therefore they adjust the variance based on your data. For example, here, the standard deviation for the prior of the intercept was 204.11.</p>
<p><strong>5. Calculating adjusted scales</strong></p>
<p>The adjusted scale for the intercept is calculated as 10 times the standard deviation of your dependent variable. We use 10, because this is the default scale used by rstanarm for intercept. For predictors, the scale is calculated as 2.5 divided by the standard deviation of your predictor times the standard deviation of the dependent variable. Just like with the intercept, we use 2.5 because this is the default scale used for predictors. Look again at the priors for the intercept and predictor in the children’s IQ model. By taking 10 times the standard deviation of kid_score, our dependent variable, we get the adjusted scale of 204.11. Similarly, by taking 2.5 divided by the standard deviation of the mom’s IQ times the standard deviation of the the children’s IQ, we get that adjusted scale of 3.40.</p>
<p><strong>6. Unadjusted Priors</strong></p>
<p>rstanarm uses automatically adjusted the priors in order to ensure the the specified priors are not too informative. However, we can turn off this adjustment if we want to. To do this, we can specify autoscale as false for the intercept prior (which is prior_int), coefficients (which is just prior), the error (which is prior_aux) or any combination. After specifying autoscale equals false, we can see that there are no longer adjusted scales in the prior_summary output.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Now it’s your turn to explore priors in rstanarm.</p>
</section>
<section id="determine-prior-distributions" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="determine-prior-distributions"><span class="header-section-number">2.6</span> Determine Prior Distributions</h2>
<p>Now let’s explore the prior distributions for a Bayesian model, so that we can understand how <code>rstanarm</code> handles priors. Priors can have a large impact on our model, so it’s important to know which prior distributions were used in an estimated model. The <code>songs</code> dataset is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Estimate a model predicting <code>popularity</code> from <code>song_age</code></li>
<li>Print a summary of the prior distributions to the screen</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Estimate the model</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>stan_model <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># Print a summary of the prior distributions</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="fu">prior_summary</span>(stan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great! Now you know how to identify the prior distributions of your model! The intercept uses a normal prior with a mean of 0 and scale of 10. The coefficient for the predictor uses a normal prior with a mean of 0 and a scale of 2.5. Finally, the error variance uses an exponential priors with a rate of 1. However, notice that all priors also have an adjusted scale. In the next exercise we’ll examine how these are calculated.</p>
</section>
<section id="calculate-adjusted-scales" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="calculate-adjusted-scales"><span class="header-section-number">2.7</span> Calculate Adjusted Scales</h2>
<p>It’s important to understand how <code>rstanarm</code> calculates adjusted scales for prior distributions, as priors can have a large impact on our estimates if not used in an appropriate manner. Calculate what the adjusted scales should be using the already loaded <code>songs</code> data.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the adjusted scale of the intercept.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Calculate the adjusted scale for the intercept</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="dv">10</span> <span class="sc">*</span> <span class="fu">sd</span>(songs<span class="sc">$</span>popularity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Calculate the adjusted scale of the <code>song_age</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Calculate the adjusted scale for `song_age`</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>(<span class="fl">2.5</span> <span class="sc">/</span> <span class="fu">sd</span>(songs<span class="sc">$</span>song_age)) <span class="sc">*</span> <span class="fu">sd</span>(songs<span class="sc">$</span>popularity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>What would be the adjusted scale of <code>valence</code> if it were in the model?</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># Calculate the adjusted scale for `valence`</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>(<span class="fl">2.5</span> <span class="sc">/</span> <span class="fu">sd</span>(songs<span class="sc">$</span>valence)) <span class="sc">*</span> <span class="fu">sd</span>(songs<span class="sc">$</span>popularity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome! Great work calculating those adjusted scales! These scales are exactly the same as the adjusted scales that we saw in the previous exercise.</p>
</section>
<section id="unadjusted-priors" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="unadjusted-priors"><span class="header-section-number">2.8</span> Unadjusted Priors</h2>
<p>Now let’s specify a model that doesn’t use adjusted scales for prior distributions, so that we alter <code>rstanarm</code> default behavior. This will allow us to have more direct control over the information going into the estimation. The <code>songs</code> data is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Predict <code>popularity</code> from <code>song_age</code></li>
<li>Tell <code>rstanarm</code> not to autoscale the parameters</li>
<li>Print a prior summary to confirm there was no adjustment</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Estimate the model with unadjusted scales</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>no_scale <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb15-3"><a href="#cb15-3"></a>    <span class="at">prior_intercept =</span> <span class="fu">normal</span>(<span class="at">autoscale =</span> <span class="cn">FALSE</span>),</span>
<span id="cb15-4"><a href="#cb15-4"></a>    <span class="at">prior =</span> <span class="fu">normal</span>(<span class="at">autoscale =</span> <span class="cn">FALSE</span>),</span>
<span id="cb15-5"><a href="#cb15-5"></a>    <span class="at">prior_aux =</span> <span class="fu">exponential</span>(<span class="at">autoscale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb15-6"><a href="#cb15-6"></a>)</span>
<span id="cb15-7"><a href="#cb15-7"></a></span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="co"># Print the prior summary</span></span>
<span id="cb15-9"><a href="#cb15-9"></a><span class="fu">prior_summary</span>(no_scale)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Perfect! You’re well on your way to fully controlling the prior distributions. Notice that now that <code>autoscale = FALSE</code> has been specified, the prior summary no longer includes adjusted scales. Now that we’ve learned how to modify the default priors, we can move onto specifying priors that are entirely our own.</p>
</section>
<section id="user-specified-priors" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="user-specified-priors"><span class="header-section-number">2.9</span> User Specified Priors</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. User Specified Priors</strong></p>
<p>In the last lesson we talked about the prior distributions that rstanarm uses by default, and we can exert some control by deciding whether or not to adjust the scales of those distributions. But what if we want to use a completely different distribution? We can define a new prior distribution using the same arguments that we learned about in the last lesson.</p>
<p><strong>2. Why change the default prior?</strong></p>
<p>There are a couple of reasons we may want to specify our own prior distributions. First, there may be a lot of research suggesting the parameter should be around a certain value. In this scenario, we should take advantage of our knowledge and include this information through the prior distribution. Alternatively, we may have a parameter that we know is constrained in some way. For example, maybe a parameter has to be positive, like a variance. Here we could specify a prior distribution that is also always positive to ensure that the parameter is correctly constrained.</p>
<p><strong>3. Specify a prior</strong></p>
<p>We can be explicit about the prior distributions to be used by specifying location and scale values in the prior functions. For example, here we’ve specified the intercept prior should have a mean of zero and a standard deviation of 10.</p>
<p><strong>4. Specify a prior</strong></p>
<p>However, just like when these values are unspecified, rstanarm will adjust the prior scales using the same rules we learned about. So if you want to make sure you are using the exact distribution you specified, make sure that autoscale is set to FALSE.</p>
<p><strong>5. Specify a prior</strong></p>
<p>Using the prior arguments, we can specify different priors for different parameters. For example, here we’ve specified that the intercept should have a normally distributed prior with a mean of three and a standard deviation of two, and the predictors should have a Cauchy prior with a mean of zero and a standard deviation of 1.There are many different prior distributions that can be used. We’ve already seen the normal and exponential distributions, but there are t distributions and the Cauchy distribution as we’ve used here. We can see a full list of available distributions by looking at the priors help page. The process of how to choose a good prior distribution is beyond the scope of this course. However, a good practice is to choose a prior distribution that is consistent the expected distribution of your parameters. For example, we know from the central limit theorem that predictor coefficients are normally distributed. Therefore, it makes sense to use a normal prior for these parameters.</p>
<p><strong>6. Flat priors</strong></p>
<p>One situation we haven’t discussed is how to specify completely uninformative, or flat priors. What if we’re in a situation where we want the prior to provide no information? We can accomplish this by setting the priors to NULL. When we look at the prior summary, we can now see that a flat prior has been used. In practice this is not usually a good idea, because we are rarely in a situation where we have no prior information. In a linear regression for example, we know that the coefficients should be normally distributed. Even if we have no idea what the parameter values should be, it’s generally better practice to specify a weakly informative prior using an adjusted scale than to use a completely flat prior.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Now let’s change the priors in our Spotify model.</p>
</section>
<section id="changing-priors" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="changing-priors"><span class="header-section-number">2.10</span> Changing Priors</h2>
<p>Now let’s change the prior distributions for our Spotify model. Changing the priors allows us to specify our own beliefs about the expected values of the parameters. The <code>songs</code> dataset is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Predict <code>popularity</code> from <code>song_age</code></li>
<li>Create a model, <code>flat_prior</code> that uses flat priors for all parameters</li>
<li>Print a summary of the prior distributions to the screen</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Estimate a model with flat priors</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>flat_prior <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb16-3"><a href="#cb16-3"></a>    <span class="at">prior_intercept =</span> <span class="cn">NULL</span>, <span class="at">prior =</span> <span class="cn">NULL</span>, <span class="at">prior_aux =</span> <span class="cn">NULL</span>)</span>
<span id="cb16-4"><a href="#cb16-4"></a></span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="co"># Print a prior summary</span></span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="fu">prior_summary</span>(flat_prior)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome job! You’ve learned how to specify your own flat prior distributions! Flat priors provide no additional information to the model. This is often not the best choice, but specifying priors that provide too much information can also be problematic. We’ll explore this in the next exercise.</p>
</section>
<section id="specifying-informative-priors" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="specifying-informative-priors"><span class="header-section-number">2.11</span> Specifying informative priors</h2>
<p>Now let’s specify a custom prior so that we can have more control over our model. The <code>songs</code> dataset is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Predict <code>popularity</code> from <code>song_age</code></li>
<li>Specify a normal prior distribution for the predictor with a mean of 20 and standard deviation of 0.1</li>
<li>Print the prior summary to the screen</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># Estimate the model with an informative prior</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>inform_prior <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb17-3"><a href="#cb17-3"></a>    <span class="at">prior =</span> <span class="fu">normal</span>(<span class="at">location =</span> <span class="dv">20</span>, <span class="at">scale =</span> <span class="fl">0.1</span>, <span class="at">autoscale =</span> <span class="cn">FALSE</span>))</span>
<span id="cb17-4"><a href="#cb17-4"></a></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="co"># Print the prior summary</span></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="fu">prior_summary</span>(inform_prior)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great! Now you know how to specify a custom prior distribution! Notice in the prior summary that we’ve said the coefficient for <code>song_age</code> has a location of 20 with a very small variance. Therefore, we would expect the parameter estimate to also be very close to 20.</p>
</section>
<section id="consequences-of-informative-priors" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="consequences-of-informative-priors"><span class="header-section-number">2.12</span> Consequences of informative priors</h2>
<p>The <code>inform_prior</code> model you estimated is loaded in the environment. As a reminder, that model was specified with a normal prior on the predictor variable with a mean of 20 and standard deviation of 0.1.</p>
<blockquote class="blockquote">
<h2 id="question-4" data-number="2.13" class="anchored"><span class="header-section-number">2.13</span> <em>Question</em></h2>
<p>How did the specified prior affect the parameter estimates?<br> <br> ✅ The estimate was unaffected by the prior<br> ⬜ The estimate was slightly affected by the prior<br> ⬜ The estimate was almost the same as the prior<br></p>
</blockquote>
<p>Correct! The estimate is almost the same as the mean of the specified prior.</p>
</section>
<section id="altering-the-estimation-process" class="level2" data-number="2.14">
<h2 data-number="2.14" class="anchored" data-anchor-id="altering-the-estimation-process"><span class="header-section-number">2.14</span> Altering the estimation process</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Altering the estimation process</strong></p>
<p>So far we’ve talked about how to edit characteristics of the model such as the number and length of chains, and prior distributions. In addition to these characteristics, there are some internal options that affect how the sampling of the posterior distribution occurs. In most cases, the default options are sufficient. However, the need to alter these options comes up often enough that it’s worth discussing them. The technical details of these options are beyond the scope of this course, but we will talk about how they can be changed to resolve error message we sometimes get.</p>
<p><strong>2. Divergent transitions</strong></p>
<p>The first error message we’ll talk about concerns divergent transitions. This happens when the size betweens the steps of the estimator are too big. The Stan documentation describes this like walking down a steep hill. Taking too big of a step may make you fall. But you can take smaller steps and make it down safely, even if it takes longer. We can control the step size in Stan by sending a list to the <code>control</code> argument that specifies an adapt_delta. The adapt_delta argument can range from 0 to 1, and is set to 0-point-9-5 by default in rstanarm. By increasing the adapt_delta, we can decrease the step size, and in many cases resolve the divergent transitions error.</p>
<p><strong>3. Exceeding the Maximum Treedepth</strong></p>
<p>The second error message you may sometimes encounter is one saying that the maximum tree depth has been reached. In the sampling algorithm used by Stan and therefore rstanarm, the sampler looks for a place to “U-Turn” in a series of possible branches. Again, the specifics of this process are beyond the scope of this course. What’s important to know here is that if the sampler reachers the max tree depth before finding a good place to “U-Turn”, then the sample terminated the iteration before finding a good stopping place. Therefore the posterior distribution is not being sampled as efficiently as it should be.As with adapt_delta, we can control the maximum tree depth in Stan by sending a list to the control argument that specifies a max_treedepth. The max_treedepth is set to 10 by default in rstanarm. By increasing the max_treedepth, we allow the to look further for a good place to “U-Turn” .</p>
<p><strong>4. Tuning the estimation</strong></p>
<p>Divergent transitions and hitting the maximum tree depth are important issues to pay attention to during the estimation of a model. These errors may represent threats to the validity of your inferences about parameter values. Although these errors are uncommon for the models we’re estimating in this course, these are important concepts to understand and know how to address in practice. Luckily, rstanarm allows for these errors to be easily addressed, so that we can be sure that our parameter estimates are coming from a stable estimation.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>Now let’s practice tuning the estimation process.</p>
</section>
<section id="altering-the-estimation" class="level2" data-number="2.15">
<h2 data-number="2.15" class="anchored" data-anchor-id="altering-the-estimation"><span class="header-section-number">2.15</span> Altering the Estimation</h2>
<p>Now let’s alter the estimation options so that we can be prepared to resolve errors that may arise. It’s important for these errors to be resolved if they come up so that we can be sure we are making valid inferences. The <code>songs</code> data is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Estimate two models predicting <code>popularity</code> from <code>song_age</code></li>
<li>In the first model, set <code>adapt_delta</code> to 0.99</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Estimate the model with a new `adapt_delta`</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>adapt_model <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb18-3"><a href="#cb18-3"></a>                        <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.99</span>))</span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="co"># View summary</span></span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="fu">summary</span>(adapt_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>In the second model, set the <code>max_treedepth</code> to 15</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># Estimate the model with a new `max_treedepth`</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs,</span>
<span id="cb19-3"><a href="#cb19-3"></a>                       <span class="at">control =</span> <span class="fu">list</span>(<span class="at">max_treedepth =</span> <span class="dv">15</span>))</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="co"># View summary</span></span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="fu">summary</span>(tree_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wonderful! Now you’re prepared to handle rstanarms most common estimation errors. Notice in these summaries that nothing looks different. These options don’t alter the model itself, but instead modify the underlying estimation algorithm.</p>
</section>
</section>
<section id="assessing-model-fit" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 3. Assessing Model Fit</h1>
<p>In this chapter, we’ll learn how to determine if our estimated model fits our data and how to compare competing models.</p>
<section id="using-the-r-squared-statistic" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="using-the-r-squared-statistic"><span class="header-section-number">3.1</span> Using the R Squared statistic</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Using the R Squared statistic</strong></p>
<p>So far in the course, we’ve talked about how to estimate and modify a model. In the final two chapters, we’ll be looking at how the models we estimate can be evaluated and used to make predictions. In this chapter, we’ll focus on the first part: evaluating models. This is a critically important part of modeling building because if our model doesn’t fit, then we won’t be able to make very good predictions. We’ll start by using one of the most common measures of model fit in linear regression: the r-squared statistic.</p>
<p><strong>2. What is R squared?</strong></p>
<p>The R squared statistic is a measure of how well the independent variables in the model are able to predict the dependent variable. Specifically, the R squared statistic measures the proportion of variance in the dependent variable that can be explained by the independent variables. As with all proportions, the R squared ranges from 0 to 1, with 0 representing no variance explained, and 1 representing all the variance explained, or a deterministic model. Because of this, the R squared is also known as the coefficient of determination.The R squared is calculated as 1 minus the sum of squared residuals (called the residual sum of squares) divided by the sum of squared deviations of the data from the mean (called the total sum of squares).</p>
<p><strong>3. What is R squared?</strong></p>
<p>In other words, we take the observed value for an observation and subtract the predicted value, square it, and then sum that over all observations. That is the numerator.</p>
<p><strong>4. What is R squared?</strong></p>
<p>In the denominator, we take the observed value for an observation, subtract the mean of the observed value, square that, and sum over all observations. This variance of residuals to total variance ratio is what drives the R squared.</p>
<p><strong>5. Calculating R squared statistic</strong></p>
<p>In a frequentist regression, we can get the R squared by estimating a model with the lm() function, and saving a summary of the object. That object contains an <code>r.squared</code>, that we can pull out to view.However, we can also calculate this by hand. We can define the residual sum of squares as the variance of the residuals of lm_model and the total sum of squares as the sum of the variance of the residuals of lm_model and the variance of the predicted values of lm_model. Taking 1 minus the residual sum of square divided by the total sum of squares, gives us the same value that was saved in the lm_summary.</p>
<p><strong>6. The R squared statistic of a Bayesian Model</strong></p>
<p>In rstanarm, the R squared is not saved in the summary object as in the lm() summary function. However, we can still calculate the R squared by hand using the exact same formulas as we did for the frequentist regression. We define the residual sum of squares and the total sum of squares, and then use those to calculate the R squared value. Comparing this value to the value we got in the frequentist regression, we see that they are almost identical.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Now it’s your turn to calculate the R squared for our model using Spotify data.</p>
</section>
<section id="calculating-frequentist-r-squared" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="calculating-frequentist-r-squared"><span class="header-section-number">3.2</span> Calculating Frequentist R-squared</h2>
<p>Let’s practice calculating the R-squared. By starting with the frequentist R-squared, we can check our formulas for calculating the R-squared by hand, by checking against the value in the frequentist summary. The <code>lm_model</code> and <code>lm_summary</code> objects are already in your environment.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Print the R-squared value from the <code>lm_summary</code> object</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>lm_summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(lm_model)</span>
<span id="cb20-2"><a href="#cb20-2"></a></span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="co"># Print the R-squared from the linear model</span></span>
<span id="cb20-4"><a href="#cb20-4"></a>lm_summary<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Calculate the variance of the residuals of the <code>lm_model</code></li>
<li>Calculate the variance of the predicted values of the <code>lm_model</code></li>
<li>Calculate the R-squared value</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># Calulate sums of squares</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>ss_res <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">residuals</span>(lm_model))</span>
<span id="cb21-3"><a href="#cb21-3"></a>ss_fit <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">fitted</span>(lm_model))</span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="co"># Calculate the R-squared</span></span>
<span id="cb21-6"><a href="#cb21-6"></a><span class="dv">1</span> <span class="sc">-</span> (ss_res <span class="sc">/</span> (ss_res <span class="sc">+</span> ss_fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome! Notice we get the same value of 0.23 with both methods. This means that the <code>song_age</code> explains 23% of the variance in <code>popularity</code>. Now that we’ve learned how to manually calculate the R-squared, we can apply these formulas to the Bayesian model.</p>
</section>
<section id="r-squared-for-a-bayesian-model" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="r-squared-for-a-bayesian-model"><span class="header-section-number">3.3</span> R-squared for a Bayesian Model</h2>
<p>Now let’s calculate the R-squared for a Bayesian model so we can assess the predictions of models estimated with <code>stan_glm</code>. The <code>stan_model</code> object is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the variance of the residuals of <code>stan_model</code></li>
<li>Calculate the variance of the fitted values of <code>stan_model</code></li>
<li>Calculate the R-squared</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># Save the variance of residulas</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>ss_res <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">residuals</span>(stan_model))</span>
<span id="cb22-3"><a href="#cb22-3"></a></span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="co"># Save the variance of fitted values</span></span>
<span id="cb22-5"><a href="#cb22-5"></a>ss_fit <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">fitted</span>(stan_model))</span>
<span id="cb22-6"><a href="#cb22-6"></a></span>
<span id="cb22-7"><a href="#cb22-7"></a><span class="co"># Calculate the R-squared</span></span>
<span id="cb22-8"><a href="#cb22-8"></a><span class="dv">1</span> <span class="sc">-</span> (ss_res <span class="sc">/</span> (ss_res <span class="sc">+</span> ss_fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Good job! For the Bayesian model, we get an R-squared of 0.23, which is very close to the R-squared for the frequentist model, as would be expected given how similar the parameter estimates are. Now let’s look at some new methods for assessing model fit that are only available for Bayesian models.</p>
</section>
<section id="posterior-predictive-model-checks" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="posterior-predictive-model-checks"><span class="header-section-number">3.4</span> Posterior predictive model checks</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Posterior predictive model checks</strong></p>
<p>The R-squared statistic tells us how much variance in the dependent variable can be explained by the predictors. But this isn’t the only way to assess model fit. In fact there are several methods that are specific to Bayesian estimation. The most important among these is a class of analyses called posterior predictive model checks, which can only be calculated when a Bayesian estimation process is used.</p>
<p><strong>2. Using posterior distributions</strong></p>
<p>As the name might suggest, posterior predictive model checks make use of the posterior distributions. Recall our model predicting a child’s IQ score from their mom’s IQ score. By default, this model is estimated with 4 chains, and 2,000 iterations in each chain, with the first 1,000 iterations of each discarded for warmup. This means that we have a total of 4,000 iterations that make up our posterior distribution (1,000 from each chain). Here we can see the values of the intercept and coefficient for the <code>mom_iq</code> variable for the first 10 iterations. As we can see, the values change with each iteration, which is expected as we are randomly sampling from the posterior distribution.</p>
<p><strong>3. Posterior predictions</strong></p>
<p>We can use these posterior draws to calculate predicted score for our data. For example, we can calculate predicted scores using the parameter values at iteration 1, another set of predicted scores using the parameter values at iteration 2, and so on. Ultimately, because we have 4,000 iteration, we can calculate 4,000 predicted scores for each observation. We can get all the predicted values for all observations using the posterior_linpred function. This returns a matrix that has a row for each iteration, and a column for each observation. In this example, we can see the predicted IQ scores for the first 10 iterations for children 1 through 5. Because these scores are generated using the model parameters, these predicted scores form a distribution of what our observed scores should look like - if the specified model was correct. Thus, deviations from these predictive distributions are an indication of poor fit.</p>
<p><strong>4. Comparing score distributions</strong></p>
<p>One way we can look for deviations is to compare the distribution of predicted scores in an iteration to the distribution of observed scores. For example we can get the predicted scores for the first and second iterations by pulling the first and second rows of the matrix return by posterior_linpred. We can then look at a summary of those scores, compared the summary of the observed scores. Here we can see that the mean of the iterations is similar to the mean of the observed data, but min and max values are less extreme than in the observed data.</p>
<p><strong>5. Comparing single scores</strong></p>
<p>Similarly, we can compare individual scores to their expected distribution. For example, child 24 had an observed score of 87, which falls in line with the distribution of predicted scores for that child, as shown in column 24 of the matrix returned by posterior_linpred. In contrast, child 185 had an observed score of 111, which is quite different from the distribution of predicted scores for this student. This means that the model does not do a very good job of predicting the IQ score for this student.</p>
<p><strong>6. Let’s practice</strong></p>
<p>Now it’s your turn to use posterior predictive model checks!</p>
</section>
<section id="predicted-score-distributions" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="predicted-score-distributions"><span class="header-section-number">3.5</span> Predicted score distributions</h2>
<p>Now let’s practicing using posterior predictive scores to look at how our Spotify model fits to the real data.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the posterior predicted scores for <code>stan_model</code></li>
<li>Print a summary of the observed popularity scores in the <code>songs</code> data</li>
<li>Compare this to a summary of the 1st and 10th replications</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Calculate posterior predictive scores</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>predictions <span class="ot">&lt;-</span> <span class="fu">posterior_linpred</span>(stan_model)</span>
<span id="cb23-3"><a href="#cb23-3"></a></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="co"># Print a summary of the observed data</span></span>
<span id="cb23-5"><a href="#cb23-5"></a><span class="fu">summary</span>(songs<span class="sc">$</span>popularity)</span>
<span id="cb23-6"><a href="#cb23-6"></a></span>
<span id="cb23-7"><a href="#cb23-7"></a><span class="co"># Print a summary of the 1st replication</span></span>
<span id="cb23-8"><a href="#cb23-8"></a><span class="fu">summary</span>(predictions[<span class="dv">1</span>,])</span>
<span id="cb23-9"><a href="#cb23-9"></a></span>
<span id="cb23-10"><a href="#cb23-10"></a><span class="co"># Print a summary of the 10th replication</span></span>
<span id="cb23-11"><a href="#cb23-11"></a><span class="fu">summary</span>(predictions[<span class="dv">10</span>,])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! Now you know the basics of comparing score distributions. Notice that the observed data goes from 0 to 85, but in the simualted datasets, the predicted scores only range from about 37 to 66. However the means of all three summaries are very similar. This suggests that the model is better at predicting songs with an average level of popularity than the edge cases.</p>
</section>
<section id="distributions-for-a-single-observation" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="distributions-for-a-single-observation"><span class="header-section-number">3.6</span> Distributions for a single observation</h2>
<p>The <code>songs</code> dataset and the posterior predictions <code>predictions</code> are already loaded. Look at the observed popularity and posterior predictive sample for the popularity of song number 12.</p>
<blockquote class="blockquote">
<h2 id="question-5" data-number="3.7" class="anchored"><span class="header-section-number">3.7</span> <em>Question</em></h2>
<p>Does the observed popularity fit within the expected distribution?<br> <br> ⬜ You bet<br> ✅ No way<br> ⬜ We need more information<br></p>
</blockquote>
<p>Correct! The observed data falls outside the range of predicted scores.</p>
</section>
<section id="model-fit-with-posterior-predictive-model-checks" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="model-fit-with-posterior-predictive-model-checks"><span class="header-section-number">3.8</span> Model fit with posterior predictive model checks</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Model fit with posterior predictive model checks</strong></p>
<p>In the last lesson, we talked about how we can get predicted scores from our posterior, and compare those predictions to our observed data. However, it would be incredibly tedious to compare the observed data to every replication one at a time, or look at the results for each observation independently. In this lesson we’ll learn how we can summarize results across all replications to make global evaluations of how well the model fits the data.</p>
<p><strong>2. R squared posterior distribution</strong></p>
<p>The first measure of fit using posterior predictive model checks is one we’ve looked at before: the R-squared. With our estimated <code>stan_model</code>, we can get a posterior distribution of the R-squared using the <code>bayes_R2</code> function. We can then look at a summary to get an idea of the distribution, or create a 95% credible interval using the quantile function.</p>
<p><strong>3. R squared histogram</strong></p>
<p>We can also make a histogram of the R-squared values. From this, we can see that the true R-squared of our model is mostly likely somewhere between .1 and .3.</p>
<p><strong>4. Density overlay</strong></p>
<p>We can also compare the distributions from all replications to the observed data at one time using the <code>pp_check</code> function, and specifying a density overlay. This will generate a plot like this. Here, each light blue line represents the distribution of predicted scores from a single replications, and the dark blue line represents the observed data. If the model fits, the dark blue line should align closely with the light blue lines.</p>
<p><strong>5. Posterior predictive tests</strong></p>
<p>We can also test certain characteristics of the dependent variable. By specifying “stat” in the <code>pp_check</code> function, we can get a distribution of the mean of the dependent variable from all replications. These are the light blue bars: the means from each replication plotted as a histogram. The mean from the observed data is then plotted on top as a dark blue bar. In our example, the observed mean of kids’ IQ scores falls within the expected range of means from the posterior predictions. Therefore, we have evidence that our model fits.</p>
<p><strong>6. Posterior predictive tests</strong></p>
<p>However, the mean is just one aspect of the observed data. By changing “stat” to “stat_2d”, we can look at multiple aspects of our dependent variable: the mean and standard deviation. In this plot, each light blue dot represents the mean and standard deviation of the predicted IQ scores in one replication. The dark blue dot shows our observed data. Because the dark blue dot is inside the mass of light blue dots, we have more evidence that our model fits our data.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Now you try using posterior predictive model checks to assess the model fit of our Spotify model.</p>
</section>
<section id="r-squared-posterior" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="r-squared-posterior"><span class="header-section-number">3.9</span> R-squared Posterior</h2>
<p>First, let’s get a posterior distribution of the R-squared statistic so we can make inferences about how predictive our model is likely to be. The <code>stan_model</code> using the Spotify data is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the posterior distribution of the R-squared statistic</li>
<li>Create a histogram of the R-squared distribution</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Calculate the posterior distribution of the R-squared</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>r2_posterior <span class="ot">&lt;-</span> <span class="fu">bayes_R2</span>(stan_model)</span>
<span id="cb24-3"><a href="#cb24-3"></a></span>
<span id="cb24-4"><a href="#cb24-4"></a><span class="co"># Make a histogram of the distribution</span></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="fu">hist</span>(r2_posterior)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome! Now you can see a range for how predictive your model may be. Notice that the distribution is centered around 0.23, which is the point estimate we calculated earlier. But now, we can see that the true value of the R-squared could plausibly range from about 0.10 to 0.35</p>
</section>
<section id="posterior-predictive-testing" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="posterior-predictive-testing"><span class="header-section-number">3.10</span> Posterior Predictive Testing</h2>
<p>Now let’s practice comparing our replicated predictions to our observed data more than one at a time. By comparing our data to all replications, we can assess how well the model fits the data.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Plot the density of predicted scores from replication compared to the observed density</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># Create density comparison</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="fu">pp_check</span>(stan_model, <span class="st">"dens_overlay"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Create a scatter plot of the mean and standard deviations of the replicated predicted scores compared to the observed data</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Create scatter plot of means and standard deviations</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="fu">pp_check</span>(stan_model, <span class="st">"stat_2d"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! Now you can evaluate if your model fits the data! In the first plot, we can see there is a second mode of popularity scores around 10 that is not captured by the model, even though the peaks of the observed data and model are in similar places. In the second plot, the mean and standard deviation of the observed data is right in the middle of the expected distribution of points, indicating that these two characteristics are recovered well.</p>
</section>
<section id="bayesian-model-comparisons" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="bayesian-model-comparisons"><span class="header-section-number">3.11</span> Bayesian model comparisons</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Bayesian model comparisons</strong></p>
<p>So far we’ve looked at how to determine if a single model fits our observed data. But what do we do if we have multiple models, and we want to figure out which one fits the best? This is known as model comparison. In this lesson, we’ll talk about how we can compare two, or more, regression models that were estimated using rstanarm.</p>
<p><strong>2. The loo package</strong></p>
<p>When we estimate a model with rstanarm, we can use the loo package for model comparisons. LOO stands for “leave-one-out” which is one variety of a class of model comparison tools more widely known as cross validation. The loo package doesn’t use true cross validation. Instead, the algorithm approximates the leave-one-out cross validation. The details of how exactly this is done are beyond the scope of this course, but if you want to learn more, the loo package documentation provides many resources for exploration. In this lesson, we’ll instead focus on how the loo package can be used to compare models, and interpret the output.</p>
<p><strong>3. Using loo on a single model</strong></p>
<p>To view the LOO estimates for a model, we simply send an estimated model as the argument to the loo() function. This gives us several pieces of information. First, we see that the estimates were computed from a log-likelihood matrix that has 4000 rows (the total number of iterations in our posterior) and 434 columns (the total number of observations in the kidiq dataset). We get the LOO estimate, called elpd_loo, the effective number of parameters in the model, p_loo, and the LOO estimate converted to a deviance scale, looic. The deviance scale is simply minus two times the LOO estimate, but is more common in some fields. Finally, we’re provided with some diagnostics from the approximation algorithm. However, these numbers aren’t very useful in isolation. For example, what does an epld_loo values of minus 1878-point-5 mean? This value really only has meaning relative to the values of competing models.</p>
<p><strong>4. Model comparisons with loo</strong></p>
<p>Let’s say that we have two possible models. In the first model, we predict a kid’s IQ scores from only their mother’s IQ. In the second model, we predict the kid’s IQ score from not only their mom’s IQ, but also whether or not their mom graduated high school, and the interaction between mom’s IQ and mom’s high school completion. We want to know which model does a better job of predicting the kid’s IQ score. To do this, we can save the loo estimates from the 1 predictor and 2 predictor models, and then use the <code>compare</code> function.</p>
<p><strong>5. Model comparisons with loo</strong></p>
<p>The compare function provides use with the difference in loo estimates, along with a standard error of the difference. A positive difference score like in this example, means that the second model is favored (the model with both predictors), whereas a negative score would indicate a preference for the first model. The standard error helps us decide if the difference in meaningful. As a rule of thumb, if absolute value of the difference is less than the standard error, the models don’t perform differently, and we should choose the simpler model (the one with fewer parameters) because it is more parsimonious. If the absolute value of the difference is greater than the standard error, then we will prefer the model indicated by the test. In this example, because the difference of 6.1 is positive and greater than the standard error of 3.9, we would choose the second model with both predictors.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>Now it’s your turn to compare models.</p>
</section>
<section id="calculating-the-loo-estimate" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="calculating-the-loo-estimate"><span class="header-section-number">3.12</span> Calculating the LOO estimate</h2>
<p>Now let’s practice using the <code>loo</code> package on our Spotify model so that we can determine which model provides the best fit to our data. The <code>songs</code> dataset is already loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Estimate a model predicting <code>popularity</code> from <code>song_age</code></li>
<li>Print the LOO approximation for this model with 1 predictor</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Estimate the model with 1 predictor</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>model_1pred <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age, <span class="at">data =</span> songs)</span>
<span id="cb27-3"><a href="#cb27-3"></a></span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="co"># Print the LOO estimate for the 1 predictor model</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>loo<span class="sc">::</span><span class="fu">loo</span>(model_1pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Estimate a model predicting <code>popularity</code> from <code>song_age</code>, <code>artist_name</code>, and their interaction</li>
<li>Print the LOO approximation for this model with 2 independent variables</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Estimate the model with both predictors</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>model_2pred <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age <span class="sc">*</span> artist_name, <span class="at">data =</span> songs)</span>
<span id="cb28-3"><a href="#cb28-3"></a></span>
<span id="cb28-4"><a href="#cb28-4"></a><span class="co"># Print the LOO estimates for the 2 predictor model</span></span>
<span id="cb28-5"><a href="#cb28-5"></a>loo<span class="sc">::</span><span class="fu">loo</span>(model_2pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome! In the summary, we see that the model with two predictors has LOO approximation of -865.0, and the model with one predictor has a LOO approximation of -888.1. Because the two predictor model is higher (less negative), we would expect that model to have better predictions. However, in order to know if this increase is meaningful, we need to direclty compare the model and compare the difference to the standard error.</p>
</section>
<section id="comparing-models" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="comparing-models"><span class="header-section-number">3.13</span> Comparing models</h2>
<p>The models from the last exercise, <code>model_1pred</code> and <code>model_2pred</code> are available in the environment.</p>
<blockquote class="blockquote">
<h2 id="question-6" data-number="3.14" class="anchored"><span class="header-section-number">3.14</span> <em>Question</em></h2>
<p>According to the LOO approximation, which model should be preferred?<br> <br> ⬜ They fit equally well, prefer <code>model_1pred</code> because it has fewer parameters<br> ⬜ <code>model_1pred</code><br> ✅ <code>model_2pred</code><br> ⬜ LOO can’t give us this information<br></p>
</blockquote>
<p>Correct! The model with both predictors has a significantly better LOO approximation.</p>
</section>
</section>
<section id="presenting-and-using-a-bayesian-regression" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 4. Presenting and Using a Bayesian Regression</h1>
<p>In this chapter, we’ll learn how to use the estimated model to create visualizations of your model and make predictions for new data.</p>
<section id="visualizing-a-bayesian-model" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="visualizing-a-bayesian-model"><span class="header-section-number">4.1</span> Visualizing a Bayesian model</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Visualizing a Bayesian model</strong></p>
<p>So far we’ve learned how to estimate, customize, and evaluate a Bayesian model. The final step of the analysis pipeline is to present the results of your model. In this chapter, we’ll learn how to make predictions using new data and how to create visualizations of the model using ggplot2. With these tools, you’ll be ready to complete a fully Bayesian analysis from model creation to presentation.</p>
<p><strong>2. Saving model coefficients</strong></p>
<p>We’ll start by making a plot of our model predicting a kid’s IQ score from their mom’s IQ score. To do this, we need to save the values of the estimated parameters. Recall that we can view the parameter estimates by using the tidy() function from the broom package. To save the parameter values, we can first save the tidy summary, and then pull values from the estimate column of the data frame.</p>
<p><strong>3. Creating a plot</strong></p>
<p>We can then create a plot using ggplot2 syntax that you are already likely already familiar with. We define our dataset, kidiq, and then specify that we want mom_iq on the x-axis, and the kid’s score on the y-axis. We can then add a geom_point layer to create the scatter plot. Finally, we use geom_abline to add a straight line to the plot. In geom_abline, we specify the intercept and slope of the line to be the model parameters that we just saved. And just like that, we have a plot showing our data and the estimated regression line!</p>
<p><strong>4. Plotting uncertainty</strong></p>
<p>We can do more than a simple plot though! Because we used a Bayesian estimation, we can use the posterior distributions to also plot the uncertainty of our regression line. Just like when we learned about posterior predictive checks, we can save the posterior draws for our intercept and mom_iq predictor using the spread_draws function from the tidybayes package. This gives us the value of the intercept and slope at each draw from the posterior. Using these draws, we can plot a regression line for each iteration.</p>
<p><strong>5. Plotting uncertainty</strong></p>
<p>We start in the same way we did on the previous plot. We define our data, put mom_iq on the x-axis and kid_score on the y-axis, and then add a geom_point layer to create the scatter plot.</p>
<p><strong>6. Plotting uncertainty</strong></p>
<p>We can then plot the lines for each iteration using geom_abline, just like in the last plot. The difference now is that we define a new dataset, <code>draws</code>, which we just created and contains the values of the parameters at each iteration. We then specify that the intercept should be the “(Intercept)” column, and the slope should be the mom_iq column. Finally, because there are 4,000 lines being drawn, we can make them small and add transparency so we can tell where there is more density from our posterior predictions. This results in plot with a line for each iteration, showing the range of plausible regression lines.</p>
<p><strong>7. Plotting uncertainty</strong></p>
<p>The final step is to add our mean regression line. This can be done with the same geom_abline code that we used in our first graphic. We define the intercept and slope to be the saved model_intercept and model_slope objects respectively. Now we have a complete graphic showing our data, the estimated regression line, and the uncertainty around our estimated line.</p>
<p><strong>8. Let’s practice</strong></p>
<p>Now it’s your turn! Let make some visualizations of our Spotify model, predicting the popularity of a song from it’s age.</p>
</section>
<section id="plotting-a-bayesian-model" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="plotting-a-bayesian-model"><span class="header-section-number">4.2</span> Plotting a Bayesian model</h2>
<p>In previous exercises we have estimated a Bayesian model predicting a song’s popularity (<code>popularity</code>) from its age (<code>song_age</code>). Now let’s visualize the model. Using the <code>songs</code> dataset and <code>stan_model</code> object that are already loaded, create a visualization showing the data the estimated regression line using ggplot2.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Save a tidy summary of the model parameters to <code>tidy_coef</code></li>
<li>Pull out the estimated intercept and slope from <code>tidy_coef</code></li>
<li>Create a plot showing the data and estimate regression line with <code>song_age</code> on the x-axis and <code>popularity</code> on the y-axis</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># Load package</span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb29-3"><a href="#cb29-3"></a></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="co"># Save the model parameters</span></span>
<span id="cb29-5"><a href="#cb29-5"></a>tidy_coef <span class="ot">&lt;-</span> <span class="fu">tidy</span>(stan_model)</span>
<span id="cb29-6"><a href="#cb29-6"></a></span>
<span id="cb29-7"><a href="#cb29-7"></a><span class="co"># Extract intercept and slope</span></span>
<span id="cb29-8"><a href="#cb29-8"></a>model_intercept <span class="ot">&lt;-</span> tidy_coef<span class="sc">$</span>estimate[<span class="dv">1</span>]</span>
<span id="cb29-9"><a href="#cb29-9"></a>model_slope     <span class="ot">&lt;-</span> tidy_coef<span class="sc">$</span>estimate[<span class="dv">2</span>]</span>
<span id="cb29-10"><a href="#cb29-10"></a></span>
<span id="cb29-11"><a href="#cb29-11"></a><span class="co"># Create the plot</span></span>
<span id="cb29-12"><a href="#cb29-12"></a><span class="fu">ggplot</span>(songs, <span class="fu">aes</span>(<span class="at">x =</span> song_age, <span class="at">y =</span> popularity)) <span class="sc">+</span></span>
<span id="cb29-13"><a href="#cb29-13"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb29-14"><a href="#cb29-14"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> model_intercept, <span class="at">slope =</span> model_slope)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! In this plot, we can see that older songs are less popular than new songs, as we might expect. This is also reflected by the negative slope coefficient for <code>song_age</code> that we have seen in previous exercises. Unfortunately, this plot doesn’t show us any measure of uncertainty or confidence in our line. We’ll add this in the next exercise.</p>
</section>
<section id="plotting-model-uncertainty" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="plotting-model-uncertainty"><span class="header-section-number">4.3</span> Plotting Model Uncertainty</h2>
<p>Because we used a Bayesian estimation, we can use the posterior distributions to create a predicted regression line from each draw in our posterior samples. These lines will show the uncertainty around our overall line. The <code>songs</code> and <code>stan_model</code> objects are already loaded, along with the <code>model_intercept</code> and <code>model_slope</code> that you used in the last exercise.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Save the values of the <code>(Intercept)</code> and <code>song_age</code> from each draw from the posterior distributions of <code>stan_model</code></li>
<li>Print the values to the screen</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Load package</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="fu">library</span>(tidybayes)</span>
<span id="cb30-3"><a href="#cb30-3"></a></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="co"># Save the values from each draw of the posterior distribution</span></span>
<span id="cb30-5"><a href="#cb30-5"></a>draws <span class="ot">&lt;-</span> <span class="fu">spread_draws</span>(stan_model, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="st">`</span><span class="at">song_age</span><span class="st">`</span>)</span>
<span id="cb30-6"><a href="#cb30-6"></a></span>
<span id="cb30-7"><a href="#cb30-7"></a><span class="co"># Print the `draws` data frame to the console</span></span>
<span id="cb30-8"><a href="#cb30-8"></a>draws</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Start the plot by creating a scatter plot with <code>song_age</code> on the x-axis and <code>popularity</code> on the y-axis</li>
<li>Add points to the plot</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Save the values from each draw of the posterior distribution</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>draws <span class="ot">&lt;-</span> <span class="fu">spread_draws</span>(stan_model, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="st">`</span><span class="at">song_age</span><span class="st">`</span>)</span>
<span id="cb31-3"><a href="#cb31-3"></a></span>
<span id="cb31-4"><a href="#cb31-4"></a><span class="co"># Create the plot</span></span>
<span id="cb31-5"><a href="#cb31-5"></a><span class="fu">ggplot</span>(songs, <span class="fu">aes</span>(<span class="at">x =</span> song_age, <span class="at">y =</span> popularity)) <span class="sc">+</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>  <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="5" type="1">
<li>Plot the uncertainty by creating a regression line for each draw from the posterior distributions</li>
<li>Plot these lines in <code>"skyblue"</code> to make them stand out.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># Save the values from each draw of the posterior distribution</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>draws <span class="ot">&lt;-</span> <span class="fu">spread_draws</span>(stan_model, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="st">`</span><span class="at">song_age</span><span class="st">`</span>)</span>
<span id="cb32-3"><a href="#cb32-3"></a></span>
<span id="cb32-4"><a href="#cb32-4"></a><span class="co"># Create the plot</span></span>
<span id="cb32-5"><a href="#cb32-5"></a><span class="fu">ggplot</span>(songs, <span class="fu">aes</span>(<span class="at">x =</span> song_age, <span class="at">y =</span> popularity)) <span class="sc">+</span></span>
<span id="cb32-6"><a href="#cb32-6"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb32-7"><a href="#cb32-7"></a>    <span class="fu">geom_abline</span>(<span class="at">data =</span> draws, <span class="fu">aes</span>(<span class="at">intercept =</span> <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="at">slope =</span> song_age),</span>
<span id="cb32-8"><a href="#cb32-8"></a>                <span class="at">size =</span> <span class="fl">0.1</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">color =</span> <span class="st">"skyblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="7" type="1">
<li>Add the final regression line by a plotting a line with final estimated intercept and slope</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># Save the values from each draw of the posterior distribution</span></span>
<span id="cb33-2"><a href="#cb33-2"></a>draws <span class="ot">&lt;-</span> <span class="fu">spread_draws</span>(stan_model, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="st">`</span><span class="at">song_age</span><span class="st">`</span>)</span>
<span id="cb33-3"><a href="#cb33-3"></a></span>
<span id="cb33-4"><a href="#cb33-4"></a><span class="co"># Create the plot</span></span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="fu">ggplot</span>(songs, <span class="fu">aes</span>(<span class="at">x =</span> song_age, <span class="at">y =</span> popularity)) <span class="sc">+</span></span>
<span id="cb33-6"><a href="#cb33-6"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb33-7"><a href="#cb33-7"></a>    <span class="fu">geom_abline</span>(<span class="at">data =</span> draws, <span class="fu">aes</span>(<span class="at">intercept =</span> <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="at">slope =</span> song_age),</span>
<span id="cb33-8"><a href="#cb33-8"></a>                <span class="at">size =</span> <span class="fl">0.1</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">color =</span> <span class="st">"skyblue"</span>) <span class="sc">+</span></span>
<span id="cb33-9"><a href="#cb33-9"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept =</span> model_intercept, <span class="at">slope =</span> model_slope)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome work! In our final plot we can see the data, the estimated regression line, and the uncertainty around the line. This is a ton of information conveyed in easily understandable plot that took only a few lines of code to create!</p>
</section>
<section id="making-predictions" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="making-predictions"><span class="header-section-number">4.4</span> Making predictions</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Making predictions</strong></p>
<p>In the last lesson we talked about how to visualize your model. For the rest of this chapter, we’ll be looking at how we can use our estimated model to make predictions for observations in our dataset, and for new data.</p>
<p><strong>2. Making predictions for observed data</strong></p>
<p>We’ll start by getting predictions for the observations in our kidiq dataset. We first estimate a model predicting the child’s IQ from the mom’s IQ and whether or not their mom completed high school. We can then get a predicted score at each iteration of the model for each kid by using the posterior_predict function. This returns a matrix that has a row for every iteration and a column for each observation. This means that we can get a posterior distribution of the predicted score for each student, just like when we were doing posterior predictive model checks in chapter three. In chapter three, we got these predictions using the posterior_linpred function. The benefit of using the <code>posterior_predict</code> function now is that we can get predictions for new data that weren’t used to estimated the model.</p>
<p><strong>3. Making predictions for new data</strong></p>
<p>To make predictions for new data, we first have to create the data we want to predict. For this example, we will predict the IQ for two children whose mothers both had an IQ of 110, one who completed high school, and one who didn’t. For these predictions, we create a new data frame with the same variable names as our observed data. Our data frame then has two columns, one for each predictor in our model, and two rows, one for each prediction that we want to make.</p>
<p><strong>4. Making predictions for new data</strong></p>
<p>After creating the new data for predictions, we can supply this data frame to the newdata argument of the posterior_predict function. This creates predictions for the new data at all 4,000 draws from the posterior distributions. Here we can see the predicted scores for these observations at the first 10 iterations. We can also look at a summary for each column. Looking at the summaries, we see that the predicted scores for the observations with a mother who completed high school are consistently higher.</p>
<p><strong>5. Let’s practice</strong></p>
<p>Now it’s your turn to make some predictions about the popularity of songs in the Spotify data!</p>
</section>
<section id="popularity-for-observed-songs" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="popularity-for-observed-songs"><span class="header-section-number">4.5</span> Popularity for Observed Songs</h2>
<p>Let’s practice making predictions about song popularity from the Spotify <code>songs</code> data. This will get us used to the syntax we will use for making predictions for new data that was not observed.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Estimate a model predicting <code>popularity</code> from <code>song_age</code> and <code>artist_name</code></li>
<li>Print a summary of the estimated model</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a><span class="co"># Estimate the regression model</span></span>
<span id="cb34-2"><a href="#cb34-2"></a>stan_model <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(popularity <span class="sc">~</span> song_age <span class="sc">+</span> artist_name, <span class="at">data =</span> songs)</span>
<span id="cb34-3"><a href="#cb34-3"></a></span>
<span id="cb34-4"><a href="#cb34-4"></a><span class="co"># Print the model summary</span></span>
<span id="cb34-5"><a href="#cb34-5"></a><span class="fu">summary</span>(stan_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Create posterior distributions of the predicted scores for each song</li>
<li>Print the first 10 predicted scores for the first 5 songs</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># Get posteriors of predicted scores for each observation</span></span>
<span id="cb35-2"><a href="#cb35-2"></a>posteriors <span class="ot">&lt;-</span> <span class="fu">posterior_predict</span>(stan_model)</span>
<span id="cb35-3"><a href="#cb35-3"></a></span>
<span id="cb35-4"><a href="#cb35-4"></a><span class="co"># Print 10 predicted scores for 5 songs</span></span>
<span id="cb35-5"><a href="#cb35-5"></a>posteriors[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! We now have a posterior distribution for the predicted popularity of every song in our dataset. But what if we want to predict the popularity for songs that aren’t in our dataset? We’ll do this in the next exercise.</p>
</section>
<section id="popularity-for-new-songs" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="popularity-for-new-songs"><span class="header-section-number">4.6</span> Popularity for New Songs</h2>
<p>Beyoncé’s most recent album, <em>Lemonade</em>, is not in our <code>songs</code> dataset. Let’s predict how popular a song on that album would be. The <em>Lemonade</em> album was released 663 days before this dataset was created. The <code>stan_model</code> object you created in the last exercise is loaded in your environment.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a data frame of new data to be predicted including the <code>song_age</code> and <code>artist_name</code> variables</li>
<li>Create a posterior distribution for predicted popularity of a song on <em>Lemonade</em></li>
<li>Print the predicted popularity for the first 10 draws from the posterior distribution</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="co"># Create data frame of new data</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>predict_data    <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">song_age =</span> <span class="dv">663</span>, <span class="at">artist_name =</span> <span class="st">"Beyoncé"</span>)</span>
<span id="cb36-3"><a href="#cb36-3"></a></span>
<span id="cb36-4"><a href="#cb36-4"></a><span class="co"># Create posterior predictions for Lemonade album</span></span>
<span id="cb36-5"><a href="#cb36-5"></a>new_predictions <span class="ot">&lt;-</span> <span class="fu">posterior_predict</span>(stan_model, <span class="at">newdata =</span> predict_data)</span>
<span id="cb36-6"><a href="#cb36-6"></a></span>
<span id="cb36-7"><a href="#cb36-7"></a><span class="co"># Print first 10 predictions for the new data</span></span>
<span id="cb36-8"><a href="#cb36-8"></a>new_predictions[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Print a summary of the posterior predictions for the 1 new observation</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># Print a summary of the posterior distribution of predicted popularity</span></span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="fu">summary</span>(new_predictions[, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Awesome! We now have a posterior distribution for the predicted popularity of a song from the the <em>Lemonade</em> album. From the posterior summary we can see there is large range of plausible values, ranging from 10.85 to 98.45!</p>
</section>
<section id="visualizing-predictions" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="visualizing-predictions"><span class="header-section-number">4.7</span> Visualizing predictions</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Visualizing predictions</strong></p>
<p>In the last lesson we learned how to create posterior distributions for predictions of both observed and unobserved data. Earlier in this chapter we reviewed how to create visualizations for our observed data. In the final lesson of this course, we’ll learn how to create visualizations for the predictions of new data. Thus, we’ll be able to effectively communicate the output of our model in the context of out-of-sample data.</p>
<p><strong>2. Plotting new predictions</strong></p>
<p>Let’s start with the same model we were working with in the last lesson. We’re predicting a child’s IQ score from their mother’s IQ and whether or not their mother completed high school. We previously created a new data frame to predict the IQ scores of 2 kids whose mothers both had an IQ of 110, but one completed high school and the other did not. Using the <code>posterior_predict</code> function, we were able to get the posterior distributions for the prediction of each kid’s IQ score. Now we want to visualize these distributions.</p>
<p><strong>3. Formatting the data</strong></p>
<p>The first step is to format the data so that we can plot it with ggplot2. To do this, we will first convert our posterior predictions to a data frame using the <code>as.data.frame</code> function. We then set the column names to be “No HS” and “Completed HS” so that we can identify which prediction is in each column. Finally, we use the <code>gather</code> function from the tidyr package to get the data into the structure needed for ggplot2. This leaves us with 2 columns: one indicating whether HS was completed, and one with the draws from the posterior distribution.</p>
<p><strong>4. Creating the plot</strong></p>
<p>To create the plot, we call the <code>ggplot</code> function, specify that we are using the plot_posterior data frame and that we want the <code>predict</code> column to be on the x-axis. We then use <code>facet_wrap</code> to put each level of high school completion in its own plot. Using <code>ncol = 1</code>, we make the plots be stacked vertically. Finally, we use `geom_density to create a density plot. With this visualization, we can see how the mother’s completion of high school affects the predicted IQ score for the kids. The distribution for the kid with a mother who completed high school is shifted a little further to the right, indicating higher scores. However, for the most part, these distributions are very similar. This is also consistent with what we saw in the previous lesson when looking at the numerical summaries of the distributions. In those summaries, the average was slightly higher when the mother had completed high school. Thus these visualizations give us another tool to help communicate the predictions made by our model.</p>
<p><strong>5. Let’s practice</strong></p>
<p>Now let’s make some visualizations for some new predictions about song popularity using the Spotify data!</p>
</section>
<section id="format-prediction-posteriors" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="format-prediction-posteriors"><span class="header-section-number">4.8</span> Format prediction posteriors</h2>
<p>Now let’s plot some new predictions. In this exercise, we’ll predict how popular a song would be that was newly released and has a <code>song_age</code> of 0. We’re still predicting <code>popularity</code> from <code>song_age</code> and <code>artist_name</code>. The <code>new_predictions</code> object has already been created and contains the distributions for the predicted scores for a new song from Adele, Taylor Swift, and Beyoncé.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Print the predicted scores from the first 10 iterations of <code>new_predictions</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># View new data predictions</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>new_predictions[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Convert <code>new_predictions</code> to a data frame and name the columns of the data frame “Adele”, “Taylor Swift”, and “Beyoncé”.</li>
<li>Structure the data in long format, with only two columns: <code>artist_name</code> and <code>predict</code>.</li>
<li>Print the first six rows of the newly structured <code>plot_posterior</code> data frame.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># Load data</span></span>
<span id="cb39-2"><a href="#cb39-2"></a>new_predictions <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/new_predictions.rds"</span>)</span>
<span id="cb39-3"><a href="#cb39-3"></a></span>
<span id="cb39-4"><a href="#cb39-4"></a><span class="co"># Convert to data frame and rename variables</span></span>
<span id="cb39-5"><a href="#cb39-5"></a>new_predictions <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(new_predictions)</span>
<span id="cb39-6"><a href="#cb39-6"></a><span class="fu">colnames</span>(new_predictions) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Adele"</span>, <span class="st">"Taylor Swift"</span>, <span class="st">"Beyoncé"</span>)</span>
<span id="cb39-7"><a href="#cb39-7"></a></span>
<span id="cb39-8"><a href="#cb39-8"></a><span class="co"># Create tidy data structure</span></span>
<span id="cb39-9"><a href="#cb39-9"></a><span class="co"># plot_posterior &lt;- tidyr::gather(new_predictions, key = "artist_name", value = "predict")</span></span>
<span id="cb39-10"><a href="#cb39-10"></a>plot_posterior <span class="ot">&lt;-</span> tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(new_predictions, <span class="at">cols =</span>  <span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">"artist_name"</span>, <span class="at">values_to =</span> <span class="st">"predict"</span>)</span>
<span id="cb39-11"><a href="#cb39-11"></a></span>
<span id="cb39-12"><a href="#cb39-12"></a><span class="co"># Print formated data</span></span>
<span id="cb39-13"><a href="#cb39-13"></a><span class="fu">head</span>(plot_posterior)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! We now have a data frame with two columns: <code>artist_name</code> and <code>predict</code>. We can use this dataset to create of plot of the predicted popularity of songs for each artist that are brand new! We’ll make this plot in the next exercise.</p>
</section>
<section id="visualize-new-predictions" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="visualize-new-predictions"><span class="header-section-number">4.9</span> Visualize New Predictions</h2>
<p>Now that you’ve formatted the data, it’s time to create the plot of predicted popularity distributions. The <code>plot_posterior</code> data frame that you just created is already available in the environment. We will use that data frame to create a visualization to communicate the results of our analysis.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a ggplot graphic with <code>predict</code> on the x-axis</li>
<li>Wrap the graphic so that each <code>artist_name</code> gets its own plot</li>
<li>Keep all facets of the plot in one column</li>
<li>Draw a density curve for the predicted popularity</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Create plot of </span></span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="fu">ggplot</span>(plot_posterior, <span class="fu">aes</span>(<span class="at">x =</span> predict)) <span class="sc">+</span></span>
<span id="cb40-3"><a href="#cb40-3"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> artist_name, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb40-4"><a href="#cb40-4"></a>    <span class="fu">geom_density</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! You’ve create a great looking plot to visualize the predicted popularity of a new song from each of these artists. We can see that the distribution for Adele has the highest predicted popularity, followed by Taylor Swift, and then Beyoncé.</p>
</section>
<section id="conclusion" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4.10</span> Conclusion</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Conclusion</strong></p>
<p>Bayesian estimation of regressions models with rstanarm offers one solution to problem presented by inferences made with frequentist regression. I hope you have enjoyed this course and learning about how to implement a Bayesian model on your own.</p>
<p><strong>2. What we’ve learned</strong></p>
<p>In this course you’ve learned how to complete a Bayesian regression analysis from beginning to end. We started by learning how to estimate a Bayesian regression model, including the differences from a frequentist regression, and how Bayesian methods allow us to make inferences about the actual parameter values. We then explored how we can modify a Bayesian model by altering the size of our posterior distribution, changing priors, and altering the estimation algorithm.</p>
<p><strong>3. What we’ve learned</strong></p>
<p>We then learned about how to evaluate the fit of our model using the R-squared statistic, posterior predictive model checks, and model comparison. Finally, we looked how we can use our estimated model to make predictions and visualizations to communicate our results.</p>
<p><strong>4. What we’ve missed</strong></p>
<p>We’ve covered a lot in this course, but there is much more to learn. There are many topics that are important for Bayesian inference that we touched on, but were beyond the scope of this course. For example, the mathematics of posterior distribution calculations and LOO approximations, how to choose the best prior distribution, and the causes of estimation errors in the sampling algorithm. Andrew Gelman’s <em>Bayesian Data Analysis</em> gives a good overview of most of these topics. Information on the LOO approximation can be found in the documentation for the LOO package, along with accompanying resources and research articles. Finally, the Stan documentation and reference manual gives more details about the sampling algorithm, and how errors can arise.</p>
<p><strong>5. What comes next?</strong></p>
<p>This was just the beginning of Bayesian modeling. If you want to learn more, I encourage you to check out the other Bayesian data analysis courses here on DataCamp. In addition, the rstanarm website has many resources for estimating all different types of regression model including Poisson and logistic regression, and multi-level models. Finally, if you are interested in the more technical aspects of Bayesian estimation, I would highly recommend the Bayesian Data Analysis book by Andrew Gelman and his colleagues that we mentioned previously.</p>
<p><strong>6. Thank you!</strong></p>
<p>Thank you for following along with this course. I hope you continue to learn about Bayesian modeling and use it in your own work!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">27: Hyperparameter Tuning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" class="pagination-link">
        <span class="nav-page-text">29: Introduction to Spark</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Content 2022 by <a href="https://www.startupengineer.io/authors/schwarz/">Joschka Schwarz</a> <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with and <a href="https://quarto.org/">Quarto</a><br> <a href="https://www.github.com/jwarz/jwarz.github.io">View the source at GitHub</a></div>
  </div>
</footer>



<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>