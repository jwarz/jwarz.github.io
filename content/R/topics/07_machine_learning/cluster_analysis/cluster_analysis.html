<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joschka Schwarz">

<title>Joschka Schwarz - Cluster Analysis in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Joschka Schwarz</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-science" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-science">    
        <li>
    <a class="dropdown-item" href="../../../../../content/R/index.html">
 <span class="dropdown-text">R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/python/index.html">
 <span class="dropdown-text">Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/sql/index.html">
 <span class="dropdown-text">SQL</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../../slides/index.html">
 <span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../resumes/index.html">
 <span class="menu-text">Resumes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jwarz/"><i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/j-schwarz"><i class="bi bi-linkedin" role="img" aria-label="Quarto LinkedIn">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#calculating-distance-between-observations" id="toc-calculating-distance-between-observations" class="nav-link active" data-scroll-target="#calculating-distance-between-observations">1. Calculating distance between observations</a>
  <ul class="collapse">
  <li><a href="#what-is-cluster-analysis" id="toc-what-is-cluster-analysis" class="nav-link" data-scroll-target="#what-is-cluster-analysis">What is cluster analysis?</a></li>
  <li><a href="#when-to-cluster" id="toc-when-to-cluster" class="nav-link" data-scroll-target="#when-to-cluster">When to cluster?</a></li>
  <li><a href="#distance-between-two-observations" id="toc-distance-between-two-observations" class="nav-link" data-scroll-target="#distance-between-two-observations">Distance between two observations</a></li>
  <li><a href="#calculate-plot-the-distance-between-two-players" id="toc-calculate-plot-the-distance-between-two-players" class="nav-link" data-scroll-target="#calculate-plot-the-distance-between-two-players">Calculate &amp; plot the distance between two players</a></li>
  <li><a href="#using-the-dist-function" id="toc-using-the-dist-function" class="nav-link" data-scroll-target="#using-the-dist-function">Using the dist() function</a></li>
  <li><a href="#who-are-the-closest-players" id="toc-who-are-the-closest-players" class="nav-link" data-scroll-target="#who-are-the-closest-players">Who are the closest players?</a></li>
  <li><a href="#the-importance-of-scale" id="toc-the-importance-of-scale" class="nav-link" data-scroll-target="#the-importance-of-scale">The importance of scale</a></li>
  <li><a href="#effects-of-scale" id="toc-effects-of-scale" class="nav-link" data-scroll-target="#effects-of-scale">Effects of scale</a></li>
  <li><a href="#when-to-scale-data" id="toc-when-to-scale-data" class="nav-link" data-scroll-target="#when-to-scale-data">When to scale data?</a></li>
  <li><a href="#measuring-distance-for-categorical-data" id="toc-measuring-distance-for-categorical-data" class="nav-link" data-scroll-target="#measuring-distance-for-categorical-data">Measuring distance for categorical data</a></li>
  <li><a href="#calculating-distance-between-categorical-variables" id="toc-calculating-distance-between-categorical-variables" class="nav-link" data-scroll-target="#calculating-distance-between-categorical-variables">Calculating distance between categorical variables</a></li>
  <li><a href="#the-closest-observation-to-a-pair" id="toc-the-closest-observation-to-a-pair" class="nav-link" data-scroll-target="#the-closest-observation-to-a-pair">The closest observation to a pair</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering">2. Hierarchical clustering</a>
  <ul class="collapse">
  <li><a href="#comparing-more-than-two-observations" id="toc-comparing-more-than-two-observations" class="nav-link" data-scroll-target="#comparing-more-than-two-observations">Comparing more than two observations</a></li>
  <li><a href="#calculating-linkage" id="toc-calculating-linkage" class="nav-link" data-scroll-target="#calculating-linkage">Calculating linkage</a></li>
  <li><a href="#revisited-the-closest-observation-to-a-pair" id="toc-revisited-the-closest-observation-to-a-pair" class="nav-link" data-scroll-target="#revisited-the-closest-observation-to-a-pair">Revisited: The closest observation to a pair</a></li>
  <li><a href="#capturing-k-clusters" id="toc-capturing-k-clusters" class="nav-link" data-scroll-target="#capturing-k-clusters">Capturing K clusters</a></li>
  <li><a href="#assign-cluster-membership" id="toc-assign-cluster-membership" class="nav-link" data-scroll-target="#assign-cluster-membership">Assign cluster membership</a></li>
  <li><a href="#exploring-the-clusters" id="toc-exploring-the-clusters" class="nav-link" data-scroll-target="#exploring-the-clusters">Exploring the clusters</a></li>
  <li><a href="#validating-the-clusters" id="toc-validating-the-clusters" class="nav-link" data-scroll-target="#validating-the-clusters">Validating the clusters</a></li>
  <li><a href="#visualizing-the-dendrogram" id="toc-visualizing-the-dendrogram" class="nav-link" data-scroll-target="#visualizing-the-dendrogram">Visualizing the dendrogram</a></li>
  <li><a href="#comparing-average-single-complete-linkage" id="toc-comparing-average-single-complete-linkage" class="nav-link" data-scroll-target="#comparing-average-single-complete-linkage">Comparing average, single &amp; complete linkage</a></li>
  <li><a href="#height-of-the-tree" id="toc-height-of-the-tree" class="nav-link" data-scroll-target="#height-of-the-tree">Height of the tree</a></li>
  <li><a href="#cutting-the-tree" id="toc-cutting-the-tree" class="nav-link" data-scroll-target="#cutting-the-tree">Cutting the tree</a></li>
  <li><a href="#clusters-based-on-height" id="toc-clusters-based-on-height" class="nav-link" data-scroll-target="#clusters-based-on-height">Clusters based on height</a></li>
  <li><a href="#exploring-the-branches-cut-from-the-tree" id="toc-exploring-the-branches-cut-from-the-tree" class="nav-link" data-scroll-target="#exploring-the-branches-cut-from-the-tree">Exploring the branches cut from the tree</a></li>
  <li><a href="#what-do-we-know-about-our-clusters" id="toc-what-do-we-know-about-our-clusters" class="nav-link" data-scroll-target="#what-do-we-know-about-our-clusters">What do we know about our clusters?</a></li>
  <li><a href="#making-sense-of-the-clusters" id="toc-making-sense-of-the-clusters" class="nav-link" data-scroll-target="#making-sense-of-the-clusters">Making sense of the clusters</a></li>
  <li><a href="#segment-wholesale-customers" id="toc-segment-wholesale-customers" class="nav-link" data-scroll-target="#segment-wholesale-customers">Segment wholesale customers</a></li>
  <li><a href="#explore-wholesale-customer-clusters" id="toc-explore-wholesale-customer-clusters" class="nav-link" data-scroll-target="#explore-wholesale-customer-clusters">Explore wholesale customer clusters</a></li>
  <li><a href="#interpreting-the-wholesale-customer-clusters" id="toc-interpreting-the-wholesale-customer-clusters" class="nav-link" data-scroll-target="#interpreting-the-wholesale-customer-clusters">Interpreting the wholesale customer clusters</a></li>
  </ul></li>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering">3. K-means clustering</a>
  <ul class="collapse">
  <li><a href="#introduction-to-k-means" id="toc-introduction-to-k-means" class="nav-link" data-scroll-target="#introduction-to-k-means">Introduction to K-means</a></li>
  <li><a href="#k-means-on-a-soccer-field" id="toc-k-means-on-a-soccer-field" class="nav-link" data-scroll-target="#k-means-on-a-soccer-field">K-means on a soccer field</a></li>
  <li><a href="#k-means-on-a-soccer-field-part-2" id="toc-k-means-on-a-soccer-field-part-2" class="nav-link" data-scroll-target="#k-means-on-a-soccer-field-part-2">K-means on a soccer field (part 2)</a></li>
  <li><a href="#evaluating-different-values-of-k-by-eye" id="toc-evaluating-different-values-of-k-by-eye" class="nav-link" data-scroll-target="#evaluating-different-values-of-k-by-eye">Evaluating different values of K by eye</a></li>
  <li><a href="#many-ks-many-models" id="toc-many-ks-many-models" class="nav-link" data-scroll-target="#many-ks-many-models">Many K’s many models</a></li>
  <li><a href="#elbow-scree-plot" id="toc-elbow-scree-plot" class="nav-link" data-scroll-target="#elbow-scree-plot">Elbow (Scree) plot</a></li>
  <li><a href="#interpreting-the-elbow-plot" id="toc-interpreting-the-elbow-plot" class="nav-link" data-scroll-target="#interpreting-the-elbow-plot">Interpreting the elbow plot</a></li>
  <li><a href="#silhouette-analysis-observation-level-performance" id="toc-silhouette-analysis-observation-level-performance" class="nav-link" data-scroll-target="#silhouette-analysis-observation-level-performance">Silhouette analysis: observation level performance</a></li>
  <li><a href="#silhouette-analysis" id="toc-silhouette-analysis" class="nav-link" data-scroll-target="#silhouette-analysis">Silhouette analysis</a></li>
  <li><a href="#making-sense-of-the-k-means-clusters" id="toc-making-sense-of-the-k-means-clusters" class="nav-link" data-scroll-target="#making-sense-of-the-k-means-clusters">Making sense of the K-means clusters</a></li>
  <li><a href="#revisiting-wholesale-data-best-k" id="toc-revisiting-wholesale-data-best-k" class="nav-link" data-scroll-target="#revisiting-wholesale-data-best-k">Revisiting wholesale data: “Best” k</a></li>
  <li><a href="#revisiting-wholesale-data-exploration" id="toc-revisiting-wholesale-data-exploration" class="nav-link" data-scroll-target="#revisiting-wholesale-data-exploration">Revisiting wholesale data: Exploration</a></li>
  </ul></li>
  <li><a href="#case-study-national-occupational-mean-wage" id="toc-case-study-national-occupational-mean-wage" class="nav-link" data-scroll-target="#case-study-national-occupational-mean-wage">4. Case Study: National Occupational mean wage</a>
  <ul class="collapse">
  <li><a href="#occupational-wage-data" id="toc-occupational-wage-data" class="nav-link" data-scroll-target="#occupational-wage-data">Occupational wage data</a></li>
  <li><a href="#initial-exploration-of-the-data" id="toc-initial-exploration-of-the-data" class="nav-link" data-scroll-target="#initial-exploration-of-the-data">Initial exploration of the data</a></li>
  <li><a href="#hierarchical-clustering-occupation-trees" id="toc-hierarchical-clustering-occupation-trees" class="nav-link" data-scroll-target="#hierarchical-clustering-occupation-trees">Hierarchical clustering: Occupation trees</a></li>
  <li><a href="#hierarchical-clustering-preparing-for-exploration" id="toc-hierarchical-clustering-preparing-for-exploration" class="nav-link" data-scroll-target="#hierarchical-clustering-preparing-for-exploration">Hierarchical clustering: Preparing for exploration</a></li>
  <li><a href="#hierarchical-clustering-plotting-occupational-clusters" id="toc-hierarchical-clustering-plotting-occupational-clusters" class="nav-link" data-scroll-target="#hierarchical-clustering-plotting-occupational-clusters">Hierarchical clustering: Plotting occupational clusters</a></li>
  <li><a href="#reviewing-the-hc-results" id="toc-reviewing-the-hc-results" class="nav-link" data-scroll-target="#reviewing-the-hc-results">Reviewing the HC results</a></li>
  <li><a href="#k-means-elbow-analysis" id="toc-k-means-elbow-analysis" class="nav-link" data-scroll-target="#k-means-elbow-analysis">K-means: Elbow analysis</a></li>
  <li><a href="#k-means-average-silhouette-widths" id="toc-k-means-average-silhouette-widths" class="nav-link" data-scroll-target="#k-means-average-silhouette-widths">K-means: Average Silhouette Widths</a></li>
  <li><a href="#the-best-number-of-clusters" id="toc-the-best-number-of-clusters" class="nav-link" data-scroll-target="#the-best-number-of-clusters">The “best” number of clusters</a></li>
  <li><a href="#review-k-means-results" id="toc-review-k-means-results" class="nav-link" data-scroll-target="#review-k-means-results">Review K-means results</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jwarz/jwarz.github.io/edit/main/content/R/topics/07_machine_learning/cluster_analysis/cluster_analysis.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jwarz/jwarz.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Cluster Analysis in R</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joschka Schwarz </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p><strong>Short Description</strong></p>
<p>Develop a strong intuition for how hierarchical and k-means clustering work and learn how to apply them to extract insights from your data.</p>
<p><strong>Long Description</strong></p>
<p>Cluster analysis is a powerful toolkit in the data science workbench. It is used to find groups of observations (clusters) that share similar characteristics. These similarities can inform all kinds of business decisions; for example, in marketing, it is used to identify distinct groups of customers for which advertisements can be tailored. In this course, you will learn about two commonly used clustering methods - hierarchical clustering and k-means clustering. You won’t just learn how to use these methods, you’ll build a <strong> strong intuition</strong> for how they work and how to interpret their results. You’ll develop this intuition by exploring three different datasets: soccer player positions, wholesale customer spending data, and longitudinal occupational wage data.</p>
<section id="calculating-distance-between-observations" class="level1">
<h1>1. Calculating distance between observations</h1>
<p>Cluster analysis seeks to find groups of observations that are similar to one another, but the identified groups are different from each other. This similarity/difference is captured by the metric called distance. In this chapter, you will learn how to calculate the distance between observations for both continuous and categorical features. You will also develop an intuition for how the scales of your features can affect distance.</p>
<section id="what-is-cluster-analysis" class="level2">
<h2 class="anchored" data-anchor-id="what-is-cluster-analysis">What is cluster analysis?</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. What is cluster analysis?</strong></p>
<p>Hi, my name is Dima and I am very excited to have you join me in learning all about cluster analysis in R. Cluster analysis is a form of data exploration, and the key to harnessing its power lies in understanding how it works. So, in this course you won’t just learn the tools necessary to perform cluster analysis - that’s the easy part - I will work with you to build the intuition behind the underlying methods. But, before we get to the how, let’s take a moment to discuss, what is clustering?</p>
<p><strong>2. What is clustering?</strong></p>
<p>No matter whether you are working with medical data,</p>
<p><strong>3. What is clustering?</strong></p>
<p>retail data,</p>
<p><strong>4. What is clustering?</strong></p>
<p>or sports data, as a data scientist you are often presented with a bunch of data that you need to make sense of.</p>
<p><strong>5. What is clustering?</strong></p>
<p>To understand what clustering is, let’s put aside the details of our data and instead focus on the toy example</p>
<p><strong>6. What is clustering?</strong></p>
<p>where the data is represented as a matrix containing entries of card suits.</p>
<p><strong>7. What is clustering?</strong></p>
<p>To look at it another way, this matrix is composed of rows containing our observations and columns that tell us something that we measured across these observations.We will refer to these columns as the features of our observations. In cluster analysis we are interested in grouping our observations such that all members of a group are similar to one another and at the same time they are distinctly different from all members outside of this group.Imagine in this example we performed cluster analysis to find which observations are similar to one another based on what suit appears in each column.</p>
<p><strong>8. What is clustering?</strong></p>
<p>In this case we identified three groups and colored the observations accordingly.To better see these pattens, lets re-organize our observation into their respective colored clusters.</p>
<p><strong>9. What is clustering?</strong></p>
<p>Here we can start to see clear patterns that emerge. Fundamentally, this is how cluster analysis works.</p>
<p><strong>10. What is clustering?</strong></p>
<p>Or to put it another way, cluster analysis is a form of exploratory data analysis where observations are divided into meaningful groups that share common characteristics amongst each other. So what are the steps involved in performing cluster analysis?</p>
<p><strong>11. The flow of cluster analysis</strong></p>
<p>Well, first, you must make sure that your data is ready for clustering, meaning that your data does not have any missing values and that your features are on similar scales.</p>
<p><strong>12. The flow of cluster analysis</strong></p>
<p>Next, you must decide on what metric is appropriate to capture the similarity between your observations using the features that you have.</p>
<p><strong>13. The flow of cluster analysis</strong></p>
<p>Once you have calculated this you can use a clustering method to group your observations based on how similar they are to each other into clusters.</p>
<p><strong>14. The flow of cluster analysis</strong></p>
<p>But, most importantly you will need to analyze the output of these clusters to determine whether they provide any meaningful insight into your data. This often requires a deep understanding of the problem and the data that you are working with.</p>
<p><strong>15. The flow of cluster analysis</strong></p>
<p>As you can see in this flow chart, the analysis you perform on these clusters may require you to iterate on the clustering steps until you converge on a meaningful grouping of your data.</p>
<p><strong>16. Structure of this course</strong></p>
<p>The first three chapters of this course will help you unpack this process.In this chapter you will gain a deeper understanding of what it means for two observation to be similar - or more specifically ,dissimilar. You will also learn why the features of your data need to be comparable to one another.</p>
<p><strong>17. Structure of this course</strong></p>
<p>In chapters two and three you will learn how to use two commonly used clustering methods: hierarchical clustering and k-means clustering.At the end of these chapters and in chapter four you will work through two case studies where clustering analysis provides a unique perspective into the underlying data.</p>
<p><strong>18. Let’s learn!</strong></p>
<p>So, let’s begin!</p>
</section>
<section id="when-to-cluster" class="level2">
<h2 class="anchored" data-anchor-id="when-to-cluster">When to cluster?</h2>
<blockquote class="blockquote">
<h2 id="question" class="anchored"><em>Question</em></h2>
<p><strong>In which of these scenarios would clustering methods likely be appropriate?</strong><br>
<br> 1) Using consumer behavior data to identify distinct segments within a market. <br> 2) Predicting whether a given user will click on an ad.<br> 3) Identifying distinct groups of stocks that follow similar trading patterns.<br> 4) Modeling &amp; predicting GDP growth.<br> <br> ⬜ 1<br> ⬜ 2<br> ⬜ 4<br> ✅ 1 &amp; 3<br> ⬜ 2 &amp; 4<br></p>
</blockquote>
<p>That is correct, market segmentation and pattern grouping are both good examples where clustering is appropriate.</p>
<p>Coincidentally, you will get the chance to work on both of these types of problems in this course.</p>
</section>
<section id="distance-between-two-observations" class="level2">
<h2 class="anchored" data-anchor-id="distance-between-two-observations">Distance between two observations</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Distance between two observations</strong></p>
<p>Let’s begin by focusing on the question that is fundamental to all clustering analyses: How similar are two observations?</p>
<p><strong>2. Distance vs Similarity</strong></p>
<p>Or from another perspective, how dissimilar are they?</p>
<p><strong>3. Distance vs Similarity</strong></p>
<p>You see, most clustering methods measure similarity between observations using a dissmilarity metric, often referred to as the distance.These two concepts are just two sides of the same coin.If two observations have a large distance then they are less similar to one another. Likewise, if their distance value is small, then they are more similar.Naturally, we should first develop a keen intuition by what is meant by distance.</p>
<p><strong>4. Distance between two players</strong></p>
<p>So, let’s work with the scenario of players on a soccer field.</p>
<p><strong>5. Distance between two players</strong></p>
<p>In this image you see the positions of two players.How far apart are they? To answer this question we first need their coordinates.</p>
<p><strong>6. Distance between two players</strong></p>
<p>Here the blue player is positioned in the center of the field, which we will refer to as 0, 0. While the red player has a position of 12 and 9 - or twelve feet to the right of center and 9 feet up.</p>
<p><strong>7. Distance between two players</strong></p>
<p>The players in this case are our observations and their X and Y coordinates are the features of these observations. We can use these features to calculate the distance between these two players.In this case we will use a distance measurement you’re likely familiar with.</p>
<p><strong>8. Distance between two players</strong></p>
<p>Euclidean distance.</p>
<p><strong>9. Distance between two players</strong></p>
<p>Which is simply the hypotenuse of the triangle that is formed by the differences in the x and y coordinates of these players.</p>
<p><strong>10. Distance between two players</strong></p>
<p>The familiar formula to calculate this is shown here.</p>
<p><strong>11. Distance between two players</strong></p>
<p>Which if we plug in our values of x and y for both players we arrive at the euclidean distance between them.</p>
<p><strong>12. Distance between two players</strong></p>
<p>Which in this case is 15.This is the fundamental idea for calculating a measure of dissimilarity between the blue and red players.</p>
<p><strong>13. dist() function</strong></p>
<p>To do this in R, we use the dist function to calculate the euclidean distance between our observations. The function simply requires a dataframe or matrix containing your observations and features. In this case, we are working with the dataframe two players. The method by which the distance is calculated is provided by the method parameter. In this case we are using euclidean distance and specify it accordingly.As in our manual calculation we see that the distance between the red and blue players is 15.</p>
<p><strong>14. More than 2 observations</strong></p>
<p>This function becomes indespensable if we have more than 2 observations. In this case if we wanted to know the distance between 3 players we would measure the distance between the players two at a time. Running this through the dist function we see that the distance between players red and blue is 15 as before, but we also have measurements between green and blue as well as green and red. In this case, green and red have the smallest distance and hence are closest to one another.The dist function would work just as well if we have more features to use for calculating the distance.</p>
<p><strong>15. Let’s practice!</strong></p>
<p>Now, Let’s put what you’ve just learned into practice in the upcoming exercises.</p>
</section>
<section id="calculate-plot-the-distance-between-two-players" class="level2">
<h2 class="anchored" data-anchor-id="calculate-plot-the-distance-between-two-players">Calculate &amp; plot the distance between two players</h2>
<p>You’ve obtained the coordinates relative to the center of the field for two players in a soccer match and would like to calculate the distance between them.</p>
<p>In this exercise you will plot the positions of the 2 players and manually calculate the distance between them by using the Euclidean distance formula.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Plot their positions from the <code>two_players</code> data frame using <code>ggplot</code>.</li>
<li>Extract the positions of the players into two data frames <code>player1</code> and <code>player2</code>.</li>
<li>Calculate the distance between player1 and player2 by using the Euclidean distance formula</li>
</ol>
<p><span class="math display">\[\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\]</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load package</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(tibble)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Create two_players</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>two_players <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">15</span>),</span>
<span id="cb1-7"><a href="#cb1-7"></a>                      <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">10</span>))</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># Plot the positions of the players</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="fu">ggplot</span>(two_players, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> </span>
<span id="cb1-11"><a href="#cb1-11"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>  <span class="co"># Assuming a 40x60 field</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>  <span class="fu">lims</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">30</span>,<span class="dv">30</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">20</span>)) <span class="sc">+</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb1-15"><a href="#cb1-15"></a>  </span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co"># Split the players data frame into two observations </span></span>
<span id="cb1-17"><a href="#cb1-17"></a>player1 <span class="ot">&lt;-</span> two_players[<span class="dv">1</span>, ]</span>
<span id="cb1-18"><a href="#cb1-18"></a>player2 <span class="ot">&lt;-</span> two_players[<span class="dv">2</span>, ]</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="co"># Calculate and print their distance using the Euclidean Distance formula</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>player_distance <span class="ot">&lt;-</span> <span class="fu">sqrt</span>( (player1<span class="sc">$</span>x <span class="sc">-</span> player2<span class="sc">$</span>x)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (player1<span class="sc">$</span>y <span class="sc">-</span> player2<span class="sc">$</span>y)<span class="sc">^</span><span class="dv">2</span> )</span>
<span id="cb1-22"><a href="#cb1-22"></a>player_distance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent work! Using the formula is a great way to learn how distance is measured between two observations.</p>
</section>
<section id="using-the-dist-function" class="level2">
<h2 class="anchored" data-anchor-id="using-the-dist-function">Using the dist() function</h2>
<p>Using the Euclidean formula manually may be practical for 2 observations but can get more complicated rather quickly when measuring the distance between many observations.</p>
<p>The <code>dist()</code> function simplifies this process by calculating distances between our observations (rows) using their features (columns). In this case the observations are the player positions and the dimensions are their x and y coordinates.</p>
<p><em>Note: The default distance calculation for the <code>dist()</code> function is Euclidean distance</em></p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the distance between two players using the <code>dist()</code> function for the data frame <code>two_players</code></li>
<li>Calculate the distance between three players for the data frame <code>three_players</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Calculate the Distance Between two_players</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>dist_two_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(two_players)</span>
<span id="cb2-3"><a href="#cb2-3"></a>dist_two_players</span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co"># Calculate the Distance Between three_players</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>three_players <span class="ot">&lt;-</span> two_players <span class="sc">|&gt;</span> </span>
<span id="cb2-7"><a href="#cb2-7"></a>                  <span class="fu">add_row</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">20</span>)</span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a>dist_three_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(three_players)</span>
<span id="cb2-10"><a href="#cb2-10"></a>dist_three_players</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dist() function makes life easier when working with many dimensions and observations.</p>
</section>
<section id="who-are-the-closest-players" class="level2">
<h2 class="anchored" data-anchor-id="who-are-the-closest-players">Who are the closest players?</h2>
<p>You are given the data frame containing the positions of 4 players on a soccer field.</p>
<p>This data is preloaded as <code>four_players</code> in your environment and is displayed below.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;">Player</th>
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: right;">-5</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<h2 id="question-1" class="anchored"><em>Question</em></h2>
<p><strong>Which two players are closest to one another?</strong><br> <br> ⬜ 1 &amp; 2<br> ⬜ 1 &amp; 3<br> ✅ 1 &amp; 4<br> ⬜ 2 &amp; 3<br> ⬜ 2 &amp; 4<br> ⬜ 3 &amp; 4<br> ⬜ Not enough information to decide<br></p>
</blockquote>
<p>That is correct! Players 1 and 4 are the closest to one another.</p>
</section>
<section id="the-importance-of-scale" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-scale">The importance of scale</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. The importance of scale</strong></p>
<p>When calculating the distance between two players on a soccer field, you used two features, x and y. Both of these features are the coordinates of the players and both are measured in the same manner. Because of this, they are comparable to one another and can be used together to calculate the euclidean distance between the players. But, what happens when the features aren’t measured in the same manner or to put it another way, when the values of these features aren’t comparable to one another?To answer this question let’s walk through an example.</p>
<p><strong>2. Distance between individuals</strong></p>
<p>Imagine you are provided with a dataset that contains the heights and weights for a large number of men in the United States. The height feature is measured in feet and the weight feature in pounds. You are interested in calculating the distance between these individuals.Let us start by comparing observations one and two.</p>
<p><strong>3. Distance between individuals</strong></p>
<p>Both men are the same height, six feet. But they differ slightly in weight. In this case the difference is two pounds.</p>
<p><strong>4. Distance between individuals</strong></p>
<p>If we calculated the euclidean distance between them we would get a value of two. Now let’s look at observations one and three.</p>
<p><strong>5. Distance between individuals</strong></p>
<p>In this comparison, the weights are the same, but the height is different by two feet. If we calculate the distance once more…</p>
<p><strong>6. Distance between individuals</strong></p>
<p>…you guessed it. It’s also two.</p>
<p><strong>7. Distance between individuals</strong></p>
<p>The distances between both pairs are identical.If we saw these three men standing side by side, would you really believe that observation one is just as similar to three as it is to two. Of course not.Then why are their distances the same? This happens because these features are on different scales. Meaning they have different averages and different expected variability. While in these comparisons these features only vary by a magnitude of two, we intuitively know that a change in two pounds is very different than a change of two feet.So how can we adjust these features to calculate a distance that better aligns with our expectations?</p>
<p><strong>8. Scaling our features</strong></p>
<p>To do this we need to convert our features to be on a similar scale with one another.There are various methods for doing this, but for this course we will use the method called standardization.This entails updating each measurement for a feature by subtracting the average value of that feature and then dividing by its standard deviation. Doing this across our features places them on a similar scale where each feature has a mean of zero and a standard deviation of one.</p>
<p><strong>9. Distance between individuals</strong></p>
<p>Going back to the previous scenario, we can use the mean and standard deviation of the height and weight features to standardize the values for our three observations. Now, if we calculate the euclidean distances between them…</p>
<p><strong>10. Distance between individuals</strong></p>
<p>Voila, the values make sense! They agree with our intuition. One and three are much less similar to one another than one and two.</p>
<p><strong>11. scale() function</strong></p>
<p>In R we can use the scale function to standardize height and weight to the same scale.If height_weight is our matrix of observations, similar to what we’ve just seen. Using the scale function with the default parameters will normalize each feature column to a mean of 0 and a variance of 1.</p>
<p><strong>12. Let’s practice!</strong></p>
<p>In the next exercise you will have a chance to further explore how scales can affect your ability to interpret the distance value.</p>
</section>
<section id="effects-of-scale" class="level2">
<h2 class="anchored" data-anchor-id="effects-of-scale">Effects of scale</h2>
<p>You have learned that when a variable is on a larger scale than other variables in your data it may disproportionately influence the resulting distance calculated between your observations. Lets see this in action by observing a sample of data from the <code>trees</code> data set.</p>
<p>You will leverage the <code>scale()</code> function which by default centers &amp; scales our column features.</p>
<p>Our variables are the following:</p>
<ul>
<li><strong>Girth</strong> - tree diameter in inches</li>
<li><strong>Height</strong> - tree height in inches</li>
</ul>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the distance matrix for the data frame <code>three_trees</code> and store it as <code>dist_trees</code></li>
<li>Create a new variable <code>scaled_three_trees</code> where the <code>three_trees</code> data is centered &amp; scaled</li>
<li>Calculate and print the distance matrix for <code>scaled_three_trees</code> and store this as <code>dist_scaled_trees</code></li>
<li>Output both <code>dist_trees</code> and <code>dist_scaled_trees</code> matrices and observe the change of which observations have the smallest distance between the two matrices <em>(hint: they have changed)</em></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>three_trees <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">Girth  =</span> <span class="fu">c</span>(<span class="fl">8.3</span>, <span class="fl">8.6</span>, <span class="fl">10.5</span>),</span>
<span id="cb3-2"><a href="#cb3-2"></a>                      <span class="at">Height =</span> <span class="fu">c</span>(<span class="dv">840</span>, <span class="dv">780</span>, <span class="dv">864</span>))</span>
<span id="cb3-3"><a href="#cb3-3"></a>                  </span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># Calculate distance for three_trees </span></span>
<span id="cb3-5"><a href="#cb3-5"></a>dist_trees <span class="ot">&lt;-</span> <span class="fu">dist</span>(three_trees)</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co"># Scale three trees &amp; calculate the distance  </span></span>
<span id="cb3-8"><a href="#cb3-8"></a>scaled_three_trees <span class="ot">&lt;-</span> <span class="fu">scale</span>(three_trees)</span>
<span id="cb3-9"><a href="#cb3-9"></a>dist_scaled_trees  <span class="ot">&lt;-</span> <span class="fu">dist</span>(scaled_three_trees)</span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co"># Output the results of both Matrices</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="fu">print</span>(<span class="st">'Without Scaling'</span>)</span>
<span id="cb3-13"><a href="#cb3-13"></a>dist_trees</span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="fu">print</span>(<span class="st">'With Scaling'</span>)</span>
<span id="cb3-16"><a href="#cb3-16"></a>dist_scaled_trees</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that before scaling observations 1 &amp; 3 were the closest but after scaling observations 1 &amp; 2 turn out to have the smallest distance.</p>
</section>
<section id="when-to-scale-data" class="level2">
<h2 class="anchored" data-anchor-id="when-to-scale-data">When to scale data?</h2>
<blockquote class="blockquote">
<h2 id="question-2" class="anchored"><em>Question</em></h2>
<p>Below are examples of datasets and their corresponding features.<br> <br> In which of these examples would scaling <strong>not</strong> be necessary?<br> <br> ⬜ Taxi Trips - <code>tip earned ($)</code>, <code>distance traveled (km)</code>.<br> ⬜ Health Measurements of Individuals - <code>height (meters)</code>, <code>weight (grams)</code>, <code>body fat percentage (%)</code>.<br> ⬜ Student Attributes - <code>average test score (1-100)</code>, <code>distance from school (km)</code>, <code>annual household income ($)</code>.<br> ⬜ Salespeople Commissions - <code>total yearly commision ($)</code>, <code>number of trips taken</code>.<br> ✅ None of the above, they all should be scaled when measuring distance.<br></p>
</blockquote>
<p>Correct! In all of these cases it would be a good idea to scale your features.</p>
</section>
<section id="measuring-distance-for-categorical-data" class="level2">
<h2 class="anchored" data-anchor-id="measuring-distance-for-categorical-data">Measuring distance for categorical data</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Measuring distance for categorical data</strong></p>
<p>So far you have exclusively worked with one type distance metric, the euclidean distance. This is a commonly used metric and is a great starting point when working with data that is continuous. But what happens if the data you have isn’t continuous but is categorical?</p>
<p><strong>2. Binary data</strong></p>
<p>Let’s start with the most basic case of categorical features, those that are binary, meaning that the values can only be one of two possiblities. Here you are presented with survey data, let’s call it survey a. The participants of this survey were asked whether they enjoy drinking various types of alcoholic beverages. Since they can only answer yes or no we can code this binary response as TRUE or FALSE.We would be interested to learn which participants are similar to one another based on their responses. To calculate this we will use the similarity score called the Jaccard Index.</p>
<p><strong>3. Jaccard index</strong></p>
<p>This measure of similarity captures the ratio between the intersection of A and B to the union of A and B.Or more intuitively the ratio between the number of times the features of both observations are TRUE to the number of times they are ever TRUE.So going back to the previous example.</p>
<p><strong>4. Calculating Jaccard distance</strong></p>
<p>Let us calculate the Jaccard similarity for two observations one and two. They only agree in one category, beer, so for the intersection we get the value of one. While the number of categories these observations are ever true, or the union, is four.Dividing the intersection by the union we get the Jaccard similarity value of 0-point-25.But what about the distance. Well remember that distance is 1 - similarity, so in this case the distance is just 0-point-75.</p>
<p><strong>5. Calculating Jaccard distance in R</strong></p>
<p>To learn how to do this in R lets start with a subset of our data containing three observations, called survey a.In order to calculate the Jaccard distance between all three observations you just need to specify that the distance method to use in the dist() function is binary.You can see that just like our manual calculation earlier, observations 1 and 2 have a distance of 0-point-75.Now let’s expand this idea to a broader case of categorical data where we have features represented by more than two categories.</p>
<p><strong>6. More than two categories</strong></p>
<p>For survey b, we have gathered the favorite color and sport for our participants. For color their choices were red blue and green and for sport the decision was between soccer and hockey. To calculate the distance between these observations we need to represent the presence or absence of each category in a process known as dummification. Essentially we consider each feature-value pair and encode its presence or absence as a 1 or 0, which is equivalent to a TRUE or FALSE. Take a look at observation one whose favorite color was red and favorite sport is soccer. After we dummify our data, shown in the table on the right, this observation now has a value of zero for every dummified feature except for the color red and the sport soccer where the value is one.Once our data is dummified, its just a matter or calculating the Jaccard distance between the observations.</p>
<p><strong>7. Dummification in R</strong></p>
<p>To perform this preliminary step in R, we would use the dummy-dot-data-dot-frame function from the dummy library. So long as your categorical values are encoded as factors this function will convert them into binary feature value representations.</p>
<p><strong>8. Generalizing categorical distance in R</strong></p>
<p>We can leverage this to calculate the distance for our data. In this case we can see that observations 2 and 3, 1 and 4 and 3 and 4 all have a comparable distance, to one another. Which makes sense if you look back at the original data.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>Now you have the tools to handle both continuous and categorical data types. Let’s practice what you’ve learned.</p>
</section>
<section id="calculating-distance-between-categorical-variables" class="level2">
<h2 class="anchored" data-anchor-id="calculating-distance-between-categorical-variables">Calculating distance between categorical variables</h2>
<p>In this exercise you will explore how to calculate binary (Jaccard) distances. In order to calculate distances we will first have to dummify our categories using the <code>dummy.data.frame()</code> from the library <code>dummies</code></p>
<p>You will use a small collection of survey observations stored in the data frame <code>job_survey</code> with the following columns:</p>
<ul>
<li><strong>job_satisfaction</strong> Possible options: “Hi”, “Mid”, “Low”</li>
<li><strong>is_happy</strong> Possible options: “Yes”, “No”</li>
</ul>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a dummified data frame <code>dummy_survey</code></li>
<li>Generate a Jaccard distance matrix for the dummified survey data <code>dist_survey</code> using the <code>dist()</code> function using the parameter <code>method = 'binary'</code></li>
<li>Print the original data and the distance matrixthe observations with a distance of 0 in the original data (1, 2, and 4)</li>
<li>Note the observations with a distance of 0 in the original data (1, 2, and 4)</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Data</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>job_survey <span class="ot">&lt;-</span> <span class="fu">data.frame</span>( <span class="co"># Does not work with tibble</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>  </span>
<span id="cb4-4"><a href="#cb4-4"></a>  <span class="at">job_satisfaction =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Hi"</span>, <span class="dv">4</span>), <span class="st">"Mid"</span>),</span>
<span id="cb4-5"><a href="#cb4-5"></a>  <span class="at">is_happy         =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"No"</span>, <span class="dv">3</span>), <span class="st">"Yes"</span>, <span class="st">"No"</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>  </span>
<span id="cb4-7"><a href="#cb4-7"></a>)</span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="co"># Load package</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="fu">library</span>(dummies)</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="co"># Dummify the Survey Data</span></span>
<span id="cb4-13"><a href="#cb4-13"></a>dummy_survey <span class="ot">&lt;-</span> <span class="fu">dummy.data.frame</span>(job_survey)</span>
<span id="cb4-14"><a href="#cb4-14"></a></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co"># Calculate the Distance</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>dist_survey <span class="ot">&lt;-</span> <span class="fu">dist</span>(dummy_survey, <span class="at">method =</span> <span class="st">'binary'</span>)</span>
<span id="cb4-17"><a href="#cb4-17"></a></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co"># Print the Original Data</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>job_survey</span>
<span id="cb4-20"><a href="#cb4-20"></a></span>
<span id="cb4-21"><a href="#cb4-21"></a><span class="co"># Print the Distance Matrix</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>dist_survey</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! <br> Notice that this distance metric successfully captured that observations 1 and 2 are identical (distance of 0)</p>
</section>
<section id="the-closest-observation-to-a-pair" class="level2">
<h2 class="anchored" data-anchor-id="the-closest-observation-to-a-pair">The closest observation to a pair</h2>
<p>Below you see a pre-calculated distance matrix between four players on a soccer field. You can clearly see that players <strong>1</strong> &amp; <strong>4</strong> are the closest to one another with a Euclidean distance value of <font color="red">10</font>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">11.7</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">16.8</td>
<td style="text-align: right;">18.0</td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: right;">4</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">20.6</td>
<td style="text-align: right;">15.8</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<h2 id="question-3" class="anchored"><em>Question</em></h2>
<p><strong>If 1 and 4 are the closest players among the four, which player is closest to players <em>1</em> and <em>4</em>?</strong><br> <br> ⬜ Clearly its player 2!<br> ⬜ No! Player 3 makes more sense.<br> ✅ Are you kidding me? There isn’t enough information to decide.<br></p>
</blockquote>
<p>Great job! We clearly don’t have enough information to make this decision without knowing how we compare one observation to a pair of observations.</p>
<p>The decision required is known as the <strong>linkage method</strong> and which you will learn about in the next chapter!</p>
</section>
</section>
<section id="hierarchical-clustering" class="level1">
<h1>2. Hierarchical clustering</h1>
<p>This chapter will help you answer the last question from chapter 1 - how do you find groups of similar observations (clusters) in your data using the distances that you have calculated? You will learn about the fundamental principles of hierarchical clustering - the linkage criteria and the dendrogram plot - and how both are used to build clusters. You will also explore data from a wholesale distributor in order to perform market segmentation of clients using their spending habits.</p>
<section id="comparing-more-than-two-observations" class="level2">
<h2 class="anchored" data-anchor-id="comparing-more-than-two-observations">Comparing more than two observations</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Comparing more than two observations</strong></p>
<p>At the end of chapter 1 you were asked to review a question that you may not have known how to answer. Let’s start by revisiting this question.</p>
<p><strong>2. The closest observation to a pair</strong></p>
<p>You were presented with a distance matrix that contained the euclidean distances between four soccer players. You know that the closest two players are 1 and 4 with a distance value of 10.In order to cluster more than two observations together you need to determine which of these statements are true.Is observation 2 closest to the newly formed group 1, 4?Or is it observation 3?</p>
<p><strong>3. Linkage criteria: complete</strong></p>
<p>To answer this question you must decide on how to measure the distance from group 1-4 to these observations. One approach we can take is to measure the maximum distance of each observation to the two members of the group. To calculate this aggregated distance between observation two and group 1-4 we would get take the larger of the two distances from 2 to 1 and 2 to 4. The distance from 2 to 1 is 11-point-7 and the distance from 2 to 4 is 20-point-6. The larger of the two values is of course 20-point-6 and hence is our maximum distance.We can apply the same logic when comparing observation 3. Resulting in a maximum distance of 16-point-8.Using this approach we can say that based on the maximum distance, observation three is closer to group 1-4.</p>
<p><strong>4. Hierarchical clustering</strong></p>
<p>Hierarchical clustering is just a continuation of this approach. This clustering method iteratively groups the observations based on their pairwise distances until every observation is linked into one large group.The decision of how to select the closest observation to an existing group is called the linkage criteria. In the previous example we decided that observation three was the closest based on the maximum distance between it and group 1-4. The approach we used is formally called the complete linkage criteria.</p>
<p><strong>5. Grouping with linkage &amp; distance</strong></p>
<p>Let’s see the hierarchical clustering method in action using a visual representation.</p>
<p><strong>6. Grouping with linkage &amp; distance</strong></p>
<p>The distances between the four players have already been calculated and are shown.</p>
<p><strong>7. Grouping with linkage &amp; distance</strong></p>
<p>We know that players 1 and 4 have the shortest distance and will be grouped first.</p>
<p><strong>8. Grouping with linkage &amp; distance</strong></p>
<p>We are now presented with three options: add player 2 to group 1-4, add player 3 to group 1-4 or start a new group for players 2 and 3. The decision will be made based on which option results in the smallest distance.</p>
<p><strong>9. Grouping with linkage &amp; distance</strong></p>
<p>As before, 2 and 3 have a distance of 18.</p>
<p><strong>10. Grouping with linkage &amp; distance</strong></p>
<p>To calculate the distance between players 2 and group 1-4 we will use the complete linkage method, which is the maximum of the distances between observation two and each member of group 1-4. The resulting linkage-based distance is 20-point-6.</p>
<p><strong>11. Grouping with linkage &amp; distance</strong></p>
<p>Applying the same for player 3 we get a linkage distance of 16-point-8.</p>
<p><strong>12. Grouping with linkage &amp; distance</strong></p>
<p>Of these three options, the grouping of player 3 with 1 and 4 is selected because it has the smallest distance value.</p>
<p><strong>13. Grouping with linkage &amp; distance</strong></p>
<p>The next round of grouping doesn’t require any decision making, we simply aggregate observation two with group 1-3-4.</p>
<p><strong>14. Grouping with linkage &amp; distance</strong></p>
<p>Now you have an iterative binary grouping of your four observations. The order in which these observations are grouped generates a hierarchy based on distance, and hence is called hierarchical clustering.</p>
<p><strong>15. Linkage criteria</strong></p>
<p>There are many different linkage methods that have been developed but for this course you will focus on the three most commonly used ones.Complete linkage, which we’ve learned is the maximum distance between two sets. Single linkage, which is the minimum distance. And average linkage, which - you guessed it - is the average distance between two sets. As you progress through this chapter you will have a chance to see the impact this decision can make in the final clustering.</p>
<p><strong>16. Let’s practice!</strong></p>
<p>Let’s proceed with some exercises.</p>
</section>
<section id="calculating-linkage" class="level2">
<h2 class="anchored" data-anchor-id="calculating-linkage">Calculating linkage</h2>
<p>Let us revisit the example with three players on a field. The distance matrix between these three players is shown below and is available as the variable <code>dist_players</code>.</p>
<p>From this we can tell that the first group that forms is between players <strong>1</strong> &amp; <strong>2</strong>, since they are the closest to one another with a Euclidean distance value of <font color="red">11</font>.</p>
<p>Now you want to apply the three linkage methods you have learned to determine what the distance of this group is to player <strong>3</strong>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">18</td>
</tr>
</tbody>
</table>
<p><strong>Steps</strong></p>
<ol type="1">
<li><p>Calculate the distance from player 3 to the group of players 1 &amp; 2 using the following three linkage methods.</p>
<ul>
<li><strong>Complete:</strong> the resulting distance is based on the maximum.</li>
<li><strong>Single:</strong> the resulting distance is based on the minimum.</li>
<li><strong>Average:</strong> the resulting distance is based on the average.</li>
</ul></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>dist_players <span class="ot">&lt;-</span> dist_three_players</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># Extract the pair distances</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>distance_1_2 <span class="ot">&lt;-</span> dist_players[<span class="dv">1</span>]</span>
<span id="cb5-5"><a href="#cb5-5"></a>distance_1_3 <span class="ot">&lt;-</span> dist_players[<span class="dv">2</span>]</span>
<span id="cb5-6"><a href="#cb5-6"></a>distance_2_3 <span class="ot">&lt;-</span> dist_players[<span class="dv">3</span>]</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># Calculate the complete distance between group 1-2 and 3</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>complete <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">c</span>(distance_1_3, distance_2_3))</span>
<span id="cb5-10"><a href="#cb5-10"></a>complete</span>
<span id="cb5-11"><a href="#cb5-11"></a></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co"># Calculate the single distance between group 1-2 and 3</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>single <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="fu">c</span>(distance_1_3, distance_2_3))</span>
<span id="cb5-14"><a href="#cb5-14"></a>single</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co"># Calculate the average distance between group 1-2 and 3</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>average <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">c</span>(distance_1_3, distance_2_3))</span>
<span id="cb5-18"><a href="#cb5-18"></a>average</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! <br> Now you have all the knowledge you need to tackle exercise 12 from chapter 1.</p>
</section>
<section id="revisited-the-closest-observation-to-a-pair" class="level2">
<h2 class="anchored" data-anchor-id="revisited-the-closest-observation-to-a-pair">Revisited: The closest observation to a pair</h2>
<p><strong>You are now ready to answer this question!</strong></p>
<p>Below you see a pre-calculated distance matrix between four players on a soccer field. You can clearly see that players <strong>1</strong> &amp; <strong>4</strong> are the closest to one another with a Euclidean distance value of <font color="red">10</font>. This distance matrix is available for your exploration as the variable <code>dist_players</code></p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">11.7</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">16.8</td>
<td style="text-align: right;">18.0</td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: right;">4</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">20.6</td>
<td style="text-align: right;">15.8</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<h2 id="question-4" class="anchored"><em>Question</em></h2>
<p><strong>If 1 and 4 are the closest players among the four, which player is closest to players <em>1</em> and <em>4</em>?</strong><br> <br> ✅ Complete Linkage: Player 3, <br> Single &amp; Average Linkage: Player 2<br> ⬜ Complete Linkage: Player 2, <br> Single &amp; Average Linkage: Player 3<br> ⬜ Player 2 using Complete, Single &amp; Average Linkage methods<br> ⬜ Player 3 using Complete, Single &amp; Average Linkage methods<br></p>
</blockquote>
<p>This is correct, you can see that the choice of the linkage method can drastically change the result of this question.</p>
</section>
<section id="capturing-k-clusters" class="level2">
<h2 class="anchored" data-anchor-id="capturing-k-clusters">Capturing K clusters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Capturing K clusters</strong></p>
<p>In the last few exercises you explored the ways in which it is possible to group multiple observations together using linkage analysis. Now you are ready to leverage this technique to group your observations into a predefined number of clusters.So let’s revisit the soccer example with a few more players.</p>
<p><strong>2. Grouping soccer players</strong></p>
<p>In this case you have the positions of six players at the start of a game and you would like to infer which players belong to which team using hierarchical clustering.</p>
<p><strong>3. Grouping soccer players</strong></p>
<p>A euclidean distance matrix was calculated for each pair of players and is now used to group players using a complete linkage criteria.This algorithm iteratively proceeds to group the players until they are all under a single group like so…</p>
<p><strong>4. Grouping soccer players</strong></p>
<p><strong>5. Grouping soccer players</strong></p>
<p><strong>6. Grouping soccer players</strong></p>
<p><strong>7. Grouping soccer players</strong></p>
<p>Once this is completed we can work backwards to capture a desired number of clusters.At this moment, there is just one cluster.</p>
<p><strong>8. Extracting 2 clusters</strong></p>
<p>If we remove the last grouping like so.</p>
<p><strong>9. Grouping soccer players</strong></p>
<p>We have two distinct clusters.</p>
<p><strong>10. Grouping soccer players</strong></p>
<p>The red cluster contains players five and six while the blue cluster contains players one through four.Just like peeling an onion we can further split this into more parts by removing the previous linkage grouping.</p>
<p><strong>11. Grouping soccer players</strong></p>
<p>In this case it was group 1, 2 and 4 linked to player 3.</p>
<p><strong>12. Grouping soccer players</strong></p>
<p>And now we have three distinct clusters (red, blue, and green).So, the process of identifying a pre-defined number of clusters, which we will refer to as k is as simple as undoing the last k-1 steps of the linkage grouping.Now let’s learn how to do this in R.</p>
<p><strong>13. Hierarchical clustering in R</strong></p>
<p>The positions of the players are available in the data frame called players.As before, to get the euclidean distance between each pair of players we use the dist function.To perform the linkage steps we will use the hclust function which accepts a distance matrix, in our case dist_players and a linkage method. The default linkage method is the complete method. This results in a hclust object containing the linkage steps and can now be used to extract clusters.</p>
<p><strong>14. Extracting K clusters</strong></p>
<p>In order to determine which observations belong to which cluster, we use the cutree function. In this case we want to have two clusters because we know that there are two teams. So we provide the function with an hclust object and specify that we want a k of two. The output of cutree is a vector which represents which cluster each observation belongs to.We can append this back to our original data frame to do further analysis with the now clustered observations.</p>
<p><strong>15. Visualizing K Clusters</strong></p>
<p>One way we can analyze the clustering result is to plot the positions of these players and color the points based on their cluster assignment. Here we do this using ggplot.Remember that this clustering incorporated several decisions,the distance metric used was euclidean,the linkage metric used was complete and the k was 2. Changing any of these may, and likely will, impact the resulting clusters. This is why it is crucial to analyze the results to see if they actually make sense. For example in this case, the cluster analysis was aimed at identifying the teams to which the players belong to based on their positions at the start of the game. Since soccer games have the same number of players on each team, we know that the results of this clustering are incorrect and would need to consider a different distance or linkage criteria.Incorporating an understanding of your data and your problem into clustering analysis is the key to successfully leveraging this tool.</p>
<p><strong>16. Let’s practice!</strong></p>
<p>So, let’s do just that with some exercises.</p>
</section>
<section id="assign-cluster-membership" class="level2">
<h2 class="anchored" data-anchor-id="assign-cluster-membership">Assign cluster membership</h2>
<p>In this exercise you will leverage the <code>hclust()</code> function to calculate the iterative linkage steps and you will use the <code>cutree()</code> function to extract the cluster assignments for the desired number (<code>k</code>) of clusters.</p>
<p>You are given the positions of 12 players at the start of a 6v6 soccer match. This is stored in the <code>lineup</code> data frame.</p>
<p>You know that this match has two teams (k = 2), let’s use the clustering methods you learned to assign which team each player belongs in based on their position.</p>
<p><strong>Notes:</strong></p>
<ul>
<li>The linkage method can be passed via the <strong>method</strong> parameter: <code>hclust(distance_matrix, method = "complete")</code></li>
<li>Remember that in soccer opposing teams start on their half of the field.</li>
<li>Because these positions are measured using the same scale we do not need to re-scale our data.</li>
</ul>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the Euclidean distance matrix <code>dist_players</code> among all twelve players</li>
<li>Perform the <strong>complete</strong> linkage calculation for hierarchical clustering using <code>hclust</code> and store this as <code>hc_players</code></li>
<li>Build the cluster assignment vector <code>clusters_k2</code> using <code>cutree()</code> with a <code>k = 2</code></li>
<li>Append the cluster assignments as a column <code>cluster</code> to the <code>lineup</code> data frame and save the results to a new data frame called <code>lineup_k2_complete</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Load package</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Load data</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>lineup <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/lineup.rds"</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co"># Calculate the Distance</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>dist_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(lineup)</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="co"># Perform the hierarchical clustering using the complete linkage</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>hc_players <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co"># Calculate the assignment vector with a k of 2</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>clusters_k2 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_players, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co"># Create a new data frame storing these results</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>lineup_k2_complete <span class="ot">&lt;-</span> <span class="fu">mutate</span>(lineup, <span class="at">cluster =</span> clusters_k2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Fantastic job! In the next exercise we will explore this result.</p>
</section>
<section id="exploring-the-clusters" class="level2">
<h2 class="anchored" data-anchor-id="exploring-the-clusters">Exploring the clusters</h2>
<p>Because clustering analysis is always in part <strong>qualitative</strong>, it is incredibly important to have the necessary tools to explore the results of the clustering.</p>
<p>In this exercise you will explore that data frame you created in the previous exercise <code>lineup_k2_complete</code>.</p>
<p><strong>Reminder:</strong> The <code>lineup_k2_complete</code> data frame contains the x &amp; y positions of 12 players at the start of a 6v6 soccer game to which you have added clustering assignments based on the following parameters:</p>
<ul>
<li>Distance: <em>Euclidean</em></li>
<li>Number of Clusters (k): <em>2</em></li>
<li>Linkage Method: <em>Complete</em></li>
</ul>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Using <code>count()</code> from dplyr, count the number of players assigned to each cluster.</li>
<li>Using <code>ggplot()</code>, plot the positions of the players and color them by cluster assignment.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Count the cluster assignments</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">count</span>(lineup_k2_complete, cluster)</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="co"># Plot the positions of the players and color them using their cluster</span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="fu">ggplot</span>(lineup_k2_complete, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You’re doing great! <br> Think carefully about whether these results make sense to you and why.</p>
</section>
<section id="validating-the-clusters" class="level2">
<h2 class="anchored" data-anchor-id="validating-the-clusters">Validating the clusters</h2>
<p>In the plot below you see the clustering results of the same lineup data you’ve previously worked with but with some minor modifications in the clustering steps.</p>
<ul>
<li>The <strong>left plot</strong> was generated using a <code>k=2</code> and <code>method = 'average'</code></li>
<li>The <strong>right plot</strong> was generated using a <code>k=3</code> and <code>method = 'complete'</code></li>
</ul>
<p><img src="http://s3.amazonaws.com/assets.datacamp.com/production/course_5592/datasets/c2_e7_example.png" alt=""></p>
<blockquote class="blockquote">
<h2 id="question-5" class="anchored"><em>Question</em></h2>
<p><strong>If our goal is to correctly assign each player to their correct team then based on what you see in the above plot and what you know about the data set which of the statements below are correct?</strong><br> <br> ⬜ The <strong>left plot</strong> successfully clusters the players in their correct team.<br> ⬜ The <strong>right plot</strong> successfully clusters the players in their correct team.<br> ⬜ The <strong>left plot</strong> fails to correctly cluster the players; <br> because this is a 6v6 game the expection is that both clusters should have 6 members each.<br> ⬜ The <strong>right plot</strong> fails to correctly cluster the players; <br> because this is a two team match clustering into three unequal groups does not address the question correctly.<br> ✅ Answers 3 &amp; 4 are both correct.<br></p>
</blockquote>
<p>Exactly! Both the results in the left and the right plots can be deemed incorrect based on what we expect from our data.</p>
</section>
<section id="visualizing-the-dendrogram" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-dendrogram">Visualizing the dendrogram</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Visualizing the dendrogram</strong></p>
<p>As you recently learned, the process of hierarchical clustering involves iteratively grouping observations via pairwise comparisons until all observations are gathered into a single group. We can represent this grouping visually using a plot called the dendrogram, also knowns as a tree diagram.</p>
<p><strong>2. Building the dendrogram</strong></p>
<p>To build a dendrogram, let’s start with the same 6 player soccer lineup from our last video.On the left we have the positions of the players and on the right we will assemble a dendgroram as we iteratively group these observations.</p>
<p><strong>3. Building the dendrogram</strong></p>
<p>As before we can start the process of hierarchical clustering by taking the two closest observations and grouping them.</p>
<p><strong>4. Building the dendrogram</strong></p>
<p>Correspondingly we can represent this grouping in the tree diagram.</p>
<p><strong>5. Building the dendrogram</strong></p>
<p>The dendrogram encodes a very important attribute of our grouping, the distance between the observations that were grouped.This is captured by the height axis. In this case the distance between the two observations is 4 point 1 and correspondingly their shared branch is at that height.</p>
<p><strong>6. Building the dendrogram</strong></p>
<p>As before we would form the next closest group, by comparing the pairwise distances and linkage criteria-based distances among the observations and existing groups.</p>
<p><strong>7. Building the dendrogram</strong></p>
<p>The first group with more than two observations now forms for one two and four and is accordingly represented in the dendrogram.</p>
<p><strong>8. Building the dendrogram</strong></p>
<p>The common branch between these three observations again encodes distance, more specifically it is a function of linkage criteria-based distance among all three observations.This is a very important feature of the dendrogram. It allows us to say something very concrete about our grouped observations at any given height. Remember that for distance we chose euclidean distance and the linkage criteria used was the complete method, which is the maximum distance between the group members.So in this case we can look at this dendrogram and say that the members that are a part of this branch, observations one two and four, have a euclidean distance between each other of 12 or less. We will leverage this attribute of the tree in our next video, but in the mean time let’s continue to build the dendrogram.</p>
<p><strong>9. Building the dendrogram</strong></p>
<p>Iteratively joining the observations and groups.</p>
<p><strong>10. Building the dendrogram</strong></p>
<p>Until all are joined into a single group.</p>
<p><strong>11. Plotting the dendrogram</strong></p>
<p>Of course, we don’t actually do this manually in R. To visualize a dendrogram, all we need to do is plot the corresponding hclust object. In this case we will reuse the hc_players object we created in the previous video to plot our dendrogram.</p>
<p><strong>12. Let’s practice!</strong></p>
<p>Now that you know how to visualize hierarchical clustering lets explore what kind of impact the decision of linkage criteria can have on the dendrogram.</p>
</section>
<section id="comparing-average-single-complete-linkage" class="level2">
<h2 class="anchored" data-anchor-id="comparing-average-single-complete-linkage">Comparing average, single &amp; complete linkage</h2>
<p>You are now ready to analyze the clustering results of the <code>lineup</code> dataset using the dendrogram plot. This will give you a new perspective on the effect the decision of the linkage method has on your resulting cluster analysis.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Perform the linkage calculation for hierarchical clustering using the linkages: complete, single and average</li>
<li>Plot the three dendrograms side by side and review the changes</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Prepare the Distance Matrix</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>dist_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(lineup)</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="co"># Generate hclust for complete, single &amp; average linkage methods</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>hc_complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb8-6"><a href="#cb8-6"></a>hc_single   <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">"single"</span>)</span>
<span id="cb8-7"><a href="#cb8-7"></a>hc_average  <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">"average"</span>)</span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co"># Plot &amp; Label the 3 Dendrograms Side-by-Side</span></span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="co"># Hint: To see these Side-by-Side run the 4 lines together as one command</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="fu">plot</span>(hc_complete, <span class="at">main =</span> <span class="st">'Complete Linkage'</span>)</span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="fu">plot</span>(hc_single,   <span class="at">main =</span> <span class="st">'Single Linkage'</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="fu">plot</span>(hc_average,  <span class="at">main =</span> <span class="st">'Average Linkage'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent! Did you notice how the trees all look different? <br> In the coming exercises you will see how visualizing this structure can be helpful for building clusters.</p>
</section>
<section id="height-of-the-tree" class="level2">
<h2 class="anchored" data-anchor-id="height-of-the-tree">Height of the tree</h2>
<p>An advantage of working with a clustering method like hierarchical clustering is that you can describe the relationships between your observations based on both the <strong>distance metric</strong> and the <strong>linkage metric</strong> selected (the combination of which defines the height of the tree).</p>
<p><strong>Based on the code below what can you concretely say about the height of a branch in the resulting dendrogram?</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>dist_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(lineup, <span class="at">method =</span> <span class="st">'euclidean'</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a>hc_players <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">'single'</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="fu">plot</span>(hc_players)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<h2 id="question-6" class="anchored"><em>Question</em></h2>
<p><strong>All of the observations linked by this branch must have:</strong><br> <br> ⬜ a <strong>maximum Euclidean</strong> distance amongst each other less than or equal to the height of the branch.<br> ⬜ a <strong>minimum Jaccard</strong> distance amongst each other less than or equal to the height of the branch.<br> ✅ a <strong>minimum Euclidean</strong> distance amongst each other less than or equal to the height of the branch.<br></p>
</blockquote>
<p>Exactly! Based on this code we can concretely say that for a given branch on a tree all members that are a part of that branch must have a minimum Euclidean distance amongst one another equal to or less than the height of that branch.</p>
<p>In the next section you will see how this description can be put into action to generate clusters that can be described using the same logic.</p>
</section>
<section id="cutting-the-tree" class="level2">
<h2 class="anchored" data-anchor-id="cutting-the-tree">Cutting the tree</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Cutting the tree</strong></p>
<p>In the previous exercises you have learned how to plot and interpret the dendrogram. Now, let’s learn how to leverage this visualization to both identify our clusters and highlight some of their key characteristics.</p>
<p><strong>2. Cutting the tree</strong></p>
<p>Let’s continue our work with the soccer player dendrogram. Remember that the distance between the observations was calculated using euclidean distance and we used the complete linkage criteria. This means that at any given branch, all members that share this branch will have a euclidean distance amongst one another no greater than the height of that branch. We can leverage this idea to both select our clusters and also characterize the relationships of their members.</p>
<p><strong>3. Cutting the tree</strong></p>
<p>To do so we can cut our tree at any desired height. Let’s choose 15 for now. This means that we remove all links above this cut point and we create our clusters below.</p>
<p><strong>4. Cutting the tree</strong></p>
<p>In this case two clusters are formed. Using this height cutoff we can already ascribe a characteristic to them. We can say that all members of the created clusters will have a euclidean distance amongst each other no greater than our cut height of 15. This statement is a function of our choice of height, distance metric and linkage criteria. This information can be very valuable as our data gets more features and becomes harder to plot using only two dimensions.</p>
<p><strong>5. Coloring the dendrogram - height</strong></p>
<p>We can visualize the clusters that form at any given height by leveraging the dendextend library to color our dendrogram plot.To do so we first must convert the hclust object into a dendrogram object by using the function as (dot) dendrogram.The next step is to use the color_branches function from the dendextend package to color the branches based on a desired criteria. In this case we want to cut using a height of 15, we represent this using the parameter h.Finally we use the plot function to plot the newly colored dendrogram.</p>
<p><strong>6. Coloring the dendrogram - height</strong></p>
<p>We can use this visual to further explore heights at which we may want to create our clusters. Let’s say we believed a height of ten would be more appropriate, as shown in this plot with a proposed red line.</p>
<p><strong>7. Coloring the dendrogram - height</strong></p>
<p>We perform the steps to color the tree using an h equal to 10. The resulting dendrogram now has four colors for the corresponding four clusters.</p>
<p><strong>8. Coloring the dendrogram - K</strong></p>
<p>You can also leverage the color_branches to color the tree using a k criteria by just providing our desired k like so. Resulting in two clusters formed by the cutting of the last grouping.</p>
<p><strong>9. cutree() using height</strong></p>
<p>Just like color_branches can interchangeably use height or k, the cutree function we used to first make clusters can be used to assign cluster memberships using a provided height with the parameter h. As before, we can append this vector of cluster assignments to our data frame in order to empower us to do further exploration.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>Now that you know how to visualize and explore the results of your hierarchical clustering work, let’s try it out.</p>
</section>
<section id="clusters-based-on-height" class="level2">
<h2 class="anchored" data-anchor-id="clusters-based-on-height">Clusters based on height</h2>
<p>In previous exercises you have grouped your observations into clusters using a pre-defined number of clusters (<strong>k</strong>). In this exercise you will leverage the visual representation of the dendrogram in order to group your observations into clusters using a maximum height (<strong>h</strong>), below which clusters form.</p>
<p>You will work the <code>color_branches()</code> function from the <code>dendextend</code> library in order to visually inspect the clusters that form at any height along the dendrogram.</p>
<p>The <strong>hc_players</strong> has been carried over from your previous work with the soccer line-up data.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a dendrogram object <code>dend_players</code> from your <code>hclust</code> result using the function <code>as.dendrogram()</code></li>
<li>Plot the dendrogram</li>
<li>Using the <code>color_branches()</code> function create &amp; plot a new dendrogram with clusters colored by a cut height of 20</li>
<li>Repeat the above step with a height of 40</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="fu">library</span>(dendextend)</span>
<span id="cb10-2"><a href="#cb10-2"></a>dist_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(lineup, <span class="at">method =</span> <span class="st">'euclidean'</span>)</span>
<span id="cb10-3"><a href="#cb10-3"></a>hc_players   <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co"># Create a dendrogram object from the hclust variable</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>dend_players <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span>(hc_players)</span>
<span id="cb10-7"><a href="#cb10-7"></a></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co"># Plot the dendrogram</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="fu">plot</span>(dend_players)</span>
<span id="cb10-10"><a href="#cb10-10"></a></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="co"># Color branches by cluster formed from the cut at a height of 20 &amp; plot</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>dend_20 <span class="ot">&lt;-</span> <span class="fu">color_branches</span>(dend_players, <span class="at">h =</span> <span class="dv">20</span>)</span>
<span id="cb10-13"><a href="#cb10-13"></a></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="co"># Plot the dendrogram with clusters colored below height 20</span></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="fu">plot</span>(dend_20)</span>
<span id="cb10-16"><a href="#cb10-16"></a></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="co"># Color branches by cluster formed from the cut at a height of 40 &amp; plot</span></span>
<span id="cb10-18"><a href="#cb10-18"></a>dend_40 <span class="ot">&lt;-</span> <span class="fu">color_branches</span>(dend_players, <span class="at">h =</span> <span class="dv">40</span>)</span>
<span id="cb10-19"><a href="#cb10-19"></a></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="co"># Plot the dendrogram with clusters colored below height 40</span></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="fu">plot</span>(dend_40)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent! Can you see that the height that you use to cut the tree greatly influences the number of clusters and their size? Consider taking a moment to play with other values of height before continuing.</p>
</section>
<section id="exploring-the-branches-cut-from-the-tree" class="level2">
<h2 class="anchored" data-anchor-id="exploring-the-branches-cut-from-the-tree">Exploring the branches cut from the tree</h2>
<p>The <code>cutree()</code> function you used in exercises 5 &amp; 6 can also be used to cut a tree at a given height by using the <code>h</code> parameter. Take a moment to explore the clusters you have generated from the previous exercises based on the heights 20 &amp; 40.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Build the cluster assignment vector <code>clusters_h20</code> using <code>cutree()</code> with a <code>h = 20</code></li>
<li>Append the cluster assignments as a column <code>cluster</code> to the <code>lineup</code> data frame and save the results to a new data frame called <code>lineup_h20_complete</code></li>
<li>Repeat the above two steps for a height of <strong>40</strong>, generating the variables <code>clusters_h40</code> and <code>lineup_h40_complete</code></li>
<li>Use ggplot2 to create a scatter plot, colored by the cluster assignment for both heights</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>dist_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(lineup, <span class="at">method =</span> <span class="st">'euclidean'</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a>hc_players   <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># Calculate the assignment vector with a h of 20</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>clusters_h20 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_players, <span class="at">h =</span> <span class="dv">20</span>)</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="co"># Create a new data frame storing these results</span></span>
<span id="cb11-8"><a href="#cb11-8"></a>lineup_h20_complete <span class="ot">&lt;-</span> <span class="fu">mutate</span>(lineup, <span class="at">cluster =</span> clusters_h20)</span>
<span id="cb11-9"><a href="#cb11-9"></a></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="co"># Calculate the assignment vector with a h of 40</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>clusters_h40 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_players, <span class="at">h =</span> <span class="dv">40</span>)</span>
<span id="cb11-12"><a href="#cb11-12"></a></span>
<span id="cb11-13"><a href="#cb11-13"></a><span class="co"># Create a new data frame storing these results</span></span>
<span id="cb11-14"><a href="#cb11-14"></a>lineup_h40_complete <span class="ot">&lt;-</span> <span class="fu">mutate</span>(lineup, <span class="at">cluster =</span> clusters_h40)</span>
<span id="cb11-15"><a href="#cb11-15"></a></span>
<span id="cb11-16"><a href="#cb11-16"></a><span class="co"># Plot the positions of the players and color them using their cluster for height = 20</span></span>
<span id="cb11-17"><a href="#cb11-17"></a><span class="fu">ggplot</span>(lineup_h20_complete, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb11-18"><a href="#cb11-18"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb11-19"><a href="#cb11-19"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb11-20"><a href="#cb11-20"></a></span>
<span id="cb11-21"><a href="#cb11-21"></a><span class="co"># Plot the positions of the players and color them using their cluster for height = 40</span></span>
<span id="cb11-22"><a href="#cb11-22"></a><span class="fu">ggplot</span>(lineup_h40_complete, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb11-23"><a href="#cb11-23"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb11-24"><a href="#cb11-24"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great job! You can now explore your clusters using both <strong>k</strong> and <strong>h</strong> parameters.</p>
</section>
<section id="what-do-we-know-about-our-clusters" class="level2">
<h2 class="anchored" data-anchor-id="what-do-we-know-about-our-clusters">What do we know about our clusters?</h2>
<p><strong>Based on the code below, what can you concretely say about the relationships of the members within each cluster?</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>dist_players <span class="ot">&lt;-</span> <span class="fu">dist</span>(lineup, <span class="at">method =</span> <span class="st">'euclidean'</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a>hc_players   <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_players, <span class="at">method =</span> <span class="st">'complete'</span>)</span>
<span id="cb12-3"><a href="#cb12-3"></a>clusters     <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_players, <span class="at">h =</span> <span class="dv">40</span>)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<h2 id="question-7" class="anchored"><em>Question</em></h2>
<p><strong>Every member belonging to a cluster must have:</strong><br> <br> ✅ a <strong>maximum Euclidean</strong> distance to all other members of its cluster that is less than 40.<br> ⬜ a <strong>maximum Euclidean</strong> distance to all other members of its cluster that is greater than or equal to 40.<br> ⬜ a <strong>average Euclidean</strong> distance to all other members of its cluster that is less than 40.<br></p>
</blockquote>
<p>Correct! The height of any branch is determined by the linkage and distance decisions (in this case complete linkage and Euclidean distance). While the members of the clusters that form below a desired height have a maximum linkage+distance amongst themselves that is less than the desired height.</p>
</section>
<section id="making-sense-of-the-clusters" class="level2">
<h2 class="anchored" data-anchor-id="making-sense-of-the-clusters">Making sense of the clusters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Making sense of the clusters</strong></p>
<p>Over the last series of exercises, you have developed the tools you need to run hierarchical clustering and the intuition to understand the impact of each step. Now you will have a chance to use these skills by clustering a new dataset.</p>
<p><strong>2. Wholesale dataset</strong></p>
<p>You will work with a series of 45 records of customer spending from a wholesale distributor. For each customer record you will have 3 features, spending on Milk, Grocery and Frozen Food.</p>
<p><strong>3. Wholesale dataset</strong></p>
<p>The dataset will look like this.You will notice that unlike the soccer positions data set, where we only have two features (x and y), this dataset has three features. The consequence of this is that we can’t simply explore what the clusters mean from a two dimensional plot.</p>
<p><strong>4. Exploring more than 2 dimensions</strong></p>
<p>There are several approaches to overcome this. Once you have assigned the cluster memberships you can make multiple plots with feature pairs and use color to show the difference in clusters. This can be helpful, but only captures one angle of the complex interactions at a time. Also this approach can quickly get out of hand when the number of features expands.Alternatively, you can use dimensionality reduction methods such as principal component analysis in order to plot your multi-dimensional data onto two dimensions and color the points using the cluster assignment. This can be helpful to see if your observations clustered well and the clusters are well separated. However, this type of analysis is difficult to interpret and wouldn’t shed light on the characteristics of the clusters.Finally, you can simply explore the distribution characteristics such as the mean and median of each feature within your clusters. By comparing these summary statistics between clusters you can begin to build a narrative of what makes the observations within the cluster similar to each other while different from the observations in the other clusters.</p>
<p><strong>5. Segment the customers</strong></p>
<p>In the next series of exercises you will use this data identify the clusters of customers that form based on their spending. This is a common use case of cluster analysis where the desired outcome is to segment customers based on their behaviors. Once the segments are identified we can explore their common characteristics to gain insights into our customer base and design value-driven opportunities using this data.Let’s get started.</p>
</section>
<section id="segment-wholesale-customers" class="level2">
<h2 class="anchored" data-anchor-id="segment-wholesale-customers">Segment wholesale customers</h2>
<p>You’re now ready to use hierarchical clustering to perform market segmentation (i.e.&nbsp;use consumer characteristics to group them into subgroups).</p>
<p>In this exercise you are provided with the amount spent by 45 different clients of a wholesale distributor for the food categories of <strong>Milk</strong>, <strong>Grocery</strong> &amp; <strong>Frozen</strong>. This is stored in the data frame <code>customers_spend</code>. Assign these clients into meaningful clusters.</p>
<p><strong>Note:</strong> For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the Euclidean distance between the customers and store this in <code>dist_customers</code></li>
<li>Run hierarchical clustering using <strong>complete</strong> linkage and store in <code>hc_customers</code></li>
<li>Plot the dendrogram</li>
<li>Create a cluster assignment vector using a height of 15,000 and store it as <code>clust_customers</code></li>
<li>Generate a new data frame <code>segment_customers</code> by appending the cluster assignment as the column <code>cluster</code> to the original <code>customers_spend</code> data frame</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Load data</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>customers_spend <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/ws_customers.rds"</span>)</span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># Calculate Euclidean distance between customers</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>dist_customers <span class="ot">&lt;-</span> <span class="fu">dist</span>(customers_spend)</span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co"># Generate a complete linkage analysis </span></span>
<span id="cb13-8"><a href="#cb13-8"></a>hc_customers <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_customers, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co"># Plot the dendrogram</span></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="fu">plot</span>(hc_customers)</span>
<span id="cb13-12"><a href="#cb13-12"></a></span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="co"># Create a cluster assignment vector at h = 15000</span></span>
<span id="cb13-14"><a href="#cb13-14"></a>clust_customers <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_customers, <span class="at">h =</span> <span class="dv">15000</span>)</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="co"># Generate the segmented customers data frame</span></span>
<span id="cb13-17"><a href="#cb13-17"></a>segment_customers <span class="ot">&lt;-</span> <span class="fu">mutate</span>(customers_spend, <span class="at">cluster =</span> clust_customers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent! Let’s move on to the next exercise and explore these clusters.</p>
</section>
<section id="explore-wholesale-customer-clusters" class="level2">
<h2 class="anchored" data-anchor-id="explore-wholesale-customer-clusters">Explore wholesale customer clusters</h2>
<p>Continuing your work on the wholesale dataset you are now ready to analyze the characteristics of these clusters.</p>
<p>Since you are working with more than 2 dimensions it would be challenging to visualize a scatter plot of the clusters, instead you will rely on summary statistics to explore these clusters. In this exercise you will analyze the mean amount spent in each cluster for all three categories.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the size of each cluster using <code>count()</code>.</li>
<li>Color &amp; plot the dendrogram using the height of 15,000.</li>
<li>Calculate the average spending for each category within each cluster using the <code>summarise_all()</code> function.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>dist_customers <span class="ot">&lt;-</span> <span class="fu">dist</span>(customers_spend)</span>
<span id="cb14-2"><a href="#cb14-2"></a>hc_customers <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_customers)</span>
<span id="cb14-3"><a href="#cb14-3"></a>clust_customers <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_customers, <span class="at">h =</span> <span class="dv">15000</span>)</span>
<span id="cb14-4"><a href="#cb14-4"></a>segment_customers <span class="ot">&lt;-</span> <span class="fu">mutate</span>(customers_spend, <span class="at">cluster =</span> clust_customers)</span>
<span id="cb14-5"><a href="#cb14-5"></a></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="co"># Count the number of customers that fall into each cluster</span></span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="fu">count</span>(segment_customers, cluster)</span>
<span id="cb14-8"><a href="#cb14-8"></a></span>
<span id="cb14-9"><a href="#cb14-9"></a><span class="co"># Color the dendrogram based on the height cutoff</span></span>
<span id="cb14-10"><a href="#cb14-10"></a>dend_customers <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span>(hc_customers)</span>
<span id="cb14-11"><a href="#cb14-11"></a>dend_colored <span class="ot">&lt;-</span> <span class="fu">color_branches</span>(dend_customers, <span class="at">h =</span> <span class="dv">15000</span>)</span>
<span id="cb14-12"><a href="#cb14-12"></a></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="co"># Plot the colored dendrogram</span></span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="fu">plot</span>(dend_colored)</span>
<span id="cb14-15"><a href="#cb14-15"></a></span>
<span id="cb14-16"><a href="#cb14-16"></a><span class="co"># Calculate the mean for each category</span></span>
<span id="cb14-17"><a href="#cb14-17"></a>segment_customers <span class="sc">%&gt;%</span> </span>
<span id="cb14-18"><a href="#cb14-18"></a>  <span class="fu">group_by</span>(cluster) <span class="sc">%&gt;%</span> </span>
<span id="cb14-19"><a href="#cb14-19"></a>  <span class="fu">summarise_all</span>(<span class="fu">list</span>(mean))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! You’ve gathered a bunch of information about these clusters, now let’s see what can be interpreted from them.</p>
</section>
<section id="interpreting-the-wholesale-customer-clusters" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-wholesale-customer-clusters">Interpreting the wholesale customer clusters</h2>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;">cluster</th>
<th style="text-align: right;">Milk</th>
<th style="text-align: right;">Grocery</th>
<th style="text-align: right;">Frozen</th>
<th style="text-align: right;">cluster size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">16950</td>
<td style="text-align: right;">12891</td>
<td style="text-align: right;">991</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">2512</td>
<td style="text-align: right;">5228</td>
<td style="text-align: right;">1795</td>
<td style="text-align: right;">29</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">10452</td>
<td style="text-align: right;">22550</td>
<td style="text-align: right;">1354</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: right;">1249</td>
<td style="text-align: right;">3916</td>
<td style="text-align: right;">10888</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<h2 id="question-8" class="anchored"><em>Question</em></h2>
<p>What observations can we make about our segments based on their average spending in each category?<br> <br> ⬜ Customers in cluster 1 spent more money on Milk than any other cluster.<br> ⬜ Customers in cluster 3 spent more money on Grocery than any other cluster.<br> ⬜ Customers in cluster 4 spent more money on Frozen goods than any other cluster.<br> ⬜ The majority of customers fell into cluster 2 and did not show any excessive spending in any category.<br> ✅ All of the above.<br></p>
</blockquote>
<p>All 4 statements are reasonable, but whether they are meaningful depends heavily on the business context of the clustering.</p>
</section>
</section>
<section id="k-means-clustering" class="level1">
<h1>3. K-means clustering</h1>
<p>In this chapter, you will build an understanding of the principles behind the k-means algorithm, learn how to select the right k when it isn’t previously known, and revisit the wholesale data from a different perspective.</p>
<section id="introduction-to-k-means" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-k-means">Introduction to K-means</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Introduction to K-means</strong></p>
<p>In the last chapter you learned how to use the hierarchical clustering method to group observations. In this chapter you will learn about another popular method of clustering called k-means clustering. To learn how this method works, let’s revisit an expanded version of the soccer lineup data you have been working with.</p>
<p><strong>2. k-means</strong></p>
<p>This data consists of twelve players on a soccer field at the start of the game. At this point in the game the teams are positioned on opposite sides of the field. We would expect that clustering can be effective in identifying teams and assigning each player to the correct team. The first step of k-means clustering involves making a decision of how many clusters to generate. This is the k in k-means clustering. This can be decided on in advance based on our understanding of the data or it can be estimated from the data empirically. We will discuss the estimation methods later in this chapter. In this example we can leverage what is known about our data. Since we know that soccer is played with two teams we can use a k of 2 for the desired number of clusters. Once k is established the algorithm can proceed.</p>
<p><strong>3. k-means</strong></p>
<p>The first step in the k-means algorithm is to initialize k points at random positions in the feature space, we will refer to these points as the cluster centroids. In this data we will illustrate our two centroids using a red and a blue x.</p>
<p><strong>4. k-means</strong></p>
<p>For each observation the distance is calculated between the observation and each centroid. In k-means clustering, the distance is limited euclidean only.</p>
<p><strong>5. k-means</strong></p>
<p>The observations are initially assigned to the centroid to which they are closest to.</p>
<p><strong>6. k-means</strong></p>
<p>We can see this decision boundary represented by the color space.</p>
<p><strong>7. k-means</strong></p>
<p>The observations now have an initial assignment to one of the two clusters.</p>
<p><strong>8. k-means</strong></p>
<p>The next step involves moving the centroids to the central points of the resulting clusters.</p>
<p><strong>9. k-means</strong></p>
<p>Again, the distance of every observation is calculated to each centroid.</p>
<p><strong>10. k-means</strong></p>
<p>And they are re-assigned based on which centroid they are closest to.</p>
<p><strong>11. k-means</strong></p>
<p>This process continues until the centroids stabilize and the observations are no longer reassigned. This is the fundamental algorithm of kmeans clustering.</p>
<p><strong>12. kmeans()</strong></p>
<p>To generate the kmeans model in R you will use the function of the same name. We will continue to work with the lineup data frame that you explored in chapter two. The kmeans function is run with the data as the first argument and the desired number of clusters provided using the centers parameter. Centers in this case is synonymous with k.</p>
<p><strong>13. Assigning clusters</strong></p>
<p>Once the model is run you will want to extract the cluster assignments in order to explore their characteristics.You can extract the cluster assignments directly from the model object. The vector of assignments is stored in the model object and is aptly named cluster.As before you can append this vector to your data frame in order to further explore the results of your clustering.</p>
<p><strong>14. Let’s practice!</strong></p>
<p>Now that you know how kmeans works and how to use it in R let’s practice with the soccer lineup data.</p>
</section>
<section id="k-means-on-a-soccer-field" class="level2">
<h2 class="anchored" data-anchor-id="k-means-on-a-soccer-field">K-means on a soccer field</h2>
<p>In the previous chapter you used the <code>lineup</code> dataset to learn about <strong>hierarchical</strong> clustering, in this chapter you will use the same data to learn about <strong>k-means</strong> clustering. As a reminder, the <code>lineup</code> data frame contains the positions of 12 players at the start of a 6v6 soccer match.</p>
<p>Just like before, you know that this match has two teams on the field so you can perform a k-means analysis using <em>k = 2</em> in order to determine which player belongs to which team.</p>
<p>Note that in the <code>kmeans()</code> function <code>k</code> is specified using the <code>centers</code> parameter.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Build a k-means model called <code>model_km2</code> for the <code>lineup</code> data using the <code>kmeans()</code> function with <code>centers = 2</code></li>
<li>Extract the vector of cluster assignments from the model <code>model_km2$cluster</code> and store this in the variable <code>clust_km2</code></li>
<li>Append the cluster assignments as a column <code>cluster</code> to the <code>lineup</code> data frame and save the results to a new data frame called <code>lineup_km2</code></li>
<li>Use ggplot to plot the positions of each player on the field and color them by their cluster</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Build a kmeans model</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>model_km2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(lineup, <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># Extract the cluster assignment vector from the kmeans model</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>clust_km2 <span class="ot">&lt;-</span> model_km2<span class="sc">$</span>cluster</span>
<span id="cb15-6"><a href="#cb15-6"></a></span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="co"># Create a new data frame appending the cluster assignment</span></span>
<span id="cb15-8"><a href="#cb15-8"></a>lineup_km2 <span class="ot">&lt;-</span> <span class="fu">mutate</span>(lineup, <span class="at">cluster =</span> clust_km2)</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="co"># Plot the positions of the players and color them using their cluster</span></span>
<span id="cb15-11"><a href="#cb15-11"></a><span class="fu">ggplot</span>(lineup_km2, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb15-12"><a href="#cb15-12"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb15-13"><a href="#cb15-13"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Well done! Knowing the desired number of clusters ahead of time can be very helpful when performing a k-means analysis. In the next section we will see what happens when we use an incorrect value of k.</p>
</section>
<section id="k-means-on-a-soccer-field-part-2" class="level2">
<h2 class="anchored" data-anchor-id="k-means-on-a-soccer-field-part-2">K-means on a soccer field (part 2)</h2>
<p>In the previous exercise you successfully used the <strong>k-means</strong> algorithm to cluster the two teams from the <code>lineup</code> data frame. This time, let’s explore what happens when you use a <code>k</code> of <strong>3</strong>.</p>
<p>You will see that the algorithm will still run, but does it actually make sense in this context…</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Build a k-means model called <code>model_km3</code> for the <code>lineup</code> data using the <code>kmeans()</code> function with <code>centers = 3</code></li>
<li>Extract the vector of cluster assignments from the model <code>model_km3$cluster</code> and store this in the variable <code>clust_km3</code></li>
<li>Append the cluster assignments as a column <code>cluster</code> to the <code>lineup</code> data frame and save the results to a new data frame called <code>lineup_km3</code></li>
<li>Use ggplot to plot the positions of each player on the field and color them by their cluster</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Build a kmeans model</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>model_km3 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(lineup, <span class="at">centers =</span> <span class="dv">3</span>)</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="co"># Extract the cluster assignment vector from the kmeans model</span></span>
<span id="cb16-5"><a href="#cb16-5"></a>clust_km3 <span class="ot">&lt;-</span> model_km3<span class="sc">$</span>cluster</span>
<span id="cb16-6"><a href="#cb16-6"></a></span>
<span id="cb16-7"><a href="#cb16-7"></a><span class="co"># Create a new data frame appending the cluster assignment</span></span>
<span id="cb16-8"><a href="#cb16-8"></a>lineup_km3 <span class="ot">&lt;-</span> <span class="fu">mutate</span>(lineup, <span class="at">cluster =</span> clust_km3)</span>
<span id="cb16-9"><a href="#cb16-9"></a></span>
<span id="cb16-10"><a href="#cb16-10"></a><span class="co"># Plot the positions of the players and color them using their cluster</span></span>
<span id="cb16-11"><a href="#cb16-11"></a><span class="fu">ggplot</span>(lineup_km3, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb16-12"><a href="#cb16-12"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb16-13"><a href="#cb16-13"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Does this result make sense? Remember we only have 2 teams on the field. It’s <strong>very</strong> important to remember that k-means will run with any k that is more than 2 and less than your total observations, but it doesn’t always mean the results will be meaningful.</p>
</section>
<section id="evaluating-different-values-of-k-by-eye" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-different-values-of-k-by-eye">Evaluating different values of K by eye</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Evaluating different values of K by eye</strong></p>
<p>In the last two exercises you explored the results for two different values of k using the same data. You knew that a k of 3 was clearly incorrect because you applied content expertise to this problem by stating that there are only two teams in a game of soccer and that the teams have the same number of players. But, what happens when you don’t know in advance what the right value of k is? In this course you will learn two methods that address this challenge by estimating k empirically from the data. In this video and the accompanying exercises you will build an intuition for one of these methods, the elbow method.</p>
<p><strong>2. Total within-cluster sum of squares: k = 1</strong></p>
<p>The elbow method relies on calculating the total within cluster sum of squares across every cluster, that is the sum of euclidean distances between each observation and the centroid corresponding to the cluster to which the observation is assigned. Here this is represented by the dashed lines between the centroid and each observation. While k = 1 isn’t really clustering, it can be helpful for the elbow analysis. As such we record the total within cluster sum of squares for the value of k = 1.</p>
<p><strong>3. Total within-cluster sum of squares: k = 2</strong></p>
<p>We repeat this step for k = 2. You can already see that the dashed lines are on average shorter and we can expect the total within cluster sum of squares to drop. Which of course it does.</p>
<p><strong>4. Total within-cluster sum of squares: k = 3</strong></p>
<p>Same goes for a value of k = 3.</p>
<p><strong>5. Total within-cluster sum of squares: k = 4</strong></p>
<p>And for k = 4. We can continue this calculation so long as k is less than our total number of observations.</p>
<p><strong>6. Elbow plot</strong></p>
<p>In this case we have calculated this for values of k from one through ten. You may notice a trend that as k increases the total within cluster sum of squares keeps decreasing. This is absolutely natural and expected, just think about it, the more you segment your data the more your points just group together into smaller and more compact clusters until you obtain many clusters with only one or two members. What we are looking for is the point at which the curve beings to flatten out, affectionally referred to as the elbow. In this case we can see that there is a precipitous drop going from a k of one to two and then a leveling off when moving between a k of 2 and 3 and onward.</p>
<p><strong>7. Elbow plot</strong></p>
<p>As such we can claim that the elbow in this case occurred where k = 2 and would consider using this estimated value of k.</p>
<p><strong>8. Generating the elbow plot</strong></p>
<p>Now that you know how the elbow plot is built, let’s learn how to build it in R. The first piece you will need to know is how to calculate the total within cluster sum of squares. Conveniently, the kmeans function already takes care of this for you. All you need to do is to extract it from the model object like so.</p>
<p><strong>9. Generating the elbow plot</strong></p>
<p>Because you want to calculate this for multiple values of k you will need to create multiple models and extract their corresponding values. To do this I recommend leveraging the map double function from the purrr library. The code shown here iterates over values of k ranging from one to ten in order to build corresponding models and extract their total within-Cluster sum of squares values. You can append this vector to the corresponding vector of k values to create a data frame. 10. Generating the elbow plot</p>
<p>Which you can then use to plot the elbow plot like so.</p>
<p><strong>11. Let’s practice!</strong></p>
<p>Let’s try it out!</p>
</section>
<section id="many-ks-many-models" class="level2">
<h2 class="anchored" data-anchor-id="many-ks-many-models">Many K’s many models</h2>
<p>While the <code>lineup</code> dataset clearly has a known value of <strong>k</strong>, often times the optimal number of clusters isn’t known and must be estimated.</p>
<p>In this exercise you will leverage <code>map_dbl()</code> from the <code>purrr</code> library to run k-means using values of k ranging from 1 to 10 and extract the <strong>total within-cluster sum of squares</strong> metric from each one. This will be the first step towards visualizing the elbow plot.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>map_dbl()</code> to run <code>kmeans()</code> using the <code>lineup</code> data for k values ranging from 1 to 10 and extract the <strong>total within-cluster sum of squares</strong> value from each model: <code>model$tot.withinss</code>the resulting vector as <code>tot_withinss</code><br>
</li>
<li>Build a new data frame <code>elbow_df</code> containing the values of k and the vector of <strong>total within-cluster sum of squares</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="fu">library</span>(purrr)</span>
<span id="cb17-2"><a href="#cb17-2"></a></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="co"># Use map_dbl to run many models with varying value of k (centers)</span></span>
<span id="cb17-4"><a href="#cb17-4"></a>tot_withinss <span class="ot">&lt;-</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,  <span class="cf">function</span>(k){</span>
<span id="cb17-5"><a href="#cb17-5"></a>  model <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> lineup, <span class="at">centers =</span> k)</span>
<span id="cb17-6"><a href="#cb17-6"></a>  model<span class="sc">$</span>tot.withinss</span>
<span id="cb17-7"><a href="#cb17-7"></a>})</span>
<span id="cb17-8"><a href="#cb17-8"></a></span>
<span id="cb17-9"><a href="#cb17-9"></a><span class="co"># Generate a data frame containing both k and tot_withinss</span></span>
<span id="cb17-10"><a href="#cb17-10"></a>elbow_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb17-11"><a href="#cb17-11"></a>  <span class="at">k =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb17-12"><a href="#cb17-12"></a>  <span class="at">tot_withinss =</span> tot_withinss</span>
<span id="cb17-13"><a href="#cb17-13"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! In the next exercise you will plot the elbow plot from this data.</p>
</section>
<section id="elbow-scree-plot" class="level2">
<h2 class="anchored" data-anchor-id="elbow-scree-plot">Elbow (Scree) plot</h2>
<p>In the previous exercises you have calculated the <strong>total within-cluster sum of squares</strong> for values of <strong>k</strong> ranging from 1 to 10. You can visualize this relationship using a line plot to create what is known as an elbow plot (or scree plot).</p>
<p>When looking at an elbow plot you want to see a sharp decline from one k to another followed by a more gradual decrease in slope. The last value of k before the slope of the plot levels off suggests a “good” value of k.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Continuing your work from the previous exercise, use the values in <code>elbow_df</code> to plot a line plot showing the relationship between <strong>k</strong> and <strong>total within-cluster sum of squares</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Use map_dbl to run many models with varying value of k (centers)</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>tot_withinss <span class="ot">&lt;-</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,  <span class="cf">function</span>(k){</span>
<span id="cb18-3"><a href="#cb18-3"></a>  model <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> lineup, <span class="at">centers =</span> k)</span>
<span id="cb18-4"><a href="#cb18-4"></a>  model<span class="sc">$</span>tot.withinss</span>
<span id="cb18-5"><a href="#cb18-5"></a>})</span>
<span id="cb18-6"><a href="#cb18-6"></a></span>
<span id="cb18-7"><a href="#cb18-7"></a><span class="co"># Generate a data frame containing both k and tot_withinss</span></span>
<span id="cb18-8"><a href="#cb18-8"></a>elbow_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb18-9"><a href="#cb18-9"></a>  <span class="at">k =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb18-10"><a href="#cb18-10"></a>  <span class="at">tot_withinss =</span> tot_withinss</span>
<span id="cb18-11"><a href="#cb18-11"></a>)</span>
<span id="cb18-12"><a href="#cb18-12"></a></span>
<span id="cb18-13"><a href="#cb18-13"></a><span class="co"># Plot the elbow plot</span></span>
<span id="cb18-14"><a href="#cb18-14"></a><span class="fu">ggplot</span>(elbow_df, <span class="fu">aes</span>(<span class="at">x =</span> k, <span class="at">y =</span> tot_withinss)) <span class="sc">+</span></span>
<span id="cb18-15"><a href="#cb18-15"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb18-16"><a href="#cb18-16"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb18-17"><a href="#cb18-17"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Fantastic! You have learned how to create and visualize elbow plots as a tool for finding a “good” value of <strong>k</strong>. In the next section you will add another tool to your arsenal for finding <strong>k</strong>.</p>
</section>
<section id="interpreting-the-elbow-plot" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-elbow-plot">Interpreting the elbow plot</h2>
<p>Based on the elbow plot you generated in the previous exercise for the <code>lineup</code> data:</p>
<p><img src="http://s3.amazonaws.com/assets.datacamp.com/production/course_5724/datasets/soccer_elbow.png"></p>
<blockquote class="blockquote">
<h2 id="question-9" class="anchored"><em>Question</em></h2>
<p><strong>Which of these interpretations are valid?</strong><br> <br> ✅ Based on this plot, the <strong>k</strong> to choose is <strong>2</strong>; the elbow occurs there.<br> ⬜ The <strong>k</strong> to choose is <strong>5</strong>; this is where the trend levels off.<br> ⬜ Any value of <strong>k</strong> is valid; this plot does not clearly identify an elbow.<br> ⬜ None of the above.<br></p>
</blockquote>
<p>That is correct, you can see that there is a sharp change in the slope of this line that makes an “elbow” shape. Furthermore, this is supported by the prior knowledge that there are <strong>two</strong> teams in this data and a <strong>k</strong> of <strong>2</strong> is desired.</p>
</section>
<section id="silhouette-analysis-observation-level-performance" class="level2">
<h2 class="anchored" data-anchor-id="silhouette-analysis-observation-level-performance">Silhouette analysis: observation level performance</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Silhouette analysis: observation level performance</strong></p>
<p>In the last series of exercises you utilized the elbow method to estimate a suitable value of k.In this lesson you will learn about the silhouette analysis method. This approach provides a different lens through which you can understand the results of your cluster analysis. It can be used to determine how well each of your observations fit into its corresponding cluster and can be leveraged as an additional method for estimating the value of k.</p>
<p><strong>2. Soccer lineup with K = 3</strong></p>
<p>Continuing with our soccer lineup dataset, we will start with the observations already clustered using kmeans with a k of three.</p>
<p><strong>3. Silhouette width</strong></p>
<p>Silhouette analysis involves calculating a measurement called the silhouette width for every observation. The silhouette width consists of two parts. The within cluster distance C and the closest neighbor distance N. We’ll work with player number 3 to illustrate this calculation.</p>
<p><strong>4. Silhouette width</strong></p>
<p>The within cluster distance for an observation is the average euclidean distance from that observation to every other observation within the same cluster. In this case the distances are represented by the arrows to the other 3 members of the green cluster.</p>
<p><strong>5. Silhouette width</strong></p>
<p>The closest neighbor distance for an observation is the average distance from that observation to the points of the closest neighboring cluster.</p>
<p><strong>6. Silhouette width</strong></p>
<p>It is calculated for the red cluster like so.</p>
<p><strong>7. Silhouette width</strong></p>
<p>Then the blue cluster. The smallest average distance to our observation is then used as the closest neighbor distance. In this case the blue cluster is clearly closer.</p>
<p><strong>8. Silhouette width: S(i)</strong></p>
<p>Using the values of N and C the silhouette width can be calculated as shown here.</p>
<p><strong>9. Silhouette width: S(i)</strong></p>
<p>More importantly is the intuitive interpretation of this value.A value close to one suggests that this observation is well matched to its current cluster.A value of 0 suggests that it is on the border between two clusters and can possibly belong to either one.While a value of -1, or close to -1 suggests that this observation has a better fit with its closest neighboring cluster.What do you think is the silhouette width for player 3? It sits on the border between blue and green so I’m guessing it’s probably close to zero.</p>
<p><strong>10. Calculating S(i)</strong></p>
<p>We can calculate the silhouette width for each observation by leveraging the pam function from the cluster library. Note, that the pam function is very similar, but is not identical to kmeans. Since we are just using it to characterize our kmeans clusters we can ignore this difference.The pam function requires a data frame and a desired number of clusters provided by the parameter k. The silhouette widths can be accessed from the pam model object as shown here.</p>
<p><strong>11. Silhouette plot</strong></p>
<p>Or they can be visualized using the silhouette plot like so.In this plot the bars represent the silhouette widths for each observation. Look at observation three, like we guessed, it’s value is close to zero.</p>
<p><strong>12. Silhouette plot</strong></p>
<p>Also, note at the bottom of this plot is the average silhouette width across the twelve observations.</p>
<p><strong>13. Average silhouette width</strong></p>
<p>This measurement can be retrieved from the model object as shown here. And, it can be interpreted in a manner similar to the silhouette width for an observation.In this case the average is well above zero suggesting that most observations are well matched to their assigned cluster.Now that you have a way of measuring the effectiveness of the clustering, you can perform an analysis similar to the elbow plot and calculate the average silhouette widths for multiple values of k. The greater the average width the better the individual observations match to their clusters.</p>
<p><strong>14. Highest average silhouette width</strong></p>
<p>Similar to the elbow plot we can leverage the map double function to run pam across multiple values of k and record the average silhouette width for each, likewise we can append these measurements to a data frame.</p>
<p><strong>15. Choosing K using average silhouette width</strong></p>
<p>And use ggplot to see the relationship between k and the average silhouette width.</p>
<p><strong>16. Choosing K using average silhouette width</strong></p>
<p>Not surprisingly, the highest average silhouette width is for a k of two, and would be the recommended value based on this method.</p>
<p><strong>17. Let’s practice!</strong></p>
<p>Now that you know how silhouette analysis works, let’s try it out.</p>
</section>
<section id="silhouette-analysis" class="level2">
<h2 class="anchored" data-anchor-id="silhouette-analysis">Silhouette analysis</h2>
<p>Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from <strong>-1</strong> to <strong>1</strong> for each observation in your data and can be interpreted as follows:</p>
<ul>
<li>Values close to <strong>1</strong> suggest that the observation is well matched to the assigned cluster</li>
<li>Values close to <strong>0</strong> suggest that the observation is borderline matched between two clusters</li>
<li>Values close to <strong>-1</strong> suggest that the observations may be assigned to the wrong cluster In this exercise you will leverage the <code>pam()</code> and the <code>silhouette()</code> functions from the <code>cluster</code> library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You’ll continue working with the <code>lineup</code> dataset.</li>
</ul>
<blockquote class="blockquote">
<p>Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k = 3?</p>
</blockquote>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Generate a k-means model <code>pam_k2</code> using <code>pam()</code> with <code>k = 2</code> on the <code>lineup</code> data.</li>
<li>Plot the silhouette analysis using <code>plot(silhouette(model))</code>.</li>
<li>Repeat the first two steps for <code>k = 3</code>, saving the model as <code>pam_k3</code>.</li>
<li>Make sure to review the differences between the plots before proceeding (especially observation <strong>3</strong>) for <code>pam_k3</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="co"># Generate a k-means model using the pam() function with a k = 2</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>pam_k2 <span class="ot">&lt;-</span> <span class="fu">pam</span>(lineup, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb19-5"><a href="#cb19-5"></a></span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="co"># Plot the silhouette visual for the pam_k2 model</span></span>
<span id="cb19-7"><a href="#cb19-7"></a><span class="fu">plot</span>(<span class="fu">silhouette</span>(pam_k2))</span>
<span id="cb19-8"><a href="#cb19-8"></a></span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="co"># Generate a k-means model using the pam() function with a k = 3</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>pam_k3 <span class="ot">&lt;-</span> <span class="fu">pam</span>(lineup, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="co"># Plot the silhouette visual for the pam_k3 model</span></span>
<span id="cb19-13"><a href="#cb19-13"></a><span class="fu">plot</span>(<span class="fu">silhouette</span>(pam_k3))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! Did you notice that for k = 2, no observation has a silhouette width close to 0? What about the fact that for k = 3, observation 3 is close to 0 and is negative? This suggests that k = 3 is not the right number of clusters.</p>
</section>
<section id="making-sense-of-the-k-means-clusters" class="level2">
<h2 class="anchored" data-anchor-id="making-sense-of-the-k-means-clusters">Making sense of the K-means clusters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Making sense of the K-means clusters</strong></p>
<p>Throughout this chapter you have worked to develop an understanding and an intuition of how to use the kmeans algorithm and its associated techniques to perform clustering analysis.Now it’s time to put these tools into practice by revisiting the wholesale dataset.</p>
<p><strong>2. Wholesale dataset</strong></p>
<p>You have learned a lot since you’ve last looked at this data so let’s have a quick refresher.The wholesale dataset is an exercise in clustering the customers of a wholesale distributor. This use of clustering is also known as market segmentation. The wholesale data consists of 45 observations of client purchases for milk, grocery and frozen food. The data is stored in the data frame customers_spend.</p>
<p><strong>3. Segmenting with hierarchical clustering</strong></p>
<p>At the end of chapter two you used hierarchical clustering to segment the customers into four clusters using a height that seemed appropriate based on the structure of the tree.</p>
<p><strong>4. Segmenting with hierarchical clustering</strong></p>
<p>You then characterized these customer segments by calculating the average of their spending in each category. From this analysis you learned that segments one, three, and four consist of around five observations each and their members collectively spend more on one category relative to the others. In a real world scenario a finding like this could be used to provide more customized advertising or other targeting for these groups based on their spending habits.Do you think the result will be the be the same if you used a different method for clustering?</p>
<p><strong>5. Segmenting with K-means</strong></p>
<p>Let’s find out. In the following exercises you will leverage the kmeans tools you have learned in this chapter to:First estimate the best value of k by finding the maximum average silhouette width with respect to k.Then you will use this value of k to create a kmeans model.Finally, you will characterize these k clusters by calculating their average spending in each category like you have in the previous chapter.As you progress through these exercises feel free to look back and compare your results with the corresponding hierarchical clustering exercises. I encourage you to see if the results are different and speculate as to why that may be the case?Most importantly, you must remember that both of these clustering methods are descriptive and not prescriptive. In other words, they will provide different lenses with which you can understand your underlying data but the choice of which to use and how to correctly use it will be highly dependent on the question at hand as well as an understanding of the underlying subject matter.</p>
<p><strong>6. Let’s cluster!</strong></p>
<p>So, what are you waiting for, let’s see how kmeans segments our customers.</p>
</section>
<section id="revisiting-wholesale-data-best-k" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-wholesale-data-best-k">Revisiting wholesale data: “Best” k</h2>
<p>At the end of <strong>Chapter 2</strong> you explored wholesale distributor data <code>customers_spend</code> using hierarchical clustering. This time you will analyze this data using the k-means clustering tools covered in this chapter.</p>
<p>The first step will be to determine the <strong>“best”</strong> value of k using <strong>average silhouette width</strong>.</p>
<p>A refresher about the data: it contains records of the amount spent by 45 different clients of a wholesale distributor for the food categories of <strong>Milk</strong>, <strong>Grocery</strong> &amp; <strong>Frozen</strong>. This is stored in the data frame <code>customers_spend</code>. For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>map_dbl()</code> to run <code>pam()</code> using the <code>customers_spend</code> data for k values ranging from 2 to 10 and extract the <strong>average silhouette width</strong> value from each model: <code>model$silinfo$avg.width</code>the resulting vector as <code>sil_width</code><br>
</li>
<li>Build a new data frame <code>sil_df</code> containing the values of k and the vector of <strong>average silhouette widths</strong><br>
</li>
<li>Use the values in <code>sil_df</code> to plot a line plot showing the relationship between <strong>k</strong> and <strong>average silhouette width</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># Use map_dbl to run many models with varying value of k</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>sil_width <span class="ot">&lt;-</span> <span class="fu">map_dbl</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>,  <span class="cf">function</span>(k){</span>
<span id="cb20-3"><a href="#cb20-3"></a>  model <span class="ot">&lt;-</span> <span class="fu">pam</span>(<span class="at">x =</span> customers_spend, <span class="at">k =</span> k)</span>
<span id="cb20-4"><a href="#cb20-4"></a>  model<span class="sc">$</span>silinfo<span class="sc">$</span>avg.width</span>
<span id="cb20-5"><a href="#cb20-5"></a>})</span>
<span id="cb20-6"><a href="#cb20-6"></a></span>
<span id="cb20-7"><a href="#cb20-7"></a><span class="co"># Generate a data frame containing both k and sil_width</span></span>
<span id="cb20-8"><a href="#cb20-8"></a>sil_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb20-9"><a href="#cb20-9"></a>  <span class="at">k =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb20-10"><a href="#cb20-10"></a>  <span class="at">sil_width =</span> sil_width</span>
<span id="cb20-11"><a href="#cb20-11"></a>)</span>
<span id="cb20-12"><a href="#cb20-12"></a></span>
<span id="cb20-13"><a href="#cb20-13"></a><span class="co"># Plot the relationship between k and sil_width</span></span>
<span id="cb20-14"><a href="#cb20-14"></a><span class="fu">ggplot</span>(sil_df, <span class="fu">aes</span>(<span class="at">x =</span> k, <span class="at">y =</span> sil_width)) <span class="sc">+</span></span>
<span id="cb20-15"><a href="#cb20-15"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb20-16"><a href="#cb20-16"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb20-17"><a href="#cb20-17"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You’re doing great! From the plot I hope you noticed that k = 2 has the highest average sillhouette width and is the <strong>\“best\”</strong> value of <strong>k</strong> we will move forward with.</p>
</section>
<section id="revisiting-wholesale-data-exploration" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-wholesale-data-exploration">Revisiting wholesale data: Exploration</h2>
<p>From the previous analysis you have found that <code>k = 2</code> has the highest <strong>average silhouette width</strong>. In this exercise you will continue to analyze the wholesale customer data by building and exploring a kmeans model with <strong>2</strong> clusters.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Build a k-means model called <code>model_customers</code> for the <code>customers_spend</code> data using the <code>kmeans()</code> function with <code>centers = 2</code>.</li>
<li>Extract the vector of cluster assignments from the model <code>model_customers$cluster</code> and store this in the variable <code>clust_customers</code>.</li>
<li>Append the cluster assignments as a column <code>cluster</code> to the <code>customers_spend</code> data frame and save the results to a new data frame called <code>segment_customers</code>.</li>
<li>Calculate the size of each cluster using <code>count()</code>.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb21-2"><a href="#cb21-2"></a></span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="co"># Build a k-means model for the customers_spend with a k of 2</span></span>
<span id="cb21-4"><a href="#cb21-4"></a>model_customers <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(customers_spend, <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb21-5"><a href="#cb21-5"></a></span>
<span id="cb21-6"><a href="#cb21-6"></a><span class="co"># Extract the vector of cluster assignments from the model</span></span>
<span id="cb21-7"><a href="#cb21-7"></a>clust_customers <span class="ot">&lt;-</span> model_customers<span class="sc">$</span>cluster</span>
<span id="cb21-8"><a href="#cb21-8"></a></span>
<span id="cb21-9"><a href="#cb21-9"></a><span class="co"># Build the segment_customers data frame</span></span>
<span id="cb21-10"><a href="#cb21-10"></a>segment_customers <span class="ot">&lt;-</span> <span class="fu">mutate</span>(customers_spend, <span class="at">cluster =</span> clust_customers)</span>
<span id="cb21-11"><a href="#cb21-11"></a></span>
<span id="cb21-12"><a href="#cb21-12"></a><span class="co"># Calculate the size of each cluster</span></span>
<span id="cb21-13"><a href="#cb21-13"></a><span class="fu">count</span>(segment_customers, cluster)</span>
<span id="cb21-14"><a href="#cb21-14"></a></span>
<span id="cb21-15"><a href="#cb21-15"></a><span class="co"># Calculate the mean for each category</span></span>
<span id="cb21-16"><a href="#cb21-16"></a>segment_customers <span class="sc">%&gt;%</span> </span>
<span id="cb21-17"><a href="#cb21-17"></a>  <span class="fu">group_by</span>(cluster) <span class="sc">%&gt;%</span> </span>
<span id="cb21-18"><a href="#cb21-18"></a>  <span class="fu">summarise_all</span>(<span class="fu">list</span>(mean))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Well done! It seems that in this case <strong>cluster 1</strong> consists of individuals who proportionally spend more on <strong>Frozen</strong> food while <strong>cluster 2</strong> customers spent more on <strong>Milk</strong> and <strong>Grocery</strong>. Did you notice that when you explored this data using hierarchical clustering, the method resulted in <strong>4</strong> clusters while using k-means got you <strong>2</strong>. Both of these results are valid, but which one is appropriate for this would require more subject matter expertise. Before you proceed with the next chapter, remember that: Generating clusters is a science, but interpreting them is an art.</p>
</section>
</section>
<section id="case-study-national-occupational-mean-wage" class="level1">
<h1>4. Case Study: National Occupational mean wage</h1>
<p>In this chapter, you will apply the skills you have learned to explore how the average salary amongst professions have changed over time.</p>
<section id="occupational-wage-data" class="level2">
<h2 class="anchored" data-anchor-id="occupational-wage-data">Occupational wage data</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Occupational wage data</strong></p>
<p>There are many types of problems that are suitable for cluster analysis. In the last 3 chapters you encountered two common types of such problems. With the soccer lineup data you worked with clustering based on spatial data. With the wholesale spending data you segmented customers into clusters. In this chapter you will encounter a third type of problem. You will leverage the tools you have learned thus far to explore data that changes with time, or time-series data.</p>
<p><strong>2. Occupational wage data</strong></p>
<p>You will work with data that consists of the average incomes for twenty two occupations in the United States collected from 2001 to 2016.This corresponds to a matrix where the observations are the 22 occupations and the features of these observations are the measurements of the average income for each year.</p>
<p><strong>3. Occupational wage data</strong></p>
<p>This data is stored in the datamatrix called oes.</p>
<p><strong>4. Occupational wage data</strong></p>
<p>We can see the trends of each occupation with respect to time in this plot. So the question we must ask ourselves is which occupations cluster together?Or to put it another way are there distinct trends of observations that we can observe?</p>
<p><strong>5. Next steps: hierarchical clustering</strong></p>
<p>In the next series of exercises you will go through the necessary steps to analyze this data using hierarchical clustering.As we have discussed in chapters 1 and 2 you will:First determine if any pre-processing steps are needed for this data, such as scaling or imputation.Next you will use the post-processed data to create a distance matrix with an appropriate distance metric.Then you will use the distance matrix to build a dendrogram using a chosen linkage criteria.You will then use what you have learned from this dendrogram to select an appropriate height and extract the cluster assignments.Finally, and most importantly you will explore the resulting clusters to determine whether they make sense and what conclusions can be made from them.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>Let’s give it a shot.</p>
</section>
<section id="initial-exploration-of-the-data" class="level2">
<h2 class="anchored" data-anchor-id="initial-exploration-of-the-data">Initial exploration of the data</h2>
<p>You are presented with data from the Occupational Employment Statistics (OES) program which produces employment and wage estimates annually. This data contains the yearly average income from <strong>2001</strong> to <strong>2016</strong> for <strong>22</strong> occupation groups. You would like to use this data to identify clusters of occupations that maintained similar income trends.</p>
<p>The data is stored in your environment as the data.matrix <code>oes</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>oes <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/oes.rds"</span>)</span>
<span id="cb22-2"><a href="#cb22-2"></a>oes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before you begin to cluster this data you should determine whether any pre-processing steps (such as scaling and imputation) are necessary.</p>
<blockquote class="blockquote">
<h2 id="question-10" class="anchored"><em>Question</em></h2>
<p><strong>Leverage the functions <code>head()</code> and <code>summary()</code> to explore the <code>oes</code> data in order to determine which of the pre-processing steps below are necessary:</strong><br> <br> ⬜ <em>NA</em> values exist in the data, hence the values must be imputed or the observations with <em>NAs</em> excluded.<br> ⬜ The variables within this data are not comparable to one another and should be scaled.<br> ⬜ Categorical variables exist within this data and should be appropriately dummified.<br> ⬜ All three pre-processing steps above are necessary for this data.<br> ✅ None of these pre-processing steps are necessary for this data.<br></p>
</blockquote>
<p>Correct, there are no missing values, no categorical and the features are on the same scale. <br> Now you’re ready to cluster this data!</p>
</section>
<section id="hierarchical-clustering-occupation-trees" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering-occupation-trees">Hierarchical clustering: Occupation trees</h2>
<p>In the previous exercise you have learned that the <code>oes</code> data is ready for hierarchical clustering without any preprocessing steps necessary. In this exercise you will take the necessary steps to build a dendrogram of occupations based on their yearly average salaries and propose clusters using a height of <code>100,000</code>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the Euclidean distance between the occupations and store this in <code>dist_oes</code></li>
<li>Run hierarchical clustering using <strong>average</strong> linkage and store in <code>hc_oes</code></li>
<li>Create a denrogram object <code>dend_oes</code> from your <code>hclust</code> result using the function <code>as.dendrogram()</code></li>
<li>Plot the dendrogram</li>
<li>Using the <code>color_branches()</code> function create &amp; plot a new dendrogram with clusters colored by a cut height of 100,000</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Calculate Euclidean distance between the occupations</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>dist_oes <span class="ot">&lt;-</span> <span class="fu">dist</span>(oes, <span class="at">method =</span> <span class="st">'euclidean'</span>)</span>
<span id="cb23-3"><a href="#cb23-3"></a></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="co"># Generate an average linkage analysis </span></span>
<span id="cb23-5"><a href="#cb23-5"></a>hc_oes <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_oes, <span class="at">method =</span> <span class="st">'average'</span>)</span>
<span id="cb23-6"><a href="#cb23-6"></a></span>
<span id="cb23-7"><a href="#cb23-7"></a><span class="co"># Create a dendrogram object from the hclust variable</span></span>
<span id="cb23-8"><a href="#cb23-8"></a>dend_oes <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span>(hc_oes)</span>
<span id="cb23-9"><a href="#cb23-9"></a></span>
<span id="cb23-10"><a href="#cb23-10"></a><span class="co"># Plot the dendrogram</span></span>
<span id="cb23-11"><a href="#cb23-11"></a><span class="fu">plot</span>(dend_oes)</span>
<span id="cb23-12"><a href="#cb23-12"></a></span>
<span id="cb23-13"><a href="#cb23-13"></a><span class="co"># Color branches by cluster formed from the cut at a height of 100000</span></span>
<span id="cb23-14"><a href="#cb23-14"></a>dend_colored <span class="ot">&lt;-</span> <span class="fu">color_branches</span>(dend_oes, <span class="at">h =</span> <span class="dv">100000</span>)</span>
<span id="cb23-15"><a href="#cb23-15"></a></span>
<span id="cb23-16"><a href="#cb23-16"></a><span class="co"># Plot the colored dendrogram</span></span>
<span id="cb23-17"><a href="#cb23-17"></a><span class="fu">plot</span>(dend_colored)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Well done! Based on the dendrogram it may be reasonable to start with the three clusters formed at a height of 100,000. The members of these clusters appear to be tightly grouped but different from one another. Let’s continue this exploration.</p>
</section>
<section id="hierarchical-clustering-preparing-for-exploration" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering-preparing-for-exploration">Hierarchical clustering: Preparing for exploration</h2>
<p>You have now created a potential clustering for the <code>oes</code> data, before you can explore these clusters with ggplot2 you will need to process the <code>oes</code> data matrix into a tidy data frame with each occupation assigned its cluster.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create the <code>df_oes</code> data frame from the <code>oes</code> data.matrix, making sure to store the rowname as a column (use <code>rownames_to_column()</code> from the <code>tibble</code> library)</li>
<li>Build the cluster assignment vector <code>cut_oes</code> using <code>cutree()</code> with a <code>h = 100,000</code></li>
<li>Append the cluster assignments as a column <code>cluster</code> to the <code>df_oes</code> data frame and save the results to a new data frame called <code>clust_oes</code></li>
<li>Use the <code>gather()</code> function from the <code>tidyr()</code> library to reshape the data into a format amenable for ggplot2 analysis and save the tidied data frame as <code>gather_oes</code></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Load package</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb24-3"><a href="#cb24-3"></a></span>
<span id="cb24-4"><a href="#cb24-4"></a>dist_oes <span class="ot">&lt;-</span> <span class="fu">dist</span>(oes,        <span class="at">method =</span> <span class="st">'euclidean'</span>)</span>
<span id="cb24-5"><a href="#cb24-5"></a>hc_oes   <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_oes, <span class="at">method =</span> <span class="st">'average'</span>)</span>
<span id="cb24-6"><a href="#cb24-6"></a></span>
<span id="cb24-7"><a href="#cb24-7"></a><span class="co"># Use rownames_to_column to move the rownames into a column of the data frame</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>df_oes <span class="ot">&lt;-</span> <span class="fu">rownames_to_column</span>(<span class="fu">as.data.frame</span>(oes), <span class="at">var =</span> <span class="st">'occupation'</span>)</span>
<span id="cb24-9"><a href="#cb24-9"></a></span>
<span id="cb24-10"><a href="#cb24-10"></a><span class="co"># Create a cluster assignment vector at h = 100,000</span></span>
<span id="cb24-11"><a href="#cb24-11"></a>cut_oes <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc_oes, <span class="at">h =</span> <span class="dv">100000</span>)</span>
<span id="cb24-12"><a href="#cb24-12"></a></span>
<span id="cb24-13"><a href="#cb24-13"></a><span class="co"># Generate the segmented the oes data frame</span></span>
<span id="cb24-14"><a href="#cb24-14"></a>clust_oes <span class="ot">&lt;-</span> <span class="fu">mutate</span>(df_oes, <span class="at">cluster =</span> cut_oes)</span>
<span id="cb24-15"><a href="#cb24-15"></a></span>
<span id="cb24-16"><a href="#cb24-16"></a><span class="co"># Create a tidy data frame by gathering the year and values into two columns</span></span>
<span id="cb24-17"><a href="#cb24-17"></a>gathered_oes <span class="ot">&lt;-</span> <span class="fu">gather</span>(<span class="at">data =</span> clust_oes, </span>
<span id="cb24-18"><a href="#cb24-18"></a>                       <span class="at">key =</span> year, </span>
<span id="cb24-19"><a href="#cb24-19"></a>                       <span class="at">value =</span> mean_salary, </span>
<span id="cb24-20"><a href="#cb24-20"></a>                       <span class="sc">-</span>occupation, <span class="sc">-</span>cluster)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! You now have the data frames necessary to explore the results of this clustering</p>
</section>
<section id="hierarchical-clustering-plotting-occupational-clusters" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering-plotting-occupational-clusters">Hierarchical clustering: Plotting occupational clusters</h2>
<p>You have successfully created all the parts necessary to explore the results of this hierarchical clustering work. In this exercise you will leverage the named assignment vector <code>cut_oes</code> and the tidy data frame <code>gathered_oes</code> to analyze the resulting clusters.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>View the assignments of each occupation to their clustering by sorting the <code>cut_oes</code> vector using <code>sort()</code></li>
<li>Use ggplot2 to plot each occupation’s average income by year and color the lines by the occupation’s assigned cluster.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># View the clustering assignments by sorting the cluster assignment vector</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="fu">sort</span>(cut_oes)</span>
<span id="cb25-3"><a href="#cb25-3"></a></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co"># Plot the relationship between mean_salary and year and color the lines by the assigned cluster</span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="fu">ggplot</span>(gathered_oes, <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> mean_salary, <span class="at">color =</span> <span class="fu">factor</span>(cluster))) <span class="sc">+</span> </span>
<span id="cb25-6"><a href="#cb25-6"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> occupation)) <span class="sc">+</span></span>
<span id="cb25-7"><a href="#cb25-7"></a>    <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Cool huh! From this work it looks like both Management &amp; Legal professions (cluster 1) experienced the most rapid growth in these 15 years.’s see what we can get by exploring this data using k-means.</p>
</section>
<section id="reviewing-the-hc-results" class="level2">
<h2 class="anchored" data-anchor-id="reviewing-the-hc-results">Reviewing the HC results</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Reviewing the HC results</strong></p>
<p>Great job, you’ve successfully analyzed the occupational wage data using hierarchical clustering. Now, let’s briefly discuss these results before moving on to kmeans clustering.</p>
<p><strong>2. The dendrogram</strong></p>
<p>Remember that this dendrogram was constructed using a euclidean distance and an average linkage criteria. What this means is that at the height of any given branch, all observations belonging to that branch must have an average euclidean distance amongst each other less than or equal to the height of that branch.Rather than using a pre-determined value of k when cutting the tree you used the structure of the tree to make the decision. A height of 100,000 seems reasonable when looking at this structure and generates three clusters. However, it would just be as reasonable to go higher to create two clusters or lower to create four. To better understand the consequence of the cut height, you explored the resulting clusters to see if they make sense.</p>
<p><strong>3. The trends</strong></p>
<p>More specifically you plotted the trends of these three clusters and used color to compare and contrast them. Visually this seems to be a reasonable clustering with three distinct trends or slopes that emerge from the three clusters.</p>
<p><strong>4. Connecting the two</strong></p>
<p>Based on this analysis one observation we can make is that two occupations concurrently had a higher growth in average wages relative to the others.These are the Management and Legal occupations. Good to know when planning a career trajectory huh?</p>
<p><strong>5. Next steps: k-means clustering</strong></p>
<p>Let’s revisit this data through the lens of k-means clustering. In k-means analysis you would first need to determine if any pre-processing steps are necessary. However we have already explored this in the hierarchical clustering work and know that the data can be used as is.So the first step will be to empirically estimate the value of k using the two methods you have learned about, the elbow plot and the maximum average silhouette width.Finally, as with any good clustering analysis, you will analyze your resulting clusters to see they make sense and find out what you can learn from them.</p>
<p><strong>6. Let’s cluster!</strong></p>
<p>Let’s cluster.</p>
</section>
<section id="k-means-elbow-analysis" class="level2">
<h2 class="anchored" data-anchor-id="k-means-elbow-analysis">K-means: Elbow analysis</h2>
<p>In the previous exercises you used the dendrogram to propose a clustering that generated 3 trees. In this exercise you will leverage the k-means elbow plot to propose the “best” number of clusters.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>map_dbl()</code> to run <code>kmeans()</code> using the <code>oes</code> data for k values ranging from 1 to 10 and extract the <strong>total within-cluster sum of squares</strong> value from each model: <code>model$tot.withinss</code>the resulting vector as <code>tot_withinss</code><br>
</li>
<li>Build a new data frame <code>elbow_df</code> containing the values of k and the vector of <strong>total within-cluster sum of squares</strong><br>
</li>
<li>Use the values in <code>elbow_df</code> to plot a line plot showing the relationship between <strong>k</strong> and <strong>total within-cluster sum of squares</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Use map_dbl to run many models with varying value of k (centers)</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>tot_withinss <span class="ot">&lt;-</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,  <span class="cf">function</span>(k){</span>
<span id="cb26-3"><a href="#cb26-3"></a>  model <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> oes, <span class="at">centers =</span> k)</span>
<span id="cb26-4"><a href="#cb26-4"></a>  model<span class="sc">$</span>tot.withinss</span>
<span id="cb26-5"><a href="#cb26-5"></a>})</span>
<span id="cb26-6"><a href="#cb26-6"></a></span>
<span id="cb26-7"><a href="#cb26-7"></a><span class="co"># Generate a data frame containing both k and tot_withinss</span></span>
<span id="cb26-8"><a href="#cb26-8"></a>elbow_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb26-9"><a href="#cb26-9"></a>  <span class="at">k =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb26-10"><a href="#cb26-10"></a>  <span class="at">tot_withinss =</span> tot_withinss</span>
<span id="cb26-11"><a href="#cb26-11"></a>)</span>
<span id="cb26-12"><a href="#cb26-12"></a></span>
<span id="cb26-13"><a href="#cb26-13"></a><span class="co"># Plot the elbow plot</span></span>
<span id="cb26-14"><a href="#cb26-14"></a><span class="fu">ggplot</span>(elbow_df, <span class="fu">aes</span>(<span class="at">x =</span> k, <span class="at">y =</span> tot_withinss)) <span class="sc">+</span></span>
<span id="cb26-15"><a href="#cb26-15"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb26-16"><a href="#cb26-16"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb26-17"><a href="#cb26-17"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Fascinating! So the elbow analysis proposes a different value of <strong>k</strong>, in the next section let’s see what we can learn from Silhouette Width Analysis.</p>
</section>
<section id="k-means-average-silhouette-widths" class="level2">
<h2 class="anchored" data-anchor-id="k-means-average-silhouette-widths">K-means: Average Silhouette Widths</h2>
<p>So hierarchical clustering resulting in <strong>3</strong> clusters and the elbow method suggests <strong>2</strong>. In this exercise use <strong>average silhouette widths</strong> to explore what the “best” value of <strong>k</strong> should be.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>map_dbl()</code> to run <code>pam()</code> using the <code>oes</code> data for k values ranging from 2 to 10 and extract the <strong>average silhouette width</strong> value from each model: <code>model$silinfo$avg.width</code>the resulting vector as <code>sil_width</code><br>
</li>
<li>Build a new data frame <code>sil_df</code> containing the values of k and the vector of <strong>average silhouette widths</strong><br>
</li>
<li>Use the values in <code>sil_df</code> to plot a line plot showing the relationship between <strong>k</strong> and <strong>average silhouette width</strong></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Use map_dbl to run many models with varying value of k</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>sil_width <span class="ot">&lt;-</span> <span class="fu">map_dbl</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>,  <span class="cf">function</span>(k){</span>
<span id="cb27-3"><a href="#cb27-3"></a>  model <span class="ot">&lt;-</span> <span class="fu">pam</span>(oes, <span class="at">k =</span> k)</span>
<span id="cb27-4"><a href="#cb27-4"></a>  model<span class="sc">$</span>silinfo<span class="sc">$</span>avg.width</span>
<span id="cb27-5"><a href="#cb27-5"></a>})</span>
<span id="cb27-6"><a href="#cb27-6"></a></span>
<span id="cb27-7"><a href="#cb27-7"></a><span class="co"># Generate a data frame containing both k and sil_width</span></span>
<span id="cb27-8"><a href="#cb27-8"></a>sil_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb27-9"><a href="#cb27-9"></a>  <span class="at">k =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb27-10"><a href="#cb27-10"></a>  <span class="at">sil_width =</span> sil_width</span>
<span id="cb27-11"><a href="#cb27-11"></a>)</span>
<span id="cb27-12"><a href="#cb27-12"></a></span>
<span id="cb27-13"><a href="#cb27-13"></a><span class="co"># Plot the relationship between k and sil_width</span></span>
<span id="cb27-14"><a href="#cb27-14"></a><span class="fu">ggplot</span>(sil_df, <span class="fu">aes</span>(<span class="at">x =</span> k, <span class="at">y =</span> sil_width)) <span class="sc">+</span></span>
<span id="cb27-15"><a href="#cb27-15"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb27-16"><a href="#cb27-16"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb27-17"><a href="#cb27-17"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! It seems that this analysis results in another value of <strong>k</strong>, this time 7 is the top contender (although 2 comes very close).</p>
</section>
<section id="the-best-number-of-clusters" class="level2">
<h2 class="anchored" data-anchor-id="the-best-number-of-clusters">The “best” number of clusters</h2>
<p>You ran three different methods for finding the optimal number of clusters and their assignments and you arrived with three different answers.</p>
<p>Below you will find a comparison between the 3 clustering results (via coloring of the occupations based on the clusters to which they belong).</p>
<p><img src="http://assets.datacamp.com/production/course_5776/datasets/c4_e09.png" alt="oes_clusters"></p>
<blockquote class="blockquote">
<h2 id="question-11" class="anchored"><em>Question</em></h2>
<p><strong>What can you say about the “best” way to cluster this data?</strong><br> <br> ⬜ The clusters generated by the hierarchical clustering all have members with a Euclidean distance amongst one another less than 100,000 and hence is the <strong>best</strong> clustering method.<br> ⬜ The clusters generated using k-means with a <strong>k = 2</strong> was identified using elbow analysis and hence is the <strong>best</strong> way to cluster this data.<br> ⬜ The clusters generated using k-means with a <strong>k = 7</strong> has the largest Average Silhouette Widths among the cluster and hence is the <strong>best</strong> way to cluster this data.<br> ✅ All of the above are correct but the <strong>best</strong> way to cluster is highly dependent on how you would use this data after.<br></p>
</blockquote>
<p>All 3 statements are correct but there is no quantitative way to determine which of these clustering approaches is the right one without further exploration.</p>
</section>
<section id="review-k-means-results" class="level2">
<h2 class="anchored" data-anchor-id="review-k-means-results">Review K-means results</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Review K-means results</strong></p>
<p>I don’t know about you, but the results of the last exercise were a little unexpected! You used three approaches for finding clusters and got three completely different answers.Which of them is the right one?</p>
<p><strong>2. Three clustering results</strong></p>
<p>If there is one point that I want you to remember from this class, is that the answer is always it depends.It depends on the clustering setup, it depends on the question we are trying to answer and it depends on our understanding of the data that we are working with.To say it another way, clustering methods require a certain amount of subjectivity. They are the looking glass through which we can see a new perspective on our data but it is up to us to judiciously use this perspective.In this case if you would ask for my opinion, I would say that the analysis of the hierarchical based clustering seems to make the most sense here. The three distinct clusters of occupations grouped similar slopes of wage growth effectively while separating the unique trends that appear.</p>
<p><strong>3. Comparing the two clustering methods</strong></p>
<p>Is this always the case? What are the differences between k-means and hierarchical clustering?Well there are some fundamental differences between the two.kmeans relies exclusively on euclidean distance whereas hierarchical clustering can handle virtually any distance metric.kmeans requires a random step that may yield different results if the process is re-run, this would not occur in hierarchical clustering.to estimate the value of k, we can use silhouette analysis and the elbow method for k-means, but the same could be said for hierarchical clustering which additionally has the added benefit of leveraging the dendrogramSo why would we ever use k-means clustering instead of hierarchical clustering. The main reason is that the k-means algorithm is less computationally expensive and can be run on much larger data within a reasonable time frame. This is the reason that this algorithm maintains such wide use and popularity.</p>
<p><strong>4. What have you learned?</strong></p>
<p>I hope you enjoyed this journey to develop the tools and intuition for working with unsupervised clustering as much as I did.In chapter one you learned the central concept to all clustering, distance. You also learned how important scale can be when calculating distance.In chapter two you learned the fundamentals of hierarchical clustering where you utilized distance to iteratively build a dendrogram and then break it down into clusters.In chapter three you worked with the k-means clustering method and learned about its associated tools.You learned a lot, you should give yourself a pat on the back.</p>
<p><strong>5. A lot more to learn</strong></p>
<p>Of course this is only the beginning of your journey. These are just some of the tools you may encounter as you delve further into the world of unsupervised clustering.As a bonus, the pam function you used for silhouette analysis actually used the k-mediods method. Its very similar to k-means except that it can accommodate distance matrices with arbitrary distances just like hierarchical clustering. You should try it out.Two other methods you might be interested in are DBSCAN and Optics clustering, both are very commonly used algorithms that we sadly don’t have time for in this course but I strongly encourage you to take what you’ve learned here to pursue them.</p>
<p><strong>6. Congratulations!</strong></p>
<p>If I can leave you with one parting thought that has helped me along my path in data science. Remember that building intuition for your methods is just as, if not more important than learning how to use their associated tools. Like the explorers of old, we data scientists have a lot of uncharted waters ahead of us, best we understand how our ship works.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Content 2022 by <a href="https://www.startupengineer.io/authors/schwarz/">Joschka Schwarz</a> <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with and <a href="https://quarto.org/">Quarto</a><br> <a href="https://www.github.com/jwarz/jwarz.github.io">View the source at GitHub</a></div>
  </div>
</footer>



<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>