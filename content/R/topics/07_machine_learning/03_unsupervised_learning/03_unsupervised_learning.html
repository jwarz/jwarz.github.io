<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joschka Schwarz">

<title>Joschka Schwarz - Unsupervised Learning in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" rel="next">
<link href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" rel="prev">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

<link href="../../../../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../../../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>


</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Joschka Schwarz</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-science" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-science">    
        <li>
    <a class="dropdown-item" href="../../../../../content/R/index.html">
 <span class="dropdown-text">R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/python/index.html">
 <span class="dropdown-text">Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/sql/index.html">
 <span class="dropdown-text">SQL</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../../slides/index.html">
 <span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../resumes/index.html">
 <span class="menu-text">Resumes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jwarz/"><i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/j-schwarz"><i class="bi bi-linkedin" role="img" aria-label="Quarto LinkedIn">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Unsupervised Learning in R</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <b><i>1 Programming Basics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/01_programming_beginner/01_programming_beginner.html" class="sidebar-item-text sidebar-link">1.1: Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/02_programming_intermediate/02_programming_intermediate.html" class="sidebar-item-text sidebar-link">1.2: Intermediate R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/03_programming_tidyverse/03_programming_tidyverse.html" class="sidebar-item-text sidebar-link">1.3: Introduction to the tidyverse</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>2 Importing Data</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/01_importing_data_beginner/01_importing_data_beginner.html" class="sidebar-item-text sidebar-link">2.1: Introduction to Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/02_importing_data_intermediate/02_importing_data_intermediate.html" class="sidebar-item-text sidebar-link">2.2: Intermediate Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/03_working_with_web_data/03_working_with_web_data.html" class="sidebar-item-text sidebar-link">2.3: Working with web data</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>3 Data Wrangling</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/01_data_manipulation_with_dplyr/01_data_manipulation_with_dplyr.html" class="sidebar-item-text sidebar-link">3.1: Data Manipulation with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/02_joining_data_with_dplyr/02_joining_data_with_dplyr.html" class="sidebar-item-text sidebar-link">3.2: Joining data with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/exploratory_data_analysis_in_r/exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.3: Exploratory Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/case_study_exploratory_data_analysis_in_r/case_study_exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.4: Case Study: EDA</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/cleaning_data_in_r/cleaning_data_in_r.html" class="sidebar-item-text sidebar-link">3.5: Cleaning Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/data_manipulation_with_datatable/data_manipulation_with_datatable.html" class="sidebar-item-text sidebar-link">3.6: Data Manipulation with data.table</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/joining_data_with_datatable/joining_data_with_datatable.html" class="sidebar-item-text sidebar-link">3.7: Joining Data with data.table</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>4 Data Visualization</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/introduction_to_data_visualization_with_ggplot2/ggplot2_intro.html" class="sidebar-item-text sidebar-link">4.1: Introduction to Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/data_visualization_with_ggplot2_intermediate/ggplot2_intermediate.html" class="sidebar-item-text sidebar-link">4.2: Intermediate Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>5 Statistics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/01_statistics_beginner/01_statistics_beginner.html" class="sidebar-item-text sidebar-link">5.1: Introduction to Statistics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/02_fundamentals_of_bayesian_data_analysis/02_fundamentals_of_bayesian_data_analysis.html" class="sidebar-item-text sidebar-link">5.2: Fundamentals of Bayesian Data Analysis</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>6 Regression</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/06_regression/01_regression_beginner/01_regression_beginner.html" class="sidebar-item-text sidebar-link">6.1: Introduction to Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/06_regression/02_regression_intermediate/02_regression_intermediate.html" class="sidebar-item-text sidebar-link">6.2: Intermediate Regression</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>7 Machine Learning</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/01_supervised_learning_classification/01_supervised_learning_classification.html" class="sidebar-item-text sidebar-link">7.1: Supervised Learning: Classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="sidebar-item-text sidebar-link">7.2: Supervised Learning: Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/03_unsupervised_learning/03_unsupervised_learning.html" class="sidebar-item-text sidebar-link active">7.3: Unsupervised Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">7.4: Machine Learning in the tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/05_cluster_analysis/05_cluster_analysis.html" class="sidebar-item-text sidebar-link">7.5: Cluster Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/06_machine_learning_with_caret/06_machine_learning_with_caret.html" class="sidebar-item-text sidebar-link">7.6: Machine Learning with caret</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/07_modeling_with_tidymodels/07_modeling_with_tidymodels.html" class="sidebar-item-text sidebar-link">7.7: Modeling with tidymodels</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/08_machine_learning_with_tree-based_models/08_machine_learning_with_tree-based_models.html" class="sidebar-item-text sidebar-link">7.8: Machine Learning with tree-based Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" class="sidebar-item-text sidebar-link">7.9: Support Vector Machines</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" class="sidebar-item-text sidebar-link">7.10: Topic Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="sidebar-item-text sidebar-link">7.11: Hyperparameter Tuning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" class="sidebar-item-text sidebar-link">7.12: Bayesian Regression Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" class="sidebar-item-text sidebar-link">7.13: Introduction to Spark</a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">R Manuals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/test.html" class="sidebar-item-text sidebar-link">1: An Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/02-content.html" class="sidebar-item-text sidebar-link">2: R Data Import/Export</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/03-content.html" class="sidebar-item-text sidebar-link">3: R Installation and Administration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/04-content.html" class="sidebar-item-text sidebar-link">4: Writing R Extensions</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/05-content.html" class="sidebar-item-text sidebar-link">5: R Language Definition</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/06-content.html" class="sidebar-item-text sidebar-link">6: R Internals</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul class="collapse">
  <li><a href="#unsupervised-learning-in-r" id="toc-unsupervised-learning-in-r" class="nav-link active" data-scroll-target="#unsupervised-learning-in-r"><span class="toc-section-number">1</span>  Unsupervised learning in R</a>
  <ul class="collapse">
  <li><a href="#welcome-to-the-course" id="toc-welcome-to-the-course" class="nav-link" data-scroll-target="#welcome-to-the-course"><span class="toc-section-number">1.1</span>  Welcome to the course!</a></li>
  <li><a href="#identify-clustering-problems" id="toc-identify-clustering-problems" class="nav-link" data-scroll-target="#identify-clustering-problems"><span class="toc-section-number">1.2</span>  Identify clustering problems</a></li>
  <li><a href="#introduction-to-k-means-clustering" id="toc-introduction-to-k-means-clustering" class="nav-link" data-scroll-target="#introduction-to-k-means-clustering"><span class="toc-section-number">1.4</span>  Introduction to k-means clustering</a></li>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering"><span class="toc-section-number">1.5</span>  k-means clustering</a></li>
  <li><a href="#results-of-kmeans" id="toc-results-of-kmeans" class="nav-link" data-scroll-target="#results-of-kmeans"><span class="toc-section-number">1.6</span>  Results of kmeans()</a></li>
  <li><a href="#visualizing-and-interpreting-results-of-kmeans" id="toc-visualizing-and-interpreting-results-of-kmeans" class="nav-link" data-scroll-target="#visualizing-and-interpreting-results-of-kmeans"><span class="toc-section-number">1.7</span>  Visualizing and interpreting results of kmeans()</a></li>
  <li><a href="#how-k-means-works-and-practical-matters" id="toc-how-k-means-works-and-practical-matters" class="nav-link" data-scroll-target="#how-k-means-works-and-practical-matters"><span class="toc-section-number">1.8</span>  How k-means works and practical matters</a></li>
  <li><a href="#handling-random-algorithms" id="toc-handling-random-algorithms" class="nav-link" data-scroll-target="#handling-random-algorithms"><span class="toc-section-number">1.9</span>  Handling random algorithms</a></li>
  <li><a href="#selecting-number-of-clusters" id="toc-selecting-number-of-clusters" class="nav-link" data-scroll-target="#selecting-number-of-clusters"><span class="toc-section-number">1.10</span>  Selecting number of clusters</a></li>
  <li><a href="#introduction-to-the-pokemon-data" id="toc-introduction-to-the-pokemon-data" class="nav-link" data-scroll-target="#introduction-to-the-pokemon-data"><span class="toc-section-number">1.11</span>  Introduction to the Pokemon data</a></li>
  <li><a href="#practical-matters-working-with-real-data" id="toc-practical-matters-working-with-real-data" class="nav-link" data-scroll-target="#practical-matters-working-with-real-data"><span class="toc-section-number">1.12</span>  Practical matters: working with real data</a></li>
  <li><a href="#review-of-k-means-clustering" id="toc-review-of-k-means-clustering" class="nav-link" data-scroll-target="#review-of-k-means-clustering"><span class="toc-section-number">1.13</span>  Review of k-means clustering</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="toc-section-number">2</span>  2. Hierarchical clustering</a>
  <ul class="collapse">
  <li><a href="#introduction-to-hierarchical-clustering" id="toc-introduction-to-hierarchical-clustering" class="nav-link" data-scroll-target="#introduction-to-hierarchical-clustering"><span class="toc-section-number">2.1</span>  Introduction to hierarchical clustering</a></li>
  <li><a href="#hierarchical-clustering-with-results" id="toc-hierarchical-clustering-with-results" class="nav-link" data-scroll-target="#hierarchical-clustering-with-results"><span class="toc-section-number">2.2</span>  Hierarchical clustering with results</a></li>
  <li><a href="#selecting-number-of-clusters-1" id="toc-selecting-number-of-clusters-1" class="nav-link" data-scroll-target="#selecting-number-of-clusters-1"><span class="toc-section-number">2.3</span>  Selecting number of clusters</a></li>
  <li><a href="#interpreting-dendrogram" id="toc-interpreting-dendrogram" class="nav-link" data-scroll-target="#interpreting-dendrogram"><span class="toc-section-number">2.4</span>  Interpreting dendrogram</a></li>
  <li><a href="#cutting-the-tree" id="toc-cutting-the-tree" class="nav-link" data-scroll-target="#cutting-the-tree"><span class="toc-section-number">2.6</span>  Cutting the tree</a></li>
  <li><a href="#clustering-linkage-and-practical-matters" id="toc-clustering-linkage-and-practical-matters" class="nav-link" data-scroll-target="#clustering-linkage-and-practical-matters"><span class="toc-section-number">2.7</span>  Clustering linkage and practical matters</a></li>
  <li><a href="#linkage-methods" id="toc-linkage-methods" class="nav-link" data-scroll-target="#linkage-methods"><span class="toc-section-number">2.8</span>  Linkage methods</a></li>
  <li><a href="#comparing-linkage-methods" id="toc-comparing-linkage-methods" class="nav-link" data-scroll-target="#comparing-linkage-methods"><span class="toc-section-number">2.9</span>  Comparing linkage methods</a></li>
  <li><a href="#practical-matters-scaling" id="toc-practical-matters-scaling" class="nav-link" data-scroll-target="#practical-matters-scaling"><span class="toc-section-number">2.11</span>  Practical matters: scaling</a></li>
  <li><a href="#comparing-kmeans-and-hclust" id="toc-comparing-kmeans-and-hclust" class="nav-link" data-scroll-target="#comparing-kmeans-and-hclust"><span class="toc-section-number">2.12</span>  Comparing kmeans() and hclust()</a></li>
  <li><a href="#review-of-hierarchical-clustering" id="toc-review-of-hierarchical-clustering" class="nav-link" data-scroll-target="#review-of-hierarchical-clustering"><span class="toc-section-number">2.13</span>  Review of hierarchical clustering</a></li>
  </ul></li>
  <li><a href="#dimensionality-reduction-with-pca" id="toc-dimensionality-reduction-with-pca" class="nav-link" data-scroll-target="#dimensionality-reduction-with-pca"><span class="toc-section-number">3</span>  3. Dimensionality reduction with PCA</a>
  <ul class="collapse">
  <li><a href="#introduction-to-pca" id="toc-introduction-to-pca" class="nav-link" data-scroll-target="#introduction-to-pca"><span class="toc-section-number">3.1</span>  Introduction to PCA</a></li>
  <li><a href="#pca-using-prcomp" id="toc-pca-using-prcomp" class="nav-link" data-scroll-target="#pca-using-prcomp"><span class="toc-section-number">3.2</span>  PCA using prcomp()</a></li>
  <li><a href="#results-of-pca" id="toc-results-of-pca" class="nav-link" data-scroll-target="#results-of-pca"><span class="toc-section-number">3.3</span>  Results of PCA</a></li>
  <li><a href="#additional-results-of-pca" id="toc-additional-results-of-pca" class="nav-link" data-scroll-target="#additional-results-of-pca"><span class="toc-section-number">3.5</span>  Additional results of PCA</a></li>
  <li><a href="#visualizing-and-interpreting-pca-results" id="toc-visualizing-and-interpreting-pca-results" class="nav-link" data-scroll-target="#visualizing-and-interpreting-pca-results"><span class="toc-section-number">3.7</span>  Visualizing and interpreting PCA results</a></li>
  <li><a href="#interpreting-biplots-1" id="toc-interpreting-biplots-1" class="nav-link" data-scroll-target="#interpreting-biplots-1"><span class="toc-section-number">3.8</span>  Interpreting biplots (1)</a></li>
  <li><a href="#interpreting-biplots-2" id="toc-interpreting-biplots-2" class="nav-link" data-scroll-target="#interpreting-biplots-2"><span class="toc-section-number">3.10</span>  Interpreting biplots (2)</a></li>
  <li><a href="#variance-explained" id="toc-variance-explained" class="nav-link" data-scroll-target="#variance-explained"><span class="toc-section-number">3.12</span>  Variance explained</a></li>
  <li><a href="#visualize-variance-explained" id="toc-visualize-variance-explained" class="nav-link" data-scroll-target="#visualize-variance-explained"><span class="toc-section-number">3.13</span>  Visualize variance explained</a></li>
  <li><a href="#practical-issues-with-pca" id="toc-practical-issues-with-pca" class="nav-link" data-scroll-target="#practical-issues-with-pca"><span class="toc-section-number">3.14</span>  Practical issues with PCA</a></li>
  <li><a href="#practical-issues-scaling" id="toc-practical-issues-scaling" class="nav-link" data-scroll-target="#practical-issues-scaling"><span class="toc-section-number">3.15</span>  Practical issues: scaling</a></li>
  <li><a href="#additional-uses-of-pca-and-wrap-up" id="toc-additional-uses-of-pca-and-wrap-up" class="nav-link" data-scroll-target="#additional-uses-of-pca-and-wrap-up"><span class="toc-section-number">3.16</span>  Additional uses of PCA and wrap-up</a></li>
  </ul></li>
  <li><a href="#putting-it-all-together-with-a-case-study" id="toc-putting-it-all-together-with-a-case-study" class="nav-link" data-scroll-target="#putting-it-all-together-with-a-case-study"><span class="toc-section-number">4</span>  4. Putting it all together with a case study</a>
  <ul class="collapse">
  <li><a href="#introduction-to-the-case-study" id="toc-introduction-to-the-case-study" class="nav-link" data-scroll-target="#introduction-to-the-case-study"><span class="toc-section-number">4.1</span>  Introduction to the case study</a></li>
  <li><a href="#preparing-the-data" id="toc-preparing-the-data" class="nav-link" data-scroll-target="#preparing-the-data"><span class="toc-section-number">4.2</span>  Preparing the data</a></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis"><span class="toc-section-number">4.3</span>  Exploratory data analysis</a></li>
  <li><a href="#performing-pca" id="toc-performing-pca" class="nav-link" data-scroll-target="#performing-pca"><span class="toc-section-number">4.5</span>  Performing PCA</a></li>
  <li><a href="#interpreting-pca-results" id="toc-interpreting-pca-results" class="nav-link" data-scroll-target="#interpreting-pca-results"><span class="toc-section-number">4.6</span>  Interpreting PCA results</a></li>
  <li><a href="#variance-explained-1" id="toc-variance-explained-1" class="nav-link" data-scroll-target="#variance-explained-1"><span class="toc-section-number">4.7</span>  Variance explained</a></li>
  <li><a href="#pca-review-and-next-steps" id="toc-pca-review-and-next-steps" class="nav-link" data-scroll-target="#pca-review-and-next-steps"><span class="toc-section-number">4.8</span>  PCA review and next steps</a></li>
  <li><a href="#communicating-pca-results" id="toc-communicating-pca-results" class="nav-link" data-scroll-target="#communicating-pca-results"><span class="toc-section-number">4.9</span>  Communicating PCA results</a></li>
  <li><a href="#hierarchical-clustering-of-case-data" id="toc-hierarchical-clustering-of-case-data" class="nav-link" data-scroll-target="#hierarchical-clustering-of-case-data"><span class="toc-section-number">4.11</span>  Hierarchical clustering of case data</a></li>
  <li><a href="#results-of-hierarchical-clustering" id="toc-results-of-hierarchical-clustering" class="nav-link" data-scroll-target="#results-of-hierarchical-clustering"><span class="toc-section-number">4.12</span>  Results of hierarchical clustering</a></li>
  <li><a href="#selecting-number-of-clusters-2" id="toc-selecting-number-of-clusters-2" class="nav-link" data-scroll-target="#selecting-number-of-clusters-2"><span class="toc-section-number">4.14</span>  Selecting number of clusters</a></li>
  <li><a href="#k-means-clustering-and-comparing-results" id="toc-k-means-clustering-and-comparing-results" class="nav-link" data-scroll-target="#k-means-clustering-and-comparing-results"><span class="toc-section-number">4.15</span>  k-means clustering and comparing results</a></li>
  <li><a href="#clustering-on-pca-results" id="toc-clustering-on-pca-results" class="nav-link" data-scroll-target="#clustering-on-pca-results"><span class="toc-section-number">4.16</span>  Clustering on PCA results</a></li>
  <li><a href="#wrap-up-and-review" id="toc-wrap-up-and-review" class="nav-link" data-scroll-target="#wrap-up-and-review"><span class="toc-section-number">4.17</span>  Wrap-up and review</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jwarz/jwarz.github.io/edit/main/content/R/topics/07_machine_learning/03_unsupervised_learning/03_unsupervised_learning.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jwarz/jwarz.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Unsupervised Learning in R</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joschka Schwarz </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>Many times in machine learning, the goal is to find patterns in data without trying to make predictions. This is called unsupervised learning. One common use case of unsupervised learning is grouping consumers based on demographics and purchasing history to deploy targeted marketing campaigns. Another example is wanting to describe the unmeasured factors that most influence crime differences between cities. This course provides a basic introduction to clustering and dimensionality reduction in R from a machine learning perspective, so that you can get from data to insights as quickly as possible.</p>
<section id="unsupervised-learning-in-r" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Unsupervised learning in R</h1>
<p>The k-means algorithm is one common approach to clustering. Learn how the algorithm works under the hood, implement k-means clustering in R, visualize and interpret the results, and select the number of clusters when it’s not known ahead of time. By the end of the chapter, you’ll have applied k-means clustering to a fun “real-world” dataset!</p>
<section id="welcome-to-the-course" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="welcome-to-the-course"><span class="header-section-number">1.1</span> Welcome to the course!</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Welcome to the course!</strong></p>
<p>Hi! I’m Hank Roark, I’m a long-time data scientist and user of the R language, and I’ll be your instructor for this course on unsupervised learning in R.</p>
<p><strong>2. Chapter 1 overview</strong></p>
<p>In this first chapter I will define ‘unsupervised learning’, provide an overview of the three major types of machine learning, and you will learn how to execute one particular type of unsupervised learning using R.</p>
<p><strong>3. Types of machine learning</strong></p>
<p>There are three major types of machine learning. The first type is unsupervised learning. The goal of unsupervised learning is to find structure in unlabeled data. Unlabeled data is data without a target, without labeled responses.Contrast this with supervised learning. Supervised learning is used when you want to make predictions on labeled data, on data with a target.Types of predictions include regression, or predicting how much of something there is or could be, and classification which is predicting what type or class some thing is or could be.The final type is reinforcement learning, where a computer learns from feedback by operating in a real or synthetic environment.</p>
<p><strong>4. Labeled vs.&nbsp;unlabeled data</strong></p>
<p>Here is a quick example of the difference between labeled and unlabeled data. The table on the left is an example with three observations about shapes, each shape with three features, represented by the three columns. This table, the one on the left is an example of unlabeled data.</p>
<p><strong>5. Labeled vs.&nbsp;unlabeled data</strong></p>
<p>If an additional vector of labels is added, like the column of labels on the right hand side, labeling each observation as belonging to one of two groups, then we would have labeled data.</p>
<p><strong>6. Unsupervised learning - clustering</strong></p>
<p>Within unsupervised learning there are two major goals. The first goal is to find homogeneous subgroups within a population. As an example let us pretend we have a population of six people. Each member of this population might have some attributes, or features — some examples of features for a person might be annual income, educational attainment, and gender.</p>
<p><strong>7. Unsupervised learning - clustering</strong></p>
<p>With those three features one might find there are two homogeneous subgroups, or groups where the members are similar by some measure of similarity.</p>
<p><strong>8. Unsupervised learning - clustering</strong></p>
<p>Once the members of each group are found, we might label one group subgroup A and the other subgroup B. The process of finding homogeneous subgroups is referred to as clustering.</p>
<p><strong>9. Clustering examples</strong></p>
<p>There are many possible applications of clustering. One use case is segmenting a market of consumers or potential consumers. This is commonly done by finding groups, or clusters, of consumers based on demographic features and purchasing history.</p>
<p><strong>10. Clustering examples</strong></p>
<p>Another example of clustering would be to find groups of movies based on features of each movie and the reviews of the movies. One might do this to find movies most like another movie.</p>
<p><strong>11. Unsupervised learning - dimensionality reduction</strong></p>
<p>The second goal of unsupervised learning is to find patterns in the features of the data. One way to do this is through ‘dimensionality reduction’. Dimensionality reduction is a method to decrease the number of features to describe an observation while maintaining the maximum information content under the constraints of lower dimensionality.</p>
<p><strong>12. Unsupervised learning - dimensionality reduction</strong></p>
<p>Dimensionality reduction is often used to achieve two goals, in addition to finding patterns in the features of the data.Dimensionality reduction allows one to visually represent high dimensional data while maintaining much of the data variability. This is done because visually representing and understanding data with more than 3 or 4 features can be difficult for both the producer and consumer of the visualization.The third major reason for dimensionality reduction is as a preprocessing step for supervised learning. More on this usage will be covered later.</p>
<p><strong>13. Challenges and benefits</strong></p>
<p>Finally a few words about the challenges and benefits typical in performing unsupervised learning.In unsupervised learning there is often no single goal of the analysis. This can be presented as someone asking you, the analyst, “to find some patterns in the data.” With that challenge, unsupervised learning often demands and brings out the deep creativity of the analyst.Finally, there is much more unlabeled data than labeled data. This means there are more opportunities to apply unsupervised learning in your work.</p>
<p><strong>14. Let’s practice!</strong></p>
<p>Now it’s your turn to practice what you’ve learned.</p>
</section>
<section id="identify-clustering-problems" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="identify-clustering-problems"><span class="header-section-number">1.2</span> Identify clustering problems</h2>
<blockquote class="blockquote">
<h2 id="question" data-number="1.3" class="anchored"><span class="header-section-number">1.3</span> <em>Question</em></h2>
<p>Which of the following are clustering problems?</p>
<ol type="1">
<li>Determining how many features it takes to describe most of the variability in data</li>
<li>Determining the natural groupings of houses for sale based on size, number of bedrooms, etc.</li>
<li>Visualizing 13 dimensional data (data with 13 features)</li>
<li>Determining if there are common patterns in the demographics of people at a commerce site</li>
<li>Predicting if someone will click on a web advertisement <br> ⬜ 1, 3, and 5<br> ⬜ 2 and 3<br> ⬜ 1, 2, and 4<br> ✅ 2 and 4<br> ⬜ All 5 are clustering problems<br></li>
</ol>
</blockquote>
</section>
<section id="introduction-to-k-means-clustering" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="introduction-to-k-means-clustering"><span class="header-section-number">1.4</span> Introduction to k-means clustering</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Introduction to k-means clustering</strong></p>
<p>Now that we have some conceptual understanding of unsupervised learning and the different goals of unsupervised learning, let’s dig right in with one popular approach to unsupervised learning.</p>
<p><strong>2. k-means clustering algorithm</strong></p>
<p>K-means is a clustering algorithm, an algorithm used to find homogeneous subgroups within a population. K-means is the first of two clustering algorithms to be covered in this course.The K-means algorithm works by first assuming the number of subgroups, or clusters, in the data and then assigns each observation to one of those subgroups. In the next video, we will go deeper into how the k-means algorithm works to achieve this goal.For example, one might hypothesize that this data shown on the screen contain 2 subgroups.The k-means algorithm would assign all points in the top right hand corner to one subgroup and all observations in the bottom left hand corner to the other subgroup.</p>
<p><strong>3. k-means in R</strong></p>
<p>k-means in R comes with the base R install. Invoking k-means in R is simply a function call to kmeans() function, typically with three parameters.The first parameter is the data, represented as ‘x’ here. In k-means, like many machine learning algorithms, the data is structured in a matrix with one observation per row of the matrix and one feature in each column of the matrix.The next parameters for ‘kmeans’ is the number of predetermined groups or clusters. This parameter is called ‘centers’, for reasons that will be covered in the next video.Finally, the kmeans algorithm has a random component. The implication of this stochastic component is that a single run of kmeans may not find the optimal solution to kmeans.To overcome the random component of the algorithm, ‘kmeans’ can be run multiple times with the ‘best’ outcome across all runs being selected as the single outcome. ‘nstart’ is the parameter that specifies the number of times ‘kmeans’ will be repeated.There are other parameters to ‘kmeans’ and I encourage you to check those out in the R documentation when you are ready.</p>
<p><strong>4. First exercises</strong></p>
<p>The first exercises use synthetic data that were generated from three subgroups. But if you plot the data it might only appear to be two subgroups. Later in this chapter, you will see how k-means can be used to estimate the number of subgroups when the number of subgroups is not known a priori.Later in this first chapter of the course, you will get experience applying ‘kmeans’ with a real world, but fun, dataset.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>With that information, let’s get started on the first exercise using ‘kmeans’.</p>
</section>
<section id="k-means-clustering" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="k-means-clustering"><span class="header-section-number">1.5</span> k-means clustering</h2>
<p>We have created some two-dimensional data and stored it in a variable called <code>x</code> in your workspace. The scatter plot on the right is a visual representation of the data.</p>
<p>In this exercise, your task is to create a k-means model of the <code>x</code> data using 3 clusters, then to look at the structure of the resulting model using the <a href="https://www.rdocumentation.org/packages/base/topics/summary"><code>summary()</code></a> function.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Fit a k-means model to <code>x</code> using 3 centers and run the k-means algorithm 20 times. Store the result in <code>km.out</code>.</li>
<li>Inspect the result with the <code>summary()</code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># load data</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>x <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/x.rds"</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">plot</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-2-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Create the k-means model: km.out</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>km.out <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(x, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co"># Inspect the result</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="fu">summary</span>(km.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;              Length Class  Mode   
#&gt; cluster      300    -none- numeric
#&gt; centers        6    -none- numeric
#&gt; totss          1    -none- numeric
#&gt; withinss       3    -none- numeric
#&gt; tot.withinss   1    -none- numeric
#&gt; betweenss      1    -none- numeric
#&gt; size           3    -none- numeric
#&gt; iter           1    -none- numeric
#&gt; ifault         1    -none- numeric</code></pre>
</div>
</div>
<p>Great work!</p>
</section>
<section id="results-of-kmeans" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="results-of-kmeans"><span class="header-section-number">1.6</span> Results of kmeans()</h2>
<p>The <code>kmeans()</code> function produces several outputs. In the video, we discussed one output of modeling, the cluster membership.</p>
<p>In this exercise, you will access the <code>cluster</code> component directly. This is useful anytime you need the cluster membership for each observation of the data used to build the clustering model. A future exercise will show an example of how this cluster membership might be used to help communicate the results of k-means modeling.</p>
<p>k-means models also have a print method to give a human friendly output of basic modeling results. This is available by using <code>print()</code> or simply typing the name of the model.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Print a list of the cluster membership to the console.</li>
<li>Use a print method to print out the <code>km.out</code> model.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Print the cluster membership component of the model</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>km.out<span class="sc">$</span>cluster</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;   [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2
#&gt;  [38] 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2
#&gt;  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3
#&gt; [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#&gt; [149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#&gt; [186] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#&gt; [223] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 1 2 2 2 2
#&gt; [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2
#&gt; [297] 2 1 2 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Print the km.out object</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>km.out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; K-means clustering with 3 clusters of sizes 98, 52, 150
#&gt; 
#&gt; Cluster means:
#&gt;         [,1]        [,2]
#&gt; 1  2.2171113  2.05110690
#&gt; 2  0.6642455 -0.09132968
#&gt; 3 -5.0556758  1.96991743
#&gt; 
#&gt; Clustering vector:
#&gt;   [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2
#&gt;  [38] 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2
#&gt;  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3
#&gt; [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#&gt; [149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#&gt; [186] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#&gt; [223] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 1 2 2 2 2
#&gt; [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2
#&gt; [297] 2 1 2 2
#&gt; 
#&gt; Within cluster sum of squares by cluster:
#&gt; [1] 148.64781  95.50625 295.16925
#&gt;  (between_SS / total_SS =  87.2 %)
#&gt; 
#&gt; Available components:
#&gt; 
#&gt; [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
#&gt; [6] "betweenss"    "size"         "iter"         "ifault"</code></pre>
</div>
</div>
<p>Take a look at all the different components of a k-means model object as you may need to access them in later exercises. Because printing the whole model object to the console outputs many different things, you may wish to instead print a specific component of the model object using the <code>$</code> operator. Great work!</p>
</section>
<section id="visualizing-and-interpreting-results-of-kmeans" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="visualizing-and-interpreting-results-of-kmeans"><span class="header-section-number">1.7</span> Visualizing and interpreting results of kmeans()</h2>
<p>One of the more intuitive ways to interpret the results of k-means models is by plotting the data as a scatter plot and using color to label the samples’ cluster membership. In this exercise, you will use the standard <code>plot()</code> function to accomplish this.</p>
<p>To create a scatter plot, you can pass data with two features (i.e.&nbsp;columns) to <code>plot()</code> with an extra argument <code>col = km.out$cluster</code>, which sets the color of each point in the scatter plot according to its cluster membership.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li><p>Using the <code>plot()</code> function to create a scatter plot of data <code>x</code>:</p>
<ul>
<li>Color the dots on the scatterplot by setting the <code>col</code> argument to the <code>cluster</code> component in <code>km.out</code>.</li>
<li>Title the plot <code>"k-means with 3 clusters"</code> using the <code>main</code> argument to <code>plot()</code>.</li>
<li>Ensure there are no axis labels by specifying <code>""</code> for both the <code>xlab</code> and <code>ylab</code> arguments to <code>plot()</code>.</li>
</ul></li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Scatter plot of x</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">plot</span>(x, <span class="at">col =</span> km.out<span class="sc">$</span>cluster,</span>
<span id="cb8-3"><a href="#cb8-3"></a>     <span class="at">main =</span> <span class="st">"k-means with 3 clusters"</span>, </span>
<span id="cb8-4"><a href="#cb8-4"></a>     <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-4-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Excellent! Let’s see how the <code>kmeans()</code> function works under the hood in the next video.</p>
</section>
<section id="how-k-means-works-and-practical-matters" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="how-k-means-works-and-practical-matters"><span class="header-section-number">1.8</span> How k-means works and practical matters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. How k-means works and practical matters</strong></p>
<p>In this section I am going to help build intuition about how ‘kmeans’ works internally.</p>
<p><strong>2. Objectives</strong></p>
<p>My goal is to do this through visual understanding; if you are interested in the mathematics, there are many sources available on the web and in print. After that, I will present methods for determining the number of subgroups, or clusters, if that is not known beforehand.</p>
<p><strong>3. Observations</strong></p>
<p>Here is data with 2 features. I know that the data for this sample is originally from two subgroups.The first step in the ‘kmeans’ algorithm is to randomly assign each point to one of the two clusters.</p>
<p><strong>4. Random cluster assignment</strong></p>
<p>This is the random aspect of the kmeans algorithm. Cluster one is represented by empty green circles and cluster two is represented by empty blue triangles.The next step of ‘kmeans’ is to calculate the centers of each of the two subgroups.</p>
<p><strong>5. Cluster centers calculated</strong></p>
<p>The centers of each subgroup is the average position of all the points in that subgroup. The center for each subgroup is shown as the solid green circle and the solid blue triangle for subgroups 1 and 2 respectfully.Next, each point in the data is assigned to the cluster of the nearest center.</p>
<p><strong>6. Iteration 1 - after reassignment</strong></p>
<p>Here, you can see that all the points closest to the solid blue triangle center have been assigned to that cluster. The equivalent is true for the other subgroup.This completes one iteration of the ‘kmeans’ algorithm. The ‘kmeans’ algorithm will finish when no points change assignment.In this case, many points change cluster assignment, so another iteration will be completed.</p>
<p><strong>7. Iteration 2</strong></p>
<p>Here, we see the kmeans algorithm after completion of 2 iterations. New cluster centers have been calculated and each observation has been assigned to the cluster of the nearest center.</p>
<p><strong>8. Iteration 3</strong></p>
<p>And here is the algorithm after completion of three iterations. Again some points have changed cluster assignments so another iteration of algorithm will complete.</p>
<p><strong>9. Iteration 4</strong></p>
<p>And this is after completion of the fourth iteration.</p>
<p><strong>10. Iteration 5</strong></p>
<p>The algorithm is completed after the fifth iteration. No observations have changed assignment from the end of the fourth to the end of this iteration so the ‘kmeans’ algorithm stops. This final plot thus shows the cluster assignments for each observation and the cluster centers for each of the two clusters.There are other stopping criteria that you can specify for the ‘kmeans’ algorithm, such as stopping after some number of iterations or if the cluster centers move less than some distance.</p>
<p><strong>11. Model selection</strong></p>
<p>Because kmeans has a random component, it is run multiple times and the best solution is selected from the multiple runs. The ‘kmeans’ algorithm needs a measurement of model quality to determine the ‘best’ outcome of multiple runs.’kmeans’ in R uses the total within cluster sum of squares as that measurement. The ‘kmeans’ run with the minimum total within cluster sum of squares is considered the best model.Total within cluster sum of squares is easy to calculate. For each cluster in the model and for each observation assigned to that cluster, calculate the squared distance from the observation to the cluster center. This is just the squared Euclidean distance from plane geometry class.Sum all of the squared distances calculated and that is the total within cluster sum of squares.</p>
<p><strong>12. Model selection</strong></p>
<p>R does all of this model selection automatically. By specifying ‘nstart’ in kmeans, the algorithm will be run ‘nstart’ times and the run with the lowest total within cluster sum of squares will be the resulting model. This helps the algorithm find a global minimum instead of a local minimum, but does not guarantee that outcome. In the hands-on exercises I will show you how to determine the total within cluster sum of squares from the results of running kmeans.</p>
<p><strong>13. Running k-means multiple times</strong></p>
<p>Here is a visual example of running the ‘kmeans’ algorithm on the same data multiple times. In this case it is known that there are three clusters within the data. The graph on the top right has lowest total within cluster sum of squares.Another item of note — cluster membership is color-coded in these plots, notice the even between runs that find approximately same solution that the cluster memberships are assigned differently — this is not a big deal, just a result of the ‘kmeans’ algorithm that you should keep in mind. For repeatability, use R’s set-seed function before running ‘kmeans’ to guarantee reproducibility.</p>
<p><strong>14. Determining the best number of clusters</strong></p>
<p>If you don’t know the number of subgroups within the data beforehand, there is a way to heuristically determine the number of clusters.You could use trial and error, but instead the best approach is to run ‘kmeans’ with 1 through some number of clusters, recording the total within cluster sum of squares for each number of clusters. This is then plotted with the number of clusters on the horizontal axis and the total within cluster sum of squares on the vertical axis — this type of plot is referred to as a scree plot.</p>
<p><strong>15. Determining the best number of clusters</strong></p>
<p>There may be an ‘elbow’ in the plotted data, a place where the total within cluster sum of squares decreases much slower with the addition of another cluster. In the plot above, the elbow appears at 2 clusters. This value can then be used to approximate the number of clusters if it is not given or known beforehand.</p>
<p><strong>16. Let’s practice!</strong></p>
<p>Cool, let’s practice what you’ve learned.</p>
</section>
<section id="handling-random-algorithms" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="handling-random-algorithms"><span class="header-section-number">1.9</span> Handling random algorithms</h2>
<p>In the video, you saw how <code>kmeans()</code> randomly initializes the centers of clusters. This random initialization can result in assigning observations to different cluster labels. Also, the random initialization can result in finding different <em>local minima</em> for the k-means algorithm. This exercise will demonstrate both results.</p>
<p>At the top of each plot, the measure of model quality—total within cluster sum of squares error—will be plotted. Look for the model(s) with the lowest error to find models with the better model results.</p>
<p>Because <code>kmeans()</code> initializes observations to random clusters, it is important to set the random number generator seed for reproducibility.</p>
<p><strong>Steps</strong></p>
<p>Your task is to generate six <code>kmeans()</code> models on the data, plotting the results of each, in order to see the impact of random initializations on model results.</p>
<pre><code>* Set the random number seed to 1 with `set.seed()`.
* For each iteration of the `for` loop, run `kmeans()` on `x`. Assume the number of clusters is 3 and number of starts (`nstart`) is 1.
* Visualize the cluster memberships using the `col` argument to `plot()`. Observe how the measure of quality and cluster assignments vary among the six model runs.</code></pre>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Set up 2 x 3 plotting grid</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co"># Set seed</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>) {</span>
<span id="cb10-8"><a href="#cb10-8"></a>  <span class="co"># Run kmeans() on x with three clusters and one start</span></span>
<span id="cb10-9"><a href="#cb10-9"></a>  km.out <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(x, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">1</span>)</span>
<span id="cb10-10"><a href="#cb10-10"></a>  </span>
<span id="cb10-11"><a href="#cb10-11"></a>  <span class="co"># Plot clusters</span></span>
<span id="cb10-12"><a href="#cb10-12"></a>  <span class="fu">plot</span>(x, <span class="at">col =</span> km.out<span class="sc">$</span>cluster, </span>
<span id="cb10-13"><a href="#cb10-13"></a>       <span class="at">main =</span> km.out<span class="sc">$</span>tot.withinss, </span>
<span id="cb10-14"><a href="#cb10-14"></a>       <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>)</span>
<span id="cb10-15"><a href="#cb10-15"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-5-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Interesting! Because of the random initialization of the k-means algorithm, there’s quite some variation in cluster assignments among the six models.</p>
</section>
<section id="selecting-number-of-clusters" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="selecting-number-of-clusters"><span class="header-section-number">1.10</span> Selecting number of clusters</h2>
<p>The k-means algorithm assumes the number of clusters as part of the input. If you know the number of clusters in advance (e.g.&nbsp;due to certain business constraints) this makes setting the number of clusters easy. However, as you saw in the video, if you do not know the number of clusters and need to determine it, you will need to run the algorithm multiple times, each time with a different number of clusters. From this, you can observe how a measure of model quality changes with the number of clusters.</p>
<p>In this exercise, you will run <code>kmeans()</code> multiple times to see how model quality changes as the number of clusters changes. Plots displaying this information help to determine the number of clusters and are often referred to as <em>scree plots</em>.</p>
<p>The ideal plot will have an <em>elbow</em> where the quality measure improves more slowly as the number of clusters increases. This indicates that the quality of the model is no longer improving substantially as the model complexity (i.e.&nbsp;number of clusters) increases. In other words, the elbow indicates the number of clusters inherent in the data.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Build 15 <code>kmeans()</code> models on <code>x</code>, each with a different number of clusters (ranging from 1 to 15). Set <code>nstart = 20</code> for all model runs and save the total within cluster sum of squares for each model to the <code>i</code>th element of <code>wss</code>.</li>
<li>Run the code provided to create a scree plot of the <code>wss</code> for all 15 models.</li>
<li>Take a look at your scree plot. How many clusters are inherent in the data? Set <code>k</code> equal to the number of clusters at the location of the elbow.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Initialize total within sum of squares error: wss</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>wss <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># For 1 to 15 cluster centers</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>) {</span>
<span id="cb11-6"><a href="#cb11-6"></a>  km.out <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(x, <span class="at">centers =</span> i, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb11-7"><a href="#cb11-7"></a>  <span class="co"># Save total within sum of squares to wss variable</span></span>
<span id="cb11-8"><a href="#cb11-8"></a>  wss[i] <span class="ot">&lt;-</span> km.out<span class="sc">$</span>tot.withinss</span>
<span id="cb11-9"><a href="#cb11-9"></a>}</span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="co"># Plot total within sum of squares vs. number of clusters</span></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, wss, <span class="at">type =</span> <span class="st">"b"</span>, </span>
<span id="cb11-13"><a href="#cb11-13"></a>     <span class="at">xlab =</span> <span class="st">"Number of Clusters"</span>, </span>
<span id="cb11-14"><a href="#cb11-14"></a>     <span class="at">ylab =</span> <span class="st">"Within groups sum of squares"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-6-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Set k equal to the number of clusters corresponding to the elbow location</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span>  <span class="co"># 3 is probably OK, too</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Looking at the scree plot, it looks like there are inherently 2 or 3 clusters in the data. Awesome job!</p>
</section>
<section id="introduction-to-the-pokemon-data" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="introduction-to-the-pokemon-data"><span class="header-section-number">1.11</span> Introduction to the Pokemon data</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Introduction to the Pokemon data</strong></p>
<p>So far you have applied what you have learned about the ‘kmeans’ algorithm to synthetic data. In this final set of exercises you will apply your learning to a ‘real world’ data set.</p>
<p><strong>2. “Real” data exercise</strong></p>
<p>This data is about 800 Pokemon from the Pokemon Games (sorry, this isn’t from Pokemon Go). This may not be dataset where you have already built intuition. This is normal in data science. You may want to gain some intuition about the data by researching Pokemon in the Pokemon games, or as we will have you do, by poking around in the data.</p>
<p><strong>3. The Pokemon dataset</strong></p>
<p>The data was originally collected by Alberto Barradas and is hosted on Kaggle at the address on the screen. The data contains 6 features for each Pokemon: Hit Points, Attack, Defense, Special Attack, Special Defense, and Speed. This is unlabeled data because there is not a single outcome that we want to predict, just some measurements of each Pokemon’s abilities.For the data curious, more information on Pokemon and these features can be found at the second address on the screen. Along with exploring the data, this is another way to build intuition about the data.</p>
<p><strong>4. Data challenges</strong></p>
<p>In the next set of exercises, you will have multiple steps to complete that are typical in handling real world data.The first is determining which variables to use for clustering — it is important to consider which feature should be used in the clustering exercise. Sometimes trying multiple subsets of features is an important step to find patterns in the data.The next, and something we will delay to a later chapter, is scaling the data. If the features being used in modeling are of different units or scales, scaling the data to a common measure is often completed in order to improve the insights gained from unsupervised learning.In this example, you will be finding homogeneous subgroups of Pokemon. The number of clusters is not known beforehand so you will have to make a determination. In real world data, a nice clean elbow on the scree plot rarely exists, so as an analyst, you will have to use some judgement in this step.Finally, a common output of any analysis exercise is a visual representation of the outcomes. This can also be helpful to gain some additional intuition into the data and the resulting models.</p>
<p><strong>5. Let’s practice!</strong></p>
<p>This may seem like a lot, but we’ll guide you through step-by-step, providing hints and templates all along the way. Let’s practice.</p>
</section>
<section id="practical-matters-working-with-real-data" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="practical-matters-working-with-real-data"><span class="header-section-number">1.12</span> Practical matters: working with real data</h2>
<p>Dealing with real data is often more challenging than dealing with synthetic data. Synthetic data helps with learning new concepts and techniques, but the next few exercises will deal with data that is closer to the type of real data you might find in your professional or academic pursuits.</p>
<p>The first challenge with the Pokemon data is that there is no pre-determined number of clusters. You will determine the appropriate number of clusters, keeping in mind that in real data the <em>elbow</em> in the scree plot might be less of a sharp elbow than in synthetic data. Use your judgement on making the determination of the number of clusters.</p>
<p>The second part of this exercise includes plotting the outcomes of the clustering on two dimensions, or features, of the data. These features were chosen somewhat arbitrarily for this exercise. Think about how you would use plotting and clustering to communicate interesting groups of Pokemon to other people.</p>
<p>An additional note: this exercise utilizes the <code>iter.max</code> argument to <code>kmeans()</code>. As you’ve seen, <code>kmeans()</code> is an iterative algorithm, repeating over and over until some stopping criterion is reached. The default number of iterations for <code>kmeans()</code> is 10, which is not enough for the algorithm to converge and reach its stopping criterion, so we’ll set the number of iterations to 50 to overcome this issue. To see what happens when <code>kmeans()</code> does not converge, try running the example with a lower number of iterations (e.g.&nbsp;3). This is another example of what might happen when you encounter real data and use real cases.</p>
<p><strong>Steps</strong></p>
<p>The <code>pokemon</code> dataset, which contains observations of 800 Pokemon characters on 6 dimensions (i.e.&nbsp;features), is available.</p>
<ol type="1">
<li>Using <code>kmeans()</code> with <code>nstart = 20</code>, determine the total within sum of square errors for different numbers of clusters (between 1 and 15).</li>
<li>Pick an appropriate number of clusters based on these results from the first instruction and assign that number to <code>k</code>.</li>
<li>Create a k-means model using <code>k</code> clusters and assign it to the <code>km.out</code> variable.</li>
<li>Create a scatter plot of <code>Defense</code> vs.&nbsp;<code>Speed</code>, showing cluster membership for each observation.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Load package</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="fu">library</span>(readr)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="fu">library</span>(dplyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:stats':
#&gt; 
#&gt;     filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:base':
#&gt; 
#&gt;     intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># load data</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>pokemon <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/Pokemon.csv"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-3"><a href="#cb17-3"></a>              <span class="fu">select</span>(HitPoints<span class="sc">:</span>Speed) <span class="sc">|&gt;</span> </span>
<span id="cb17-4"><a href="#cb17-4"></a>              <span class="fu">as.matrix</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 800 Columns: 13</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr (3): Name, Type1, Type2
#&gt; dbl (9): Number, Total, HitPoints, Attack, Defense, SpecialAttack, SpecialDe...
#&gt; lgl (1): Legendary
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># Initialize total within sum of squares error: wss</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>wss <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb20-3"><a href="#cb20-3"></a></span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="co"># Look over 1 to 15 possible clusters</span></span>
<span id="cb20-5"><a href="#cb20-5"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>) {</span>
<span id="cb20-6"><a href="#cb20-6"></a>  <span class="co"># Fit the model: km.out</span></span>
<span id="cb20-7"><a href="#cb20-7"></a>  km.out <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(pokemon, <span class="at">centers =</span> i, <span class="at">nstart =</span> <span class="dv">20</span>, <span class="at">iter.max =</span> <span class="dv">50</span>)</span>
<span id="cb20-8"><a href="#cb20-8"></a>  <span class="co"># Save the within cluster sum of squares</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>  wss[i] <span class="ot">&lt;-</span> km.out<span class="sc">$</span>tot.withinss</span>
<span id="cb20-10"><a href="#cb20-10"></a>}</span>
<span id="cb20-11"><a href="#cb20-11"></a></span>
<span id="cb20-12"><a href="#cb20-12"></a><span class="co"># Produce a scree plot</span></span>
<span id="cb20-13"><a href="#cb20-13"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, wss, <span class="at">type =</span> <span class="st">"b"</span>, </span>
<span id="cb20-14"><a href="#cb20-14"></a>     <span class="at">xlab =</span> <span class="st">"Number of Clusters"</span>, </span>
<span id="cb20-15"><a href="#cb20-15"></a>     <span class="at">ylab =</span> <span class="st">"Within groups sum of squares"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-7-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># Select number of clusters (2, 3, 4 probably OK)</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>k <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="co"># Build model with k clusters: km.out</span></span>
<span id="cb21-5"><a href="#cb21-5"></a>km.out <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(pokemon, <span class="at">centers =</span> k, <span class="at">nstart =</span> <span class="dv">20</span>, <span class="at">iter.max =</span> <span class="dv">50</span>)</span>
<span id="cb21-6"><a href="#cb21-6"></a></span>
<span id="cb21-7"><a href="#cb21-7"></a><span class="co"># View the resulting model</span></span>
<span id="cb21-8"><a href="#cb21-8"></a>km.out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; K-means clustering with 3 clusters of sizes 270, 355, 175
#&gt; 
#&gt; Cluster means:
#&gt;   HitPoints   Attack   Defense SpecialAttack SpecialDefense    Speed
#&gt; 1  81.90370 96.15926  77.65556     104.12222       86.87778 94.71111
#&gt; 2  54.68732 56.93239  53.64507      52.02254       53.04789 53.58873
#&gt; 3  79.30857 97.29714 108.93143      66.71429       87.04571 57.29143
#&gt; 
#&gt; Clustering vector:
#&gt;   [1] 2 2 1 1 2 2 1 1 1 2 2 3 1 2 2 2 2 2 2 1 2 2 1 1 2 2 2 1 2 1 2 1 2 3 2 2 3
#&gt;  [38] 2 2 1 2 1 2 1 2 2 2 1 2 2 1 2 3 2 1 2 2 2 1 2 1 2 1 2 1 2 2 3 2 1 1 1 2 3
#&gt;  [75] 3 2 2 1 2 1 2 3 3 2 1 2 3 3 2 1 2 2 1 2 3 2 3 2 3 2 1 1 1 3 2 3 2 3 2 1 2
#&gt; [112] 1 2 3 3 3 2 2 3 2 3 2 3 3 3 2 1 2 3 2 1 1 1 1 1 1 3 3 3 2 3 3 3 2 2 1 1 1
#&gt; [149] 2 2 3 2 3 1 1 3 1 1 1 2 2 1 1 1 1 1 2 2 3 2 2 1 2 2 3 2 2 2 1 2 2 2 2 1 2
#&gt; [186] 1 2 2 2 2 2 2 1 2 2 1 1 3 2 2 3 1 2 2 1 2 2 2 2 2 3 1 3 2 3 1 2 2 1 2 3 2
#&gt; [223] 3 3 3 2 3 2 3 3 3 3 3 2 2 3 2 3 2 3 2 2 1 2 1 3 2 1 1 1 2 3 1 1 2 2 3 2 2
#&gt; [260] 2 3 1 1 1 3 2 2 3 3 1 1 1 2 2 1 1 2 2 1 1 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 1
#&gt; [297] 2 2 1 2 2 2 3 2 2 1 1 2 2 2 3 2 2 1 2 1 2 2 2 1 2 3 2 3 2 2 2 3 2 3 2 3 3
#&gt; [334] 3 2 2 1 2 1 1 2 2 2 2 2 2 3 2 1 1 2 1 2 1 3 3 2 1 2 2 2 1 2 1 2 3 1 1 1 1
#&gt; [371] 3 2 3 2 3 2 3 2 3 2 3 2 1 2 3 2 1 1 2 3 3 2 1 1 2 2 1 1 2 2 1 2 3 3 3 2 2
#&gt; [408] 3 1 1 2 3 3 1 3 3 3 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 2 3 3 2 2 1 2 2 1 2 2 1
#&gt; [445] 2 2 2 2 2 2 1 2 1 2 3 2 3 2 3 3 3 1 2 3 2 2 1 2 1 2 3 1 2 1 2 1 1 1 1 2 1
#&gt; [482] 2 2 1 2 3 2 2 2 2 3 2 2 1 1 2 2 1 1 2 3 2 3 2 1 3 2 1 2 2 1 3 1 1 3 3 3 1
#&gt; [519] 1 1 1 3 1 3 1 1 1 1 3 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2
#&gt; [556] 2 1 2 2 1 2 2 1 2 2 2 2 3 2 1 2 1 2 1 2 1 2 3 2 2 1 2 1 2 3 3 2 1 2 1 3 3
#&gt; [593] 2 3 3 2 2 1 3 3 2 2 1 2 2 1 2 1 2 1 1 2 2 1 2 3 1 1 2 3 2 3 1 2 3 2 3 2 1
#&gt; [630] 2 3 2 1 2 1 2 2 3 2 2 1 2 1 2 2 1 2 1 1 2 3 2 3 2 1 3 2 1 2 3 2 3 3 2 2 1
#&gt; [667] 2 1 2 2 1 2 3 3 2 3 1 2 1 3 2 1 3 2 3 2 3 3 2 3 2 3 1 3 2 2 1 2 1 1 1 1 1
#&gt; [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 2 2 1 2 2 1 2 2 2 2 1 2 2 2 2 1 2 2 1
#&gt; [741] 2 1 2 3 1 2 1 1 2 3 1 3 2 3 2 1 2 3 2 3 2 3 2 1 2 1 2 3 2 1 1 1 1 3 2 1 1
#&gt; [778] 3 2 3 2 2 2 2 3 3 3 3 2 3 2 1 1 1 3 3 1 1 1 1
#&gt; 
#&gt; Within cluster sum of squares by cluster:
#&gt; [1] 1018348.0  812079.9  709020.5
#&gt;  (between_SS / total_SS =  40.8 %)
#&gt; 
#&gt; Available components:
#&gt; 
#&gt; [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
#&gt; [6] "betweenss"    "size"         "iter"         "ifault"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Plot of Defense vs. Speed by cluster membership</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="fu">plot</span>(pokemon[, <span class="fu">c</span>(<span class="st">"Defense"</span>, <span class="st">"Speed"</span>)],</span>
<span id="cb23-3"><a href="#cb23-3"></a>     <span class="at">col =</span> km.out<span class="sc">$</span>cluster,</span>
<span id="cb23-4"><a href="#cb23-4"></a>     <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"k-means clustering of Pokemon with"</span>, k, <span class="st">"clusters"</span>),</span>
<span id="cb23-5"><a href="#cb23-5"></a>     <span class="at">xlab =</span> <span class="st">"Defense"</span>, <span class="at">ylab =</span> <span class="st">"Speed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-7-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-7-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Nice job! You’re really getting the hang of k-means clustering quickly!</p>
</section>
<section id="review-of-k-means-clustering" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="review-of-k-means-clustering"><span class="header-section-number">1.13</span> Review of k-means clustering</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Review of k-means clustering</strong></p>
<p>Congratulations! You have completed the first of four chapters in the Unsupervised Learning in R course.</p>
<p><strong>2. Chapter review</strong></p>
<p>You have covered much in a short period. As a review:You have learned the difference between supervised and unsupervised learning and about the two major goals of unsupervised learning, you have successfully created several k-means cluster models in R, gained intuition about how the k-means algorithm works, learned how to use model selection to handle the circumstance where the number of subgroups is not known beforehand, and finally, you performed clustering to gain insights on a fun real-world dataset.</p>
<p><strong>3. Coming up: chapter 2</strong></p>
<p>Coming up in Chapter 2, I will cover hierarchical clustering, which is a clustering method that is used when the number of clusters is not provided a priori.</p>
<p><strong>4. Coming up: chapter 3</strong></p>
<p>In Chapter 3, you will learn about principal components analysis, a method for doing dimensionality reduction.</p>
<p><strong>5. Coming up: chapter 4</strong></p>
<p>And finally in Chapter 4 you will apply everything you learned in the first three chapters to a data set covering breast cancer tumor observations. This will reinforce your learning from the earlier chapters and help you apply these skills to your own datasets.</p>
<p><strong>6. See you in the next chapter!</strong></p>
<p>See you in the next chapter.</p>
</section>
</section>
<section id="hierarchical-clustering" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 2. Hierarchical clustering</h1>
<p>Hierarchical clustering is another popular method for clustering. The goal of this chapter is to go over how it works, how to use it, and how it compares to k-means clustering.</p>
<section id="introduction-to-hierarchical-clustering" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction-to-hierarchical-clustering"><span class="header-section-number">2.1</span> Introduction to hierarchical clustering</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Introduction to hierarchical clustering</strong></p>
<p>In this chapter, you will learn about a different method of clustering called hierarchical clustering.</p>
<p><strong>2. Hierarchical clustering</strong></p>
<p>Hierarchical clustering is used when the number of clusters is not known ahead of time. This is different from kmeans clustering where you first have to specify the number of clusters and then execute the algorithm. There are two approaches to hierarchical clustering: bottom-up and top-down. This course will focus on bottom-up clustering.</p>
<p><strong>3. Simple example</strong></p>
<p>To gain intuition about this, let’s start with a simple example of five observations each with two features.</p>
<p><strong>4. Five clusters</strong></p>
<p>Bottom-up hierarchical clustering starts by assigning each point to its own cluster. So in this example, there are five clusters because there are five points. They are color-coded for reference.</p>
<p><strong>5. Four clusters</strong></p>
<p>The next step of bottom-up hierarchical clustering is to find the closest two clusters and to join them together into a single cluster. In this example, we now have four clusters since the purple and orange clusters are combined into a single cluster.</p>
<p><strong>6. Three clusters</strong></p>
<p>This process continues iteratively, finding the next pair of clusters that are closest to each other and combining them into a single cluster. Here you can see the green and red clusters are combined, resulting in a total of three clusters at this step in the algorithm.</p>
<p><strong>7. Two clusters</strong></p>
<p>Again, the hierarchical cluster algorithm continues by joining the two closest clusters together into a single cluster.</p>
<p><strong>8. One cluster</strong></p>
<p>This continues until there is only one cluster. Once there is only a single cluster, the hierarchical clustering algorithm stops. I have skipped a few details, like how distance is measured between clusters – those details are not needed right now; but I will cover them in the next few videos.</p>
<p><strong>9. Hierarchical clustering in R</strong></p>
<p>Performing hierarchical clustering in R requires only one parameter – the distance between the observations. There are many ways to calculate the distance between observations – for this class we will use standard Euclidean distance. This is calculated using the dist() function in R. The parameter to dist() is a matrix of the same structure as other matrices used in machine learning: one observation per row, one feature per column. The resultant distance matrix is then passed in as the ‘d’ parameter to the hclust() function in R. This will then return a hierarchical clustering model for interrogation and use. There are a few more parameters available in the hclust() function, but this is enough to get started with creating models. We will cover other typical options later, and as before when you are ready the R documentation for hclust() is a good resource.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>OK, let’s practice what you’ve learned in the coming exercises.</p>
</section>
<section id="hierarchical-clustering-with-results" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="hierarchical-clustering-with-results"><span class="header-section-number">2.2</span> Hierarchical clustering with results</h2>
<p>In this exercise, you will create your first hierarchical clustering model using the <a href="https://www.rdocumentation.org/packages/stats/topics/hclust"><code>hclust()</code></a> function.</p>
<p>We have created some data that has two dimensions and placed it in a variable called <code>x</code>. Your task is to create a hierarchical clustering model of <code>x</code>. Remember from the video that the first step to hierarchical clustering is determining the similarity between observations, which you will do with the <a href="https://www.rdocumentation.org/packages/stats/topics/dist"><code>dist()</code></a> function.</p>
<p>You will look at the structure of the resulting model using the <code>summary()</code> function.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Fit a hierarchical clustering model to <code>x</code> using the <code>hclust()</code> function. Store the result in <code>hclust.out</code>.</li>
<li>Inspect the result with the <code>summary()</code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># load data</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>x2 <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/x2.rds"</span>)</span>
<span id="cb24-3"><a href="#cb24-3"></a></span>
<span id="cb24-4"><a href="#cb24-4"></a><span class="co"># Create hierarchical clustering model: hclust.out</span></span>
<span id="cb24-5"><a href="#cb24-5"></a>hclust.out <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x2))</span>
<span id="cb24-6"><a href="#cb24-6"></a></span>
<span id="cb24-7"><a href="#cb24-7"></a><span class="co"># Inspect the result</span></span>
<span id="cb24-8"><a href="#cb24-8"></a><span class="fu">summary</span>(hclust.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;             Length Class  Mode     
#&gt; merge       98     -none- numeric  
#&gt; height      49     -none- numeric  
#&gt; order       50     -none- numeric  
#&gt; labels       0     -none- NULL     
#&gt; method       1     -none- character
#&gt; call         2     -none- call     
#&gt; dist.method  1     -none- character</code></pre>
</div>
</div>
<p>Awesome! Now that you’ve made your first hierarchical clustering model, let’s learn how to use it to solve problems.</p>
</section>
<section id="selecting-number-of-clusters-1" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="selecting-number-of-clusters-1"><span class="header-section-number">2.3</span> Selecting number of clusters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Selecting number of clusters</strong></p>
<p>Now that you have created your first hierarchical cluster model in R, let’s take the next step in how to interpret and use a hierarchical cluster model.</p>
<p><strong>2. Interpreting results</strong></p>
<p>If you look at the summary of the hierarchical clustering model the output is somewhat technical and opaque. In all honesty it’s not that useful of a summary.</p>
<p><strong>3. Dendrogram</strong></p>
<p>To remedy this issue, first let’s build a little more intuition about the hierarchical clustering algorithm.I will be using the same example of 5 observations each with the 2 features you’ve seen before. These are color-coded and presented in the 2-dimensional plane on the left.The same five points are also presented on the right. On this side we will be building a tree, a dendrogram, which represents the clustering process.</p>
<p><strong>4. Dendrogram</strong></p>
<p>As described before, every observation is made a cluster. Then the closest two clusters are joined together into a single cluster. This is equivalent to two points being joined on the tree representation of the clusters.</p>
<p><strong>5. Dendrogram</strong></p>
<p>This process continues, finding the closest two clusters and joining them into a new cluster. The distance between the clusters is represented as the height of the horizontal line on the dendrogram.</p>
<p><strong>6. Dendrogram</strong></p>
<p>The next iteration then joins the next two closest clusters.</p>
<p><strong>7. Dendrogram</strong></p>
<p>This algorithm continues until only one cluster is remaining. This also completes the tree representation, the dendrogram, of the results of the hierarchical clustering algorithm.</p>
<p><strong>8. Dendrogram plotting in R</strong></p>
<p>To create the dendrogram in R, the output of the hclust() function, the model, is passed into R’s plot() function.The next step typical in hierarchical clustering is to determine the number of clusters you want in the model. This is one of the key model selection steps for this algorithm. A way to think about this is as drawing a cut line at a particular ‘height’ or distance, between the clusters.Choosing the number of clusters based on distance between the clusters is equivalent to drawing a line on the dendrogram at a height equal to the desired distance between clusters. This is done using the abline() function in R, using the ‘h’ parameter to specify the height to draw the line, and optionally a color for the line, using the parameter ‘c-o-l’.Here I show the results of abline() with a horizontal red line. Specifying height of the line is the equivalent of specifying that you want clusters that are no further apart than that height. Distance between cluster can be any metric, but throughout this course we will be using Euclidean distance.In this example, the result is two clusters with the blue, purple, and orange observations assigned to cluster 1 and the red and green observations assigned to cluster 2.</p>
<p><strong>9. Tree “cutting” in R</strong></p>
<p>Finally, to make cluster assignments for each observation in the cluster, you can use the “cut tree” function in R. The “cut tree” function takes as its parameters the hierarchical cluster model and either the height at which to cut the dendrogram tree, the ‘h’ parameter, or the number of clusters you want to maintain, the ‘k’ parameter. The results are a vector with a numeric cluster assignment for each observation.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>Ok, now it’s your turn to practice what you’ve learned.</p>
</section>
<section id="interpreting-dendrogram" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="interpreting-dendrogram"><span class="header-section-number">2.4</span> Interpreting dendrogram</h2>
<p>The plot displayed to the right shows the <code>hclust.out</code> model you constructed in the previous exercise. We’ve drawn horizontal lines at heights of 3.5, 4.5, 6.9, and 9.0, respectively.</p>
<blockquote class="blockquote">
<h2 id="question-1" data-number="2.5" class="anchored"><span class="header-section-number">2.5</span> <em>Question</em></h2>
<p>Which cut point yields 3 clusters?<br> <br> ⬜ 3.5<br> ⬜ 4.5<br> ✅ 6.9<br> ⬜ 9.0<br></p>
</blockquote>
<p>Correct! If you cut the tree at a height of 6.9, you’re left with 3 branches representing 3 distinct clusters.</p>
</section>
<section id="cutting-the-tree" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="cutting-the-tree"><span class="header-section-number">2.6</span> Cutting the tree</h2>
<p>Remember from the video that <a href="https://www.rdocumentation.org/packages/stats/topics/cutree"><code>cutree()</code></a> is the R function that cuts a hierarchical model. The <code>h</code> and <code>k</code> arguments to <code>cutree()</code> allow you to cut the tree based on a certain height <code>h</code> or a certain number of clusters <code>k</code>.</p>
<p>In this exercise, you will use <code>cutree()</code> to cut the hierarchical model you created earlier based on each of these two criteria.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Cut the <code>hclust.out</code> model at height 7.</li>
<li>Cut the <code>hclust.out</code> model to create 3 clusters.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Cut by height</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="fu">cutree</span>(hclust.out, <span class="at">h =</span> <span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;  [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 2 2 2
#&gt; [39] 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Cut by number of clusters</span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="fu">cutree</span>(hclust.out, <span class="at">k =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;  [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 2 2 2
#&gt; [39] 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
</div>
</div>
<p>If you’re wondering what the output means, remember, there are 50 observations in the original dataset <code>x</code>. The output of each <code>cutree()</code> call represents the cluster assignments for each observation in the original dataset. Great work!</p>
</section>
<section id="clustering-linkage-and-practical-matters" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="clustering-linkage-and-practical-matters"><span class="header-section-number">2.7</span> Clustering linkage and practical matters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Clustering linkage and practical matters</strong></p>
<p>I have a couple more details of hierarchical clustering to cover before wrapping up this chapter.</p>
<p><strong>2. Linking clusters in hierarchical clustering</strong></p>
<p>The first detail to cover is how the distance between clusters is determined. As soon as the first two observations are combined into a cluster, the hierarchical clustering algorithm needs rules about how to measure the distance between clusters.There are four methods available in R to measure the distance, or similarity, between clusters.The first method, and the method that is the default to the hclust() function, is the ‘complete’ method. In the complete method, the distance – or similarity – is determined pairwise between all observations in cluster 1 and cluster 2, and the largest distance is used as the distance between the clusters.The second common linkage method is the single method. Again, the pairwise similarity is calculated between points in each cluster, and the smallest such similarity is used as the distance between the clusters.The third common linkage method is the average method. This method uses the average of the pairwise similarities as the distance between the two clusters.The final method works a little different. In the ‘centroid’ method the centroid of cluster 1 is calculated and the centroid of cluster 2 is calculated, and the distance between the clusters is the distance between the centroids.</p>
<p><strong>3. Linking methods: complete and average</strong></p>
<p>In practice, the choice of linkage is one of those model parameters you will need to choose based on the insights provided by the distance methods. As a rule of thumb, ‘Complete’ and ‘Average’ tend to produce more balanced trees and are the most commonly used.Here we see the same data with complete linkage on the left hand side and average linkage on the right hand side.</p>
<p><strong>4. Linking method: single</strong></p>
<p>‘Single’ tends to produce trees where observations are fused in one at a time, producing unbalanced trees.</p>
<p><strong>5. Linking method: centroid</strong></p>
<p>‘Centroid’ linkage can create inversions where clusters are fused below either of the individual clusters; this is undesirable behavior – as such, this method is used much less often than the others.You can see the inversion here in the clusters with the boxes around them. Those clusters have been fused into the tree below where the individual clusters have been fused.</p>
<p><strong>6. Linkage in R</strong></p>
<p>Specifying linkage in R is only a matter of specifying the ‘method’ parameter in the call to the hclust() function. The value of the parameter is a string specifying the linkage method – here we show creating hierarchical cluster models with complete, average, and single linkages.</p>
<p><strong>7. Practical matters</strong></p>
<p>As a final practical matter, many of the machine learning methods, including kmeans and hierarchical clustering, are sensitive to the data which is on different scales or units of measurement.To resolve this, the data is transformed through a linear transformation before performing clustering. This transformation subtracts the mean of a feature from each of the observations, and divides each feature observation by the standard deviation of the feature. This is sometimes referred to as normalization and has the effect of producing a population where the normalized feature has a mean of zero and a standard deviation of one.If you know any of the features are on different scales or units of measure, then it is customary to normalize all the features. Even when the same scales and units of measures are used, it is good practice to check the variability of the means and standard deviations of the features. If the means and standard deviations vary across the features scaling is in order.</p>
<p><strong>8. Practical matters</strong></p>
<p>To check the means of all the features the colMeans() function is used, passing in the data matrix. Because features are in the columns this will return the mean value of each feature in the given observations. To calculate the standard deviation of each feature, the apply() function is used, applying the sd() function to each column, or axis 2, of the matrix.</p>
<p><strong>9. Practical matters</strong></p>
<p>Producing a matrix where all features have been normalized is done by passing the original matrix to the scale() method in R. The output is a matrix of the same size, with each feature normalized. Here we show checking that the normalized matrix has column means which are zero, within floating point precision, and column standard deviations of 1.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>Alright, let’s get some practice.</p>
</section>
<section id="linkage-methods" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="linkage-methods"><span class="header-section-number">2.8</span> Linkage methods</h2>
<p>In this exercise, you will produce hierarchical clustering models using different linkages and plot the dendrogram for each, observing the overall structure of the trees.</p>
<p>You’ll be asked to interpret the results in the next exercise.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Produce three hierarchical clustering models on <code>x</code> using the <code>"complete"</code>, <code>"average"</code>, and <code>"single"</code> linkage methods, respectively.</li>
<li>Plot a dendrogram for each model, using titles of <code>"Complete"</code>, <code>"Average"</code>, and <code>"Single"</code>, respectively.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Cluster using complete linkage: hclust.complete</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>hclust.complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x2), <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb30-3"><a href="#cb30-3"></a></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="co"># Cluster using average linkage: hclust.average</span></span>
<span id="cb30-5"><a href="#cb30-5"></a>hclust.average <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x2), <span class="at">method =</span> <span class="st">"average"</span>)</span>
<span id="cb30-6"><a href="#cb30-6"></a></span>
<span id="cb30-7"><a href="#cb30-7"></a><span class="co"># Cluster using single linkage: hclust.single</span></span>
<span id="cb30-8"><a href="#cb30-8"></a>hclust.single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x2), <span class="at">method =</span> <span class="st">"single"</span>)</span>
<span id="cb30-9"><a href="#cb30-9"></a></span>
<span id="cb30-10"><a href="#cb30-10"></a><span class="co"># Plot dendrogram of hclust.complete</span></span>
<span id="cb30-11"><a href="#cb30-11"></a><span class="fu">plot</span>(hclust.complete, <span class="at">main =</span> <span class="st">"Complete"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-10-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Plot dendrogram of hclust.average</span></span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="fu">plot</span>(hclust.average, <span class="at">main =</span> <span class="st">"Average"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-10-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-10-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># Plot dendrogram of hclust.single</span></span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="fu">plot</span>(hclust.single, <span class="at">main =</span> <span class="st">"Single"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-10-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-10-3.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Before moving on, make sure to toggle through the plots to compare and contrast the three dendrograms you created. You’ll learn about the implications of these differences in the next exercise. Excellent work!</p>
</section>
<section id="comparing-linkage-methods" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="comparing-linkage-methods"><span class="header-section-number">2.9</span> Comparing linkage methods</h2>
<p>The models you created in the last exercise—<code>hclust.complete</code>, <code>hclust.average</code>, and <code>hclust.single</code>—are available in your workspace.</p>
<blockquote class="blockquote">
<h2 id="question-2" data-number="2.10" class="anchored"><span class="header-section-number">2.10</span> <em>Question</em></h2>
<p>Which linkage(s) produce balanced trees?<br> <br> ⬜ Complete only<br> ⬜ Average only<br> ⬜ Single only<br> ⬜ Average and single<br> ✅ Complete and average<br> ⬜ All three<br></p>
</blockquote>
<p>Right! Whether you want balanced or unbalanced trees for your hierarchical clustering model depends on the context of the problem you’re trying to solve. Balanced trees are essential if you want an even number of observations assigned to each cluster. On the other hand, if you want to detect outliers, for example, an unbalanced tree is more desirable because pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters.</p>
</section>
<section id="practical-matters-scaling" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="practical-matters-scaling"><span class="header-section-number">2.11</span> Practical matters: scaling</h2>
<p>Recall from the video that clustering real data may require <em>scaling</em> the features if they have different distributions. So far in this chapter, you have been working with synthetic data that did not need scaling.</p>
<p>In this exercise, you will go back to working with “real” data, the <code>pokemon</code> dataset introduced in the first chapter. You will observe the distribution (mean and standard deviation) of each feature, scale the data accordingly, then produce a hierarchical clustering model using the complete linkage method.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Observe the mean of each variable in <code>pokemon</code> using the <code>colMeans()</code> function.</li>
<li>Observe the standard deviation of each variable using the <code>apply()</code> and <code>sd()</code> functions. Since the variables are the columns of your matrix, make sure to specify 2 as the <code>MARGIN</code> argument to <code>apply()</code>.</li>
<li>Scale the <code>pokemon</code> data using the <code>scale()</code> function and store the result in <code>pokemon.scaled</code>.</li>
<li>Create a hierarchical clustering model of the <code>pokemon.scaled</code> data using the complete linkage method. Manually specify the <code>method</code> argument and store the result in <code>hclust.pokemon</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># View column means</span></span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="fu">colMeans</span>(pokemon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
#&gt;       69.25875       79.00125       73.84250       72.82000       71.90250 
#&gt;          Speed 
#&gt;       68.27750</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># View column standard deviations</span></span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="fu">apply</span>(pokemon, <span class="dv">2</span>, sd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;      HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
#&gt;       25.53467       32.45737       31.18350       32.72229       27.82892 
#&gt;          Speed 
#&gt;       29.06047</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># Scale the data</span></span>
<span id="cb37-2"><a href="#cb37-2"></a>pokemon.scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(pokemon)</span>
<span id="cb37-3"><a href="#cb37-3"></a></span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="co"># Create hierarchical clustering model: hclust.pokemon</span></span>
<span id="cb37-5"><a href="#cb37-5"></a>hclust.pokemon <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(pokemon.scaled), <span class="at">method =</span> <span class="st">"complete"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s quickly recap what you just did. You first checked to see if the column means and standard deviations vary. Because they do, you scaled the data, converted the scaled data to a similarity matrix and passed it into the <code>hclust()</code> function. Great work!</p>
</section>
<section id="comparing-kmeans-and-hclust" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="comparing-kmeans-and-hclust"><span class="header-section-number">2.12</span> Comparing kmeans() and hclust()</h2>
<p>Comparing k-means and hierarchical clustering, you’ll see the two methods produce different cluster memberships. This is because the two algorithms make different assumptions about how the data is generated. In a more advanced course, we could choose to use one model over another based on the quality of the models’ assumptions, but for now, it’s enough to observe that they are different.</p>
<p>This exercise will have you compare results from the two models on the <code>pokemon</code> dataset to see how they differ.</p>
<p><strong>Steps</strong></p>
<p>The results from running k-means clustering on the <code>pokemon</code> data (for 3 clusters) are stored as <code>km.pokemon</code>.</p>
<ol type="1">
<li>Using <code>cutree()</code> on <code>hclust.pokemon</code>, assign cluster membership to each observation. Assume three clusters and assign the result to a vector called <code>cut.pokemon</code>.</li>
<li>Using <code>table()</code>, compare cluster membership between the two clustering methods. Recall that the different components of k-means model objects can be accessed with the <code>$</code> operator.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># Load data</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="co"># km.pokemon &lt;- kmeans(scale(pokemon), centers = k)</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>km.pokemon <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/km.pokemon.rds"</span>)</span>
<span id="cb38-4"><a href="#cb38-4"></a></span>
<span id="cb38-5"><a href="#cb38-5"></a><span class="co"># Apply cutree() to hclust.pokemon: cut.pokemon</span></span>
<span id="cb38-6"><a href="#cb38-6"></a>cut.pokemon <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hclust.pokemon, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb38-7"><a href="#cb38-7"></a></span>
<span id="cb38-8"><a href="#cb38-8"></a><span class="co"># Compare methods</span></span>
<span id="cb38-9"><a href="#cb38-9"></a><span class="fu">table</span>(km.pokemon<span class="sc">$</span>cluster, cut.pokemon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;    cut.pokemon
#&gt;       1   2   3
#&gt;   1 204   9   1
#&gt;   2 342   1   0
#&gt;   3 242   1   0</code></pre>
</div>
</div>
<p>Looking at the table, it looks like the hierarchical clustering model assigns most of the observations to cluster 1, while the k-means algorithm distributes the observations relatively evenly among all clusters. It’s important to note that there’s no consensus on which method produces better clusters. The job of the analyst in unsupervised clustering is to observe the cluster assignments and make a judgment call as to which method provides more insights into the data. Excellent job!</p>
</section>
<section id="review-of-hierarchical-clustering" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="review-of-hierarchical-clustering"><span class="header-section-number">2.13</span> Review of hierarchical clustering</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Review of hierarchical clustering</strong></p>
<p>Congratulations, you have now successfully completed the clustering chapters.</p>
<p><strong>2. Hierarchical clustering review</strong></p>
<p>In this latest chapter you have learned to create hierarchical clustering models in R,</p>
<p><strong>3. Linking methods: complete and average</strong></p>
<p>describe how different linkage methods between clusters affect the results of hierarchical clustering,</p>
<p><strong>4. Hierarchical clustering</strong></p>
<p>have gained some intuition</p>
<p><strong>5. Iterating</strong></p>
<p>about how the hierarchical clustering algorithm functions,</p>
<p><strong>6. Dendrogram</strong></p>
<p>saw how to visually represent the results of hierarchical clustering as a dendrogram,</p>
<p><strong>7. How k-means and hierarchical clustering differ</strong></p>
<p>learned the difference between kmeans and hierarchical clustering methods and their outputs,</p>
<p><strong>8. Practical matters</strong></p>
<p>and you have successfully applied hierarchical clustering to the Pokemon dataset, dealing with practical matters like scaling the data.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>Let’s continue to the third chapter!</p>
</section>
</section>
<section id="dimensionality-reduction-with-pca" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 3. Dimensionality reduction with PCA</h1>
<p>Principal component analysis, or PCA, is a common approach to dimensionality reduction. Learn exactly what PCA does, visualize the results of PCA with biplots and scree plots, and deal with practical issues such as centering and scaling the data before performing PCA.</p>
<section id="introduction-to-pca" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction-to-pca"><span class="header-section-number">3.1</span> Introduction to PCA</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Introduction to PCA</strong></p>
<p><strong>2. Two methods of clustering</strong></p>
<p>Thus far in this course you have learned about two techniques for performing clustering to find similar subgroups within an overall population.The next type of unsupervised machine learning which will be covered is dimensionality reduction. Dimensionality reduction has two main goals: to find structure within features and to aid in visualization.</p>
<p><strong>3. Dimensionality reduction</strong></p>
<p>In this course, I will be covering one particular and popular method of dimensionality reduction – principal components analysis.Principal component analysis has three goals -First, PCA will find a linear combination of the original features. A linear combination just means it takes some fraction of some or all of the features and adds them together. These new features are called principal components.Second, in those new features, PCA will maintain as much variance as it can from the original data for a given number of principal components.Third, the new features are uncorrelated, or orthogonal to each other.</p>
<p><strong>4. PCA intuition</strong></p>
<p>As with clustering methods, lets build up some intuition about how principal components analysis achieves these goals by studying the simplest example possible.Here we show a data set with two dimensions, or features – x on the horizontal axis and y on the vertical axis.Each point on the plane represents an observation in the data set, plotted as points.The goal of PCA would be to find a lower dimensional representation of this data that maintains and describes the maximum amount of variance from the original data. Because the original data is of 2 dimensions, the lower dimensional representation will be only one dimension, or feature. That is, PCA will help us map this data from 2 features to 1 feature while maintaining as much data variability as possible.</p>
<p><strong>5. PCA intuition</strong></p>
<p>The first step is to fit a regression line through the data. This line is determined such that it explains all the data with the minimum residual error when those points are mapped to the line.This line is the first principal component of this data.</p>
<p><strong>6. PCA intuition</strong></p>
<p>Now we have a new dimension along the line. Each point is then mapped on the line – projected onto the line. This projected value on the new line is sometimes referred to as the component scores or the factor scores.</p>
<p><strong>7. Visualization of high dimensional data</strong></p>
<p>Principal components analysis is also often used to aid in visualization of data of high dimensions. Data with more than 3 or 4 features is often difficult to develop an effective visualization for communication, and can put high cognitive loads on the consumers of the visualization.</p>
<p><strong>8. Visualization</strong></p>
<p>Here I am showing the well known iris data set. The iris data set consists of four physical measures of a population of three different types of iris flowers.On the left hand side, each of the four variables is scatter-plotted versus each of the other three variables. The different types of the flowers are plotted as different colors.The right hand side is the result of using PCA to map the four original variables to one variable. On the right hand side, the component scores of the original data is shown, using only the first principal component from the iris data set. The first principal component maintains 92% of the variability of the original data.</p>
<p><strong>9. PCA in R</strong></p>
<p>Creating a PCA model in R uses the ‘prcomp’ function. The first parameter this function is the original data, one observation per row, one feature per column, as in other machine learning applications.The scale parameter indicates if the data should be scaled to standard deviation of one (1) before performing PCA – more on this in a later video. The center parameters indicates if the data should be centered around zero (0) before performing PCA. I highly recommend also leaving this parameter as TRUE.Finally, performing summary on the output of ‘prcomp’ Indicates the variance explained by each principal component, the cumulative proportion of variance explained as each principal component is added to the previous principal component, and the standard deviations of the principal components.Again, you are encouraged to look at the R documentation for the prcomp() function when you are ready to dig into the options and results of this function more.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>Ok, enough, time to create your first principal components model.</p>
</section>
<section id="pca-using-prcomp" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="pca-using-prcomp"><span class="header-section-number">3.2</span> PCA using prcomp()</h2>
<p>In this exercise, you will create your first PCA model and observe the diagnostic results.</p>
<p>We have loaded the Pokemon data from earlier, which has four dimensions, and placed it in a variable called <code>pokemon</code>. Your task is to create a PCA model of the data, then to inspect the resulting model using the <code>summary()</code> function.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a PCA model of the data in <code>pokemon</code>, setting <code>scale</code> to <code>TRUE</code>. Store the result in <code>pr.out</code>.</li>
<li>Inspect the result with the <code>summary()</code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Load data</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>pokemon <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/pokemon50.rds"</span>)</span>
<span id="cb40-3"><a href="#cb40-3"></a></span>
<span id="cb40-4"><a href="#cb40-4"></a><span class="co"># Perform scaled PCA: pr.out</span></span>
<span id="cb40-5"><a href="#cb40-5"></a>pr.out <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(pokemon, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb40-6"><a href="#cb40-6"></a></span>
<span id="cb40-7"><a href="#cb40-7"></a><span class="co"># Inspect model output</span></span>
<span id="cb40-8"><a href="#cb40-8"></a><span class="fu">summary</span>(pr.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Importance of components:
#&gt;                           PC1    PC2    PC3     PC4
#&gt; Standard deviation     1.4420 1.0013 0.7941 0.53595
#&gt; Proportion of Variance 0.5199 0.2507 0.1577 0.07181
#&gt; Cumulative Proportion  0.5199 0.7705 0.9282 1.00000</code></pre>
</div>
</div>
<p>Nice job!</p>
</section>
<section id="results-of-pca" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="results-of-pca"><span class="header-section-number">3.3</span> Results of PCA</h2>
<p>This exercise will check your understanding of the <code>summary()</code> of a PCA model. Your model from the last exercise, <code>pr.out</code>, and the <code>pokemon</code> dataset are still available in your workspace.</p>
<blockquote class="blockquote">
<h2 id="question-3" data-number="3.4" class="anchored"><span class="header-section-number">3.4</span> <em>Question</em></h2>
<p>What is the minimum number of principal components that are required to describe at least 75% of the cumulative variance in this dataset?<br> <br> ⬜ 1<br> ✅ 2<br> ⬜ 3<br> ⬜ 4<br></p>
</blockquote>
<p>Right! The first two principal components describe around 77% of the variance.</p>
</section>
<section id="additional-results-of-pca" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="additional-results-of-pca"><span class="header-section-number">3.5</span> Additional results of PCA</h2>
<p>PCA models in R produce additional diagnostic and output components:</p>
<ul>
<li><code>center</code>: the column means used to center to the data, or <code>FALSE</code> if the data weren’t centered</li>
<li><code>scale</code>: the column standard deviations used to scale the data, or <code>FALSE</code> if the data weren’t scaled</li>
<li><code>rotation</code>: the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components</li>
<li><code>x</code>: the value of each observation in the original dataset projected to the principal components</li>
</ul>
<p>You can access these the same as other model components. For example, use <code>pr.out$rotation</code> to access the <code>rotation</code> component.</p>
<blockquote class="blockquote">
<h2 id="question-4" data-number="3.6" class="anchored"><span class="header-section-number">3.6</span> <em>Question</em></h2>
<p>Which of the following statements is <strong>not</strong> correct regarding the <code>pr.out</code> model fit on the <code>pokemon</code> data?<br> <br> ⬜ The <code>x</code> component is a table with the same dimensions as the original data.<br> ⬜ The data were centered prior to performing PCA.<br> ⬜ The data were scaled prior to performing PCA.<br> ✅ The directions of the principal component vectors are presented in a table with the same dimensions as the original data.<br></p>
</blockquote>
<p>Right! Calling <code>dim()</code> on <code>pr.out$rotation</code> and <code>pokemon</code>, you can see they have different dimensions.</p>
</section>
<section id="visualizing-and-interpreting-pca-results" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="visualizing-and-interpreting-pca-results"><span class="header-section-number">3.7</span> Visualizing and interpreting PCA results</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Visualizing and interpreting PCA results</strong></p>
<p>Great work on making your first PCA model. In this video, we will explore some additional visualizations often used to understand PCA models.</p>
<p><strong>2. Biplot</strong></p>
<p>The first of the two visualizations is known as a biplot. This plot shows all of the original observations as points plotted in the first two principal components. A biplot also shows the original features as vectors mapped onto the first two principal components.In this case, the original features of petal length and petal width are in the same direction in the first two principal components, indicating that these two features are correlated in the original data.</p>
<p><strong>3. Scree plot</strong></p>
<p>The second type of plot is a scree plot. Scree plots for PCA either show the proportion of variance explained by each principal component, as on the left hand side here. Or they show the cumulative percentage of variance explained as the number of principal components increases (until all of the original variance is explained when the number of principal components equals the number of features in the original data). This is shown on the right hand side.</p>
<p><strong>4. Biplots in R</strong></p>
<p>Creating a biplot in R from the results of ‘prcomp’ only requires passing the PCA model into the function ‘biplot’.</p>
<p><strong>5. Scree plots in R</strong></p>
<p>Building up the scree plots requires a few additional steps. First, the standard deviations of each principle component is accessed through the ‘sdev’ component of the PCA model. Because we want variance instead of standard deviation – and variance is defined as the square of standard deviation – we must take the square of each element of ‘sdev’.Finally, the proportion of variance for each principle component is determined by dividing by the total variance explained. This is then plotted using R’s base plot,</p>
<p><strong>6. Scree plot</strong></p>
<p>or your favorite plotting function.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Ok, let’s get started creating some visualizations to help interpret PCA models. We will guide you along the way.</p>
</section>
<section id="interpreting-biplots-1" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="interpreting-biplots-1"><span class="header-section-number">3.8</span> Interpreting biplots (1)</h2>
<p>As stated in the video, the <a href="https://www.rdocumentation.org/packages/stats/topics/biplot"><code>biplot()</code></a> function plots both the principal components loadings and the mapping of the observations to their first two principal component values. The next couple of exercises will check your interpretation of the <code>biplot()</code> visualization.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a><span class="fu">biplot</span>(pr.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-14-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-5" data-number="3.9" class="anchored"><span class="header-section-number">3.9</span> <em>Question</em></h2>
<p>Using the <code>biplot()</code> of the <code>pr.out</code> model, which two original variables have approximately the same loadings in the first two principal components?<br> <br> ✅ <code>Attack</code> and <code>HitPoints</code><br> ⬜ <code>Attack</code> and <code>Speed</code><br> ⬜ <code>Speed</code> and <code>Defense</code><br> ⬜ <code>HitPoints</code> and <code>Defense</code><br></p>
</blockquote>
<p>Good job! Continue to the next exercise.</p>
</section>
<section id="interpreting-biplots-2" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="interpreting-biplots-2"><span class="header-section-number">3.10</span> Interpreting biplots (2)</h2>
<p>In the last exercise, you saw that <code>Attack</code> and <code>HitPoints</code> have approximately the same loadings in the first two principal components.</p>
<blockquote class="blockquote">
<h2 id="question-6" data-number="3.11" class="anchored"><span class="header-section-number">3.11</span> <em>Question</em></h2>
<p>Again using the <code>biplot()</code> of the <code>pr.out</code> model, which two Pokemon are the least similar in terms of the second principal component?<br> <br> ✅ Kadabra and Torkoal<br> ⬜ Magnemite and Origin<br> ⬜ Forme and Goodra<br> ⬜ Marowak and Golem<br></p>
</blockquote>
<p>Nice one!</p>
</section>
<section id="variance-explained" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="variance-explained"><span class="header-section-number">3.12</span> Variance explained</h2>
<p>The second common plot type for understanding PCA models is a <em>scree plot</em>. A scree plot shows the variance explained as the number of principal components increases. Sometimes the cumulative variance explained is plotted as well.</p>
<p>In this and the next exercise, you will prepare data from the <code>pr.out</code> model you created at the beginning of the chapter for use in a scree plot. Preparing the data for plotting is required because there is not a built-in function in R to create this type of plot.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Assign to the variable <code>pr.var</code> the square of the standard deviations of the principal components (i.e.&nbsp;the variance). The standard deviation of the principal components is available in the <code>sdev</code> component of the PCA model object.</li>
<li>Assign to the variable <code>pve</code> the proportion of the variance explained, calculated by dividing <code>pr.var</code> by the total variance explained by all principal components.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="co"># Variability of each principal component: pr.var</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>pr.var <span class="ot">&lt;-</span> pr.out<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb43-3"><a href="#cb43-3"></a></span>
<span id="cb43-4"><a href="#cb43-4"></a><span class="co"># Variance explained by each principal component: pve</span></span>
<span id="cb43-5"><a href="#cb43-5"></a>pve <span class="ot">&lt;-</span> pr.var <span class="sc">/</span> <span class="fu">sum</span>(pr.var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work!</p>
</section>
<section id="visualize-variance-explained" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="visualize-variance-explained"><span class="header-section-number">3.13</span> Visualize variance explained</h2>
<p>Now you will create a scree plot showing the proportion of variance explained by each principal component, as well as the cumulative proportion of variance explained.</p>
<p>Recall from the video that these plots can help to determine the number of principal components to retain. One way to determine the number of principal components to retain is by looking for an elbow in the scree plot showing that as the number of principal components increases, the rate at which variance is explained decreases substantially. In the absence of a clear elbow, you can use the scree plot as a guide for setting a threshold.</p>
<p><strong>Steps</strong></p>
<p>The proportion of variance explained is still available in the <code>pve</code> object.</p>
<ol type="1">
<li>Use <code>plot()</code> to plot the proportion of variance explained by each principal component.</li>
<li>Use <code>plot()</code> and <code>cumsum()</code> (cumulative sum) to plot the cumulative proportion of variance explained as a function of the number principal components.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># Plot variance explained for each principal component</span></span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="fu">plot</span>(pve, <span class="at">xlab =</span> <span class="st">"Principal Component"</span>,</span>
<span id="cb44-3"><a href="#cb44-3"></a>     <span class="at">ylab =</span> <span class="st">"Proportion of Variance Explained"</span>,</span>
<span id="cb44-4"><a href="#cb44-4"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"b"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-16-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a><span class="co"># Plot cumulative proportion of variance explained</span></span>
<span id="cb45-2"><a href="#cb45-2"></a><span class="fu">plot</span>(<span class="fu">cumsum</span>(pve), <span class="at">xlab =</span> <span class="st">"Principal Component"</span>,</span>
<span id="cb45-3"><a href="#cb45-3"></a>     <span class="at">ylab =</span> <span class="st">"Cumulative Proportion of Variance Explained"</span>,</span>
<span id="cb45-4"><a href="#cb45-4"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"b"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-16-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-16-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Awesome! Notice that when the number of principal components is equal to the number of original features in the data, the cumulative proportion of variance explained is 1.</p>
</section>
<section id="practical-issues-with-pca" class="level2" data-number="3.14">
<h2 data-number="3.14" class="anchored" data-anchor-id="practical-issues-with-pca"><span class="header-section-number">3.14</span> Practical issues with PCA</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Practical issues with PCA</strong></p>
<p>Before PCA is wrapped up there are some practical issues of using PCA on real world data that will be helpful to know.</p>
<p><strong>2. Practical issues with PCA</strong></p>
<p>There are three types of items that need to be considered to complete a successful principal components analysis.The first of these, is dealing with scaling the data. I’ll cover this in more detail in just a moment.The second item that sometimes needs to be considered is what to do with observations that have missing data in one or more of the features. There are many ways to address this issue, with one of the simplest approaches being to not include, or drop, observations with missing data.A more complex approach to dealing with missing data is to estimate or impute the missing values. While I will not go into more detail on this item, I wanted you to be aware of these strategies.The third practical matter is how to handle observations with features that are categories – that is, features that are not numerical. The first strategy is the simplest – do not include the categorical features in modeling.The second strategy is more involved and requires using one of many methods to encode the categorical features as numbers. This is more detail than I want to cover in this introductory course, but wanted to make sure you were aware of them if the situation presents itself.</p>
<p><strong>3. mtcars dataset</strong></p>
<p>Now let’s dig into the importance of scaling. Here we will look at the mtcars dataset. This dataset has information about various car models, things like miles per gallon, horsepower, and number of cylinders. Each feature is in different units of measure.</p>
<p><strong>4. Scaling</strong></p>
<p>As with clustering, when features are measured in different units or scales, it is often required to center the data by subtracting the means of each feature, and dividing each feature by its standard deviation to normalize the data.Here it’s shown that the means of the features and the standard deviations of the features vary quite a bit, indicating that centering and scaling the data are in order before performing PCA.</p>
<p><strong>5. Importance of scaling data</strong></p>
<p>A way to understand the importance of scaling the data is to review the biplots of the mtcars data with and without scaling.In the example on the left hand side, without scaling, the displacement and horsepower features are the features with the largest loadings in the first two principal components, with all the other features being overwhelmed. This is because those two features, displacement and horsepower, have the most variance in the data… but that is true only because they are on a different unit of measure from the other features.On the right hand side is the same original data on a biplot but with scaling performed before doing PCA. This biplot now shows a more even distribution of the loading vectors.</p>
<p><strong>6. Scaling and PCA in R</strong></p>
<p>Unlike the clustering algorithms, prcomp in R has the option to perform scaling and centering directly in the principal components algorithm. There are two parameters, center, and scale, used to perform this in R. Setting these to TRUE or FALSE will perform or not perform scaling and centering.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>The coming exercises will help you practice what you’ve learned.</p>
</section>
<section id="practical-issues-scaling" class="level2" data-number="3.15">
<h2 data-number="3.15" class="anchored" data-anchor-id="practical-issues-scaling"><span class="header-section-number">3.15</span> Practical issues: scaling</h2>
<p>You saw in the video that scaling your data before doing PCA changes the results of the PCA modeling. Here, you will perform PCA with and without scaling, then visualize the results using biplots.</p>
<p>Sometimes scaling is appropriate when the variances of the variables are substantially different. This is commonly the case when variables have different units of measurement, for example, degrees Fahrenheit (temperature) and miles (distance). Making the decision to use scaling is an important step in performing a principal component analysis.</p>
<p><strong>Steps</strong></p>
<p>The same Pokemon dataset is available in your workspace as <code>pokemon</code>, but one new variable has been added: <code>Total</code>.</p>
<ol type="1">
<li>There is some code at the top of the editor to calculate the mean and standard deviation of each variable in the model. Run this code to see how the scale of the variables differs in the original data.</li>
<li>Create a PCA model of <code>pokemon</code> with scaling, assigning the result to <code>pr.with.scaling</code>.</li>
<li>Create a PCA model of <code>pokemon</code> without scaling, assigning the result to <code>pr.without.scaling</code>.</li>
<li>Use <code>biplot()</code> to plot both models (one at a time) and compare their outputs.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1"></a><span class="co"># Load data</span></span>
<span id="cb46-2"><a href="#cb46-2"></a>pokemon <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"data/pokemon50_total.rds"</span>)</span>
<span id="cb46-3"><a href="#cb46-3"></a></span>
<span id="cb46-4"><a href="#cb46-4"></a><span class="co"># Mean of each variable</span></span>
<span id="cb46-5"><a href="#cb46-5"></a><span class="fu">colMeans</span>(pokemon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;     Total HitPoints    Attack   Defense     Speed 
#&gt;    448.82     71.08     81.22     78.44     66.58</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1"></a><span class="co"># Standard deviation of each variable</span></span>
<span id="cb48-2"><a href="#cb48-2"></a><span class="fu">apply</span>(pokemon, <span class="dv">2</span>, sd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;     Total HitPoints    Attack   Defense     Speed 
#&gt; 119.32321  25.62193  33.03078  32.05809  27.51036</code></pre>
</div>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a><span class="co"># PCA model with scaling: pr.with.scaling</span></span>
<span id="cb50-2"><a href="#cb50-2"></a>pr.with.scaling <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(pokemon, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb50-3"><a href="#cb50-3"></a></span>
<span id="cb50-4"><a href="#cb50-4"></a><span class="co"># PCA model without scaling: pr.without.scaling</span></span>
<span id="cb50-5"><a href="#cb50-5"></a>pr.without.scaling <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(pokemon, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb50-6"><a href="#cb50-6"></a></span>
<span id="cb50-7"><a href="#cb50-7"></a><span class="co"># Create biplots of both for comparison</span></span>
<span id="cb50-8"><a href="#cb50-8"></a><span class="fu">biplot</span>(pr.with.scaling)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-17-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a><span class="fu">biplot</span>(pr.without.scaling)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-17-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-17-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Good job! The new <code>Total</code> column contains much more variation, on average, than the other four columns, so it has a disproportionate effect on the PCA model when scaling is not performed. After scaling the data, there’s a much more even distribution of the loading vectors.</p>
</section>
<section id="additional-uses-of-pca-and-wrap-up" class="level2" data-number="3.16">
<h2 data-number="3.16" class="anchored" data-anchor-id="additional-uses-of-pca-and-wrap-up"><span class="header-section-number">3.16</span> Additional uses of PCA and wrap-up</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Additional uses of PCA and wrap-up</strong></p>
<p>Congratulations again, you have completed the exercises for principal components analysis.</p>
<p><strong>2. Dimensionality reduction</strong></p>
<p>During this chapter you have learned how to perform PCA in R and have gained some geometric intuition about how the algorithm works.</p>
<p><strong>3. Data visualization</strong></p>
<p>You have learned how PCA can be used to help visualize high dimensionality data,</p>
<p><strong>4. Interpreting PCA results</strong></p>
<p>how to interpret PCA models by making and reading biplots and scree plots,</p>
<p><strong>5. Importance of data scaling</strong></p>
<p>and how to deal with one of the most common practical issues when doing PCA: centering and scaling.</p>
<p><strong>6. Up next</strong></p>
<p>In the next and final chapter, you will apply everything you have learned about unsupervised learning to understanding a real-world data set.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Let’s get started!</p>
</section>
</section>
<section id="putting-it-all-together-with-a-case-study" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 4. Putting it all together with a case study</h1>
<p>The goal of this chapter is to guide you through a complete analysis using the unsupervised learning techniques covered in the first three chapters. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses.</p>
<section id="introduction-to-the-case-study" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction-to-the-case-study"><span class="header-section-number">4.1</span> Introduction to the case study</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Introduction to the case study</strong></p>
<p>Thus far you have successfully learned how to do two types of clustering, kmeans and hierarchical, and one type of dimensionality reduction, principal component analysis.</p>
<p><strong>2. Objectives</strong></p>
<p>This chapter is a little different. In this chapter I am going to guide you through a complete analysis using the unsupervised learning techniques you have just learned. I have three reasons for doing this: One, reinforce what you have already learned. Two, add in a few steps not covered before, such as getting and preparing the data, and seeing if the results of unsupervised learning would make good features for supervised learning, and Three, emphasize the creativity required to be successful at unsupervised learning.</p>
<p><strong>3. Example use case</strong></p>
<p>The dataset you will be using in this analysis was published in a paper by Bennett and Mangasarian. Their data consisted of measurements of nuclei of cells of human breast masses. Each observation, or row, is of a single mass or group of cells and consists of ten features. Each feature is a summary statistic of measurements from the cells in that mass.There is also a target variable, or label, in the dataset. The label would be used if you were doing modeling using supervised learning – it will not be used for modeling during this analysis.</p>
<p><strong>4. Analysis</strong></p>
<p>At a high level you will complete six steps during this analysis. Downloading and preparing the data for modeling, doing some high level exploratory analysis, performing principal component analysis and using visualizations and other mechanisms to interpret the results, completing two types of clustering, understanding and comparing the two types, and finally, combining Principal Component Analysis as a preprocessing step to clustering.During the coding exercises, you will be guided through each step.</p>
<p><strong>5. Review: PCA in R</strong></p>
<p>The exercises immediately following this video include completing principal components analysis on the data. As a reminder, the function in R to do principal component analysis is ‘prcomp’, and takes as its parameters a matrix of the data with each observation as a row of the matrix and one feature per column of the matrix, plus the options for scaling and centering the data. As covered earlier, if the data uses different scales or units of measure, centering and scaling the data before performing PCA can improve the results of the analysis. And don’t forget that the summary function in R, with the output of prcomp() as the input, provides important information about the amount of variability described by each principal component.</p>
<p><strong>6. Unsupervised learning is open-ended</strong></p>
<p>The analysis you are going to step through is but one path that could have been taken – as you complete this chapter you might want to give thought to what other approaches you might take when presented with an analysis using unsupervised learning.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>I hope you’ll have fun with the next exercises. We’ll help you with hints and templates along the way.</p>
</section>
<section id="preparing-the-data" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="preparing-the-data"><span class="header-section-number">4.2</span> Preparing the data</h2>
<p>Unlike prior chapters, where we prepared the data for you for unsupervised learning, the goal of this chapter is to step you through a more realistic and complete workflow.</p>
<p>Recall from the video that the first step is to download and prepare the data.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>read.csv()</code> function to download the CSV (comma-separated values) file containing the data from the URL provided. Assign the result to <code>wisc.df</code>.</li>
<li>Use <code>as.matrix()</code> to convert the features of the data (in columns 3 through 32) to a matrix. Store this in a variable called <code>wisc.data</code>.</li>
<li>Assign the row names of <code>wisc.data</code> the values currently contained in the <code>id</code> column of <code>wisc.df</code>. While not strictly required, this will help you keep track of the different observations throughout the modeling process.</li>
<li>Finally, set a vector called <code>diagnosis</code> to be <code>1</code> if a diagnosis is malignant (<code>"M"</code>) and <code>0</code> otherwise. Note that R coerces <code>TRUE</code> to 1 and <code>FALSE</code> to 0.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1"></a><span class="co"># Download the data: wisc.df</span></span>
<span id="cb52-2"><a href="#cb52-2"></a>wisc_df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/WisconsinCancer.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; New names:
#&gt; • `` -&gt; `...33`</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning: One or more parsing issues, call `problems()` on your data frame for details,
#&gt; e.g.:
#&gt;   dat &lt;- vroom(...)
#&gt;   problems(dat)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 569 Columns: 33
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr  (1): diagnosis
#&gt; dbl (31): id, radius_mean, texture_mean, perimeter_mean, area_mean, smoothne...
#&gt; lgl  (1): ...33
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a><span class="co"># Convert the features of the data: wisc.data</span></span>
<span id="cb56-2"><a href="#cb56-2"></a>wisc_data <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(wisc_df[<span class="dv">3</span><span class="sc">:</span><span class="dv">32</span>])</span>
<span id="cb56-3"><a href="#cb56-3"></a></span>
<span id="cb56-4"><a href="#cb56-4"></a><span class="co"># Set the row names of wisc.data</span></span>
<span id="cb56-5"><a href="#cb56-5"></a><span class="fu">row.names</span>(wisc_data) <span class="ot">&lt;-</span> wisc_df<span class="sc">$</span>id</span>
<span id="cb56-6"><a href="#cb56-6"></a></span>
<span id="cb56-7"><a href="#cb56-7"></a><span class="co"># Create diagnosis vector</span></span>
<span id="cb56-8"><a href="#cb56-8"></a>diagnosis <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(wisc_df<span class="sc">$</span>diagnosis <span class="sc">==</span> <span class="st">"M"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great work! You’ve successfully prepared the data for exploratory data analysis.</p>
</section>
<section id="exploratory-data-analysis" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="exploratory-data-analysis"><span class="header-section-number">4.3</span> Exploratory data analysis</h2>
<p>The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data.</p>
<p>The variables you created before, <code>wisc.data</code> and <code>diagnosis</code>, are still available in your workspace. Explore the data to answer the following questions:</p>
<blockquote class="blockquote">
<h2 id="question-7" data-number="4.4" class="anchored"><span class="header-section-number">4.4</span> <em>Question</em></h2>
<ol type="1">
<li>How many observations are in this dataset?<br></li>
<li>How many variables/features in the data are suffixed with _mean?<br></li>
<li>How many of the observations have a malignant diagnosis?<br> <br> ⬜ 569, 5, 112<br> ⬜ 30, 10, 212<br> ✅ 569, 10, 212<br> ⬜ 30, 5, 112<br></li>
</ol>
</blockquote>
</section>
<section id="performing-pca" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="performing-pca"><span class="header-section-number">4.5</span> Performing PCA</h2>
<p>The next step in your analysis is to perform PCA on <code>wisc.data</code>.</p>
<p>You saw in the last chapter that it’s important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data:</p>
<ol type="1">
<li>The input variables use different units of measurement.</li>
<li>The input variables have <em>significantly</em> different variances.</li>
</ol>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Check the mean and standard deviation of the features of the data to determine if the data should be scaled. Use the <code>colMeans()</code> and <code>apply()</code> functions like you’ve done before.</li>
<li>Execute PCA on the <code>wisc.data</code>, scaling if appropriate, and assign the model to <code>wisc.pr</code>.</li>
<li>Inspect a summary of the results with the <code>summary()</code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1"></a><span class="co"># Check column means and standard deviations</span></span>
<span id="cb57-2"><a href="#cb57-2"></a><span class="fu">colMeans</span>(wisc_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;             radius_mean            texture_mean          perimeter_mean 
#&gt;            1.412729e+01            1.928965e+01            9.196903e+01 
#&gt;               area_mean         smoothness_mean        compactness_mean 
#&gt;            6.548891e+02            9.636028e-02            1.043410e-01 
#&gt;          concavity_mean     concave points_mean           symmetry_mean 
#&gt;            8.879932e-02            4.891915e-02            1.811619e-01 
#&gt;  fractal_dimension_mean               radius_se              texture_se 
#&gt;            6.279761e-02            4.051721e-01            1.216853e+00 
#&gt;            perimeter_se                 area_se           smoothness_se 
#&gt;            2.866059e+00            4.033708e+01            7.040979e-03 
#&gt;          compactness_se            concavity_se       concave points_se 
#&gt;            2.547814e-02            3.189372e-02            1.179614e-02 
#&gt;             symmetry_se    fractal_dimension_se            radius_worst 
#&gt;            2.054230e-02            3.794904e-03            1.626919e+01 
#&gt;           texture_worst         perimeter_worst              area_worst 
#&gt;            2.567722e+01            1.072612e+02            8.805831e+02 
#&gt;        smoothness_worst       compactness_worst         concavity_worst 
#&gt;            1.323686e-01            2.542650e-01            2.721885e-01 
#&gt;    concave points_worst          symmetry_worst fractal_dimension_worst 
#&gt;            1.146062e-01            2.900756e-01            8.394582e-02</code></pre>
</div>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1"></a><span class="fu">apply</span>(wisc_data, <span class="dv">2</span>, sd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;             radius_mean            texture_mean          perimeter_mean 
#&gt;            3.524049e+00            4.301036e+00            2.429898e+01 
#&gt;               area_mean         smoothness_mean        compactness_mean 
#&gt;            3.519141e+02            1.406413e-02            5.281276e-02 
#&gt;          concavity_mean     concave points_mean           symmetry_mean 
#&gt;            7.971981e-02            3.880284e-02            2.741428e-02 
#&gt;  fractal_dimension_mean               radius_se              texture_se 
#&gt;            7.060363e-03            2.773127e-01            5.516484e-01 
#&gt;            perimeter_se                 area_se           smoothness_se 
#&gt;            2.021855e+00            4.549101e+01            3.002518e-03 
#&gt;          compactness_se            concavity_se       concave points_se 
#&gt;            1.790818e-02            3.018606e-02            6.170285e-03 
#&gt;             symmetry_se    fractal_dimension_se            radius_worst 
#&gt;            8.266372e-03            2.646071e-03            4.833242e+00 
#&gt;           texture_worst         perimeter_worst              area_worst 
#&gt;            6.146258e+00            3.360254e+01            5.693570e+02 
#&gt;        smoothness_worst       compactness_worst         concavity_worst 
#&gt;            2.283243e-02            1.573365e-01            2.086243e-01 
#&gt;    concave points_worst          symmetry_worst fractal_dimension_worst 
#&gt;            6.573234e-02            6.186747e-02            1.806127e-02</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1"></a><span class="co"># Execute PCA, scaling if appropriate: wisc.pr</span></span>
<span id="cb61-2"><a href="#cb61-2"></a>wisc_pr <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(wisc_data, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb61-3"><a href="#cb61-3"></a></span>
<span id="cb61-4"><a href="#cb61-4"></a><span class="co"># Look at summary of results</span></span>
<span id="cb61-5"><a href="#cb61-5"></a><span class="fu">summary</span>(wisc_pr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Importance of components:
#&gt;                           PC1    PC2     PC3     PC4     PC5     PC6     PC7
#&gt; Standard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880 0.82172
#&gt; Proportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025 0.02251
#&gt; Cumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759 0.91010
#&gt;                            PC8    PC9    PC10   PC11    PC12    PC13    PC14
#&gt; Standard deviation     0.69037 0.6457 0.59219 0.5421 0.51104 0.49128 0.39624
#&gt; Proportion of Variance 0.01589 0.0139 0.01169 0.0098 0.00871 0.00805 0.00523
#&gt; Cumulative Proportion  0.92598 0.9399 0.95157 0.9614 0.97007 0.97812 0.98335
#&gt;                           PC15    PC16    PC17    PC18    PC19    PC20   PC21
#&gt; Standard deviation     0.30681 0.28260 0.24372 0.22939 0.22244 0.17652 0.1731
#&gt; Proportion of Variance 0.00314 0.00266 0.00198 0.00175 0.00165 0.00104 0.0010
#&gt; Cumulative Proportion  0.98649 0.98915 0.99113 0.99288 0.99453 0.99557 0.9966
#&gt;                           PC22    PC23   PC24    PC25    PC26    PC27    PC28
#&gt; Standard deviation     0.16565 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987
#&gt; Proportion of Variance 0.00091 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005
#&gt; Cumulative Proportion  0.99749 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997
#&gt;                           PC29    PC30
#&gt; Standard deviation     0.02736 0.01153
#&gt; Proportion of Variance 0.00002 0.00000
#&gt; Cumulative Proportion  1.00000 1.00000</code></pre>
</div>
</div>
</section>
<section id="interpreting-pca-results" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="interpreting-pca-results"><span class="header-section-number">4.6</span> Interpreting PCA results</h2>
<p>Now you’ll use some visualizations to better understand your PCA model. You were introduced to one of these visualizations, the biplot, in an earlier chapter.</p>
<p>You’ll run into some common challenges with using biplots on real-world data containing a non-trivial number of observations and variables, then you’ll look at some alternative visualizations. You are encouraged to experiment with additional visualizations before moving on to the next exercise.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a biplot of the <code>wisc.pr</code> data. <em>What stands out to you about this plot? Is it easy or difficult to understand? Why?</em></li>
<li>Execute the code to scatter plot each observation by principal components 1 and 2, coloring the points by the diagnosis.</li>
<li>Repeat the same for principal components 1 and 3. <em>What do you notice about these plots?</em></li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1"></a><span class="co"># Create a biplot of wisc.pr</span></span>
<span id="cb63-2"><a href="#cb63-2"></a><span class="fu">biplot</span>(wisc_pr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-20-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1"></a><span class="co"># Scatter plot observations by components 1 and 2</span></span>
<span id="cb64-2"><a href="#cb64-2"></a><span class="fu">plot</span>(wisc_pr<span class="sc">$</span>x[, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)], <span class="at">col =</span> (diagnosis <span class="sc">+</span> <span class="dv">1</span>), </span>
<span id="cb64-3"><a href="#cb64-3"></a>     <span class="at">xlab =</span> <span class="st">"PC1"</span>, <span class="at">ylab =</span> <span class="st">"PC2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-20-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-20-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1"></a><span class="co"># Repeat for components 1 and 3</span></span>
<span id="cb65-2"><a href="#cb65-2"></a><span class="fu">plot</span>(wisc_pr<span class="sc">$</span>x[, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)], <span class="at">col =</span> (diagnosis <span class="sc">+</span> <span class="dv">1</span>), </span>
<span id="cb65-3"><a href="#cb65-3"></a>     <span class="at">xlab =</span> <span class="st">"PC1"</span>, <span class="at">ylab =</span> <span class="st">"PC3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-20-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-20-3.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Excellent work! Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.</p>
</section>
<section id="variance-explained-1" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="variance-explained-1"><span class="header-section-number">4.7</span> Variance explained</h2>
<p>In this exercise, you will produce scree plots showing the proportion of variance explained as the number of principal components increases. The data from PCA must be prepared for these plots, as there is not a built-in function in R to create them directly from the PCA model.</p>
<p>As you look at these plots, ask yourself if there’s an elbow in the amount of variance explained that might lead you to pick a natural number of principal components. If an obvious elbow does not exist, as is typical in real-world datasets, consider how else you might determine the number of principal components to retain based on the scree plot.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Calculate the variance of each principal component by squaring the <code>sdev</code> component of <code>wisc.pr</code>. Save the result as an object called <code>pr.var</code>.</li>
<li>Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called <code>pve</code>.</li>
<li>Create a plot of variance explained for each principal component.</li>
<li>Using the <code>cumsum()</code> function, create a plot of cumulative proportion of variance explained.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1"></a><span class="co"># Set up 1 x 2 plotting grid</span></span>
<span id="cb66-2"><a href="#cb66-2"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-3"><a href="#cb66-3"></a></span>
<span id="cb66-4"><a href="#cb66-4"></a><span class="co"># Calculate variability of each component</span></span>
<span id="cb66-5"><a href="#cb66-5"></a>pr_var <span class="ot">&lt;-</span> wisc_pr<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb66-6"><a href="#cb66-6"></a></span>
<span id="cb66-7"><a href="#cb66-7"></a><span class="co"># Variance explained by each principal component: pve</span></span>
<span id="cb66-8"><a href="#cb66-8"></a>pve <span class="ot">&lt;-</span> pr_var <span class="sc">/</span> <span class="fu">sum</span>(pr_var)</span>
<span id="cb66-9"><a href="#cb66-9"></a></span>
<span id="cb66-10"><a href="#cb66-10"></a><span class="co"># Plot variance explained for each principal component</span></span>
<span id="cb66-11"><a href="#cb66-11"></a><span class="fu">plot</span>(pve, <span class="at">xlab =</span> <span class="st">"Principal Component"</span>, </span>
<span id="cb66-12"><a href="#cb66-12"></a>     <span class="at">ylab =</span> <span class="st">"Proportion of Variance Explained"</span>, </span>
<span id="cb66-13"><a href="#cb66-13"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"b"</span>)</span>
<span id="cb66-14"><a href="#cb66-14"></a></span>
<span id="cb66-15"><a href="#cb66-15"></a><span class="co"># Plot cumulative proportion of variance explained</span></span>
<span id="cb66-16"><a href="#cb66-16"></a><span class="fu">plot</span>(<span class="fu">cumsum</span>(pve), <span class="at">xlab =</span> <span class="st">"Principal Component"</span>, </span>
<span id="cb66-17"><a href="#cb66-17"></a>     <span class="at">ylab =</span> <span class="st">"Cumulative Proportion of Variance Explained"</span>, </span>
<span id="cb66-18"><a href="#cb66-18"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"b"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="03_unsupervised_learning_files/figure-html/unnamed-chunk-21-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="03_unsupervised_learning_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Great work! Before moving on, answer the following question: What is the minimum number of principal components needed to explain 80% of the variance in the data? Write it down as you may need this in the next exercise :)</p>
</section>
<section id="pca-review-and-next-steps" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="pca-review-and-next-steps"><span class="header-section-number">4.8</span> PCA review and next steps</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. PCA review and next steps</strong></p>
<p>Before moving on, let me quickly review the analysis thus far and get you on the way to the final steps.</p>
<p><strong>2. Review thus far</strong></p>
<p>So far in this chapter you have completed 2 new steps typical in unsupervised analysis: downloading the data and performing some basic exploratory analysis. These are steps you will have to do in any machine learning work.You also completed a rather detailed principal component analysis and learned a bit about some latent, or unseen, variables that might exist in the observations.</p>
<p><strong>3. Next steps</strong></p>
<p>As a reminder, the next (and final) steps in this particular analysis are to complete two types of clustering on the data, and combine PCA and clustering together. The first exercise on hierarchical clustering also has you compare the results of the clustering to the diagnosis – if you were doing supervised learning, this step would provide you insights as to if the clusters would be useful features.Next, there is a comparison of the results of the two types of clustering; this type of work is done to contrast the results of the two algorithms and see if they produce similar or different sub-groupings.Finally, you’ll combine PCA and clustering. PCA is often used as a preprocessing step for different types of machine learning – when done that way it creates a type of regularization that helps avoid overfitting the data. In a coming exercise, you will see how PCA affects the results of clustering.</p>
<p><strong>4. Review: hierarchical clustering in R</strong></p>
<p>Just some quick reminders on hierarchical clustering:The R function for hierarchical clustering is hclust. hclust takes a matrix of the pair wise distance between observations as its input. You will continue to use Euclidean distance for this exercise.</p>
<p><strong>5. Review: k-means in R</strong></p>
<p>And to do kmeans in R, use the kmeans function. The kmeans function takes a matrix of the data, the same matrix you prepared earlier in this chapter. kmeans also requires the number of clusters to be decided before the algorithm is run, which is specified using the centers parameter to kmeans.Recall that the kmeans algorithm has a stochastic or random aspect. To improve the chances of finding a global minimum kmeans is run repeatedly keeping the ‘best’ results, as measured by total within cluster sum of squares, from all the runs – the number of times kmeans is run is specified by the nstart parameter to kmeans.</p>
<p><strong>6. Let’s practice!</strong></p>
<p>Ok, let’s get started.</p>
</section>
<section id="communicating-pca-results" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="communicating-pca-results"><span class="header-section-number">4.9</span> Communicating PCA results</h2>
<p>This exercise will check your understanding of the PCA results, in particular the loadings and variance explained. The loadings, represented as vectors, explain the mapping from the original features to the principal components. The principal components are naturally ordered from the most variance explained to the least variance explained.</p>
<p>The variables you created before—<code>wisc.data</code>, <code>diagnosis</code>, <code>wisc.pr</code>, and <code>pve</code>—are still available.</p>
<blockquote class="blockquote">
<h2 id="question-8" data-number="4.10" class="anchored"><span class="header-section-number">4.10</span> <em>Question</em></h2>
<p>For the first principal component, what is the component of the loading vector for the feature <code>concave.points_mean</code>? What is the minimum number of principal components required to explain 80% of the variance of the data?<br> <br> ✅ -0.26085376, 5<br> ⬜ -0.25088597, 2<br> ⬜ 0.034767500, 4<br> ⬜ 0.26085376, 5<br></p>
</blockquote>
</section>
<section id="hierarchical-clustering-of-case-data" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="hierarchical-clustering-of-case-data"><span class="header-section-number">4.11</span> Hierarchical clustering of case data</h2>
<p>The goal of this exercise is to do hierarchical clustering of the observations. Recall from Chapter 2 that this type of clustering does not assume in advance the number of natural groups that exist in the data.</p>
<p>As part of the preparation for hierarchical clustering, distance between all pairs of observations are computed. Furthermore, there are different ways to <em>link</em> clusters together, with <em>single</em>, <em>complete</em>, and <em>average</em> being the most common linkage methods.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Scale the <code>wisc.data</code> data and assign the result to <code>data.scaled</code>.</li>
<li>Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to <code>data.dist</code>.</li>
<li>Create a hierarchical clustering model using complete linkage. Manually specify the <code>method</code> argument to <code>hclust()</code> and assign the results to <code>wisc.hclust</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1"></a><span class="co"># Scale the wisc.data data: data.scaled</span></span>
<span id="cb67-2"><a href="#cb67-2"></a>data_scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(wisc_data)</span>
<span id="cb67-3"><a href="#cb67-3"></a></span>
<span id="cb67-4"><a href="#cb67-4"></a><span class="co"># Calculate the (Euclidean) distances: data.dist</span></span>
<span id="cb67-5"><a href="#cb67-5"></a>data_dist <span class="ot">&lt;-</span> <span class="fu">dist</span>(data_scaled)</span>
<span id="cb67-6"><a href="#cb67-6"></a></span>
<span id="cb67-7"><a href="#cb67-7"></a><span class="co"># Create a hierarchical clustering model: wisc.hclust</span></span>
<span id="cb67-8"><a href="#cb67-8"></a>wisc_hclust <span class="ot">&lt;-</span> <span class="fu">hclust</span>(data_dist, <span class="at">method =</span> <span class="st">"complete"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="results-of-hierarchical-clustering" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="results-of-hierarchical-clustering"><span class="header-section-number">4.12</span> Results of hierarchical clustering</h2>
<p>Let’s use the hierarchical clustering model you just created to determine a height (or distance between clusters) where a certain number of clusters exists. The variables you created before—<code>wisc.data</code>, <code>diagnosis</code>, <code>wisc.pr</code>, <code>pve</code>, and <code>wisc.hclust</code>—are all available in your workspace.</p>
<blockquote class="blockquote">
<h2 id="question-9" data-number="4.13" class="anchored"><span class="header-section-number">4.13</span> <em>Question</em></h2>
<p>Using the <code>plot()</code> function, what is the height at which the clustering model has 4 clusters?<br> <br> ✅ 20<br> ⬜ 4<br> ⬜ 10<br> ⬜ 24<br></p>
</blockquote>
</section>
<section id="selecting-number-of-clusters-2" class="level2" data-number="4.14">
<h2 data-number="4.14" class="anchored" data-anchor-id="selecting-number-of-clusters-2"><span class="header-section-number">4.14</span> Selecting number of clusters</h2>
<p>In this exercise, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing <em>unsupervised</em> learning like this, a target variable isn’t available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model.</p>
<p>When performing <em>supervised</em> learning—that is, when you’re trying to predict some target variable of interest and that target variable is available in the original data—using clustering to create new features may or may not improve the performance of the final model. This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use <code>cutree()</code> to cut the tree so that it has 4 clusters. Assign the output to the variable <code>wisc_hclust_clusters</code>.</li>
<li>Use the <code>table()</code> function to compare the cluster membership to the actual diagnoses.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1"></a><span class="co"># Cut tree so that it has 4 clusters: wisc.hclust.clusters</span></span>
<span id="cb68-2"><a href="#cb68-2"></a>wisc_hclust_clusters <span class="ot">&lt;-</span> <span class="fu">cutree</span>(wisc_hclust, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb68-3"><a href="#cb68-3"></a></span>
<span id="cb68-4"><a href="#cb68-4"></a><span class="co"># Compare cluster membership to actual diagnoses</span></span>
<span id="cb68-5"><a href="#cb68-5"></a><span class="fu">table</span>(wisc_hclust_clusters, diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     diagnosis
#&gt; wisc_hclust_clusters   0   1
#&gt;                    1  12 165
#&gt;                    2   2   5
#&gt;                    3 343  40
#&gt;                    4   0   2</code></pre>
</div>
</div>
<p>Four clusters were picked after some exploration. Before moving on, you may want to explore how different numbers of clusters affect the ability of the hierarchical clustering to separate the different diagnoses. Great job!</p>
</section>
<section id="k-means-clustering-and-comparing-results" class="level2" data-number="4.15">
<h2 data-number="4.15" class="anchored" data-anchor-id="k-means-clustering-and-comparing-results"><span class="header-section-number">4.15</span> k-means clustering and comparing results</h2>
<p>As you now know, there are two main types of clustering: hierarchical and k-means.</p>
<p>In this exercise, you will create a k-means clustering model on the Wisconsin breast cancer data and compare the results to the actual diagnoses and the results of your hierarchical clustering model. Take some time to see how each clustering model performs in terms of separating the two diagnoses and how the clustering models compare to each other.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li><p><code>wisc.data</code>, <code>diagnosis</code>, and <code>wisc.hclust.clusters</code> are still available.</p>
<ul>
<li>Create a k-means model on <code>wisc.data</code>, assigning the result to <code>wisc.km</code>. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data and repeat the algorithm 20 times to find a well performing model.</li>
<li>Use the <code>table()</code> function to compare the cluster membership of the k-means model to the actual diagnoses contained in the <code>diagnosis</code> vector. <em>How well does k-means separate the two diagnoses?</em></li>
<li>Use the <code>table()</code> function to compare the cluster membership of the k-means model to the hierarchical clustering model. Recall the cluster membership of the hierarchical clustering model is contained in <code>wisc.hclust.clusters</code>.</li>
</ul></li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1"></a><span class="co"># Create a k-means model on wisc.data: wisc.km</span></span>
<span id="cb70-2"><a href="#cb70-2"></a>wisc_km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="fu">scale</span>(wisc_data), <span class="at">centers =</span> <span class="dv">2</span>, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb70-3"><a href="#cb70-3"></a></span>
<span id="cb70-4"><a href="#cb70-4"></a><span class="co"># Compare k-means to actual diagnoses</span></span>
<span id="cb70-5"><a href="#cb70-5"></a><span class="fu">table</span>(wisc_km<span class="sc">$</span>cluster, diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;    diagnosis
#&gt;       0   1
#&gt;   1 343  37
#&gt;   2  14 175</code></pre>
</div>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1"></a><span class="co"># Compare k-means to hierarchical clustering</span></span>
<span id="cb72-2"><a href="#cb72-2"></a><span class="fu">table</span>(wisc_km<span class="sc">$</span>cluster, wisc_hclust_clusters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;    wisc_hclust_clusters
#&gt;       1   2   3   4
#&gt;   1  17   0 363   0
#&gt;   2 160   7  20   2</code></pre>
</div>
</div>
<p>Nice! Looking at the second table you generated, it looks like clusters 1, 2, and 4 from the hierarchical clustering model can be interpreted as the cluster 1 equivalent from the k-means algorithm, and cluster 3 can be interpreted as the cluster 2 equivalent.</p>
</section>
<section id="clustering-on-pca-results" class="level2" data-number="4.16">
<h2 data-number="4.16" class="anchored" data-anchor-id="clustering-on-pca-results"><span class="header-section-number">4.16</span> Clustering on PCA results</h2>
<p>In this final exercise, you will put together several steps you used earlier and, in doing so, you will experience some of the creativity that is typical in unsupervised learning.</p>
<p>Recall from earlier exercises that the PCA model required significantly fewer features to describe 80% and 95% of the variability of the data. In addition to <em>normalizing</em> data and potentially avoiding overfitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques.</p>
<p>Let’s see if PCA improves or degrades the performance of hierarchical clustering.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with complete linkage. Assign the results to <code>wisc_pr_hclust</code>.</li>
<li>Cut this hierarchical clustering model into 4 clusters and assign the results to <code>wisc_pr_hclust_clusters</code>.</li>
<li>Using <code>table()</code>, compare the results from your new hierarchical clustering model with the actual diagnoses. How well does the newly created model with four clusters separate out the two diagnoses?</li>
<li>How well do the k-means and hierarchical clustering models you created in previous exercises do in terms of separating the diagnoses? Again, use the <code>table()</code> function to compare the output of each model with the vector containing the actual diagnoses.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1"></a><span class="co"># Create a hierarchical clustering model: wisc.pr.hclust</span></span>
<span id="cb74-2"><a href="#cb74-2"></a>wisc_pr_hclust <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(wisc_pr<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]), <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb74-3"><a href="#cb74-3"></a></span>
<span id="cb74-4"><a href="#cb74-4"></a><span class="co"># Cut model into 4 clusters: wisc_pr_hclust_clusters</span></span>
<span id="cb74-5"><a href="#cb74-5"></a>wisc_pr_hclust_clusters <span class="ot">&lt;-</span> <span class="fu">cutree</span>(wisc_pr_hclust, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb74-6"><a href="#cb74-6"></a></span>
<span id="cb74-7"><a href="#cb74-7"></a><span class="co"># Compare to actual diagnoses</span></span>
<span id="cb74-8"><a href="#cb74-8"></a><span class="fu">table</span>(wisc_pr_hclust_clusters, diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                        diagnosis
#&gt; wisc_pr_hclust_clusters   0   1
#&gt;                       1   5 113
#&gt;                       2 350  97
#&gt;                       3   2   0
#&gt;                       4   0   2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1"></a><span class="co"># Compare to k-means and hierarchical</span></span>
<span id="cb76-2"><a href="#cb76-2"></a><span class="fu">table</span>(wisc_hclust_clusters, diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;                     diagnosis
#&gt; wisc_hclust_clusters   0   1
#&gt;                    1  12 165
#&gt;                    2   2   5
#&gt;                    3 343  40
#&gt;                    4   0   2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1"></a><span class="fu">table</span>(wisc_km<span class="sc">$</span>cluster, diagnosis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;    diagnosis
#&gt;       0   1
#&gt;   1 343  37
#&gt;   2  14 175</code></pre>
</div>
</div>
</section>
<section id="wrap-up-and-review" class="level2" data-number="4.17">
<h2 data-number="4.17" class="anchored" data-anchor-id="wrap-up-and-review"><span class="header-section-number">4.17</span> Wrap-up and review</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Wrap-up and review</strong></p>
<p>Congratulations! You have completed the final case study chapter and the course on unsupervised learning in R. I imagine you are ready to be done, so let’s quickly wrap up.</p>
<p><strong>2. Case study wrap-up</strong></p>
<p>This last chapter presented an entire data analysis using unsupervised learning, from beginning to end. With the knowledge of how to gather and explore a data set, and a creative approach to modeling using unsupervised learning you are prepared to tackle real world problems.</p>
<p><strong>3. Types of clustering</strong></p>
<p>During this course you have learned how to perform kmeans and hierarchical clustering using R, resulting in finding homogeneous subgroups within a population.</p>
<p><strong>4. Dimensionality reduction</strong></p>
<p>You’ve learned to decrease the dimensions of data while maintaining maximum data variability using principal components analysis.</p>
<p><strong>5. Model selection</strong></p>
<p>Also, you’ve seen how to deal with challenges you are likely to experience in your work, such as variable and model selection,</p>
<p><strong>6. Interpreting PCA results</strong></p>
<p>interpreting the results of your modeling,</p>
<p><strong>7. Importance of scaling data</strong></p>
<p>and the importance of scaling and centering your data.</p>
<p><strong>8. Course review</strong></p>
<p>All of this has been done using only a few methods and constructs in the R system, demonstrating some of its known strengths for data analysis work.</p>
<p><strong>9. Dendrogram</strong></p>
<p>Also, you have gained intuition about how each of the algorithms works internally,</p>
<p><strong>10. Strengths and weaknesses of each algorithm</strong></p>
<p>the technical strengths and weakness of each algorithm, and how and when to do model selection.</p>
<p><strong>11. Course review</strong></p>
<p>Finally, you completed an example use case from beginning to end.</p>
<p><strong>12. Hone your skills!</strong></p>
<p>I hope you’ve enjoyed learning about unsupervised learning in R. The best way to hone your skills is to practice applying them to interesting real-world datasets that you’re motivated to explore. Thanks!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">7.2: Supervised Learning: Regression</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" class="pagination-link">
        <span class="nav-page-text">7.4: Machine Learning in the tidyverse</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Content 2022 by <a href="https://www.startupengineer.io/authors/schwarz/">Joschka Schwarz</a> <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with and <a href="https://quarto.org/">Quarto</a><br> <a href="https://www.github.com/jwarz/jwarz.github.io">View the source at GitHub</a></div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","openEffect":"zoom","closeEffect":"zoom","selector":".lightbox","loop":true});</script>



<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>