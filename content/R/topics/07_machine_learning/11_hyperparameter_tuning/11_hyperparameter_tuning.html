<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joschka Schwarz">

<title>Joschka Schwarz - Hyperparameter Tuning in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" rel="next">
<link href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" rel="prev">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

<link href="../../../../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../../../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>


</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Joschka Schwarz</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-science" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-science">    
        <li>
    <a class="dropdown-item" href="../../../../../content/R/index.html">
 <span class="dropdown-text">R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/python/index.html">
 <span class="dropdown-text">Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../content/sql/index.html">
 <span class="dropdown-text">SQL</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../../slides/index.html">
 <span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../resumes/index.html">
 <span class="menu-text">Resumes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jwarz/"><i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/j-schwarz"><i class="bi bi-linkedin" role="img" aria-label="Quarto LinkedIn">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Hyperparameter Tuning in R</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <b><i>1 Programming Basics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/01_programming_beginner/01_programming_beginner.html" class="sidebar-item-text sidebar-link">1.1: Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/02_programming_intermediate/02_programming_intermediate.html" class="sidebar-item-text sidebar-link">1.2: Intermediate R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/01_basics/03_programming_tidyverse/03_programming_tidyverse.html" class="sidebar-item-text sidebar-link">1.3: Introduction to the tidyverse</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>2 Importing Data</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/01_importing_data_beginner/01_importing_data_beginner.html" class="sidebar-item-text sidebar-link">2.1: Introduction to Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/02_importing_data_intermediate/02_importing_data_intermediate.html" class="sidebar-item-text sidebar-link">2.2: Intermediate Importing Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/02_importing_data/03_working_with_web_data/03_working_with_web_data.html" class="sidebar-item-text sidebar-link">2.3: Working with web data</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>3 Data Wrangling</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/01_data_manipulation_with_dplyr/01_data_manipulation_with_dplyr.html" class="sidebar-item-text sidebar-link">3.1: Data Manipulation with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/02_joining_data_with_dplyr/02_joining_data_with_dplyr.html" class="sidebar-item-text sidebar-link">3.2: Joining data with dplyr</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/exploratory_data_analysis_in_r/exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.3: Exploratory Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/case_study_exploratory_data_analysis_in_r/case_study_exploratory_data_analysis_in_r.html" class="sidebar-item-text sidebar-link">3.4: Case Study: EDA</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/cleaning_data_in_r/cleaning_data_in_r.html" class="sidebar-item-text sidebar-link">3.5: Cleaning Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/data_manipulation_with_datatable/data_manipulation_with_datatable.html" class="sidebar-item-text sidebar-link">3.6: Data Manipulation with data.table</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/03_data_wrangling/joining_data_with_datatable/joining_data_with_datatable.html" class="sidebar-item-text sidebar-link">3.7: Joining Data with data.table</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>4 Data Visualization</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/introduction_to_data_visualization_with_ggplot2/ggplot2_intro.html" class="sidebar-item-text sidebar-link">4.1: Introduction to Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/04_data_visualization/data_visualization_with_ggplot2_intermediate/ggplot2_intermediate.html" class="sidebar-item-text sidebar-link">4.2: Intermediate Data Visualization with ggplot2</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>5 Statistics</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/01_statistics_beginner/01_statistics_beginner.html" class="sidebar-item-text sidebar-link">5.1: Introduction to Statistics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/02_foundations_of_probability_in_r/02_foundations_of_probability_in_r.html" class="sidebar-item-text sidebar-link">5.2: Foundations of Probability</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/03_regression_beginner/03_regression_beginner.html" class="sidebar-item-text sidebar-link">5.3: Introduction to Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/04_regression_intermediate/04_regression_intermediate.html" class="sidebar-item-text sidebar-link">5.4: Intermediate Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/06_modeling_with_data_in_the_tidyverse/06_modeling_with_data_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">5.5: Modeling with Data in the Tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/09_experimental_design/09_experimental_design.html" class="sidebar-item-text sidebar-link">5.6: Experimental Design</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/10_ab_testing_in_r/10_ab_testing_in_r.html" class="sidebar-item-text sidebar-link">5.7: A/B Testing</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/19_fundamentals_of_bayesian_data_analysis/19_fundamentals_of_bayesian_data_analysis.html" class="sidebar-item-text sidebar-link">5.8: Fundamentals of Bayesian Data Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/05_statistics/21_factor_analysis_in_r/21_factor_analysis_in_r.html" class="sidebar-item-text sidebar-link">5.9: Factor Analysis</a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <b><i>7 Machine Learning</i></b>
  </li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/01_supervised_learning_classification/01_supervised_learning_classification.html" class="sidebar-item-text sidebar-link">7.1: Supervised Learning: Classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/02_supervised_learning_regression/02_supervised_learning_regression.html" class="sidebar-item-text sidebar-link">7.2: Supervised Learning: Regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/03_unsupervised_learning/03_unsupervised_learning.html" class="sidebar-item-text sidebar-link">7.3: Unsupervised Learning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/04_machine_learning_in_the_tidyverse/04_machine_learning_in_the_tidyverse.html" class="sidebar-item-text sidebar-link">7.4: Machine Learning in the tidyverse</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/05_cluster_analysis/05_cluster_analysis.html" class="sidebar-item-text sidebar-link">7.5: Cluster Analysis</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/06_machine_learning_with_caret/06_machine_learning_with_caret.html" class="sidebar-item-text sidebar-link">7.6: Machine Learning with caret</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/07_modeling_with_tidymodels/07_modeling_with_tidymodels.html" class="sidebar-item-text sidebar-link">7.7: Modeling with tidymodels</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/08_machine_learning_with_tree-based_models/08_machine_learning_with_tree-based_models.html" class="sidebar-item-text sidebar-link">7.8: Machine Learning with tree-based Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/09_support_vector_machines/09_support_vector_machines.html" class="sidebar-item-text sidebar-link">7.9: Support Vector Machines</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" class="sidebar-item-text sidebar-link">7.10: Topic Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.html" class="sidebar-item-text sidebar-link active">7.11: Hyperparameter Tuning</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" class="sidebar-item-text sidebar-link">7.12: Bayesian Regression Modeling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/topics/07_machine_learning/13_spark_with_sparklyr_in_R/13_spark_with_sparklyr_in_R.html" class="sidebar-item-text sidebar-link">7.13: Introduction to Spark</a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">R Manuals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/test.html" class="sidebar-item-text sidebar-link">1: An Introduction to R</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/02-content.html" class="sidebar-item-text sidebar-link">2: R Data Import/Export</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/03-content.html" class="sidebar-item-text sidebar-link">3: R Installation and Administration</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/04-content.html" class="sidebar-item-text sidebar-link">4: Writing R Extensions</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/05-content.html" class="sidebar-item-text sidebar-link">5: R Language Definition</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../content/R/r-manuals/06-content.html" class="sidebar-item-text sidebar-link">6: R Internals</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul class="collapse">
  <li><a href="#introduction-to-hyperparameters" id="toc-introduction-to-hyperparameters" class="nav-link active" data-scroll-target="#introduction-to-hyperparameters"><span class="toc-section-number">1</span>  Introduction to hyperparameters</a>
  <ul class="collapse">
  <li><a href="#parameters-vs-hyperparameters" id="toc-parameters-vs-hyperparameters" class="nav-link" data-scroll-target="#parameters-vs-hyperparameters"><span class="toc-section-number">1.1</span>  Parameters vs hyperparameters</a></li>
  <li><a href="#model-parameters-vs.-hyperparameters" id="toc-model-parameters-vs.-hyperparameters" class="nav-link" data-scroll-target="#model-parameters-vs.-hyperparameters"><span class="toc-section-number">1.2</span>  Model parameters vs.&nbsp;hyperparameters</a></li>
  <li><a href="#hyperparameters-in-linear-models" id="toc-hyperparameters-in-linear-models" class="nav-link" data-scroll-target="#hyperparameters-in-linear-models"><span class="toc-section-number">1.3</span>  Hyperparameters in linear models</a></li>
  <li><a href="#what-are-the-coefficients" id="toc-what-are-the-coefficients" class="nav-link" data-scroll-target="#what-are-the-coefficients"><span class="toc-section-number">1.5</span>  What are the coefficients?</a></li>
  <li><a href="#recap-of-machine-learning-basics" id="toc-recap-of-machine-learning-basics" class="nav-link" data-scroll-target="#recap-of-machine-learning-basics"><span class="toc-section-number">1.6</span>  Recap of machine learning basics</a></li>
  <li><a href="#machine-learning-with-caret" id="toc-machine-learning-with-caret" class="nav-link" data-scroll-target="#machine-learning-with-caret"><span class="toc-section-number">1.7</span>  Machine learning with caret</a></li>
  <li><a href="#resampling-schemes" id="toc-resampling-schemes" class="nav-link" data-scroll-target="#resampling-schemes"><span class="toc-section-number">1.8</span>  Resampling schemes</a></li>
  <li><a href="#hyperparameter-tuning-in-caret" id="toc-hyperparameter-tuning-in-caret" class="nav-link" data-scroll-target="#hyperparameter-tuning-in-caret"><span class="toc-section-number">1.10</span>  Hyperparameter tuning in caret</a></li>
  <li><a href="#hyperparameters-in-stochastic-gradient-boosting" id="toc-hyperparameters-in-stochastic-gradient-boosting" class="nav-link" data-scroll-target="#hyperparameters-in-stochastic-gradient-boosting"><span class="toc-section-number">1.11</span>  Hyperparameters in Stochastic Gradient Boosting</a></li>
  <li><a href="#changing-the-number-of-hyperparameters-to-tune" id="toc-changing-the-number-of-hyperparameters-to-tune" class="nav-link" data-scroll-target="#changing-the-number-of-hyperparameters-to-tune"><span class="toc-section-number">1.13</span>  Changing the number of hyperparameters to tune</a></li>
  <li><a href="#tune-hyperparameters-manually" id="toc-tune-hyperparameters-manually" class="nav-link" data-scroll-target="#tune-hyperparameters-manually"><span class="toc-section-number">1.14</span>  Tune hyperparameters manually</a></li>
  </ul></li>
  <li><a href="#hyperparameter-tuning-with-caret" id="toc-hyperparameter-tuning-with-caret" class="nav-link" data-scroll-target="#hyperparameter-tuning-with-caret"><span class="toc-section-number">2</span>  2. Hyperparameter tuning with caret</a>
  <ul class="collapse">
  <li><a href="#hyperparameter-tuning-in-caret-1" id="toc-hyperparameter-tuning-in-caret-1" class="nav-link" data-scroll-target="#hyperparameter-tuning-in-caret-1"><span class="toc-section-number">2.1</span>  Hyperparameter tuning in caret</a></li>
  <li><a href="#finding-hyperparameters" id="toc-finding-hyperparameters" class="nav-link" data-scroll-target="#finding-hyperparameters"><span class="toc-section-number">2.2</span>  Finding hyperparameters</a></li>
  <li><a href="#cartesian-grid-search-in-caret" id="toc-cartesian-grid-search-in-caret" class="nav-link" data-scroll-target="#cartesian-grid-search-in-caret"><span class="toc-section-number">2.4</span>  Cartesian grid search in caret</a></li>
  <li><a href="#plot-hyperparameter-model-output" id="toc-plot-hyperparameter-model-output" class="nav-link" data-scroll-target="#plot-hyperparameter-model-output"><span class="toc-section-number">2.6</span>  Plot hyperparameter model output</a></li>
  <li><a href="#grid-vs.-random-search" id="toc-grid-vs.-random-search" class="nav-link" data-scroll-target="#grid-vs.-random-search"><span class="toc-section-number">2.7</span>  Grid vs.&nbsp;Random Search</a></li>
  <li><a href="#grid-search-with-range-of-hyperparameters" id="toc-grid-search-with-range-of-hyperparameters" class="nav-link" data-scroll-target="#grid-search-with-range-of-hyperparameters"><span class="toc-section-number">2.8</span>  Grid search with range of hyperparameters</a></li>
  <li><a href="#find-train-option-for-random-search" id="toc-find-train-option-for-random-search" class="nav-link" data-scroll-target="#find-train-option-for-random-search"><span class="toc-section-number">2.9</span>  Find train() option for random search</a></li>
  <li><a href="#random-search-with-caret" id="toc-random-search-with-caret" class="nav-link" data-scroll-target="#random-search-with-caret"><span class="toc-section-number">2.11</span>  Random search with caret</a></li>
  <li><a href="#adaptive-resampling" id="toc-adaptive-resampling" class="nav-link" data-scroll-target="#adaptive-resampling"><span class="toc-section-number">2.13</span>  Adaptive resampling</a></li>
  <li><a href="#advantages-of-adaptive-resampling" id="toc-advantages-of-adaptive-resampling" class="nav-link" data-scroll-target="#advantages-of-adaptive-resampling"><span class="toc-section-number">2.14</span>  Advantages of Adaptive Resampling</a></li>
  <li><a href="#adaptive-resampling-with-caret" id="toc-adaptive-resampling-with-caret" class="nav-link" data-scroll-target="#adaptive-resampling-with-caret"><span class="toc-section-number">2.16</span>  Adaptive Resampling with caret</a></li>
  </ul></li>
  <li><a href="#hyperparameter-tuning-with-mlr" id="toc-hyperparameter-tuning-with-mlr" class="nav-link" data-scroll-target="#hyperparameter-tuning-with-mlr"><span class="toc-section-number">3</span>  3. Hyperparameter tuning with mlr</a>
  <ul class="collapse">
  <li><a href="#machine-learning-with-mlr" id="toc-machine-learning-with-mlr" class="nav-link" data-scroll-target="#machine-learning-with-mlr"><span class="toc-section-number">3.1</span>  Machine learning with mlr</a></li>
  <li><a href="#machine-learning-with-mlr-1" id="toc-machine-learning-with-mlr-1" class="nav-link" data-scroll-target="#machine-learning-with-mlr-1"><span class="toc-section-number">3.2</span>  Machine Learning with mlr</a></li>
  <li><a href="#modeling-with-mlr" id="toc-modeling-with-mlr" class="nav-link" data-scroll-target="#modeling-with-mlr"><span class="toc-section-number">3.4</span>  Modeling with mlr</a></li>
  <li><a href="#grid-and-random-search-with-mlr" id="toc-grid-and-random-search-with-mlr" class="nav-link" data-scroll-target="#grid-and-random-search-with-mlr"><span class="toc-section-number">3.5</span>  Grid and random search with mlr</a></li>
  <li><a href="#random-search-with-mlr" id="toc-random-search-with-mlr" class="nav-link" data-scroll-target="#random-search-with-mlr"><span class="toc-section-number">3.6</span>  Random search with mlr</a></li>
  <li><a href="#perform-hyperparameter-tuning-with-mlr" id="toc-perform-hyperparameter-tuning-with-mlr" class="nav-link" data-scroll-target="#perform-hyperparameter-tuning-with-mlr"><span class="toc-section-number">3.7</span>  Perform hyperparameter tuning with mlr</a></li>
  <li><a href="#evaluating-hyperparameters-with-mlr" id="toc-evaluating-hyperparameters-with-mlr" class="nav-link" data-scroll-target="#evaluating-hyperparameters-with-mlr"><span class="toc-section-number">3.9</span>  Evaluating hyperparameters with mlr</a></li>
  <li><a href="#why-to-evaluate-tuning" id="toc-why-to-evaluate-tuning" class="nav-link" data-scroll-target="#why-to-evaluate-tuning"><span class="toc-section-number">3.10</span>  Why to evaluate tuning?</a></li>
  <li><a href="#evaluating-hyperparameter-tuning-results" id="toc-evaluating-hyperparameter-tuning-results" class="nav-link" data-scroll-target="#evaluating-hyperparameter-tuning-results"><span class="toc-section-number">3.12</span>  Evaluating hyperparameter tuning results</a></li>
  <li><a href="#advanced-tuning-with-mlr" id="toc-advanced-tuning-with-mlr" class="nav-link" data-scroll-target="#advanced-tuning-with-mlr"><span class="toc-section-number">3.13</span>  Advanced tuning with mlr</a></li>
  <li><a href="#define-advanced-tuning-controls" id="toc-define-advanced-tuning-controls" class="nav-link" data-scroll-target="#define-advanced-tuning-controls"><span class="toc-section-number">3.14</span>  Define advanced tuning controls</a></li>
  <li><a href="#define-aggregated-measures" id="toc-define-aggregated-measures" class="nav-link" data-scroll-target="#define-aggregated-measures"><span class="toc-section-number">3.16</span>  Define aggregated measures</a></li>
  <li><a href="#setting-hyperparameters" id="toc-setting-hyperparameters" class="nav-link" data-scroll-target="#setting-hyperparameters"><span class="toc-section-number">3.17</span>  Setting hyperparameters</a></li>
  </ul></li>
  <li><a href="#hyperparameter-tuning-with-h2o" id="toc-hyperparameter-tuning-with-h2o" class="nav-link" data-scroll-target="#hyperparameter-tuning-with-h2o"><span class="toc-section-number">4</span>  4. Hyperparameter tuning with h2o</a>
  <ul class="collapse">
  <li><a href="#machine-learning-with-h2o" id="toc-machine-learning-with-h2o" class="nav-link" data-scroll-target="#machine-learning-with-h2o"><span class="toc-section-number">4.1</span>  Machine learning with h2o</a></li>
  <li><a href="#prepare-data-for-modelling-with-h2o" id="toc-prepare-data-for-modelling-with-h2o" class="nav-link" data-scroll-target="#prepare-data-for-modelling-with-h2o"><span class="toc-section-number">4.2</span>  Prepare data for modelling with h2o</a></li>
  <li><a href="#modeling-with-h2o" id="toc-modeling-with-h2o" class="nav-link" data-scroll-target="#modeling-with-h2o"><span class="toc-section-number">4.3</span>  Modeling with h2o</a></li>
  <li><a href="#grid-and-random-search-with-h2o" id="toc-grid-and-random-search-with-h2o" class="nav-link" data-scroll-target="#grid-and-random-search-with-h2o"><span class="toc-section-number">4.4</span>  Grid and random search with h2o</a></li>
  <li><a href="#grid-search-with-h2o" id="toc-grid-search-with-h2o" class="nav-link" data-scroll-target="#grid-search-with-h2o"><span class="toc-section-number">4.5</span>  Grid search with h2o</a></li>
  <li><a href="#random-search-with-h2o" id="toc-random-search-with-h2o" class="nav-link" data-scroll-target="#random-search-with-h2o"><span class="toc-section-number">4.7</span>  Random search with h2o</a></li>
  <li><a href="#stopping-criteria" id="toc-stopping-criteria" class="nav-link" data-scroll-target="#stopping-criteria"><span class="toc-section-number">4.8</span>  Stopping criteria</a></li>
  <li><a href="#automatic-machine-learning-with-h2o" id="toc-automatic-machine-learning-with-h2o" class="nav-link" data-scroll-target="#automatic-machine-learning-with-h2o"><span class="toc-section-number">4.10</span>  Automatic machine learning with H2O</a></li>
  <li><a href="#automl-in-h2o" id="toc-automl-in-h2o" class="nav-link" data-scroll-target="#automl-in-h2o"><span class="toc-section-number">4.11</span>  AutoML in h2o</a></li>
  <li><a href="#scoring-the-leaderboard" id="toc-scoring-the-leaderboard" class="nav-link" data-scroll-target="#scoring-the-leaderboard"><span class="toc-section-number">4.12</span>  Scoring the leaderboard</a></li>
  <li><a href="#extract-h2o-models-and-evaluate-performance" id="toc-extract-h2o-models-and-evaluate-performance" class="nav-link" data-scroll-target="#extract-h2o-models-and-evaluate-performance"><span class="toc-section-number">4.13</span>  Extract h2o models and evaluate performance</a></li>
  <li><a href="#wrap-up" id="toc-wrap-up" class="nav-link" data-scroll-target="#wrap-up"><span class="toc-section-number">4.16</span>  Wrap-up</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jwarz/jwarz.github.io/edit/main/content/R/topics/07_machine_learning/11_hyperparameter_tuning/11_hyperparameter_tuning.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jwarz/jwarz.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Hyperparameter Tuning in R</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Joschka Schwarz </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>For many machine learning problems, simply running a model out-of-the-box and getting a prediction is not enough; you want the best model with the most accurate prediction. One way to perfect your model is with hyperparameter tuning, which means optimizing the settings for that specific model. In this course, you will work with the caret, mlr and h2o packages to find the optimal combination of hyperparameters in an efficient manner using grid search, random search, adaptive resampling and automatic machine learning (AutoML). Furthermore, you will work with different datasets and tune different supervised learning models, such as random forests, gradient boosting machines, support vector machines, and even neural nets. Get ready to tune!</p>
<section id="introduction-to-hyperparameters" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction to hyperparameters</h1>
<p>Why do we use the strange word “hyperparameter”? What makes it hyper? Here, you will understand what model parameters are, and why they are different from hyperparameters in machine learning. You will then see why we would want to tune them and how the default setting of caret automatically includes hyperparameter tuning.</p>
<section id="parameters-vs-hyperparameters" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="parameters-vs-hyperparameters"><span class="header-section-number">1.1</span> Parameters vs hyperparameters</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Parameters vs hyperparameters</strong></p>
<p>Welcome to this course on hyperparameter tuning in R.In this course you will learn:- what hyperparameters are and what makes them different from regular parameters- why hyperparameter tuning is an important step towards optimizing your machine learning models- and how you can apply hyperparameter tuning with the packages caret, mlr and h2o.</p>
<p><strong>2. About me</strong></p>
<p>My name is Shirin and I started out as a traditional biologist. I spent a lot of time in the lab. But eventually, it became clear that what I really enjoyed above all else was working with data. That’s why I spent two years as a bioinformatics Postdoc at the University of Münster in Germany before I started working as a Data Scientist for codecentric. I also write a Data Science blog where I play around with different datasets, analyses, and visualization techniques.</p>
<p><strong>3. “Hyper”parameters vs model parameters</strong></p>
<p>So, why do we use the strange word “hyper-parameter”? And how are hyperparameters different from model parameters?In this chapter, we will work with a dataset about breast cancer patient samples. 10 features describe the diagnosis of benign or malignant tissue masses. Here, we use them to build a classification model.Let’s have a look at a simple linear model.</p>
<p><strong>4. Let’s start simple: Model parameters in a linear model</strong></p>
<p>A linear model models the relationship between variables by fitting a linear function. Here, we will pick two features at random: perimeter_worst &amp; fractal_dimension_mean and look at their linear relationship. We could, of course, make our linear model much more complex by adding additional features and more complex interactions, but for this purpose, we will keep it simple.The summary function will give us an overview of the fitted linear model and its results, like residuals, coefficients, and statistics.</p>
<p><strong>5. Let’s start simple: Model parameters in a linear model</strong></p>
<p>The results of our fitted linear model give the model parameters.Thus, model <strong>parameters</strong> are the <strong>result</strong> of model fitting.In machine learning, we use the word training instead of model fitting, so we can say that the model parameters are being fit (or found) during training.Let’s look again at our linear model: here we want to find the coefficients, which we can think of as the slope and intercept of our model.</p>
<p><strong>6. Coefficients in a linear model</strong></p>
<p>Slope and intercept are best understood when visualized; they describe the best line through our data points.slope describes the steepness of this line, whileintercept describes the point where our line crosses the y-axis.</p>
<p><strong>7. Model parameters vs hyperparameters in a linear model</strong></p>
<p>Okay, you now know what model parameters are. But what about hyperparameters?Hyperparameters are defined before training, they specify HOW the training is supposed to happen, this means they define options in function calls:We can find out which options to define by looking at the arguments or formals of a function or by going to its help page.In our linear model, <code>method</code> is a hyperparameter.</p>
<p><strong>8. Parameters vs hyperparameters in machine learning</strong></p>
<p>So, to recap:Model parameters were found during training, like coefficients. In machine learning these could be the weights of a neural network.Hyperparameters were defined before training; these could be the learning rate in a neural net or the number of trees in a random forest.</p>
<p><strong>9. Why tune hyperparameters?</strong></p>
<p>But why would we want to tune hyperparameters?Imagine we are creating a fantasy football team: we want to find the best combination of players to maximize our chances of winning.In machine learning, we have hyperparameters - which we could think of like fantasy football players; each hyperparameter can take a range of values - just as players can be assigned different positions on the field. Just like with our fantasy football team, we want to find the best combination of hyperparameters, so that our model performs as well as possible.</p>
<p><strong>10. Let’s practice!</strong></p>
<p>Time to put this into practice.</p>
</section>
<section id="model-parameters-vs.-hyperparameters" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="model-parameters-vs.-hyperparameters"><span class="header-section-number">1.2</span> Model parameters vs.&nbsp;hyperparameters</h2>
<!--
LO: The student knows how to build a linear model and how to extract coefficients
-->
<p>In order to perform hyperparameter tuning, it is important to really understand what hyperparameters are (and what they are not). So let’s look at <strong>model parameters versus hyperparameters</strong> in detail.</p>
<p>Note: The Breast Cancer Wisconsin (Diagnostic) dataset has been loaded as <code>breast_cancer_data</code> for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use this dataset to fit a <strong>linear model</strong> with <code>concavity_mean</code> as response and <code>symmetry_mean</code> as predictor variable.</li>
<li>Look at the <code>summary()</code> of this linear model.</li>
<li>Extract the <strong>coefficients</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load data</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(readr)</span>
<span id="cb1-3"><a href="#cb1-3"></a>breast_cancer_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/breast_cancer_data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 100 Columns: 11
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr  (1): diagnosis
#&gt; dbl (10): concavity_mean, symmetry_mean, fractal_dimension_mean, perimeter_s...
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Fit a linear model on the breast_cancer_data.</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>linear_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(concavity_mean <span class="sc">~</span> symmetry_mean,</span>
<span id="cb3-3"><a href="#cb3-3"></a>                    <span class="at">data =</span> breast_cancer_data)</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co"># Look at the summary of the linear_model.</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="fu">summary</span>(linear_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = concavity_mean ~ symmetry_mean, data = breast_cancer_data)
#&gt; 
#&gt; Residuals:
#&gt;       Min        1Q    Median        3Q       Max 
#&gt; -0.201877 -0.039201 -0.008432  0.030655  0.226150 
#&gt; 
#&gt; Coefficients:
#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   -0.15311    0.04086  -3.747 0.000303 ***
#&gt; symmetry_mean  1.33366    0.21257   6.274 9.57e-09 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 0.06412 on 98 degrees of freedom
#&gt; Multiple R-squared:  0.2866, Adjusted R-squared:  0.2793 
#&gt; F-statistic: 39.36 on 1 and 98 DF,  p-value: 9.575e-09</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Extract the coefficients.</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>linear_model<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;   (Intercept) symmetry_mean 
#&gt;    -0.1531055     1.3336568</code></pre>
</div>
</div>
<p>Good job! You know how to build a linear model and how to extract coefficients.</p>
</section>
<section id="hyperparameters-in-linear-models" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="hyperparameters-in-linear-models"><span class="header-section-number">1.3</span> Hyperparameters in linear models</h2>
<!--
LO: The student knows the difference between model parameters and hyperparameters
-->
<p>Note that hyperparameters can be found in the <strong>help section</strong> for a function, while model parameters are part of the output of a function.</p>
<blockquote class="blockquote">
<h2 id="question" data-number="1.4" class="anchored"><span class="header-section-number">1.4</span> <em>Question</em></h2>
<p>Which of the following is a <strong>hyperparameter</strong> in the linear model from your last exercise?<br> <br> ✅ Weights<br> ⬜ Coefficients<br> ⬜ Residuals<br> ⬜ Intercept<br></p>
</blockquote>
<p>Correct! In the ‘Arguments’ section of the help function for <code>lm</code> we learn that weights are an optional vector to be used in the fitting process.</p>
</section>
<section id="what-are-the-coefficients" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="what-are-the-coefficients"><span class="header-section-number">1.5</span> What are the coefficients?</h2>
<!--
LO: The student knows the difference between model parameters and hyperparameters
-->
<p>To get a good feel for the difference between fitted model parameters and hyperparameters, we are going to take a closer look at those fitted parameters: in our simple linear model, the <strong>coefficients</strong>. The dataset <code>breast_cancer_data</code> has already been loaded for you and the linear model call was run as in the previous lesson, so you can directly access the object <code>linear_model</code>.</p>
<p>In our linear model, we can extract the coefficients in the following way: <code>linear_model$coefficients</code>. And we can <strong>visualize the relationship</strong> we modeled with a plot.</p>
<p><strong>Remember</strong>, that a linear model has the basic formula: <code>y = x * slope + intercept</code></p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Explore the coefficients of the <code>linear_model</code> in the console.</li>
<li>Plot the regression line with <code>ggplot2</code>.</li>
<li>Assign the correct coefficients to <code>slope</code> and <code>intercept</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="co"># Plot linear relationship.</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="fu">ggplot</span>(<span class="at">data =</span> breast_cancer_data, </span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="fu">aes</span>(<span class="at">x =</span> symmetry_mean, <span class="at">y =</span> concavity_mean)) <span class="sc">+</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">"grey"</span>) <span class="sc">+</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> linear_model<span class="sc">$</span>coefficients[<span class="dv">2</span>], </span>
<span id="cb7-8"><a href="#cb7-8"></a>              <span class="at">intercept =</span> linear_model<span class="sc">$</span>coefficients[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-3-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Perfect! You understand that coefficients represent the slope and intercept of the fitted model formula.</p>
</section>
<section id="recap-of-machine-learning-basics" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="recap-of-machine-learning-basics"><span class="header-section-number">1.6</span> Recap of machine learning basics</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Recap of machine learning basics</strong></p>
<p>Good job, you now have a solid understanding of the differences between model parameters and hyperparameters. But before we jump into actively tuning the hyperparameters, we will briefly recap the basics of machine learning in R.We will use the caret package first, because it automatically performs a basic hyperparameter tuning for you with every training run.</p>
<p><strong>2. Machine learning with caret - splitting data</strong></p>
<p>But before we look into this automatic hyperparameter tuning in caret, we need to prepare our data for training.First, we will divide our data into training and test sets.Caret makes this step easy with the <code>createDataPartition</code> function. It lets us give a class label vector as input for stratified partitioning of the data; this is important because we want to have a roughly equal ratio of classes in our training and test set. With the argument <code>p</code> we tell the function what proportion of the data should go into the training set, here 70%. The index that will be created can then be used for subsetting the original dataset.How much of the data you want to keep for training can be part of the optimization process. There are really no strict rules on how to split the data but you want to make sure that you have enough training powerand that you have a representative test set. With a small dataset such as this, 70% is a common number, but you will also often see 80 or 90% training data.</p>
<p><strong>3. Train a machine learning model with caret</strong></p>
<p>Here, I will not go into additional steps of the machine learning workflow, like feature engineering, preprocessing, normalization, balancing classes, etc. Just keep in mind that in a real-world scenario, you would at least want to think about incorporating these steps into your workflow.Our validation scheme is defined in the <code>trainControl()</code> function: we will do 5 times 3 repeated cross-validation, which means repeating 3-fold cross-validation 5 times. This scheme is then given as an argument in the <code>train()</code> function.In <code>caret</code> we can train machine learning models with a large number of different algorithms; we define this with the argument <code>method</code> in the <code>train()</code> function. Here, we will train a Random Forest model, which is abbreviated <code>rf</code>.<code>train()</code> also wants to know which data and which features to use. Our dataset is the training set that we created before. The features are given with a formula: the class or response variable (here <strong>diagnosis</strong>) is written before and features after the tilde. For features, we write a dot here, which indicates that we want to use all remaining columns as features in our model.In addition, I want to know how long my model took to train. For this, I am using the tictoc package, which will return the runtime between tic and toc.As we can see, our model took about 1.4 seconds to train.</p>
<p><strong>4. Automatic hyperparameter tuning in caret</strong></p>
<p>Here is the random forest model we just trained. In the output we can already see hyperparameter tuning in action as caret performs it automatically with different options for the hyperparameter mtry –&gt; you will learn more about that in the next lesson!What’s important to note here is that caret compares different hyperparameters on the training and validation data only.Do NOT be tempted to measure your model performance on the test data during hyperparameter tuning as that would give you an overly optimistic and biased performance evaluation!</p>
<p><strong>5. Let’s start modeling!</strong></p>
<p>Now, it’s your turn to start modeling!</p>
</section>
<section id="machine-learning-with-caret" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="machine-learning-with-caret"><span class="header-section-number">1.7</span> Machine learning with caret</h2>
<!--
LO: The student knows how to split data into training and test sets for modeling
-->
<!--
LO: The student knows how to use cross-validation in caret
-->
<!--
LO: The student knows how to train a machine learning model with caret
-->
<p>Before we can train machine learning models and tune hyperparameters, we need to <strong>prepare the data</strong>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the <code>caret</code> package to create an index with 70% of the <code>breast_cancer_data</code> to create a <strong>training set</strong> and <strong>stratify the partitions</strong> by the response variable <code>diagnosis</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Load package</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">library</span>(caret)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Lade nötiges Paket: lattice</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Create partition index</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(breast_cancer_data<span class="sc">$</span>diagnosis, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Use the index you created in the previous step to <strong>partition</strong> the <code>breast_cancer_data</code> in training and test sets.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Subset `breast_cancer_data` with index</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>bc_train_data <span class="ot">&lt;-</span> breast_cancer_data[index, ]</span>
<span id="cb11-3"><a href="#cb11-3"></a>bc_test_data  <span class="ot">&lt;-</span> breast_cancer_data[<span class="sc">-</span>index, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Define a <strong>repeated cross-validation</strong> scheme for <code>caret</code> with 5 folds and 3 repeats.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Define 3x5 folds repeated cross-validation</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Use the <code>caret</code> package to <strong>train a Stochastic Gradient Boosting model</strong> and add the <strong>repeated cross-validation</strong> scheme that you defined in the last step to the <code>train</code> function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Required package: gbm</span></span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="co"># Run the train() function</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>gbm_model <span class="ot">&lt;-</span> <span class="fu">train</span>(diagnosis <span class="sc">~</span> ., </span>
<span id="cb13-5"><a href="#cb13-5"></a>                   <span class="at">data      =</span> bc_train_data, </span>
<span id="cb13-6"><a href="#cb13-6"></a>                   <span class="at">method    =</span> <span class="st">"gbm"</span>, </span>
<span id="cb13-7"><a href="#cb13-7"></a>                   <span class="at">trControl =</span> fitControl,</span>
<span id="cb13-8"><a href="#cb13-8"></a>                   <span class="at">verbose   =</span> <span class="cn">FALSE</span>)</span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co"># Look at the model</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>gbm_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Stochastic Gradient Boosting 
#&gt; 
#&gt; 80 samples
#&gt; 10 predictors
#&gt;  2 classes: 'B', 'M' 
#&gt; 
#&gt; No pre-processing
#&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) 
#&gt; Summary of sample sizes: 64, 64, 64, 64, 64, 64, ... 
#&gt; Resampling results across tuning parameters:
#&gt; 
#&gt;   interaction.depth  n.trees  Accuracy   Kappa    
#&gt;   1                   50      0.9208333  0.8416667
#&gt;   1                  100      0.9166667  0.8333333
#&gt;   1                  150      0.9166667  0.8333333
#&gt;   2                   50      0.9166667  0.8333333
#&gt;   2                  100      0.9083333  0.8166667
#&gt;   2                  150      0.9083333  0.8166667
#&gt;   3                   50      0.9083333  0.8166667
#&gt;   3                  100      0.9041667  0.8083333
#&gt;   3                  150      0.9041667  0.8083333
#&gt; 
#&gt; Tuning parameter 'shrinkage' was held constant at a value of 0.1
#&gt; 
#&gt; Tuning parameter 'n.minobsinnode' was held constant at a value of 10
#&gt; Accuracy was used to select the optimal model using the largest value.
#&gt; The final values used for the model were n.trees = 50, interaction.depth =
#&gt;  1, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
</div>
</div>
<p>Very good! You know the basics of building models with caret.</p>
</section>
<section id="resampling-schemes" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="resampling-schemes"><span class="header-section-number">1.8</span> Resampling schemes</h2>
<p>In the previous exercise, you defined a 3x5 folds repeated cross-validation resampling scheme with the following code:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<h2 id="question-1" data-number="1.9" class="anchored"><span class="header-section-number">1.9</span> <em>Question</em></h2>
<p>Which of the following is <strong>NOT</strong> a valid resampling method in <code>caret</code>?<br> <br> ⬜ <code>boot</code><br> ✅ <code>adaboost</code><br> ⬜ <code>cv</code><br> ⬜ <code>LGOCV</code><br></p>
</blockquote>
<p>Correct! <code>adaboost</code> is an implementation of the AdaBoost optimization algorithm from Freund and Shapire (1997) and not a resampling scheme.</p>
</section>
<section id="hyperparameter-tuning-in-caret" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="hyperparameter-tuning-in-caret"><span class="header-section-number">1.10</span> Hyperparameter tuning in caret</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Hyperparameter tuning with caret</strong></p>
<p>Caret makes hyperparameter tuning very easy. By default, it performs automatic tuning for you with every training run. But you can also manually define how you want to tune your models.</p>
<p><strong>2. Automatic hyperparameter tuning in caret</strong></p>
<p>Here you see the random forest model with the <code>rf</code> method from before. In the output we see that we only have one hyperparameter to tune:<code>mtry</code> defines the number of variables that are randomly sampled as candidates at each split. caret automatically tried three different <code>mtry</code> values and includes the performance of each with the output. The best model is chosen with the metric <code>accuracy</code>, which in this case was for mtry = 6.</p>
<p><strong>3. Hyperparameters are specific to model algorithms</strong></p>
<p>Different algorithms have different hyperparameters. You might be wondering how you would know which hyperparameters you can tune with these different <code>method</code> in caret.If you know the model abbreviation, you can use the modellookup function.But the easiest way is to use the online documentation for caret. Click this link on the slides to go to the page.There, you will find an overview of the different algorithms you can set as <code>method</code> in the train function. This table includes the name of the model, the string you need to put into the train function, whether it can be used for classification or regression and what the original R package of the implementation is.But most importantly, you will find which hyperparameters can be tuned. Here, I will not discuss the mathematics behind hyperparameters, instead, I will focus on HOW to perform the hyperparameter tuning.</p>
<p><strong>4. Hyperparameters in Support Vector Machines (SVM)</strong></p>
<p>Let’s change things up a bit and build a Support Vector Machine with Polynomial Kernel similar to the Random Forest model from before: this time I am using the <code>svmPoly</code> model. And I am again calculating the training time.</p>
<p><strong>5. Hyperparameters in Support Vector Machines (SVM)</strong></p>
<p>When we examine the model object again, we see that this time <code>caret</code> performed a more complex hyperparameter tuning. If we have more than one hyperparameter to tune, <code>train</code> automatically creates a grid of tuning parameters.By default, caret tries all possible combinations of three hyperparameters, in our model:- degree being 1, 2, or 3- scale being 0.001, 0.01 or 0.1- and c being 0.25, 0.5 or 1Because the output shows the performance for every possible combination of hyperparameters, the output is too long to fit on this slide and I am only showing the best model with degree of 1, scale of 0.1 and c equal to 1.</p>
<p><strong>6. Defining hyperparameters for automatic tuning</strong></p>
<p>We can also set the option <code>tuneLength</code> to specify the number of different values to try for each hyperparameter, for example 5. Now, caret tries all possible combinations of five hyperparameters:- degree being 1, 2, 3, 4 or 5- scale being 1e-03, 1e-02, 1e-01, 1e+00 and 1e+01- and c being 0.25, 0.5, 1, 2 or 4The best model now has degree, scale and c of 1.</p>
<p><strong>7. Manual hyperparameter tuning in caret</strong></p>
<p>Of course, you could also manually try out different hyperparameters. This, we can do with the option <code>tuneGrid()</code>, to which we can feed a grid of hyperparameters. This grid is defined with the <code>expand.grid()</code> function.If we use that function, we need to define all hyperparameters. Let’s see what happens if we set the degree to 4 and keep scale and c at 1 and retrain the model.</p>
<p><strong>8. Manual hyperparameter tuning in caret</strong></p>
<p>This time, we only trained with one combination of hyperparameters, so our output gives the performance for these hyperparameters only!</p>
<p><strong>9. It’s your turn!</strong></p>
<p>Now it’s your turn to apply simple hyperparameter tuning in caret.</p>
</section>
<section id="hyperparameters-in-stochastic-gradient-boosting" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="hyperparameters-in-stochastic-gradient-boosting"><span class="header-section-number">1.11</span> Hyperparameters in Stochastic Gradient Boosting</h2>
<!--
LO: The student knows how to find out which hyperparameters can be tuned in a given caret method
-->
<p>In the previous lesson, you built a Stochastic Gradient Boosting model in caret. A similar model as the one from before has been preloaded as <code>gbm_model</code>.</p>
<blockquote class="blockquote">
<h2 id="question-2" data-number="1.12" class="anchored"><span class="header-section-number">1.12</span> <em>Question</em></h2>
<p>In order to optimize this model, you want to <strong>tune its hyperparameters</strong>. Which of the following is NOT a hyperparameter of the <code>gbm</code> method?<br> <br> ⬜ n.trees<br> ⬜ n.minobsinnode<br> ✅ na.action<br> ⬜ interaction.depth<br></p>
</blockquote>
<p>Correct! <code>na.action</code> is not a hyperparameter; it is a function to specify the action to be taken if NAs are found.</p>
</section>
<section id="changing-the-number-of-hyperparameters-to-tune" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="changing-the-number-of-hyperparameters-to-tune"><span class="header-section-number">1.13</span> Changing the number of hyperparameters to tune</h2>
<!--
LO: The student knows how to change the number of hyperparameters that are tuned automatically by `caret`
-->
<p>When we examine the model object closely, we can see that <code>caret</code> already did some <strong>automatic hyperparameter</strong> tuning for us: <code>train</code> automatically creates a <strong>grid of tuning parameters</strong>. By default, if <code>p</code> is the number of tuning parameters, the grid size is 3^p.&nbsp;But we can also <strong>specify the number</strong> of different values to try for each hyperparameter.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Test <strong>four</strong> different values for each hyperparameter with automatic tuning in <code>caret</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Load package</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="co"># Set seed.</span></span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="co"># Start timer.</span></span>
<span id="cb16-7"><a href="#cb16-7"></a><span class="fu">tic</span>()</span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="co"># Train model.</span></span>
<span id="cb16-9"><a href="#cb16-9"></a>gbm_model <span class="ot">&lt;-</span> <span class="fu">train</span>(diagnosis <span class="sc">~</span> ., </span>
<span id="cb16-10"><a href="#cb16-10"></a>                   <span class="at">data       =</span> bc_train_data, </span>
<span id="cb16-11"><a href="#cb16-11"></a>                   <span class="at">method     =</span> <span class="st">"gbm"</span>, </span>
<span id="cb16-12"><a href="#cb16-12"></a>                   <span class="at">trControl  =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">3</span>),</span>
<span id="cb16-13"><a href="#cb16-13"></a>                   <span class="at">verbose    =</span> <span class="cn">FALSE</span>,</span>
<span id="cb16-14"><a href="#cb16-14"></a>                   <span class="at">tuneLength =</span> <span class="dv">4</span>)</span>
<span id="cb16-15"><a href="#cb16-15"></a><span class="co"># Stop timer.</span></span>
<span id="cb16-16"><a href="#cb16-16"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 0.705 sec elapsed</code></pre>
</div>
</div>
<p>Great! You can now perform a simple hyperparameter-tuning with caret.</p>
</section>
<section id="tune-hyperparameters-manually" class="level2" data-number="1.14">
<h2 data-number="1.14" class="anchored" data-anchor-id="tune-hyperparameters-manually"><span class="header-section-number">1.14</span> Tune hyperparameters manually</h2>
<!--
LO: The student knows how to manually define hyperparameters in caret
-->
<p>If you already know which hyperparameter values you want to set, you can also <strong>manually define</strong> hyperparameters as a <strong>grid</strong>. Go to <code>modelLookup("gbm")</code> or search for <code>gbm</code> in the <a href="https://topepo.github.io/caret/available-models.html">list of available models in caret</a> and check under <strong>Tuning Parameters</strong>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define the following <strong>hyperparameter grid</strong> for a Gradient Boosting Model: the number of trees as 200; the tree complexity as 1; the learning rate as 0.1 and the minimum number of training set samples in a node to commence splitting as 10.</li>
<li>Apply the grid to the <code>train()</code> function of <code>caret</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Define hyperparameter grid.</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>hyperparams <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">n.trees           =</span> <span class="dv">200</span>, </span>
<span id="cb18-3"><a href="#cb18-3"></a>                           <span class="at">interaction.depth =</span> <span class="dv">1</span>, </span>
<span id="cb18-4"><a href="#cb18-4"></a>                           <span class="at">shrinkage         =</span> <span class="fl">0.1</span>, </span>
<span id="cb18-5"><a href="#cb18-5"></a>                           <span class="at">n.minobsinnode    =</span> <span class="dv">10</span>)</span>
<span id="cb18-6"><a href="#cb18-6"></a></span>
<span id="cb18-7"><a href="#cb18-7"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb18-8"><a href="#cb18-8"></a><span class="co"># Apply hyperparameter grid to train().</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>gbm_model <span class="ot">&lt;-</span> <span class="fu">train</span>(diagnosis <span class="sc">~</span> ., </span>
<span id="cb18-10"><a href="#cb18-10"></a>                   <span class="at">data      =</span> bc_train_data, </span>
<span id="cb18-11"><a href="#cb18-11"></a>                   <span class="at">method    =</span> <span class="st">"gbm"</span>, </span>
<span id="cb18-12"><a href="#cb18-12"></a>                   <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">3</span>),</span>
<span id="cb18-13"><a href="#cb18-13"></a>                   <span class="at">verbose   =</span> <span class="cn">FALSE</span>,</span>
<span id="cb18-14"><a href="#cb18-14"></a>                   <span class="at">tuneGrid  =</span> hyperparams)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great job! You have made it through the first chapter. Now, you will learn how to use more advanced methods to optimize hyperparameters.</p>
</section>
</section>
<section id="hyperparameter-tuning-with-caret" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 2. Hyperparameter tuning with caret</h1>
<p>In this chapter, you will learn how to tune hyperparameters with a Cartesian grid. Then, you will implement faster and more efficient approaches. You will use Random Search and adaptive resampling to tune the parameter grid, in a way that concentrates on values in the neighborhood of the optimal settings.</p>
<section id="hyperparameter-tuning-in-caret-1" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="hyperparameter-tuning-in-caret-1"><span class="header-section-number">2.1</span> Hyperparameter tuning in caret</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Hyperparameter tuning in caret</strong></p>
<p>Welcome back to Chapter 2. Let’s dive deeper into how to perform hyperparameter tuning with caret.</p>
<p><strong>2. Voter dataset from US 2016 election</strong></p>
<p>The dataset we’ll be working with is from a survey about the 2016 US presidential election. We will use the attribute “turnout16_2016” to predict whether or not a person voted in that election.</p>
<p><strong>3. Let’s train another model with caret</strong></p>
<p>Let’s train another machine learning model with caret using gradient boosting with repeated cross-validation. Keep in mind that in reality, you would want to address the problem of having unbalanced classes but let’s focus on hyperparameter tuning for now. Our model takes about 33 seconds to run.</p>
<p><strong>4. Let’s train another model with caret</strong></p>
<p>When we explore our model, we see that caret tuned interaction depth and the number of trees based on default values. For a quick base-line model, this is fine but what if we wanted to manually define hyperparameters?</p>
<p><strong>5. Cartesian grid search with caret</strong></p>
<p>In the previous chapter, you used the <code>expand.grid()</code> function to manually define single values for every hyperparameter. The same function can be used to define a grid of hyper-parameters because it creates a grid of all possible combinations of hyperparameters given!For the number of trees and tree complexity, we’ll compare different values. Shrinkage and the minimum number of observations per node is kept constant.We can now train our model just as before. But this time, we will use the tuneGrid parameter and feed our grid to it.If we perform Cartesian grid search, every combination of hyperparameters in our grid will be evaluated.You see that this model took much longer to train. You will see in the following examples that hyperparameter tuning takes some time and computational power - so be prepared to exercise some patience!</p>
<p><strong>6. Cartesian grid search with caret</strong></p>
<p>The output will look similar to before with automatic hyperparameter tuning. We again get a table with accuracy and kappa values for all tested combinations of hyperparameters and a final result written below this table.Our model performance did not improve compared to before but we only tested a small range of hyperparameter values. In your real-world projects, you would test a much larger range of values but here, we will focus on learning the concepts behind hyperparameter tuning techniques.</p>
<p><strong>7. Plot hyperparameter models</strong></p>
<p>We can also plot our hyperparameters with the plot function and define the metric and plot type we want to visualize. Per default, we will see accuracy and line plots. Every line represents a different hyperparameter for the maximum tree depth. The colors of the lines correspond to this as well. On the x-axis, we see the number of boosting iterations, which comes from the hyperparameter n.trees that we defined to be either 100, 200 or 250. And the y-axis shows the accuracy of the model given these hyperparameter combinations. Alternatively, we can plot the Kappa metrics and show them as a heatmap. Kappa is another metric used to evaluate the performance of classification models. It compares an Observed Accuracy with an Expected Accuracy. Kappa values need to be considered in the context of the problem but generally, we want to achieve high Kappa values. The Kappa values are shown on the color scale, while the x-axis shows the number of trees and the y-axis the max tree depth.Here, our Kappa values don’t look very good - the reason is that our data was strongly unbalanced, so the accuracy for always assigning the majority class will already be very high. So, we can conclude that while having pretty good accuracy, our model did not in fact perform much better than random.</p>
<p><strong>8. Test it out for yourself!</strong></p>
<p>Alright, enough theory - go test it out yourself!</p>
</section>
<section id="finding-hyperparameters" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="finding-hyperparameters"><span class="header-section-number">2.2</span> Finding hyperparameters</h2>
<p>Finding out <strong>which</strong> hyperparameters you can tune with a given algorithm or function is the most important prerequisite for actually tuning your models!</p>
<blockquote class="blockquote">
<h2 id="question-3" data-number="2.3" class="anchored"><span class="header-section-number">2.3</span> <em>Question</em></h2>
<p>Which <strong>hyperparameters</strong> can you tune with a simple <strong>Generalized Linear Model</strong> that uses the <code>glm</code> method?<br> <br> ⬜ tau<br> ⬜ alpha, lambda<br> ✅ None<br> ⬜ cost, Loss<br></p>
</blockquote>
<p>Correct! A simple glm has no hyperparameters that can be tuned.</p>
</section>
<section id="cartesian-grid-search-in-caret" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="cartesian-grid-search-in-caret"><span class="header-section-number">2.4</span> Cartesian grid search in caret</h2>
<!--
LO: The student knows how to define a Cartesian grid of hyperparameters in caret
-->
<p>In chapter 1, you learned how to use the <code>expand.grid()</code> function to manually define hyperparameters. The same function can also be used to <strong>define a grid of hyperparameters</strong>.</p>
<p>The <code>voters_train_data</code> dataset has already been preprocessed to make it a bit smaller so training will run faster; it has now 80 observations and balanced classes and has been loaded for you. And the <code>trainControl</code> object has been defined with repeated cross-validation:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># Load data</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>voters_train_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/voters_train_data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 80 Columns: 40
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr  (1): turnout16_2016
#&gt; dbl (39): RIGGED_SYSTEM_1_2016, RIGGED_SYSTEM_2_2016, RIGGED_SYSTEM_3_2016, ...
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>,</span>
<span id="cb21-2"><a href="#cb21-2"></a>                           <span class="at">number =</span> <span class="dv">3</span>,</span>
<span id="cb21-3"><a href="#cb21-3"></a>                           <span class="at">repeats =</span> <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define a <strong>Cartesian grid</strong> of Support Vector Machine hyperparameters with the following combinations: <code>degree</code> should be 1, 2, or 3, <code>scale</code> should be 0.1, 0.01 or 0.001 and <code>C</code> should be held constant at 0.5.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># Define Cartesian grid</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>man_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">degree =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), </span>
<span id="cb22-3"><a href="#cb22-3"></a>                        <span class="at">scale  =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>), </span>
<span id="cb22-4"><a href="#cb22-4"></a>                        <span class="at">C      =</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Use the Cartesian grid you defined in the previous step to train a <strong>Support Vector Machines with Polynomial Kernel</strong> in <code>caret</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Required package: kernlab</span></span>
<span id="cb23-2"><a href="#cb23-2"></a></span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="co"># Start timer, set seed &amp; train model</span></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="fu">tic</span>()</span>
<span id="cb23-5"><a href="#cb23-5"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb23-6"><a href="#cb23-6"></a>svm_model_voters_grid <span class="ot">&lt;-</span> <span class="fu">train</span>(turnout16_2016 <span class="sc">~</span> ., </span>
<span id="cb23-7"><a href="#cb23-7"></a>                   <span class="at">data      =</span> voters_train_data, </span>
<span id="cb23-8"><a href="#cb23-8"></a>                   <span class="at">method    =</span> <span class="st">"svmPoly"</span>, </span>
<span id="cb23-9"><a href="#cb23-9"></a>                   <span class="at">trControl =</span> fitControl,</span>
<span id="cb23-10"><a href="#cb23-10"></a>                   <span class="at">verbose   =</span> <span class="cn">FALSE</span>,</span>
<span id="cb23-11"><a href="#cb23-11"></a>                   <span class="at">tuneGrid  =</span> man_grid)</span>
<span id="cb23-12"><a href="#cb23-12"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 1.743 sec elapsed</code></pre>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-4" data-number="2.5" class="anchored"><span class="header-section-number">2.5</span> <em>Question</em></h2>
<p>Explore the <code>svm_model_voters_grid</code> model object: <strong>Which hyperparameter combination was best?</strong><br> <br> ⬜ degree 3 &amp; scale 0.010<br> ✅ degree 1 &amp; scale 0.100]<br> ⬜ degree 1 &amp; scale 0.001<br> ⬜ degree 2 &amp; scale 0.100<br></p>
</blockquote>
<p>Correct. This was the best hyperparameter combination in our model.</p>
</section>
<section id="plot-hyperparameter-model-output" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="plot-hyperparameter-model-output"><span class="header-section-number">2.6</span> Plot hyperparameter model output</h2>
<!--
LO: The student knows how to plot the hyperparameter tuning results
-->
<p>In the previous exercise, you defined a <strong>Cartesian grid of hyperparameters</strong> and used it to train a Support Vector Machine model. The same code as before has been run in the background, so you can directly work with the <code>svm_model_voters_grid</code> model object. The <code>caret</code> package has also been loaded.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li><strong>Plot</strong> the model object with <strong>default arguments</strong>: Accuracy and line-plot.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># Plot default</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="fu">plot</span>(svm_model_voters_grid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-15-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Add another plot where you <strong>plot Kappa values</strong> and use a <strong>level-plot</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Plot Kappa level-plot</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="fu">plot</span>(svm_model_voters_grid, <span class="at">metric =</span> <span class="st">"Kappa"</span>, <span class="at">plotType =</span> <span class="st">"level"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-16-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Very good! Plotting is generally a good way to explore your model output and hyperparameter tuning results.</p>
</section>
<section id="grid-vs.-random-search" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="grid-vs.-random-search"><span class="header-section-number">2.7</span> Grid vs.&nbsp;Random Search</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Grid vs.&nbsp;Random Search</strong></p>
<p>Defining a hyperparameter grid is easy, right? So let’s learn even more about it.</p>
<p><strong>2. Grid search continued</strong></p>
<p>What we did in the previous lesson was <strong>fixing</strong> the learning rate and the minimum number of training set samples in a node to commence splitting. For the number of trees and for tree complexity, we compared different options.Then we trained a model with repeated cross-validationand gradient boosting.</p>
<p><strong>3. Grid search with hyperparameter ranges</strong></p>
<p>But what if we don’t want to define a set of distinct values for each hyperparameter but instead want to define a range of values?This is easy as well, just use the seq() function and define at which value you want to start, at which value to stop and by what increments you want to go between start and stop.Here you see that you can now end up with non-integer values in your grid and that the grid will quickly grow much bigger.</p>
<p><strong>4. Grid search with many hyperparameter options</strong></p>
<p>Let’s see what happens if we use this grid in a gradient boosting model the same way we used the Cartesian grid before - as input to the tunegrid argument in train. 240 seconds to train! I’m sure you can see how quickly you would end up with a training that takes forever to run if you keep increasing the hyperparameters you want to tune!</p>
<p><strong>5. Cartesian grid vs random search</strong></p>
<p>So far, we have always compared all possible combinations of hyperparameters in our predefined grid. This method was called Cartesian grid search.Here, you see a different way to plot the hyperparameter tuning results: instead of using the base R plot function, you can also feed the model object to the ggplot function and see a similar line plot of hyperparameter combinations and their corresponding accuracies.Even though we want to compare as many hyperparameters as possible in order to find the most optimal combination for our model, using Cartesian grid search will become slow and computationally expensive very quickly.So let’s look at a faster alternative - random search.With random search we no longer test all possible combinations of different hyperparameters; instead we will randomly pick a specified number of hyperparameter combinations by chance and only evaluate those regarding model performance.</p>
<p><strong>6. Random search in caret</strong></p>
<p>To use random search, another option is available in caret’s trainControl function called search. Possible inputs to this argument are “grid” and “random”. The built-in models contained in caret contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the tuneLength option in train. We already got to know tuneLength in the first chapter, where we used to to define the number of tuning parameters to compare in caret’s automatic tuning function. There, all possible combinations of these hyperparameters were compared. Here we use it to define how many randomly picked hyperparameter combinations to evaluate. 5 is of course too few and in reality, you would want to test at least 100. But again, for demonstration purposes we go with the time-saving version, here,which still takes almost 1 minute to train.</p>
<p><strong>7. Random search in caret</strong></p>
<p>This is how the model output looks like now: 5 randomly picked hyperparameter combinations and their corresponding accuracy and kappa values are shown.And - as always - the final values are given in the text at the bottom of the printed output.One important thing to note is this: in caret, random search can NOT be combined with grid search! This means that the tunelength argument cannot be used to sample from a customized grid.</p>
<p><strong>8. Let’s get coding!</strong></p>
<p>Alright, now it’s your turn to try out what you’ve learned!</p>
</section>
<section id="grid-search-with-range-of-hyperparameters" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="grid-search-with-range-of-hyperparameters"><span class="header-section-number">2.8</span> Grid search with range of hyperparameters</h2>
<!--
LO: The student knows how to define a Cartesian grid with a range of hyperparameters in caret
-->
<p>In chapter 1, you learned how to use the <code>expand.grid()</code> function to manually define a set of hyperparameters. The same function can also be used to define a <strong>grid with ranges</strong> of hyperparameters.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define a <strong>grid</strong> with the neural network hyperparameter <strong>size ranging from 1 to 5</strong> with a <strong>step-size of 1</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Define the grid with hyperparameter ranges</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>big_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">size =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">1</span>, <span class="at">to =</span> <span class="dv">5</span>, <span class="at">by =</span> <span class="dv">1</span>),</span>
<span id="cb27-3"><a href="#cb27-3"></a>                        <span class="at">decay =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb27-4"><a href="#cb27-4"></a>big_grid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["size"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["decay"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"0"},{"1":"2","2":"0"},{"1":"3","2":"0"},{"1":"4","2":"0"},{"1":"5","2":"0"},{"1":"1","2":"1"},{"1":"2","2":"1"},{"1":"3","2":"1"},{"1":"4","2":"1"},{"1":"5","2":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Specifically define the <code>trainControl</code> function to perform <strong>grid search</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Train control with grid search</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method  =</span> <span class="st">"repeatedcv"</span>,</span>
<span id="cb28-3"><a href="#cb28-3"></a>                           <span class="at">number  =</span> <span class="dv">3</span>,</span>
<span id="cb28-4"><a href="#cb28-4"></a>                           <span class="at">repeats =</span> <span class="dv">5</span>,</span>
<span id="cb28-5"><a href="#cb28-5"></a>                           <span class="at">search  =</span> <span class="st">"grid"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Train a <strong>regular Neural Network</strong> in caret.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># Required package: nnet</span></span>
<span id="cb29-2"><a href="#cb29-2"></a></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="co"># Train neural net</span></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="fu">tic</span>()</span>
<span id="cb29-5"><a href="#cb29-5"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb29-6"><a href="#cb29-6"></a>nn_model_voters_big_grid <span class="ot">&lt;-</span> <span class="fu">train</span>(turnout16_2016 <span class="sc">~</span> ., </span>
<span id="cb29-7"><a href="#cb29-7"></a>                   <span class="at">data      =</span> voters_train_data, </span>
<span id="cb29-8"><a href="#cb29-8"></a>                   <span class="at">method    =</span> <span class="st">"nnet"</span>, </span>
<span id="cb29-9"><a href="#cb29-9"></a>                   <span class="at">trControl =</span> fitControl,</span>
<span id="cb29-10"><a href="#cb29-10"></a>                   <span class="co"># verbose   = FALSE,</span></span>
<span id="cb29-11"><a href="#cb29-11"></a>                   <span class="at">trace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb29-12"><a href="#cb29-12"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 1.935 sec elapsed</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>And finally: feed the <code>big_grid</code> to this Neural Network for tuning.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Define the grid with hyperparameter ranges</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>big_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">size =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">1</span>, <span class="at">to =</span> <span class="dv">5</span>, <span class="at">by =</span> <span class="dv">1</span>), <span class="at">decay =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb31-3"><a href="#cb31-3"></a></span>
<span id="cb31-4"><a href="#cb31-4"></a><span class="co"># Train control with grid search</span></span>
<span id="cb31-5"><a href="#cb31-5"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>, <span class="at">number =</span> <span class="dv">3</span>, <span class="at">repeats =</span> <span class="dv">5</span>, <span class="at">search =</span> <span class="st">"grid"</span>)</span>
<span id="cb31-6"><a href="#cb31-6"></a></span>
<span id="cb31-7"><a href="#cb31-7"></a><span class="co"># Train neural net</span></span>
<span id="cb31-8"><a href="#cb31-8"></a><span class="fu">tic</span>()</span>
<span id="cb31-9"><a href="#cb31-9"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb31-10"><a href="#cb31-10"></a>nn_model_voters_big_grid <span class="ot">&lt;-</span> <span class="fu">train</span>(turnout16_2016 <span class="sc">~</span> ., </span>
<span id="cb31-11"><a href="#cb31-11"></a>                   <span class="at">data      =</span> voters_train_data, </span>
<span id="cb31-12"><a href="#cb31-12"></a>                   <span class="at">method    =</span> <span class="st">"nnet"</span>, </span>
<span id="cb31-13"><a href="#cb31-13"></a>                   <span class="at">trControl =</span> fitControl,</span>
<span id="cb31-14"><a href="#cb31-14"></a>                   <span class="co"># verbose   = FALSE,</span></span>
<span id="cb31-15"><a href="#cb31-15"></a>                   <span class="at">trace     =</span> <span class="cn">FALSE</span>,</span>
<span id="cb31-16"><a href="#cb31-16"></a>                   <span class="at">tuneGrid  =</span> big_grid)</span>
<span id="cb31-17"><a href="#cb31-17"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 1.862 sec elapsed</code></pre>
</div>
</div>
<p>Great! You understood the complete workflow of defining a hyperparameter grid and using it in the caret <code>train()</code> function.</p>
</section>
<section id="find-train-option-for-random-search" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="find-train-option-for-random-search"><span class="header-section-number">2.9</span> Find train() option for random search</h2>
<p>In the video for this chapter, I showed you how to perform a grid search or random search with <code>caret</code>.</p>
<blockquote class="blockquote">
<h2 id="question-5" data-number="2.10" class="anchored"><span class="header-section-number">2.10</span> <em>Question</em></h2>
<p>Which <strong>argument</strong> do you need to set in combination with <code>trainControl(search = "random")</code> in order to perform <strong>random search</strong>?<br> <br> ⬜ method<br> ✅ tuneLength<br> ⬜ preProcess<br> ⬜ tuneGrid<br></p>
</blockquote>
<p>Correct! Tune length defines the number of (randomly sampled) tuning parameter combinations to compare.</p>
</section>
<section id="random-search-with-caret" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="random-search-with-caret"><span class="header-section-number">2.11</span> Random search with caret</h2>
<!--
LO: The student knows how to perform random hyperparameter search in caret
-->
<p>Now you are going to perform a <strong>random search</strong> instead of grid search!</p>
<p>As before, the small <code>voters_train_data</code> dataset has been loaded for you, as have the <code>caret</code> and <code>tictoc</code> packages.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define a training control object with <strong>random search</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># Train control with random search</span></span>
<span id="cb33-2"><a href="#cb33-2"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method  =</span> <span class="st">"repeatedcv"</span>,</span>
<span id="cb33-3"><a href="#cb33-3"></a>                           <span class="at">number  =</span> <span class="dv">3</span>,</span>
<span id="cb33-4"><a href="#cb33-4"></a>                           <span class="at">repeats =</span> <span class="dv">5</span>,</span>
<span id="cb33-5"><a href="#cb33-5"></a>                           <span class="at">search  =</span> <span class="st">"random"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Compare <strong>six random hyperparameter combinations</strong> in the neural network below.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a><span class="co"># Test 6 random hyperparameter combinations</span></span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="fu">tic</span>()</span>
<span id="cb34-3"><a href="#cb34-3"></a>nn_model_voters_big_grid <span class="ot">&lt;-</span> <span class="fu">train</span>(turnout16_2016 <span class="sc">~</span> ., </span>
<span id="cb34-4"><a href="#cb34-4"></a>                   <span class="at">data       =</span> voters_train_data, </span>
<span id="cb34-5"><a href="#cb34-5"></a>                   <span class="at">method     =</span> <span class="st">"nnet"</span>, </span>
<span id="cb34-6"><a href="#cb34-6"></a>                   <span class="at">trControl  =</span> fitControl,</span>
<span id="cb34-7"><a href="#cb34-7"></a>                   <span class="co"># verbose    = FALSE,</span></span>
<span id="cb34-8"><a href="#cb34-8"></a>                   <span class="at">trace      =</span> <span class="cn">FALSE</span>,</span>
<span id="cb34-9"><a href="#cb34-9"></a>                   <span class="at">tuneLength =</span> <span class="dv">6</span>)</span>
<span id="cb34-10"><a href="#cb34-10"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 3.076 sec elapsed</code></pre>
</div>
</div>
<p>You just performed random search with hyperparameter values that were picked by <code>caret</code>.</p>
<blockquote class="blockquote">
<h2 id="question-6" data-number="2.12" class="anchored"><span class="header-section-number">2.12</span> <em>Question</em></h2>
<p>How could you <strong>define your own grid</strong> of hyperparameter values from which to sample randomly?<br> <br> ⬜ By setting both arguments <code>tuneGrid</code> and <code>tuneLength</code> in <code>caret::train</code>.<br> ⬜ By changing the <code>method</code> argument in <code>trainControl</code>.<br> ✅ In <code>caret</code>, it is not possible to perform a random search on a defined grid.<br> ⬜ By setting the <code>randomGrid</code> argument in <code>caret::train</code>.<br></p>
</blockquote>
<p>Correct! In <code>caret</code>, it is not possible to perform a random search on a defined grid. You will learn how to do this with other packages in the next two chapters.</p>
</section>
<section id="adaptive-resampling" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="adaptive-resampling"><span class="header-section-number">2.13</span> Adaptive resampling</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Adaptive resampling</strong> Good job completing the exercises! You have now seen how hyperparameter grids work when tuning your models. However, both grid and random search are not very efficient - nor fast! Adaptive Resampling is a technique that can be used instead.</p>
<p><strong>2. What is Adaptive Resampling?</strong> With grid search and random search the performance of different hyperparameter combinations is evaluated. Which combination wins, e.g.&nbsp;which has the highest accuracy, is determined at the very end. That way, many of the tested combinations will perform badly. And testing hyperparameter combinations will continue, even if the best combination has already been found. With adaptive resampling, hyperparameter combinations are resampled with values that are close to combinations that performed well and combinations that are sub-optimal will not be tested at all. This way, each round of tested hyperparameters will zero in on the optimal combination of hyperparameters. This makes adaptive resampling faster and more efficient. A detailed explanation of adaptive resampling and how it is implemented in caret can be found in this paper by Max Kuhn. You can again click on this link to get to the paper.</p>
<p><strong>3. Adaptive resampling in caret</strong> Adaptive resampling is implemented in caret, so it is very easy for us to use. We simply need to modify our trainControl function with the following settings: As method we define adaptive_cv to use adaptive resampling with cross-validation. By default, a “grid” search would be performed but here, we define search as random. Then we define the adaptive resampling process with - min, which determines the minimum number of resamples used for each hyperparameter. Per default, caret uses a min value of 5. The larger we set min, the slower the resampling process will be but we increase our likelihood of finding the optimal hyperparameter combination. - alpha defines the confidence level that we want to use to remove hyperparameters. Usually, changing alpha does not influence the result that much. - with method, we set the resampling method. It can be either a simple linear model, as we use here with gls. Or we could use a Bradley-Terry model, which would be advised if we have a large number of hyperparameters to test or if we expect our model accuracy to be close to one and not vary much between hyperparameter combinations. It is therefore useful for fine-tuning models that are already pretty good. - and finally, complete let’s us specify whether we want to generate a full resampling set if an optimal solution is found before resampling is completed. Setting complete as FALSE would save time and we would still get the optimal combination of hyperparameters - but we won’t know the final estimated performance measure for our model. This is how the final traincontrol function will look like for adaptive resampling.</p>
<p><strong>4. Adaptive resampling in caret</strong> We can now use our as such defined trainControl with carets train function - just as we did before. What we additionally need to define in train is again the setting tuneLength, which will define the maximum number of hyperparameter combinations we want to compare. Here I’ll be using 7, which is, of course, again a rather low number. In your real-world experiments, you will most likely want to compare at least 100 combinations. But you see that even an efficient method like adaptive resampling still takes time to perform its magic.</p>
<p><strong>5. Adaptive resampling</strong> Here you see the lower part of the output of our model trained with adaptive resampling. It has again the same structure as our caret models from before. And we also get the final values used, which give an accuracy of 96% but a low Kappa value.</p>
<p><strong>6. Let’s get coding!</strong> Now it’s your turn!</p>
</section>
<section id="advantages-of-adaptive-resampling" class="level2" data-number="2.14">
<h2 data-number="2.14" class="anchored" data-anchor-id="advantages-of-adaptive-resampling"><span class="header-section-number">2.14</span> Advantages of Adaptive Resampling</h2>
<p>You have heard a lot about advanced tuning with <strong>Adaptive Resampling</strong> in <code>caret</code> in the video you just saw.</p>
<blockquote class="blockquote">
<h2 id="question-7" data-number="2.15" class="anchored"><span class="header-section-number">2.15</span> <em>Question</em></h2>
<p>Which of the following statements about the Adaptive Resampling technique is <strong>NOT true</strong>?<br> <br> ✅ Adaptive Resampling will find better hyperparameter combinations than grid or random search.<br> ⬜ Adaptive Resampling is faster than cartesian grid search.<br> ⬜ With Adaptive Resampling hyperparameter combinations are resampled near values that perform well.<br> ⬜ Adaptive Resampling is more efficient than grid and random search.<br></p>
</blockquote>
<p>Correct! Adaptive Resampling does not necessarily find better hyperparameter combinations, it is just more efficient at searching.</p>
</section>
<section id="adaptive-resampling-with-caret" class="level2" data-number="2.16">
<h2 data-number="2.16" class="anchored" data-anchor-id="adaptive-resampling-with-caret"><span class="header-section-number">2.16</span> Adaptive Resampling with caret</h2>
<!--
LO: The student knows how to perform Adaptive Resampling in caret
-->
<p>Now you are going to train a model on the voter’s dataset using <strong>Adaptive Resampling</strong>!</p>
<p>As before, the small <code>voters_train_data</code> dataset has been loaded for you, as have the <code>caret</code> and <code>tictoc</code> packages.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define a <code>trainControl()</code> function for <strong>performing Adaptive Resampling</strong> with 3x3 repeated cross-validation.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="co"># Define trainControl function</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method  =</span> <span class="st">"adaptive_cv"</span>,</span>
<span id="cb36-3"><a href="#cb36-3"></a>                           <span class="at">number  =</span> <span class="dv">3</span>, </span>
<span id="cb36-4"><a href="#cb36-4"></a>                           <span class="at">repeats =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Change the <strong>resampling scheme</strong> from the default grid method to random search for Adaptive Resampling.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># Define trainControl function</span></span>
<span id="cb37-2"><a href="#cb37-2"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method  =</span> <span class="st">"adaptive_cv"</span>,</span>
<span id="cb37-3"><a href="#cb37-3"></a>                           <span class="at">number  =</span> <span class="dv">3</span>, </span>
<span id="cb37-4"><a href="#cb37-4"></a>                           <span class="at">repeats =</span> <span class="dv">3</span>,</span>
<span id="cb37-5"><a href="#cb37-5"></a>                           <span class="at">search  =</span> <span class="st">"random"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Define the <strong>Adaptive Resampling options</strong> <em>minimum number of resamples</em> as 3 and use the <em>Bradley Terry</em> method.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># Define trainControl function</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method   =</span> <span class="st">"adaptive_cv"</span>,</span>
<span id="cb38-3"><a href="#cb38-3"></a>                           <span class="at">number   =</span> <span class="dv">3</span>, </span>
<span id="cb38-4"><a href="#cb38-4"></a>                           <span class="at">repeats  =</span> <span class="dv">3</span>,</span>
<span id="cb38-5"><a href="#cb38-5"></a>                           <span class="at">adaptive =</span> <span class="fu">list</span>(<span class="at">min =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">method =</span> <span class="st">"BT"</span>, <span class="at">complete =</span> <span class="cn">FALSE</span>),</span>
<span id="cb38-6"><a href="#cb38-6"></a>                           <span class="at">search   =</span> <span class="st">"random"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Change the <strong>maximum number of tuning parameter combinations</strong> that will be generated by random search from its default of 3 to 6 and train the <strong>Neural Network</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># Required package: BradleyTerry2</span></span>
<span id="cb39-2"><a href="#cb39-2"></a></span>
<span id="cb39-3"><a href="#cb39-3"></a><span class="co"># Start timer &amp; train model</span></span>
<span id="cb39-4"><a href="#cb39-4"></a><span class="fu">tic</span>()</span>
<span id="cb39-5"><a href="#cb39-5"></a>svm_model_voters_ar <span class="ot">&lt;-</span> <span class="fu">train</span>(turnout16_2016 <span class="sc">~</span> ., </span>
<span id="cb39-6"><a href="#cb39-6"></a>                   <span class="at">data       =</span> voters_train_data, </span>
<span id="cb39-7"><a href="#cb39-7"></a>                   <span class="at">method     =</span> <span class="st">"nnet"</span>, </span>
<span id="cb39-8"><a href="#cb39-8"></a>                   <span class="at">trControl  =</span> fitControl,</span>
<span id="cb39-9"><a href="#cb39-9"></a>                   <span class="co"># verbose  = FALSE,</span></span>
<span id="cb39-10"><a href="#cb39-10"></a>                   <span class="at">trace      =</span> <span class="cn">FALSE</span>,</span>
<span id="cb39-11"><a href="#cb39-11"></a>                   <span class="at">tuneLength =</span> <span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Lade nötigen Namensraum: BradleyTerry2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in reshapeWide(data, idvar = idvar, timevar = timevar, varying =
#&gt; varying, : multiple rows match for model_id=m6: first taken</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in reshapeWide(data, idvar = idvar, timevar = timevar, varying =
#&gt; varying, : multiple rows match for model_id=m5: first taken</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in reshapeWide(data, idvar = idvar, timevar = timevar, varying =
#&gt; varying, : multiple rows match for model_id=m1: first taken</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in reshapeWide(data, idvar = idvar, timevar = timevar, varying =
#&gt; varying, : multiple rows match for model_id=m2: first taken</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in reshapeWide(data, idvar = idvar, timevar = timevar, varying =
#&gt; varying, : multiple rows match for model_id=m3: first taken</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in reshapeWide(data, idvar = idvar, timevar = timevar, varying =
#&gt; varying, : multiple rows match for model_id=m4: first taken</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Lade nötigen Namensraum: BradleyTerry2
#&gt; Lade nötigen Namensraum: BradleyTerry2
#&gt; Lade nötigen Namensraum: BradleyTerry2
#&gt; Lade nötigen Namensraum: BradleyTerry2
#&gt; Lade nötigen Namensraum: BradleyTerry2
#&gt; Lade nötigen Namensraum: BradleyTerry2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 2.044 sec elapsed</code></pre>
</div>
</div>
<p>Good job! You have mastered the final hyperparameter tuning method in caret: Adaptive Resampling!</p>
</section>
</section>
<section id="hyperparameter-tuning-with-mlr" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> 3. Hyperparameter tuning with mlr</h1>
<p>Here, you will use another package for machine learning that has very convenient hyperparameter tuning functions. You will define a Cartesian grid or perform Random Search, as well as advanced techniques. You will also learn different ways to plot and evaluate models with different hyperparameters.</p>
<section id="machine-learning-with-mlr" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="machine-learning-with-mlr"><span class="header-section-number">3.1</span> Machine learning with mlr</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Machine learning with mlr</strong></p>
<p>In order to fully understand the concepts of hyperparameter tuning, we will look at implementations in different contexts and packages. Here, I will introduce the package mlr.</p>
<p><strong>2. The mlr package</strong></p>
<p>mlr is another very popular framework for machine learning in R. It provides methods for a number of algorithms and machine learning tasks, including supervised classification and regression.mlr also allows you to easily perform hyperparameter tuning.However, mlr has a slightly different way of defining machine learning tasks. Therefore, we will use this lesson to go over the basics of machine learning with mlr before we dive into hyperparameter tuning.The workflow for training models with mlr follows three steps:First, you need to define the task, then you define the learner and only then can you fit the model. We will go over each step in detail.You can follow this link to find out more about mlr and how it works.</p>
<p><strong>3. New dataset: User Knowledge Data</strong></p>
<p>The dataset we’ll be working with in this chapter is a real-world dataset on students’ knowledge status about the subject of Electrical DC Machines.It consists of 150 observations and 6 variables. The five features are - STG (The degree of study time for goal object materials)- SCG (The degree of repetition for goal object materials) - STR (The degree of study time for related objects with goal object) - LPR (The exam performance for related objects with goal object)- and PEG (The exam performance for goal objects)The response variable UNS (The knowledge level of the student) can be one of three classes: Low, Middle or high.</p>
<p><strong>4. Tasks in mlr for supervised learning</strong></p>
<p>Tasks define the data and - in case of supervised learning - the response variable or target. mlr has a number of different tasks you can choose from:regressionclassificationmulti-label classificationand cost-sensitive classificationHere, we’ll focus on tasks for supervised learning.To create a classification task, we are using the makeClassifTask function. If you want to know which task functions you can define, either check the mlr manual or - if you are using RStudio - start typing make in your console and autocomplete will suggest different functions.</p>
<p><strong>5. Learners in mlr</strong></p>
<p>Next, we’ll need to define our learner. You can find out which learners you can choose from by calling the listlearners function. This will return a table of available learners and the name you will need to use as class if you want to create a learner object for it.By convention, all classification learners start with “classif.”, all regression learners with “regr.” and all multilabel classification learners start with “multilabel.”.Here, we want to create a learner for a deep neural network from the h2o package.We create a new object with the makelearner function and “classif.h2o.deeplearning”. In this function, we could also define whether we wanted labels or predictions as output, set hyperparameters, and more. Check the help function for makelearner to find out more.Sometimes, you might have classification models where your target column contains more or fewer factor levels than the target column in your test or validation data. This can lead to problems but if you set fix.factors.prediction to TRUE a factor level for missing data is added.And you can also define whether to return predicted labels or probabilities.</p>
<p><strong>6. Model fitting in mlr</strong></p>
<p>Finally, we can take our task and our learner and use them with the train function of mlr to fit our model.We are again using the tictoc package to calculate how long the runtimes are for our models: Here, the training took about 4 seconds.</p>
<p><strong>7. Let’s practice!</strong></p>
<p>Alright, now it’s your turn to try out mlr!</p>
</section>
<section id="machine-learning-with-mlr-1" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="machine-learning-with-mlr-1"><span class="header-section-number">3.2</span> Machine Learning with mlr</h2>
<p>Note: The <code>mlr</code> package is already loaded.</p>
<blockquote class="blockquote">
<h2 id="question-8" data-number="3.3" class="anchored"><span class="header-section-number">3.3</span> <em>Question</em></h2>
<p>Which of the following is <strong>NOT</strong> a step in the <code>mlr</code> modelling workflow? <br> <br> ⬜ Defining a learner.<br> ⬜ Defining a task.<br> ⬜ Fitting a model.<br> ✅ Converting the response variable to a factor.<br></p>
</blockquote>
<p>Correct! Converting the response variable to a factor is not a necessary step of the <code>mlr</code> workflow.</p>
</section>
<section id="modeling-with-mlr" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="modeling-with-mlr"><span class="header-section-number">3.4</span> Modeling with mlr</h2>
<!--
LO: The student knows how to train a machine learning model with mlr
-->
<p>As you have seen in the video just now, <code>mlr</code> is yet another popular machine learning package in R that comes with many functions to do hyperparameter tuning. Here, you are going to go over the <strong>basic workflow</strong> for training models with <code>mlr</code>.</p>
<p>The <code>knowledge_train_data</code> dataset has already been loaded for you, as have the packages <code>mlr</code>, <code>tidyverse</code> and <code>tictoc</code>. <strong>Remember</strong> that starting to type in the console will suggest autocompleting options for functions and packages.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a <strong>regular classification task</strong> with the <code>knowledge_train_data</code> and target <code>UNS</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a><span class="co"># Load package</span></span>
<span id="cb50-2"><a href="#cb50-2"></a><span class="fu">library</span>(mlr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Lade nötiges Paket: ParamHelpers</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning message: 'mlr' is in 'maintenance-only' mode since July 2019.
#&gt; Future development will only happen in 'mlr3'
#&gt; (&lt;https://mlr3.mlr-org.com&gt;). Due to the focus on 'mlr3' there might be
#&gt; uncaught bugs meanwhile in {mlr} - please consider switching.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'mlr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Das folgende Objekt ist maskiert 'package:caret':
#&gt; 
#&gt;     train</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1"></a><span class="co"># Load data</span></span>
<span id="cb55-2"><a href="#cb55-2"></a>knowledge_train_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/knowledge_train_data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 120 Columns: 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; chr (1): UNS
#&gt; dbl (5): STG, SCG, STR, LPR, PEG
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1"></a><span class="co"># Create classification task</span></span>
<span id="cb58-2"><a href="#cb58-2"></a>task <span class="ot">&lt;-</span> <span class="fu">makeClassifTask</span>(<span class="at">data   =</span> knowledge_train_data, </span>
<span id="cb58-3"><a href="#cb58-3"></a>                        <span class="at">target =</span> <span class="st">"UNS"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in makeTask(type = type, data = data, weights = weights, blocking =
#&gt; blocking, : Provided data is not a pure data.frame but from class spec_tbl_df,
#&gt; hence it will be converted.</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Ask for a <strong>list of all learners</strong> you can use with mlr. Note that you are converting the output to a data frame and selecting only the columns <code>class</code>, <code>short.name</code> and <code>package</code> so that the output fits the page.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1"></a><span class="co"># load package</span></span>
<span id="cb60-2"><a href="#cb60-2"></a><span class="fu">library</span>(dplyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; Attache Paket: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:stats':
#&gt; 
#&gt;     filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Die folgenden Objekte sind maskiert von 'package:base':
#&gt; 
#&gt;     intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1"></a><span class="co"># Call the list of learners</span></span>
<span id="cb64-2"><a href="#cb64-2"></a><span class="fu">listLearners</span>() <span class="sc">%&gt;%</span></span>
<span id="cb64-3"><a href="#cb64-3"></a> <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb64-4"><a href="#cb64-4"></a> <span class="fu">select</span>(class, short.name, package) <span class="sc">%&gt;%</span></span>
<span id="cb64-5"><a href="#cb64-5"></a> <span class="fu">filter</span>(<span class="fu">grepl</span>(<span class="st">"classif."</span>, class))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in listLearners.character(): The following learners could not be constructed, probably because their packages are not installed:
#&gt; classif.FDboost,classif.IBk,classif.J48,classif.JRip,classif.LiblineaRL1L2SVC,classif.LiblineaRL1LogReg,classif.LiblineaRL2L1SVC,classif.LiblineaRL2LogReg,classif.LiblineaRL2SVC,classif.LiblineaRMultiClassSVC,classif.OneR,classif.PART,classif.RRF,classif.ada,classif.adaboostm1,classif.bartMachine,classif.boosting,classif.bst,classif.cforest,classif.clusterSVM,classif.ctree,classif.dbnDNN,classif.dcSVM,classif.evtree,classif.fdausc.glm,classif.fdausc.kernel,classif.fdausc.knn,classif.fdausc.np,classif.fgam,classif.fnn,classif.gamboost,classif.gaterSVM,classif.glmboost,classif.kknn,classif.mda,classif.mlp,classif.neuralnet,classif.nnTrain,classif.pamr,classif.penalized,classif.plr,classif.plsdaCaret,classif.rFerns,classif.rda,classif.rotationForest,classif.saeDNN,classif.sda,classif.sparseLDA,cluster.Cobweb,cluster.EM,cluster.FarthestFirst,cluster.MiniBatchKmeans,cluster.SimpleKMeans,cluster.XMeans,cluster.cmeans,cluster.dbscan,cluster.kmeans,multilabel.cforest,multilabel.rFerns,regr.FDboost,regr.IBk,regr.LiblineaRL2L1SVR,regr.LiblineaRL2L2SVR,regr.RRF,regr.bartMachine,regr.bcart,regr.bgp,regr.bgpllm,regr.blm,regr.brnn,regr.bst,regr.btgp,regr.btgpllm,regr.btlm,regr.cforest,regr.crs,regr.ctree,regr.evtree,regr.fgam,regr.fnn,regr.frbs,regr.gamboost,regr.glmboost,regr.kknn,regr.km,regr.laGP,regr.mars,regr.mob,regr.pcr,regr.penalized,regr.plsr,regr.rsm,surv.cforest,surv.gamboost,surv.glmboost
#&gt; Check ?learners to see which packages you need or install mlr with all suggestions.</code></pre>
</div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["class"],"name":[1],"type":["chr"],"align":["left"]},{"label":["short.name"],"name":[2],"type":["chr"],"align":["left"]},{"label":["package"],"name":[3],"type":["chr"],"align":["left"]}],"data":[{"1":"classif.C50","2":"C50","3":"C50"},{"1":"classif.FDboost","2":"FDboost","3":"FDboost,mboost"},{"1":"classif.IBk","2":"ibk","3":"RWeka"},{"1":"classif.J48","2":"j48","3":"RWeka"},{"1":"classif.JRip","2":"jrip","3":"RWeka"},{"1":"classif.LiblineaRL1L2SVC","2":"liblinl1l2svc","3":"LiblineaR"},{"1":"classif.LiblineaRL1LogReg","2":"liblinl1logreg","3":"LiblineaR"},{"1":"classif.LiblineaRL2L1SVC","2":"liblinl2l1svc","3":"LiblineaR"},{"1":"classif.LiblineaRL2LogReg","2":"liblinl2logreg","3":"LiblineaR"},{"1":"classif.LiblineaRL2SVC","2":"liblinl2svc","3":"LiblineaR"},{"1":"classif.LiblineaRMultiClassSVC","2":"liblinmulticlasssvc","3":"LiblineaR"},{"1":"classif.OneR","2":"oner","3":"RWeka"},{"1":"classif.PART","2":"part","3":"RWeka"},{"1":"classif.RRF","2":"RRF","3":"RRF"},{"1":"classif.ada","2":"ada","3":"ada,rpart"},{"1":"classif.adaboostm1","2":"adaboostm1","3":"RWeka"},{"1":"classif.bartMachine","2":"bartmachine","3":"bartMachine"},{"1":"classif.binomial","2":"binomial","3":"stats"},{"1":"classif.boosting","2":"adabag","3":"adabag,rpart"},{"1":"classif.bst","2":"bst","3":"bst,rpart"},{"1":"classif.cforest","2":"cforest","3":"party"},{"1":"classif.clusterSVM","2":"clusterSVM","3":"SwarmSVM,LiblineaR"},{"1":"classif.ctree","2":"ctree","3":"party"},{"1":"classif.cvglmnet","2":"cvglmnet","3":"glmnet"},{"1":"classif.dbnDNN","2":"dbn.dnn","3":"deepnet"},{"1":"classif.dcSVM","2":"dcSVM","3":"SwarmSVM,e1071"},{"1":"classif.earth","2":"fda","3":"earth,stats"},{"1":"classif.evtree","2":"evtree","3":"evtree"},{"1":"classif.fdausc.glm","2":"fdausc.glm","3":"fda.usc"},{"1":"classif.fdausc.kernel","2":"fdausc.kernel","3":"fda.usc"},{"1":"classif.fdausc.knn","2":"fdausc.knn","3":"fda.usc"},{"1":"classif.fdausc.np","2":"fdausc.np","3":"fda.usc"},{"1":"classif.featureless","2":"featureless","3":"mlr"},{"1":"classif.fgam","2":"FGAM","3":"refund"},{"1":"classif.fnn","2":"fnn","3":"FNN"},{"1":"classif.gamboost","2":"gamboost","3":"mboost"},{"1":"classif.gaterSVM","2":"gaterSVM","3":"SwarmSVM"},{"1":"classif.gausspr","2":"gausspr","3":"kernlab"},{"1":"classif.gbm","2":"gbm","3":"gbm"},{"1":"classif.glmboost","2":"glmboost","3":"mboost"},{"1":"classif.glmnet","2":"glmnet","3":"glmnet"},{"1":"classif.h2o.deeplearning","2":"h2o.dl","3":"h2o"},{"1":"classif.h2o.gbm","2":"h2o.gbm","3":"h2o"},{"1":"classif.h2o.glm","2":"h2o.glm","3":"h2o"},{"1":"classif.h2o.randomForest","2":"h2o.rf","3":"h2o"},{"1":"classif.kknn","2":"kknn","3":"kknn"},{"1":"classif.knn","2":"knn","3":"class"},{"1":"classif.ksvm","2":"ksvm","3":"kernlab"},{"1":"classif.lda","2":"lda","3":"MASS"},{"1":"classif.logreg","2":"logreg","3":"stats"},{"1":"classif.lssvm","2":"lssvm","3":"kernlab"},{"1":"classif.lvq1","2":"lvq1","3":"class"},{"1":"classif.mda","2":"mda","3":"mda"},{"1":"classif.mlp","2":"mlp","3":"RSNNS"},{"1":"classif.multinom","2":"multinom","3":"nnet"},{"1":"classif.naiveBayes","2":"nbayes","3":"e1071"},{"1":"classif.neuralnet","2":"neuralnet","3":"neuralnet"},{"1":"classif.nnTrain","2":"nn.train","3":"deepnet"},{"1":"classif.nnet","2":"nnet","3":"nnet"},{"1":"classif.pamr","2":"pamr","3":"pamr"},{"1":"classif.penalized","2":"penalized","3":"penalized"},{"1":"classif.plr","2":"plr","3":"stepPlr"},{"1":"classif.plsdaCaret","2":"plsdacaret","3":"caret,pls"},{"1":"classif.probit","2":"probit","3":"stats"},{"1":"classif.qda","2":"qda","3":"MASS"},{"1":"classif.rFerns","2":"rFerns","3":"rFerns"},{"1":"classif.randomForest","2":"rf","3":"randomForest"},{"1":"classif.ranger","2":"ranger","3":"ranger"},{"1":"classif.rda","2":"rda","3":"klaR"},{"1":"classif.rotationForest","2":"rotationForest","3":"rotationForest"},{"1":"classif.rpart","2":"rpart","3":"rpart"},{"1":"classif.saeDNN","2":"sae.dnn","3":"deepnet"},{"1":"classif.sda","2":"sda","3":"sda"},{"1":"classif.sparseLDA","2":"sparseLDA","3":"sparseLDA,MASS,elasticnet"},{"1":"classif.svm","2":"svm","3":"e1071"},{"1":"classif.xgboost","2":"xgboost","3":"xgboost"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Find the correct classifier for <strong>Random Forest</strong> in the previous output and <strong>build a learner</strong> with this <code>randomForest</code> classifier.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1"></a><span class="co"># Create learner</span></span>
<span id="cb66-2"><a href="#cb66-2"></a>lrn <span class="ot">&lt;-</span> <span class="fu">makeLearner</span>(<span class="st">"classif.randomForest"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>In your learner, change the following settings from the default: have <strong>class probabilities</strong> as output and add a <strong>factor for missing data</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1"></a><span class="co"># Create learner</span></span>
<span id="cb67-2"><a href="#cb67-2"></a>lrn <span class="ot">&lt;-</span> <span class="fu">makeLearner</span>(<span class="st">"classif.randomForest"</span>, </span>
<span id="cb67-3"><a href="#cb67-3"></a>                   <span class="at">predict.type =</span> <span class="st">"prob"</span>, </span>
<span id="cb67-4"><a href="#cb67-4"></a>                   <span class="at">fix.factors.prediction =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Good job! You successfully trained a Random Forest with mlr. Now, let’s look into hyperparameter tuning.</p>
</section>
<section id="grid-and-random-search-with-mlr" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="grid-and-random-search-with-mlr"><span class="header-section-number">3.5</span> Grid and random search with mlr</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Grid and random search with mlr</strong></p>
<p>Now that you have the basics of machine learning with the mlr package fresh on your mind, let’s look into hyperparameter tuning with grid and random search in mlr.</p>
<p><strong>2. Hyperparameter tuning with mlr</strong></p>
<p>For hyperparameter tuning in mlr you have to define three things:- the search space, this means the hyperparameter values you want to compare and tune- the tuning method, which could be grid or random search- the resampling schemeLet’s look at each step in turn.</p>
<p><strong>3. Defining the search space</strong></p>
<p>First, we will define the hyperparameter search space for our learner.To do this, we use the makeParamSet function. Within this main function, we have several support functions for defining different types of parameter spaces:- makeNumericParam let’s us define numeric ranges between specified values- makeIntegerParam does the same just with integers- makeDiscreteParam is for defining discrete lists of values to search- makeLogicalParam let’s us define TRUE/FALSE values- and makeDiscreteVectorParam for vectors of discrete parametersNow, let’s look at what hyperparameters we have in a deep learning model.If we enter the learner class into the getParamSet function, it will return a table with hyperparameters for this function and tell us what type they are, what default values will be set, their lower and upper boundaries, if they are tunable, plus some additional information.</p>
<p><strong>4. Defining the search space</strong></p>
<p>So, let’s take the makeParamSet function and create entries for the hyperparameters hidden, activation and l1 and l2.</p>
<p><strong>5. Defining the tuning method</strong></p>
<p>Next, we define the tuning method. The two basic versions you can choose from are grid search, where every combination of hyperparameters will be compared, and random search, where a subset of our specified values will be tested randomly.The two respective functions are makeTuneControlGrid and makeTuneControlRandom. When we look at the objects we created with these two functions, it will print a summary to the console that tells us what settings were chosen (in our case, the default values).The maxit argument of the makeTuneControlRandom function determines the number of iterations for random search - per default 100.One thing is important to note: Grid search can only deal with discrete parameter sets, defined with the makeDiscreteParam function, while random search can deal with all types of parameter sets.</p>
<p><strong>6. Define resampling strategy</strong></p>
<p>The final step is to define the resampling strategy. With the makeResampleDesc function you can create a description object for a resampling strategy, like cross-validation, repeated cross-validation, leave-one-out, bootstraping or holdout. Alternatively, you can store a set of integer vectors for training and test sets with the makeResampleInstance function.Here, we will use 3 x 5-fold repeated cross-validation to measure the performance of a specific parameter combination. If we want to predict training AND validation data (in mlr called test) during resampling to detect overfitting, we choose predict = both.We now have these three objects in R:- cross_val- param_set- and ctrl_gridSo, now we take our task and learner object and combine all in the tuneParams function, which will perform the hyperparameter tuning and model fitting.</p>
<p><strong>7. Tuning hyperparameters</strong></p>
<p>We could additionally define performance evaluation metrics but if we don’t, mlr will use the Mean misclassification error rate (mmce) by default for classification tasks. See getDefaultMeasure to find the default measure for a task. Here, you see the output that is printed to the console during training. We will get feedback for every hyperparameter that was tested, as well as the time it took to run.The last row returns the best hyperparameter result. In this case 2 hidden layers, Tanh activation and l1=0.11 and l2=0.1.The entire training process took about 30 seconds.</p>
<p><strong>8. Let’s practice!</strong></p>
<p>Now, it’s your turn again!</p>
</section>
<section id="random-search-with-mlr" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="random-search-with-mlr"><span class="header-section-number">3.6</span> Random search with mlr</h2>
<!--
LO: The student knows how to prepare hyperparameter tuning with random search in mlr
-->
<p>Now, you are going to perform <strong>hyperparameter tuning with random search</strong>. You will prepare the different functions and objects you need to tune your model in the next exercise.</p>
<p>The <code>knowledge_train_data</code> dataset has already been loaded for you, as have the packages <code>mlr</code>, <code>tidyverse</code> and <code>tictoc</code>. Remember to look into the function that lists all learners if you are unsure about the name of a learner.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Get the <strong>parameter set</strong> for neural networks of the <code>nnet</code> package.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1"></a><span class="co"># Get the parameter set for neural networks of the nnet package</span></span>
<span id="cb68-2"><a href="#cb68-2"></a><span class="fu">getParamSet</span>(<span class="st">"classif.nnet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;            Type len    Def      Constr Req Tunable Trafo
#&gt; size    integer   -      3    0 to Inf   -    TRUE     -
#&gt; maxit   integer   -    100    1 to Inf   -    TRUE     -
#&gt; skip    logical   -  FALSE           -   -    TRUE     -
#&gt; rang    numeric   -    0.7 -Inf to Inf   -    TRUE     -
#&gt; decay   numeric   -      0 -Inf to Inf   -    TRUE     -
#&gt; Hess    logical   -  FALSE           -   -    TRUE     -
#&gt; trace   logical   -   TRUE           -   -   FALSE     -
#&gt; MaxNWts integer   -   1000    1 to Inf   -   FALSE     -
#&gt; abstol  numeric   - 0.0001 -Inf to Inf   -    TRUE     -
#&gt; reltol  numeric   -  1e-08 -Inf to Inf   -    TRUE     -</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Define a <strong>set of discrete parameters:</strong> start with defining <code>size</code> to be either 2, 3 or 5.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1"></a><span class="co"># Define set of parameters</span></span>
<span id="cb70-2"><a href="#cb70-2"></a>param_set <span class="ot">&lt;-</span> <span class="fu">makeParamSet</span>(</span>
<span id="cb70-3"><a href="#cb70-3"></a>  <span class="fu">makeDiscreteParam</span>(<span class="st">"size"</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">5</span>))</span>
<span id="cb70-4"><a href="#cb70-4"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Define a set of <strong>numeric parameters</strong>: add ranges for <code>decay</code> from 0.0001 to 0.1.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1"></a><span class="co"># Define set of parameters</span></span>
<span id="cb71-2"><a href="#cb71-2"></a>param_set <span class="ot">&lt;-</span> <span class="fu">makeParamSet</span>(</span>
<span id="cb71-3"><a href="#cb71-3"></a>  <span class="fu">makeDiscreteParam</span>(<span class="st">"size"</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">5</span>)),</span>
<span id="cb71-4"><a href="#cb71-4"></a>  <span class="fu">makeNumericParam</span>(<span class="st">"decay"</span>, <span class="at">lower =</span> <span class="fl">0.0001</span>, <span class="at">upper =</span> <span class="fl">0.1</span>)</span>
<span id="cb71-5"><a href="#cb71-5"></a>)</span>
<span id="cb71-6"><a href="#cb71-6"></a></span>
<span id="cb71-7"><a href="#cb71-7"></a><span class="co"># Print parameter set</span></span>
<span id="cb71-8"><a href="#cb71-8"></a><span class="fu">print</span>(param_set)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;           Type len Def        Constr Req Tunable Trafo
#&gt; size  discrete   -   -         2,3,5   -    TRUE     -
#&gt; decay  numeric   -   - 0.0001 to 0.1   -    TRUE     -</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Define a <strong>random search tuning method</strong> with default values.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1"></a><span class="co"># Define a random search tuning method.</span></span>
<span id="cb73-2"><a href="#cb73-2"></a>ctrl_random <span class="ot">&lt;-</span> <span class="fu">makeTuneControlRandom</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Very good! You know how to prepare hyperparameter tuning with random search in mlr.</p>
</section>
<section id="perform-hyperparameter-tuning-with-mlr" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="perform-hyperparameter-tuning-with-mlr"><span class="header-section-number">3.7</span> Perform hyperparameter tuning with mlr</h2>
<!--
LO: The student knows how to do hyperparameter tuning with random search in mlr
-->
<p>Now, you can combine the prepared functions and objects from the previous exercise to actually perform <strong>hyperparameter tuning with random search</strong>. The <code>knowledge_train_data</code> dataset has already been loaded for you, as have the packages <code>mlr</code>, <code>tidyverse</code> and <code>tictoc</code>. And the following code has also been run already:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1"></a><span class="co"># Define task</span></span>
<span id="cb74-2"><a href="#cb74-2"></a>task <span class="ot">&lt;-</span> <span class="fu">makeClassifTask</span>(<span class="at">data =</span> knowledge_train_data, </span>
<span id="cb74-3"><a href="#cb74-3"></a>                        <span class="at">target =</span> <span class="st">"UNS"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in makeTask(type = type, data = data, weights = weights, blocking =
#&gt; blocking, : Provided data is not a pure data.frame but from class spec_tbl_df,
#&gt; hence it will be converted.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1"></a><span class="co"># Define learner</span></span>
<span id="cb76-2"><a href="#cb76-2"></a>lrn <span class="ot">&lt;-</span> <span class="fu">makeLearner</span>(<span class="st">"classif.nnet"</span>, <span class="at">predict.type =</span> <span class="st">"prob"</span>, <span class="at">fix.factors.prediction =</span> <span class="cn">TRUE</span>)</span>
<span id="cb76-3"><a href="#cb76-3"></a></span>
<span id="cb76-4"><a href="#cb76-4"></a><span class="co"># Define set of parameters</span></span>
<span id="cb76-5"><a href="#cb76-5"></a>param_set <span class="ot">&lt;-</span> <span class="fu">makeParamSet</span>(</span>
<span id="cb76-6"><a href="#cb76-6"></a>  <span class="fu">makeDiscreteParam</span>(<span class="st">"size"</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">5</span>)),</span>
<span id="cb76-7"><a href="#cb76-7"></a>  <span class="fu">makeNumericParam</span>(<span class="st">"decay"</span>, <span class="at">lower =</span> <span class="fl">0.0001</span>, <span class="at">upper =</span> <span class="fl">0.1</span>)</span>
<span id="cb76-8"><a href="#cb76-8"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Change the <strong>maximum number of iterations</strong> for random search to 6. Note, that 6 is a very low number; we use it so that calculation won’t take forever to complete here; usually, you would set the number much higher (the default is 100).</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1"></a><span class="co"># Define a random search tuning method.</span></span>
<span id="cb77-2"><a href="#cb77-2"></a>ctrl_random <span class="ot">&lt;-</span> <span class="fu">makeTuneControlRandom</span>(<span class="at">maxit =</span> <span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Define a <strong>3 x 3 repeated cross-validation</strong> scheme. Note, that these are very low numbers that we only use to keep calculation time down, in reality you would want to use larger values.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1"></a><span class="co"># Define a 3 x 3 repeated cross-validation scheme</span></span>
<span id="cb78-2"><a href="#cb78-2"></a>cross_val <span class="ot">&lt;-</span> <span class="fu">makeResampleDesc</span>(<span class="st">"RepCV"</span>, <span class="at">folds =</span> <span class="dv">3</span> <span class="sc">*</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Fill in the missing information with the <strong>appropriate objects</strong> you defined in this and the previous exercise in order to run the hyperparameter tuning. Don’t be concerned if running the code will take a bit, hyperparameter tuning takes time!</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1"></a><span class="co"># Tune hyperparameters</span></span>
<span id="cb79-2"><a href="#cb79-2"></a><span class="fu">tic</span>()</span>
<span id="cb79-3"><a href="#cb79-3"></a>lrn_tune <span class="ot">&lt;-</span> <span class="fu">tuneParams</span>(lrn,</span>
<span id="cb79-4"><a href="#cb79-4"></a>                       task,</span>
<span id="cb79-5"><a href="#cb79-5"></a>                       <span class="at">resampling =</span> cross_val,</span>
<span id="cb79-6"><a href="#cb79-6"></a>                       <span class="at">control =</span> ctrl_random,</span>
<span id="cb79-7"><a href="#cb79-7"></a>                       <span class="at">par.set =</span> param_set,</span>
<span id="cb79-8"><a href="#cb79-8"></a>                       <span class="at">show.info =</span> F) <span class="sc">|&gt;</span> </span>
<span id="cb79-9"><a href="#cb79-9"></a>                       </span>
<span id="cb79-10"><a href="#cb79-10"></a>                       <span class="co"># Supress output</span></span>
<span id="cb79-11"><a href="#cb79-11"></a>                       <span class="fu">capture.output</span>()</span>
<span id="cb79-12"><a href="#cb79-12"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 4.657 sec elapsed</code></pre>
</div>
</div>
<blockquote class="blockquote">
<h2 id="question-9" data-number="3.8" class="anchored"><span class="header-section-number">3.8</span> <em>Question</em></h2>
<p>What’s the <strong>default performance metric</strong> used by <code>tuneParams()</code> in the previous exercises?<br> <br> ✅ <code>mmce</code> (mean misclassification rate)<br> ⬜ <code>acc</code> (accuracy)<br> ⬜ <code>rmse</code> (root mean squared error)<br> ⬜ <code>kappa</code> (Kappa)<br></p>
</blockquote>
<p>Great! You know how to perform hyperparameter tuning with random search in mlr.</p>
</section>
<section id="evaluating-hyperparameters-with-mlr" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="evaluating-hyperparameters-with-mlr"><span class="header-section-number">3.9</span> Evaluating hyperparameters with mlr</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Evaluating hyperparameters with mlr</strong></p>
<p>In this lesson, I will show different ways to plot and evaluate models with different hyperparameters.</p>
<p><strong>2. Evaluation of our results can tell us:</strong></p>
<p>Generally, we want to evaluate hyperparameter tuning to assess:- How different hyperparameters affect model performance.- Which hyperparameters have particularly strong or weak impact.- Whether our hyperparameter search converged, i.e.&nbsp;whether we can be reasonably confident that we found the most optimal combination (or close to it).</p>
<p><strong>3. Recap</strong></p>
<p>Let’s look at our former example: hyperparameter tuning with deep learning from the h2o package.Let’s set up the tuning just as before, before we discuss how to evaluate the results: create a grid of hyperparameters, define tune control, resampling strategy, task and the learner.A small variation is our resampling scheme: this time, I am using a holdout set, which is much faster to train than repeated cross validation. The default split is 2 / 3.</p>
<p><strong>4. Evaluating the tuning results</strong></p>
<p>Here, you see the tuning result, i.e.&nbsp;the best combination of hyperparameters from our random set: 1 hidden layer with 10 nodes, Rectifier activation and l1 and l2 regularization of 0.541 and 0.229.Mean misclassification error was 0.16, which is okay.With the generateHyperParsEffectData function, we generate the results object.Because we are tuning more than 2 hyperparameters, we need to set partial dependence to TRUE.The output of this function gives a summary of the settings: which hyperparameters were tuned, which measure was used to evaluate them, what optimizer was used and whether we used nested cross validation.And we get a table with different hyperparameters that were tested. In our case, we see five randomly picked hyperparameter combinations, what values were chosen for each, as well as the mmce and the execution time. The entire table can be called with generateHyperParsEffectData$data.</p>
<p><strong>5. Plotting hyperparameter tuning results</strong></p>
<p>The hyperparameter effect data can also be plotted with the plotHyperParsEffect function. A plot can often make it easier to grasp the overall information and evaluate how well different hyperparameters performed in our model.The function takes a few inputs:- the generated hyperparameter effect data- a regression method to calculate partial dependence. Here, I am choosing random Forest but any regression method from the mlr repertoire can be used.- The x axis can show any of our hyperparameters. Here I chose l1. The y axis can show the remaining metrics, like mmce or iteration. Optionally, you can choose a z variable, in this case the number of hidden layers which is shown with different colors.- By default, plotHyperParsEffect will create a scatter plot, we can change that to a line plot, heatmap or contour plot, with the plot.type argument.</p>
<p><strong>6. Now it’s your turn!</strong></p>
<p>Alright, now it’s your turn to evaluate hyperparameters!</p>
</section>
<section id="why-to-evaluate-tuning" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="why-to-evaluate-tuning"><span class="header-section-number">3.10</span> Why to evaluate tuning?</h2>
<blockquote class="blockquote">
<h2 id="question-10" data-number="3.11" class="anchored"><span class="header-section-number">3.11</span> <em>Question</em></h2>
<p>What can you learn from <strong>evaluating hyperparameter tuning</strong> results?<br> <br> ⬜ How different model parameters were learned.<br> ✅ Which hyperparameters have a strong effect on model performance.<br> ⬜ Whether your hyperparameters are the best possible combination for your task.<br> ⬜ Whether the learning rate converged.<br></p>
</blockquote>
<p>Correct! Evaluating hyperparameter tuning results will tell you which hyperparameters have a strong effect on model performance.</p>
</section>
<section id="evaluating-hyperparameter-tuning-results" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="evaluating-hyperparameter-tuning-results"><span class="header-section-number">3.12</span> Evaluating hyperparameter tuning results</h2>
<!--
LO: The student knows how to do evaluate hyperparameter tuning results in mlr
-->
<p>Here, you will <strong>evaluate the results of a hyperparameter tuning run</strong> for a decision tree trained with the <code>rpart</code> package. And the following code has also been run:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1"></a>task <span class="ot">&lt;-</span> <span class="fu">makeClassifTask</span>(<span class="at">data =</span> knowledge_train_data, </span>
<span id="cb81-2"><a href="#cb81-2"></a>                        <span class="at">target =</span> <span class="st">"UNS"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in makeTask(type = type, data = data, weights = weights, blocking =
#&gt; blocking, : Provided data is not a pure data.frame but from class spec_tbl_df,
#&gt; hence it will be converted.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1"></a>lrn <span class="ot">&lt;-</span> <span class="fu">makeLearner</span>(<span class="at">cl =</span> <span class="st">"classif.rpart"</span>, <span class="at">fix.factors.prediction =</span> <span class="cn">TRUE</span>)</span>
<span id="cb83-2"><a href="#cb83-2"></a></span>
<span id="cb83-3"><a href="#cb83-3"></a>param_set <span class="ot">&lt;-</span> <span class="fu">makeParamSet</span>(</span>
<span id="cb83-4"><a href="#cb83-4"></a>  <span class="fu">makeIntegerParam</span>(<span class="st">"minsplit"</span>,  <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="dv">30</span>),</span>
<span id="cb83-5"><a href="#cb83-5"></a>  <span class="fu">makeIntegerParam</span>(<span class="st">"minbucket"</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="dv">30</span>),</span>
<span id="cb83-6"><a href="#cb83-6"></a>  <span class="fu">makeIntegerParam</span>(<span class="st">"maxdepth"</span>,  <span class="at">lower =</span> <span class="dv">3</span>, <span class="at">upper =</span> <span class="dv">10</span>)</span>
<span id="cb83-7"><a href="#cb83-7"></a>)</span>
<span id="cb83-8"><a href="#cb83-8"></a></span>
<span id="cb83-9"><a href="#cb83-9"></a>ctrl_random <span class="ot">&lt;-</span> <span class="fu">makeTuneControlRandom</span>(<span class="at">maxit =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Create a <strong>holdout validation resampling scheme</strong> with the default proportion of 2/3 to use in the tuning process below.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1"></a><span class="co"># Create holdout sampling</span></span>
<span id="cb84-2"><a href="#cb84-2"></a>holdout <span class="ot">&lt;-</span> <span class="fu">makeResampleDesc</span>(<span class="st">"Holdout"</span>)</span>
<span id="cb84-3"><a href="#cb84-3"></a></span>
<span id="cb84-4"><a href="#cb84-4"></a><span class="co"># Perform tuning</span></span>
<span id="cb84-5"><a href="#cb84-5"></a>lrn_tune <span class="ot">&lt;-</span> <span class="fu">tuneParams</span>(<span class="at">learner =</span> lrn, <span class="at">task =</span> task, <span class="at">resampling =</span> holdout, <span class="at">control =</span> ctrl_random, <span class="at">par.set =</span> param_set)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune] Started tuning learner classif.rpart for parameter set:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt;              Type len Def  Constr Req Tunable Trafo
#&gt; minsplit  integer   -   - 1 to 30   -    TRUE     -
#&gt; minbucket integer   -   - 1 to 30   -    TRUE     -
#&gt; maxdepth  integer   -   - 3 to 10   -    TRUE     -</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; With control class: TuneControlRandom</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Imputation value: 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 1: minsplit=23; minbucket=22; maxdepth=3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 1: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 2: minsplit=5; minbucket=16; maxdepth=9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 2: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 3: minsplit=9; minbucket=25; maxdepth=10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 3: mmce.test.mean=0.2500000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 4: minsplit=7; minbucket=14; maxdepth=9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 4: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 5: minsplit=2; minbucket=10; maxdepth=6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 5: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 6: minsplit=30; minbucket=4; maxdepth=3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 6: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 7: minsplit=8; minbucket=6; maxdepth=4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 7: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 8: minsplit=16; minbucket=11; maxdepth=4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 8: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 9: minsplit=9; minbucket=18; maxdepth=5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 9: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 10: minsplit=16; minbucket=10; maxdepth=9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 10: mmce.test.mean=0.1000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune] Result: minsplit=30; minbucket=4; maxdepth=3 : mmce.test.mean=0.1000000</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Generate <strong>hyperparameter effect data</strong> for the <code>lrn_tune</code> object and use <strong>partial dependence</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1"></a><span class="co"># Generate hyperparameter effect data</span></span>
<span id="cb110-2"><a href="#cb110-2"></a>hyperpar_effects <span class="ot">&lt;-</span> <span class="fu">generateHyperParsEffectData</span>(lrn_tune, <span class="at">partial.dep =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li><strong>Plot</strong> the hyperparameter effects with “regr.glm” to learn the partial dependence and plot <code>minsplit</code> on the x-axis against the <code>mmce</code> of the test data on the y-axis.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1"></a><span class="co"># Required package: mmpf</span></span>
<span id="cb111-2"><a href="#cb111-2"></a></span>
<span id="cb111-3"><a href="#cb111-3"></a><span class="co"># Plot hyperparameter effects</span></span>
<span id="cb111-4"><a href="#cb111-4"></a>p <span class="ot">&lt;-</span> <span class="fu">plotHyperParsEffect</span>(hyperpar_effects, </span>
<span id="cb111-5"><a href="#cb111-5"></a>                    <span class="at">partial.dep.learn =</span> <span class="st">"regr.glm"</span>,</span>
<span id="cb111-6"><a href="#cb111-6"></a>                    <span class="at">x =</span> <span class="st">"minsplit"</span>, <span class="at">y =</span> <span class="st">"mmce.test.mean"</span>, <span class="at">z =</span> <span class="st">"maxdepth"</span>,</span>
<span id="cb111-7"><a href="#cb111-7"></a>                    <span class="at">plot.type =</span> <span class="st">"line"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1"></a>p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-44-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="11_hyperparameter_tuning_files/figure-html/unnamed-chunk-44-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Good job! You know how to do evaluate hyperparameter tuning results in mlr.</p>
</section>
<section id="advanced-tuning-with-mlr" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="advanced-tuning-with-mlr"><span class="header-section-number">3.13</span> Advanced tuning with mlr</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Advanced tuning with mlr</strong></p>
<p>Now, I will show you some advanced functions for hyperparameter tuning in mlr.</p>
<p><strong>2. Advanced tuning controls</strong></p>
<p>In the previous lesson, you got to know grid search and random search. But we can also use more advanced methods, like- CMA Evolution Strategy, which is based on the concept of creating variation from the hyperparameter values in each iteration and choosing those with highest fitness for the next round. You can think of it as “Survival of the fittest” for hyperparameters.- We can also predefine a complete data frame of hyperparameters- Or use Generalized simulated annealing. Our hyperparameter search space for the model can be thought of as a complex non-linear function where the best hyperparameters will be found in its global minimum, which GenSA aims to find.- Another tuning control is iterated F-racing for automated configuration of algorithms to find the most optimal hyperparameter values in an optimization task.- And we could also use model-based or Bayesian optimization. As the name suggests, MBO uses Bayesian statistics to approximate the objective function. MBO works in conjunction with the functions <code>makeMBOControl</code> and <code>setMBOControlTermination</code>Check the help for each function to find out which hyperparameters can be defined with each function. Some allow only discrete values, while others can’t deal with dependencies. An example for dependent hyperparameters is degree in Support Vector Machines, which only works with a polynomial kernel.</p>
<p><strong>3. Choosing evaluation metrics</strong></p>
<p>Until now, we didn’t define performance metrics and used the defaults. For classification, this was the Mean misclassification error (mmce).But we can also define one or more metric with the measure argument to tuneParams, which can take one value or a list of values. Let’s look at an example:If we pass a list, the first element is used to optimize against during hyperparameter tuning, while the remaining elements of the list will only be evaluated and returned.For additional details, have a look at the Advanced Tuning section of the mlr package documentation. Here you see part of the output of our tuning run. You get information about the iteration number, hyperparameters and the metrics measured on the test (or more accurately the validation) data.</p>
<p><strong>4. Choosing evaluation metrics</strong></p>
<p>We can also define more complex metrics with the setAggregation function, which additionally returns the standard deviation of a metric, aggregated after resampling. In our example, accuracy, aggregated by the mean performance values on the training set is used for optimization, while the mmce is evaluated.When we look at the output, we see that we get information about the performance not only for the test set but also for the training set.If the available performance metrics are not suitable for your particular problem, you can use the makeMeasure function to construct custom measures.</p>
<p><strong>5. Nested cross-validation &amp; nested resampling</strong></p>
<p>Another advanced approach is nested cross validation. Here, we use the makeTuneWrapper function instead of tuneParams to customize our base learner and add a hyperparameter search strategy.We can use this wrapper either directly with the train function, where tuning and resampling are performed and a final model is fit with the best hyperparameter combination. These can be extracted with getTuneResult.Or we can add an additional layer of cross-validation with the resample function, where we pass a second tuning control to the resampling argument.</p>
<p><strong>6. Choose hyperparameters from a tuning set</strong></p>
<p>And finally, we can extract the hyperparameters of our learner object and use the setHyperPars function to specifically define a set of hyperparameters. These can then be used just as before with the fit function, which will return a trained model that can be used for prediction on new data.</p>
<p><strong>7. It’s your turn!</strong></p>
<p>Great, now it’s your turn again!</p>
</section>
<section id="define-advanced-tuning-controls" class="level2" data-number="3.14">
<h2 data-number="3.14" class="anchored" data-anchor-id="define-advanced-tuning-controls"><span class="header-section-number">3.14</span> Define advanced tuning controls</h2>
<blockquote class="blockquote">
<h2 id="question-11" data-number="3.15" class="anchored"><span class="header-section-number">3.15</span> <em>Question</em></h2>
<p>How do you define a <strong>model-based / Bayesian</strong> hyperparameter optimization strategy?<br> <br> ⬜ <code>makeTuneControlCMAES</code><br> ⬜ <code>makeTuneControlGenSA</code><br> ⬜ <code>makeTuneControlIrace</code><br> ✅ <code>makeTuneControlMBO</code><br></p>
</blockquote>
<p>Correct! <code>makeTuneControlMBO</code> is for model-based / Bayesian optimization.</p>
</section>
<section id="define-aggregated-measures" class="level2" data-number="3.16">
<h2 data-number="3.16" class="anchored" data-anchor-id="define-aggregated-measures"><span class="header-section-number">3.16</span> Define aggregated measures</h2>
<!--
LO: The student knows how to define aggregated measures for hyperparameter optimization
-->
<p>Now, you are going to define <strong>performance measures</strong>. And the following code has also been run:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1"></a>task <span class="ot">&lt;-</span> <span class="fu">makeClassifTask</span>(<span class="at">data =</span> knowledge_train_data, </span>
<span id="cb113-2"><a href="#cb113-2"></a>                        <span class="at">target =</span> <span class="st">"UNS"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in makeTask(type = type, data = data, weights = weights, blocking =
#&gt; blocking, : Provided data is not a pure data.frame but from class spec_tbl_df,
#&gt; hence it will be converted.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1"></a>lrn <span class="ot">&lt;-</span> <span class="fu">makeLearner</span>(<span class="at">cl =</span> <span class="st">"classif.nnet"</span>, <span class="at">fix.factors.prediction =</span> <span class="cn">TRUE</span>)</span>
<span id="cb115-2"><a href="#cb115-2"></a></span>
<span id="cb115-3"><a href="#cb115-3"></a>param_set <span class="ot">&lt;-</span> <span class="fu">makeParamSet</span>(</span>
<span id="cb115-4"><a href="#cb115-4"></a>  <span class="fu">makeIntegerParam</span>(<span class="st">"size"</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="dv">5</span>),</span>
<span id="cb115-5"><a href="#cb115-5"></a>  <span class="fu">makeIntegerParam</span>(<span class="st">"maxit"</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="dv">300</span>),</span>
<span id="cb115-6"><a href="#cb115-6"></a>  <span class="fu">makeNumericParam</span>(<span class="st">"decay"</span>, <span class="at">lower =</span> <span class="fl">0.0001</span>, <span class="at">upper =</span> <span class="dv">1</span>)</span>
<span id="cb115-7"><a href="#cb115-7"></a>)</span>
<span id="cb115-8"><a href="#cb115-8"></a></span>
<span id="cb115-9"><a href="#cb115-9"></a>ctrl_random <span class="ot">&lt;-</span> <span class="fu">makeTuneControlRandom</span>(<span class="at">maxit =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Use the <code>setAggregation</code> function, which <strong>aggregates the standard deviation</strong> of performance metrics.</li>
<li>Apply <code>setAggregation</code> to the <strong>mean misclassification error</strong> and <strong>accuracy after resampling</strong>.</li>
<li><strong>Optimize</strong> your model by mean misclassification error. Remember that the <strong>first argument</strong> is used for optimization.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1"></a><span class="co"># Create holdout sampling</span></span>
<span id="cb116-2"><a href="#cb116-2"></a>holdout <span class="ot">&lt;-</span> <span class="fu">makeResampleDesc</span>(<span class="st">"Holdout"</span>, <span class="at">predict =</span> <span class="st">"both"</span>)</span>
<span id="cb116-3"><a href="#cb116-3"></a></span>
<span id="cb116-4"><a href="#cb116-4"></a><span class="co"># Perform tuning</span></span>
<span id="cb116-5"><a href="#cb116-5"></a>lrn_tune <span class="ot">&lt;-</span> <span class="fu">tuneParams</span>(<span class="at">learner =</span> lrn, </span>
<span id="cb116-6"><a href="#cb116-6"></a>                       <span class="at">task =</span> task, </span>
<span id="cb116-7"><a href="#cb116-7"></a>                       <span class="at">resampling =</span> holdout, </span>
<span id="cb116-8"><a href="#cb116-8"></a>                       <span class="at">control =</span> ctrl_random, </span>
<span id="cb116-9"><a href="#cb116-9"></a>                       <span class="at">par.set =</span> param_set,</span>
<span id="cb116-10"><a href="#cb116-10"></a>                       <span class="at">measures =</span> <span class="fu">list</span>(mmce, <span class="fu">setAggregation</span>(mmce, train.mean), acc, <span class="fu">setAggregation</span>(acc, train.mean)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune] Started tuning learner classif.nnet for parameter set:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt;          Type len Def      Constr Req Tunable Trafo
#&gt; size  integer   -   -      1 to 5   -    TRUE     -
#&gt; maxit integer   -   -    1 to 300   -    TRUE     -
#&gt; decay numeric   -   - 0.0001 to 1   -    TRUE     -</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; With control class: TuneControlRandom</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Imputation value: 1Imputation value: InfImputation value: -0Imputation value: Inf</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 1: size=3; maxit=206; decay=0.419</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  30
#&gt; initial  value 96.426127 
#&gt; iter  10 value 74.568100
#&gt; iter  20 value 73.100246
#&gt; iter  30 value 73.092719
#&gt; final  value 73.092692 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 1: mmce.test.mean=0.1500000,mmce.train.mean=0.2000000,acc.test.mean=0.8500000,acc.train.mean=0.8000000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 2: size=2; maxit=191; decay=0.71</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  21
#&gt; initial  value 102.263186 
#&gt; iter  10 value 83.097035
#&gt; iter  20 value 82.086385
#&gt; final  value 82.086252 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 2: mmce.test.mean=0.2750000,mmce.train.mean=0.3125000,acc.test.mean=0.7250000,acc.train.mean=0.6875000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 3: size=1; maxit=90; decay=0.0304</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  12
#&gt; initial  value 105.909403 
#&gt; iter  10 value 51.826666
#&gt; iter  20 value 32.162957
#&gt; iter  30 value 32.128445
#&gt; final  value 32.128442 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 3: mmce.test.mean=0.0000000,mmce.train.mean=0.0500000,acc.test.mean=1.0000000,acc.train.mean=0.9500000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 4: size=5; maxit=47; decay=0.505</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  48
#&gt; initial  value 92.605998 
#&gt; iter  10 value 76.947819
#&gt; iter  20 value 75.804846
#&gt; iter  30 value 75.776186
#&gt; iter  40 value 75.776082
#&gt; iter  40 value 75.776081
#&gt; iter  40 value 75.776081
#&gt; final  value 75.776081 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 4: mmce.test.mean=0.1750000,mmce.train.mean=0.2250000,acc.test.mean=0.8250000,acc.train.mean=0.7750000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 5: size=4; maxit=9; decay=0.28</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  39
#&gt; initial  value 97.059713 
#&gt; final  value 72.888407 
#&gt; stopped after 9 iterations</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 5: mmce.test.mean=0.4250000,mmce.train.mean=0.3625000,acc.test.mean=0.5750000,acc.train.mean=0.6375000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 6: size=3; maxit=242; decay=0.505</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  30
#&gt; initial  value 107.807702 
#&gt; iter  10 value 77.060303
#&gt; iter  20 value 76.549742
#&gt; iter  30 value 76.499535
#&gt; final  value 76.499522 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 6: mmce.test.mean=0.1750000,mmce.train.mean=0.2250000,acc.test.mean=0.8250000,acc.train.mean=0.7750000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 7: size=4; maxit=157; decay=0.145</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  39
#&gt; initial  value 106.929482 
#&gt; iter  10 value 65.860449
#&gt; iter  20 value 53.059710
#&gt; iter  30 value 50.568108
#&gt; iter  40 value 50.435407
#&gt; iter  50 value 50.411180
#&gt; iter  60 value 50.408403
#&gt; iter  70 value 50.407844
#&gt; final  value 50.407580 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 7: mmce.test.mean=0.0500000,mmce.train.mean=0.0750000,acc.test.mean=0.9500000,acc.train.mean=0.9250000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 8: size=5; maxit=106; decay=0.411</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  48
#&gt; initial  value 96.594375 
#&gt; iter  10 value 73.283384
#&gt; iter  20 value 71.943440
#&gt; iter  30 value 71.691969
#&gt; iter  40 value 71.687468
#&gt; final  value 71.687448 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 8: mmce.test.mean=0.1250000,mmce.train.mean=0.1875000,acc.test.mean=0.8750000,acc.train.mean=0.8125000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 9: size=5; maxit=269; decay=0.663</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  48
#&gt; initial  value 103.390387 
#&gt; iter  10 value 81.672960
#&gt; iter  20 value 80.373430
#&gt; iter  30 value 80.253895
#&gt; iter  40 value 80.244497
#&gt; final  value 80.244480 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 9: mmce.test.mean=0.2250000,mmce.train.mean=0.2875000,acc.test.mean=0.7750000,acc.train.mean=0.7125000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-x] 10: size=3; maxit=43; decay=0.863</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  30
#&gt; initial  value 96.237197 
#&gt; iter  10 value 85.598330
#&gt; iter  20 value 84.667091
#&gt; iter  30 value 84.651343
#&gt; final  value 84.651329 
#&gt; converged</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune-y] 10: mmce.test.mean=0.3000000,mmce.train.mean=0.3250000,acc.test.mean=0.7000000,acc.train.mean=0.6750000; time: 0.0 min</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; [Tune] Result: size=1; maxit=90; decay=0.0304 : mmce.test.mean=0.0000000,mmce.train.mean=0.0500000,acc.test.mean=1.0000000,acc.train.mean=0.9500000</code></pre>
</div>
</div>
<p>Very good! You know how to define aggregated measures for hyperparameter optimization.</p>
</section>
<section id="setting-hyperparameters" class="level2" data-number="3.17">
<h2 data-number="3.17" class="anchored" data-anchor-id="setting-hyperparameters"><span class="header-section-number">3.17</span> Setting hyperparameters</h2>
<!--
LO: The student knows how to set hyperparameters in mlr
-->
<p>And finally, you are going to set specific hyperparameters, which you might have found by examining your tuning results from before, The <code>knowledge_train_data</code> dataset has already been loaded for you, as have the packages <code>mlr</code> and <code>tidyverse</code>. And the following code has also been run:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1"></a>task <span class="ot">&lt;-</span> <span class="fu">makeClassifTask</span>(<span class="at">data =</span> knowledge_train_data, </span>
<span id="cb152-2"><a href="#cb152-2"></a>                        <span class="at">target =</span> <span class="st">"UNS"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Warning in makeTask(type = type, data = data, weights = weights, blocking =
#&gt; blocking, : Provided data is not a pure data.frame but from class spec_tbl_df,
#&gt; hence it will be converted.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1"></a>lrn <span class="ot">&lt;-</span> <span class="fu">makeLearner</span>(<span class="at">cl =</span> <span class="st">"classif.nnet"</span>, <span class="at">fix.factors.prediction =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Set the following hyperparameters for a neural net: One <strong>hidden layer</strong>, <strong>maximum number of iterations</strong> of 150 and 0 <strong>decay</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1"></a><span class="co"># Set hyperparameters</span></span>
<span id="cb155-2"><a href="#cb155-2"></a>lrn_best <span class="ot">&lt;-</span> <span class="fu">setHyperPars</span>(lrn, <span class="at">par.vals =</span> <span class="fu">list</span>(<span class="at">size =</span> <span class="dv">1</span>, </span>
<span id="cb155-3"><a href="#cb155-3"></a>                                              <span class="at">maxit =</span> <span class="dv">150</span>, </span>
<span id="cb155-4"><a href="#cb155-4"></a>                                              <span class="at">decay =</span> <span class="dv">0</span>))</span>
<span id="cb155-5"><a href="#cb155-5"></a></span>
<span id="cb155-6"><a href="#cb155-6"></a><span class="co"># Train model</span></span>
<span id="cb155-7"><a href="#cb155-7"></a>model_best <span class="ot">&lt;-</span> <span class="fu">train</span>(lrn_best, task)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # weights:  12
#&gt; initial  value 137.402550 
#&gt; iter  10 value 69.480261
#&gt; iter  20 value 33.034771
#&gt; iter  30 value 16.942039
#&gt; iter  40 value 13.941801
#&gt; iter  50 value 13.560082
#&gt; iter  60 value 13.358721
#&gt; iter  70 value 13.347518
#&gt; iter  80 value 13.155062
#&gt; iter  90 value 13.154252
#&gt; iter 100 value 13.143890
#&gt; iter 110 value 13.119652
#&gt; iter 120 value 13.119398
#&gt; iter 130 value 13.116703
#&gt; iter 140 value 13.115863
#&gt; iter 150 value 13.115290
#&gt; final  value 13.115290 
#&gt; stopped after 150 iterations</code></pre>
</div>
</div>
<p>Great! You know how to set specific hyperparameters in mlr.</p>
</section>
</section>
<section id="hyperparameter-tuning-with-h2o" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> 4. Hyperparameter tuning with h2o</h1>
<p>In this final chapter, you will use h2o, another package for machine learning with very convenient hyperparameter tuning functions. You will use it to train different models and define a Cartesian grid. Then, You will implement a Random Search use stopping criteria. Finally, you will learn AutoML, an h2o interface which allows for very fast and convenient model and hyperparameter tuning with just one function.</p>
<section id="machine-learning-with-h2o" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="machine-learning-with-h2o"><span class="header-section-number">4.1</span> Machine learning with h2o</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Machine learning with H2O</strong></p>
<p>In the previous chapters, you learned what hyperparameters are and how you can tune them with <code>caret</code> and <code>mlr</code>. There are many other popular machine learning packages and the one I want to present in this chapter is h2o.</p>
<p><strong>2. What is H2O?</strong></p>
<p>H2O is an open source machine learning platform you can use with R and the h2o package.What makes h2o special compared to caret and mlr is that h2o is designed for scalability. This means that the implementations of h2o’s machine learning algorithms can be trained on distributed clusters. That’s why you need to initiate an h2o cluster with the h2o.init() function - if you are not working on remote clusters, like Spark or Hadoop, you will initiate a local cluster on your machine.Another very useful feature of h2o is AutoML for automatic model comparison and hyperparameter tuning.</p>
<p><strong>3. New dataset: seeds data</strong></p>
<p>In this chapter, we will be working on a dataset with measurements of geometrical properties of wheat seed kernels. These measurements are area, perimeter, compactness, kernel length and width, asymmetry and kernel grove.We also know the seed type, that describes three different varieties of wheat. In this dataset we have 50 instances for each of the three seed varieties, denoted with 1, 2 and 3.</p>
<p><strong>4. Preparing the data for modeling with H2O</strong></p>
<p>Before we can start, we need to pass our data to the H2O instance. If you load data in from a file, you can directly load them as h2o frames, but often, we want to preprocess data with other R packages. In this case, we use the as.h2o function to convert an R object to an h2o frame.Next, it is good practice to define the names of the features (<code>x</code>) and the target variable (<code>y</code>). These names correspond to the column names of our dataset.In this dataset, the target seed type has been encoded numerically as 1, 2 and 3. Because we want to use the seed type for classification, we need to convert the target into factors.</p>
<p><strong>5. Training, validation and test sets</strong></p>
<p>h2o also contains a function for splitting data into training, validation and test sets.The h2o.splitframe function takes: - the h2o frame to split and- the ratios for how many instances should go into the created subsets (here, we want to have 70% of the data in the training set, 15% in the validation set and the remaining 15% in the test set.We can use the summary function on our response variable to compare the ratios in the different subsets. summary for h2o frames takes the additional exact_quantiles argument, which - if set to TRUE - computes exact quantiles. Per default, it use approximations.</p>
<p><strong>6. Model training with H2O</strong></p>
<p>h2o contains a number of different algorithms that can be trained in a distributed fashion:- Gradient Boosted models- Generalized linear models- Random Forest models- and Neural NetworksThese functions can take many arguments and hyperparameters. You can find them all by looking at the help for each function.</p>
<p><strong>7. Model training with H2O</strong></p>
<p>In this gradient boosting example, I am telling h2o which column in my data is the target ‘y’ and which features ‘x’ I want to include in the model. And I give the training and validation data.Here is the beginning of the model output with a summary of the final hyperparameters.</p>
<p><strong>8. Evaluate model performance with H2O</strong></p>
<p>h2o also includes functions for evaluating model performance. The h2o.performance function calculates a set of metrics on a new h2o frame, here we use the test data.We can use additional functions to extract components from this model metrics object, like the confusion matrix and logloss.If we want to use our model to generate predictions, we use the h2o(dot)predict function.</p>
<p><strong>9. Let’s practice!</strong></p>
<p>Let’s practice!</p>
</section>
<section id="prepare-data-for-modelling-with-h2o" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="prepare-data-for-modelling-with-h2o"><span class="header-section-number">4.2</span> Prepare data for modelling with h2o</h2>
<!--
LO: The student knows how to prepare data for machine learning with h2o
-->
<p>In order to train models with <code>h2o</code>, you need to <strong>prepare the data</strong> according to h2o’s specific needs. Here, you will go over a common data preparation workflow in <code>h2o</code>.</p>
<p>The <code>h2o</code> library has already been loaded for you, as has the <code>seeds_train_data</code> object.</p>
<p>This chapter uses functions that can take some time to run, so don’t be surprised if it takes a little longer than usual to submit your answer. On rare occurrences, you may get a server error. If this is the case, just reload the page.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Convert the <code>seeds_train_data</code> object to an <strong>H2O frame</strong>. Note, that you only need to give arguments that don’t have a default value in the function.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1"></a><span class="co"># Load data</span></span>
<span id="cb157-2"><a href="#cb157-2"></a>seeds_train_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/seeds_train_data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; Rows: 105 Columns: 8
#&gt; ── Column specification ────────────────────────────────────────────────────────
#&gt; Delimiter: ","
#&gt; dbl (8): area, perimeter, compactness, kernel_length, kernel_width, asymmetr...
#&gt; 
#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.
#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1"></a><span class="co"># Load package</span></span>
<span id="cb159-2"><a href="#cb159-2"></a><span class="fu">library</span>(h2o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>#&gt; 
#&gt; ----------------------------------------------------------------------
#&gt; 
#&gt; Your next step is to start H2O:
#&gt;     &gt; h2o.init()
#&gt; 
#&gt; For H2O package documentation, ask for help:
#&gt;     &gt; ??h2o
#&gt; 
#&gt; After starting H2O, you can use the Web UI at http://localhost:54321
#&gt; For more information visit https://docs.h2o.ai
#&gt; 
#&gt; ----------------------------------------------------------------------
#&gt; 
#&gt; 
#&gt; Attache Paket: 'h2o'
#&gt; 
#&gt; Die folgenden Objekte sind maskiert von 'package:stats':
#&gt; 
#&gt;     cor, sd, var
#&gt; 
#&gt; Die folgenden Objekte sind maskiert von 'package:base':
#&gt; 
#&gt;     %*%, %in%, &amp;&amp;, apply, as.factor, as.numeric, colnames, colnames&lt;-,
#&gt;     ifelse, is.character, is.factor, is.numeric, log, log10, log1p,
#&gt;     log2, round, signif, trunc, ||</code></pre>
</div>
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1"></a><span class="co"># Initialise h2o cluster</span></span>
<span id="cb161-2"><a href="#cb161-2"></a><span class="fu">h2o.init</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; H2O is not running yet, starting it now...
#&gt; 
#&gt; Note:  In case of errors look at the following log files:
#&gt;     /var/folders/_t/jtmqlfxs6lz2jk_9ndj50w1r0000gp/T//RtmpAL0Cle/file9b59699745ea/h2o_jschwarz_started_from_r.out
#&gt;     /var/folders/_t/jtmqlfxs6lz2jk_9ndj50w1r0000gp/T//RtmpAL0Cle/file9b594a6f3bce/h2o_jschwarz_started_from_r.err
#&gt; 
#&gt; 
#&gt; Starting H2O JVM and connecting: ... Connection successful!
#&gt; 
#&gt; R is connected to the H2O cluster: 
#&gt;     H2O cluster uptime:         2 seconds 730 milliseconds 
#&gt;     H2O cluster timezone:       Europe/Berlin 
#&gt;     H2O data parsing timezone:  UTC 
#&gt;     H2O cluster version:        3.38.0.1 
#&gt;     H2O cluster version age:    2 months and 19 days  
#&gt;     H2O cluster name:           H2O_started_from_R_jschwarz_gnh329 
#&gt;     H2O cluster total nodes:    1 
#&gt;     H2O cluster total memory:   16.00 GB 
#&gt;     H2O cluster total cores:    16 
#&gt;     H2O cluster allowed cores:  16 
#&gt;     H2O cluster healthy:        TRUE 
#&gt;     H2O Connection ip:          localhost 
#&gt;     H2O Connection port:        54321 
#&gt;     H2O Connection proxy:       NA 
#&gt;     H2O Internal Security:      FALSE 
#&gt;     R Version:                  R version 4.2.2 (2022-10-31)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1"></a><span class="co"># Convert data to h2o frame</span></span>
<span id="cb163-2"><a href="#cb163-2"></a>seeds_train_data_hf <span class="ot">&lt;-</span> <span class="fu">as.h2o</span>(seeds_train_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Define <strong>vectors of target</strong> (<code>y</code>) and <strong>feature names</strong> (<code>x</code>).</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1"></a><span class="co"># Identify target and features</span></span>
<span id="cb165-2"><a href="#cb165-2"></a>y <span class="ot">&lt;-</span> <span class="st">"seed_type"</span></span>
<span id="cb165-3"><a href="#cb165-3"></a>x <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="fu">colnames</span>(seeds_train_data_hf), y)</span>
<span id="cb165-4"><a href="#cb165-4"></a></span>
<span id="cb165-5"><a href="#cb165-5"></a>seeds_train_data_hf[, y] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(seeds_train_data_hf[, y])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Split the <code>seeds_train_data_hf</code> into a <strong>training</strong> and <strong>validation</strong> set and use the default proportion of 75% instances in one set, 25% in the other.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb166-1"><a href="#cb166-1"></a><span class="co"># Split data into train &amp; validation sets</span></span>
<span id="cb166-2"><a href="#cb166-2"></a>sframe <span class="ot">&lt;-</span> <span class="fu">h2o.splitFrame</span>(seeds_train_data_hf, <span class="at">seed =</span> <span class="dv">42</span>)</span>
<span id="cb166-3"><a href="#cb166-3"></a>train <span class="ot">&lt;-</span> sframe[[<span class="dv">1</span>]]</span>
<span id="cb166-4"><a href="#cb166-4"></a>valid <span class="ot">&lt;-</span> sframe[[<span class="dv">2</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Calculate the <strong>ratio of the target variable</strong> in the training set using <strong>exact quantiles</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb167"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1"></a><span class="co"># Calculate ratios of the target variable</span></span>
<span id="cb167-2"><a href="#cb167-2"></a><span class="fu">summary</span>(train<span class="sc">$</span>seed_type, <span class="at">exact_quantiles =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;  seed_type
#&gt;  2:30     
#&gt;  1:26     
#&gt;  3:26</code></pre>
</div>
</div>
<p>Good job! You successfully prepared data for machine learning with h2o.</p>
</section>
<section id="modeling-with-h2o" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="modeling-with-h2o"><span class="header-section-number">4.3</span> Modeling with h2o</h2>
<!--
LO: The student knows how to train machine learning models with h2o
-->
<p>In the last exercise, you successfully prepared data for modeling with h2o. Now, you can use this data to <strong>train a model</strong>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Train a <strong>Random Forest</strong> model with <code>h2o</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb169-1"><a href="#cb169-1"></a><span class="co"># Train random forest model</span></span>
<span id="cb169-2"><a href="#cb169-2"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">h2o.randomForest</span>(<span class="at">x =</span> x,</span>
<span id="cb169-3"><a href="#cb169-3"></a>                             <span class="at">y =</span> y,</span>
<span id="cb169-4"><a href="#cb169-4"></a>                             <span class="at">training_frame =</span> train,</span>
<span id="cb169-5"><a href="#cb169-5"></a>                             <span class="at">validation_frame =</span> valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Calculate <strong>model performance</strong> of the Random Forest model on the <strong>validation</strong> data.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb171-1"><a href="#cb171-1"></a><span class="co"># Calculate model performance</span></span>
<span id="cb171-2"><a href="#cb171-2"></a>perf <span class="ot">&lt;-</span> <span class="fu">h2o.performance</span>(rf_model, <span class="at">valid =</span> <span class="cn">TRUE</span>)</span>
<span id="cb171-3"><a href="#cb171-3"></a>perf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; H2OMultinomialMetrics: drf
#&gt; ** Reported on validation data. **
#&gt; 
#&gt; Validation Set Metrics: 
#&gt; =====================
#&gt; 
#&gt; Extract validation frame with `h2o.getFrame("RTMP_sid_9c1e_9")`
#&gt; MSE: (Extract with `h2o.mse`) 0.04930212
#&gt; RMSE: (Extract with `h2o.rmse`) 0.2220408
#&gt; Logloss: (Extract with `h2o.logloss`) 0.1522815
#&gt; Mean Per-Class Error: 0.07407407
#&gt; AUC: (Extract with `h2o.auc`) NaN
#&gt; AUCPR: (Extract with `h2o.aucpr`) NaN
#&gt; R^2: (Extract with `h2o.r2`) 0.9370028
#&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`)
#&gt; =========================================================================
#&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
#&gt;        1 2 3  Error     Rate
#&gt; 1      8 0 1 0.1111 =  1 / 9
#&gt; 2      0 5 0 0.0000 =  0 / 5
#&gt; 3      1 0 8 0.1111 =  1 / 9
#&gt; Totals 9 5 9 0.0870 = 2 / 23
#&gt; 
#&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)`
#&gt; =======================================================================
#&gt; Top-3 Hit Ratios: 
#&gt;   k hit_ratio
#&gt; 1 1  0.913044
#&gt; 2 2  1.000000
#&gt; 3 3  1.000000</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Extract the <strong>confusion matrix</strong> from the model performance object.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb173-1"><a href="#cb173-1"></a><span class="co"># Extract confusion matrix</span></span>
<span id="cb173-2"><a href="#cb173-2"></a><span class="fu">h2o.confusionMatrix</span>(perf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["1"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["2"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["3"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Error"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Rate"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"8","2":"0","3":"1","4":"0.11111111","5":"1 / 9","_rn_":"1"},{"1":"0","2":"5","3":"0","4":"0.00000000","5":"0 / 5","_rn_":"2"},{"1":"1","2":"0","3":"8","4":"0.11111111","5":"1 / 9","_rn_":"3"},{"1":"9","2":"5","3":"9","4":"0.08695652","5":"2 / 23","_rn_":"Totals"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<ol start="4" type="1">
<li>Extract the <code>logloss</code> of the model performance object.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb174-1"><a href="#cb174-1"></a><span class="co"># Extract logloss</span></span>
<span id="cb174-2"><a href="#cb174-2"></a><span class="fu">h2o.logloss</span>(perf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; [1] 0.1522815</code></pre>
</div>
</div>
<p>Great! You know how to train a random forest model with h2o.</p>
</section>
<section id="grid-and-random-search-with-h2o" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="grid-and-random-search-with-h2o"><span class="header-section-number">4.4</span> Grid and random search with h2o</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Grid and random search with H2O</strong></p>
<p>As caret and mlr, h2o supports Cartesian grid search and random search.</p>
<p><strong>2. Hyperparameters in H2O models</strong></p>
<p>We can find an overview of all hyperparameters in the help for each model function. A few of the hyperparameters for gradient boosting models are- the number of trees- maximum tree depth- fewest allowed observations in a leaf and- learning rate, optionally with scaling</p>
<p><strong>3. Preparing our data for modeling with H2O</strong></p>
<p>Before we define hyperparameter grids, let’s briefly go over how we prepared our data:- we converted the data to an h2o frame- defined features and target and- split into training, validation &amp; test data</p>
<p><strong>4. Defining a hyperparameter grid</strong></p>
<p>For cartesian grid search, we define a list of values for the hyperparameters, we want to tune.This list, can be fed into the h2o(dot)grid function as input to the hyper_params argument.Additional arguments to give h2o(doc)grid can be- the algorithm, here gbm- a model id,- features and target- training and validation data- a seed for random number generationh2o will now train a model for every possible combination of hyperparameters from our grid.After training is complete, we can look at the tuning results with the h2o(dot)getGrid function.</p>
<p><strong>5. Examining a grid object</strong></p>
<p>To examine the tuning results, h2o(dot)getGrid uses the grid id which we defined in the h2o.grid function before. We also give a metric and the order by which to sort the different models.The summary of this grid object tells us which hyperparameters were tuned, how many models were trained (and how many failed).</p>
<p><strong>6. Extracting the best model from a grid</strong></p>
<p>All 27 models in our grid are given a unique model id, which we can use to extract any of the models with the h2o(dot)getModel function.Usually, we will want to extract the best model. Because we sorted by decreasing accuracy, the first model in the sorted table returned by get.grid will have the highest accuracy.Model summary will give us an overview of the hyperparameters used.</p>
<p><strong>7. Extracting the best model from a grid</strong></p>
<p>The extracted best model can now be treated as a regular h2o model. We can for example use the h2o(dot)performance function to evaluate the performance on test data.</p>
<p><strong>8. Random search with H2O</strong></p>
<p>Random search in h2o also takes a list of hyperparameter values, just as the grid search example before. In contrast, we now don’t want to train models for every possible combination of our defined grid, but only for a randomly sampled subset of our hyperparameters.Thus, we need an additional list that defines the search criterium “random discrete”. We have several options for controlling how the random search is performed- set maximum number of models to train with max_models- define stopping metric, rounds and tolerance- or set the maximum run time, as here with 60 secondsThis search criteria object can then be passed onto the search_criteria argument of h2o(dot)grid.</p>
<p><strong>9. Stopping criteria</strong></p>
<p>Instead of defining the maximum run time or the maximum number of model to train during random search, we can define early stopping criteria.Early stopping is calculated on the validation data and works with three arguments:- stopping_metric- stopping_rounds and - stopping_tolerance.If the performance metric doesn’t improve for a number of consecutive rounds by at least the stopping tolerance, model training will stop. In this example here, training will stop if 6 times in a row, the mean per class error does not improve by at least 0.0001.For stopping metric, we can choose between several options, like- mean residual deviance (the default for regression tasks)- logloss (the default for classification tasks)- mean squared error MSE, etc.The remaining training code and model output looks just like before.</p>
<p><strong>10. Time to practice!</strong></p>
<p>Time to practice!</p>
</section>
<section id="grid-search-with-h2o" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="grid-search-with-h2o"><span class="header-section-number">4.5</span> Grid search with h2o</h2>
<!--
LO: The student knows how to use grid search for model tuning with h2o
-->
<p>Now that you successfully trained a Random Forest model with <code>h2o</code>, you can apply the same concepts to training all other algorithms, like <strong>Deep Learning</strong>. In this exercise, you are going to apply <strong>a grid search</strong> to tune a model. The <code>h2o</code> library has already been loaded and initialized for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Start defining a <strong>grid of hyperparameters for deep learning</strong> with <code>h2o</code>: for <strong>learning rate</strong> use the values 0.001, 0.005 and 0.01. For an overview of all hyperparameters to use, go to the <strong>help</strong> for <code>h2o.deeplearning</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1"></a><span class="co"># Define hyperparameters</span></span>
<span id="cb176-2"><a href="#cb176-2"></a>dl_params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">rate =</span> <span class="fu">c</span>(<span class="fl">0.001</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Add values for the <strong>number of training iterations</strong> of 5, 10 and 15 to the tuning grid.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb177"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb177-1"><a href="#cb177-1"></a><span class="co"># Define hyperparameters</span></span>
<span id="cb177-2"><a href="#cb177-2"></a>dl_params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">epochs =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>),</span>
<span id="cb177-3"><a href="#cb177-3"></a>                  <span class="at">rate =</span> <span class="fu">c</span>(<span class="fl">0.001</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>And finally, add two options for the <strong>number nodes in two hidden layers</strong>: 50 &amp; 50 and 100 &amp; 100. Note that the argument takes <strong>two values</strong> here, so you need to create a <strong>list within the list</strong> for the two hidden layers of one option.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb178-1"><a href="#cb178-1"></a><span class="co"># Define hyperparameters</span></span>
<span id="cb178-2"><a href="#cb178-2"></a>dl_params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">hidden =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">50</span>), <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">100</span>)),</span>
<span id="cb178-3"><a href="#cb178-3"></a>                  <span class="at">epochs =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>),</span>
<span id="cb178-4"><a href="#cb178-4"></a>                  <span class="at">rate =</span> <span class="fu">c</span>(<span class="fl">0.001</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<h2 id="question-12" data-number="4.6" class="anchored"><span class="header-section-number">4.6</span> <em>Question</em></h2>
<p>Which <strong>argument</strong> of the <code>h2o.grid()</code> function takes the hyperparameter grid as input?<br> <br> ✅ <code>hyper_params</code><br> ⬜ <code>algorithm</code><br> ⬜ <code>is_supervised</code><br> ⬜ <code>do_hyper_params_check</code><br></p>
</blockquote>
<p>Correct! You know how to use grid search for tuning a deep learning model with h2o.</p>
</section>
<section id="random-search-with-h2o" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="random-search-with-h2o"><span class="header-section-number">4.7</span> Random search with h2o</h2>
<!--
LO: The student knows how to use random search for model tuning with h2o
-->
<p>Next, you will use <strong>random search</strong>.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define a <strong>search criteria</strong> object that defines <strong>random search</strong> with a <strong>maximum runtime of 10 seconds</strong>.</li>
<li>Add this search criteria object at the appropriate place in the <code>h2o.grid</code> function to <strong>train the random models</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb179"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1"></a><span class="co"># Define search criteria</span></span>
<span id="cb179-2"><a href="#cb179-2"></a>search_criteria <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">strategy =</span> <span class="st">"RandomDiscrete"</span>, </span>
<span id="cb179-3"><a href="#cb179-3"></a>                        <span class="at">max_runtime_secs =</span> <span class="dv">10</span>, <span class="co"># this is way too short &amp; only used to keep runtime short!</span></span>
<span id="cb179-4"><a href="#cb179-4"></a>                        <span class="at">seed =</span> <span class="dv">42</span>)</span>
<span id="cb179-5"><a href="#cb179-5"></a></span>
<span id="cb179-6"><a href="#cb179-6"></a><span class="co"># Train with random search</span></span>
<span id="cb179-7"><a href="#cb179-7"></a>dl_grid <span class="ot">&lt;-</span> <span class="fu">h2o.grid</span>(<span class="st">"deeplearning"</span>, </span>
<span id="cb179-8"><a href="#cb179-8"></a>                    <span class="at">grid_id =</span> <span class="st">"dl_grid"</span>,</span>
<span id="cb179-9"><a href="#cb179-9"></a>                    <span class="at">x =</span> x, </span>
<span id="cb179-10"><a href="#cb179-10"></a>                    <span class="at">y =</span> y,</span>
<span id="cb179-11"><a href="#cb179-11"></a>                    <span class="at">training_frame =</span> train,</span>
<span id="cb179-12"><a href="#cb179-12"></a>                    <span class="at">validation_frame =</span> valid,</span>
<span id="cb179-13"><a href="#cb179-13"></a>                    <span class="at">seed =</span> <span class="dv">42</span>,</span>
<span id="cb179-14"><a href="#cb179-14"></a>                    <span class="at">hyper_params =</span> dl_params,</span>
<span id="cb179-15"><a href="#cb179-15"></a>                    <span class="at">search_criteria =</span> search_criteria)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=======                                                               |  10%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |===============================================================       |  91%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<p>Nice! You know how to use random search for tuning a deep learning model with h2o.</p>
</section>
<section id="stopping-criteria" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="stopping-criteria"><span class="header-section-number">4.8</span> Stopping criteria</h2>
<!--
LO: The student knows how to use stopping criteria in random search for model tuning with h2o
-->
<p>In random search, you can also define <strong>stopping criteria</strong> instead of a maximum runtime. The <code>h2o</code> library and <code>seeds_train_data</code> has already been loaded and initialized for you.</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Modify the hyperparameter search by <strong>adding misclassification rate</strong> as the <strong>stopping metric</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb181"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1"></a><span class="co"># Define early stopping</span></span>
<span id="cb181-2"><a href="#cb181-2"></a>stopping_params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">strategy =</span> <span class="st">"RandomDiscrete"</span>, </span>
<span id="cb181-3"><a href="#cb181-3"></a>                        <span class="at">stopping_metric =</span> <span class="st">"misclassification"</span>,</span>
<span id="cb181-4"><a href="#cb181-4"></a>                        <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Add <strong>3 stopping rounds</strong> to the search criteria object.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb182-1"><a href="#cb182-1"></a><span class="co"># Define early stopping</span></span>
<span id="cb182-2"><a href="#cb182-2"></a>stopping_params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">strategy =</span> <span class="st">"RandomDiscrete"</span>, </span>
<span id="cb182-3"><a href="#cb182-3"></a>                        <span class="at">stopping_metric =</span> <span class="st">"misclassification"</span>,</span>
<span id="cb182-4"><a href="#cb182-4"></a>                        <span class="at">stopping_rounds =</span> <span class="dv">3</span>, </span>
<span id="cb182-5"><a href="#cb182-5"></a>                        <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>And finally, add a <strong>stopping tolerance</strong> of 0.1 to the search criteria..</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1"></a><span class="co"># Define early stopping</span></span>
<span id="cb183-2"><a href="#cb183-2"></a>stopping_params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">strategy =</span> <span class="st">"RandomDiscrete"</span>, </span>
<span id="cb183-3"><a href="#cb183-3"></a>                        <span class="at">stopping_metric =</span> <span class="st">"misclassification"</span>,</span>
<span id="cb183-4"><a href="#cb183-4"></a>                        <span class="at">stopping_rounds =</span> <span class="dv">2</span>, </span>
<span id="cb183-5"><a href="#cb183-5"></a>                        <span class="at">stopping_tolerance =</span> <span class="fl">0.1</span>,</span>
<span id="cb183-6"><a href="#cb183-6"></a>                        <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<h2 id="question-13" data-number="4.9" class="anchored"><span class="header-section-number">4.9</span> <em>Question</em></h2>
<p>Which <strong>argument</strong> of the <code>h2o.grid()</code> function takes the stopping criteria as input?<br> <br> ⬜ <code>algorithm</code><br> ⬜ <code>grid_id</code><br> ⬜ <code>strategy</code><br> ✅ <code>search_criteria</code><br></p>
</blockquote>
<p>Correct! You know how to use stopping criteria in random search.</p>
</section>
<section id="automatic-machine-learning-with-h2o" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="automatic-machine-learning-with-h2o"><span class="header-section-number">4.10</span> Automatic machine learning with H2O</h2>
<p>Theory. Coming soon …</p>
<p><strong>1. Automatic machine learning with H2O</strong></p>
<p>The final and most convenient function for advanced model tuning is h2o’s automatic machine learning functionality.</p>
<p><strong>2. Automatic Machine Learning (AutoML)</strong></p>
<p>Automatic Machine Learning goes one step beyond regular hyperparameter tuning: instead of tuning only one model type or algorithm, AutoML performs tuning for a number of different algorithms, as well as hyperparameters.AutoML makes finding the best (or almost best) model extremely fast and easybecause it is all combined in a single function, that only needs a dataset, the target in case of classification and a time or model number limit that tells it how long to train models for.</p>
<p><strong>3. AutoML in H2O</strong></p>
<p>AutoML trains a number of different algorithms during a default classification run in this specific order:- 1 generalized linear model- 1 distributed random forest- 1 extremely randomized tree- 3 XGBoost- 5 gradient boosting machines- a Neural Net, a random grid of XGBoost, a random grid of GBMs, a random grid of Neural Nets and- 2 Stacked EnsemblesOne of the 2 ensembles is calculated from all models, the other only from the best models of each family of algorithms.In case you want to exclude algorithms, you can define a list of these algorithms in your automl run. In some cases, it is recommended to exclude tree-based algorithms.</p>
<p><strong>4. Hyperparameter tuning in H2O’s AutoML</strong></p>
<p>For all algorithms where multiple models are run, automl automatically tests a range of hyperparameters for different arguments.These are some of the hyperparameters for gradient boosting modelsand for neural nets.XGBoost hyperparameters are similar to GBM.You can find out more about each hyperparameter by going to the help for the original h2o model functions. For gradient boosting that would be h2o.gbm.Random Forest and Extremely Randomized Trees are not grid searched because only one model is trained for each.</p>
<p><strong>5. Using AutoML with H2O</strong></p>
<p>Here you see the h2o.automl function in action.automl uses the same arguments as regular h2o algorithms: x, y, training_frame, validation_frame, etc.As before with random search, AutoML needs stopping criteria. These can either be the maximum run time (as here 60 seconds), which will define the time spent on grid searches or the maximum number of models. We could also give both and automl will stop when it reaches either of the criteria.Note, that training the stacked ensembles doesn’t count to the maximum run time, nor the maximum number of models to train, they will always be calculated at the end.With sort_metric, we define how the models should be sorted in the final output, called the leaderboard. The best model according to this metric, here logloss, will be on position 1 in the leaderboard. You can choose- area under the curve (default for binary classification)- mean per class error (default for multinomial classification)- mean residual deviance (default for regression)and more.As always, the help functions will give you additional details.</p>
<p><strong>6. Viewing the AutoML leaderboard</strong></p>
<p>This is how the leaderboard from the previous automl run looks like. You can extract it by calling automl_model@leaderboard.The leaderboard contains all models that were trained, as well as the model id and performance metrics, like - mean_per_class_error- logloss- root mean squared error- or mean squared errorThe second column of the leaderboard shows the metric that was used for ranking, in our case mean per class error, as specified in the automl function.If you don’t specify a leaderboard dataset, metrics will be calculated on 5-fold cross-validation results.Follow this link to read the complete documentation.</p>
<p><strong>7. Extracting models from AutoML leaderboard</strong></p>
<p>You can extract individual models from automl (usually, the best model) via their model_id, which you find the in the leaderboard.Models you extracted can again be treated just as you would any other h2o model, e.g.&nbsp;for predictions.</p>
<p><strong>8. Get ready for your last round of exercises!</strong></p>
<p>Get ready for your last round of exercises!</p>
</section>
<section id="automl-in-h2o" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="automl-in-h2o"><span class="header-section-number">4.11</span> AutoML in h2o</h2>
<!--
LO: The student knows how to perform automatic machine learning with h2o
-->
<p>A very convenient functionality of <code>h2o</code> is automatic machine learning (<strong>AutoML</strong>).</p>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Define an <strong>automatic machine learning training</strong> with <strong>maximum runtime</strong> of 10 seconds. Note: 10 seconds is of course not enough for “real-life” but training for hours would take too long for the purpose of this exercise otherwise.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb184-1"><a href="#cb184-1"></a><span class="co"># Run automatic machine learning</span></span>
<span id="cb184-2"><a href="#cb184-2"></a>automl_model <span class="ot">&lt;-</span> <span class="fu">h2o.automl</span>(<span class="at">x =</span> x, </span>
<span id="cb184-3"><a href="#cb184-3"></a>                           <span class="at">y =</span> y,</span>
<span id="cb184-4"><a href="#cb184-4"></a>                           <span class="at">training_frame =</span> train,</span>
<span id="cb184-5"><a href="#cb184-5"></a>                           <span class="at">max_runtime_secs =</span> <span class="dv">10</span>,</span>
<span id="cb184-6"><a href="#cb184-6"></a>                           <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=========                                                             |  13%
  |                                                                            
  |=======================                                               |  33%
  |                                                                            
  |======================================                                |  54%
  |                                                                            
  |====================================================                  |  74%
  |                                                                            
  |==================================================================    |  94%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Use <strong>mean per class error</strong> as the sorting metric in the leaderboard.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb186-1"><a href="#cb186-1"></a><span class="co"># Run automatic machine learning</span></span>
<span id="cb186-2"><a href="#cb186-2"></a>automl_model <span class="ot">&lt;-</span> <span class="fu">h2o.automl</span>(<span class="at">x =</span> x, </span>
<span id="cb186-3"><a href="#cb186-3"></a>                           <span class="at">y =</span> y,</span>
<span id="cb186-4"><a href="#cb186-4"></a>                           <span class="at">training_frame =</span> train,</span>
<span id="cb186-5"><a href="#cb186-5"></a>                           <span class="at">max_runtime_secs =</span> <span class="dv">10</span>,</span>
<span id="cb186-6"><a href="#cb186-6"></a>                           <span class="at">sort_metric =</span> <span class="st">"mean_per_class_error"</span>,</span>
<span id="cb186-7"><a href="#cb186-7"></a>                           <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=======                                                               |  10%
#&gt; 11:21:55.384: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 82.0.
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |===================================                                   |  51%
  |                                                                            
  |==================================================                    |  71%
  |                                                                            
  |================================================================      |  91%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Have the leaderboard be calculated on <strong>3-fold cross-validated data</strong>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb188-1"><a href="#cb188-1"></a><span class="co"># Run automatic machine learning</span></span>
<span id="cb188-2"><a href="#cb188-2"></a>automl_model <span class="ot">&lt;-</span> <span class="fu">h2o.automl</span>(<span class="at">x =</span> x, </span>
<span id="cb188-3"><a href="#cb188-3"></a>                           <span class="at">y =</span> y,</span>
<span id="cb188-4"><a href="#cb188-4"></a>                           <span class="at">training_frame =</span> train,</span>
<span id="cb188-5"><a href="#cb188-5"></a>                           <span class="at">max_runtime_secs =</span> <span class="dv">10</span>,</span>
<span id="cb188-6"><a href="#cb188-6"></a>                           <span class="at">sort_metric =</span> <span class="st">"mean_per_class_error"</span>,</span>
<span id="cb188-7"><a href="#cb188-7"></a>                           <span class="at">nfolds =</span> <span class="dv">3</span>,</span>
<span id="cb188-8"><a href="#cb188-8"></a>                           <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=======                                                               |  10%
#&gt; 11:22:10.648: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 82.0.
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |===================================                                   |  51%
  |                                                                            
  |==================================================                    |  71%
  |                                                                            
  |================================================================      |  91%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Have the <strong>leaderboard be calculated based on the validation data</strong> instead of on cross-validation results.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb190-1"><a href="#cb190-1"></a><span class="co"># Run automatic machine learning</span></span>
<span id="cb190-2"><a href="#cb190-2"></a>automl_model <span class="ot">&lt;-</span> <span class="fu">h2o.automl</span>(<span class="at">x =</span> x, </span>
<span id="cb190-3"><a href="#cb190-3"></a>                           <span class="at">y =</span> y,</span>
<span id="cb190-4"><a href="#cb190-4"></a>                           <span class="at">training_frame =</span> train,</span>
<span id="cb190-5"><a href="#cb190-5"></a>                           <span class="at">max_runtime_secs =</span> <span class="dv">10</span>,</span>
<span id="cb190-6"><a href="#cb190-6"></a>                           <span class="at">sort_metric =</span> <span class="st">"mean_per_class_error"</span>,</span>
<span id="cb190-7"><a href="#cb190-7"></a>                           <span class="at">leaderboard_frame =</span> valid,</span>
<span id="cb190-8"><a href="#cb190-8"></a>                           <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=======                                                               |  10%
#&gt; 11:22:26.222: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 82.0.
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |===================================                                   |  51%
  |                                                                            
  |==================================================                    |  71%
  |                                                                            
  |================================================================      |  91%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<p>Very good! You know how to use AutoML to perform automatic machine learning with h2o.</p>
</section>
<section id="scoring-the-leaderboard" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="scoring-the-leaderboard"><span class="header-section-number">4.12</span> Scoring the leaderboard</h2>
<p>H2O’s AutoML function returns the model results in the so called <strong>leaderboard</strong>.</p>
<blockquote class="blockquote">
<p><em>Question</em> <strong>Without cross-validation</strong>, which datasets do you have to define if you want to: <br> * use the <strong>training data</strong> as is * use half of the validation data for <strong>validation</strong> * and use the other half of the validation data for <strong>scoring the leaderboard</strong> <br> ⬜ <code>training_frame</code><br> ⬜ <code>training_frame</code> + <code>leaderboard_frame</code><br> ✅ <code>training_frame</code> + <code>validation_frame</code><br> ⬜ <code>training_frame</code> + <code>validation_frame</code> + <code>leaderboard_frame</code><br></p>
</blockquote>
<p>Correct! With <code>training_frame</code> + <code>validation_frame</code>, training data is used as is, validation set is split 50/50 into validation and leaderboard data.</p>
</section>
<section id="extract-h2o-models-and-evaluate-performance" class="level2" data-number="4.13">
<h2 data-number="4.13" class="anchored" data-anchor-id="extract-h2o-models-and-evaluate-performance"><span class="header-section-number">4.13</span> Extract h2o models and evaluate performance</h2>
<!--
LO: Student knows how to extract the best model from AutoML
-->
<p>In this final exercise, you will extract the <strong>best model</strong> from the <strong>AutoML leaderboard</strong>. The <code>h2o</code> library and <code>test</code> (???) data has been loaded and the following code has been run:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb192"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1"></a>automl_model <span class="ot">&lt;-</span> <span class="fu">h2o.automl</span>(<span class="at">x =</span> x, </span>
<span id="cb192-2"><a href="#cb192-2"></a>                           <span class="at">y =</span> y,</span>
<span id="cb192-3"><a href="#cb192-3"></a>                           <span class="at">training_frame =</span> train,</span>
<span id="cb192-4"><a href="#cb192-4"></a>                           <span class="at">nfolds =</span> <span class="dv">3</span>,</span>
<span id="cb192-5"><a href="#cb192-5"></a>                           <span class="at">max_runtime_secs =</span> <span class="dv">60</span>,</span>
<span id="cb192-6"><a href="#cb192-6"></a>                           <span class="at">sort_metric =</span> <span class="st">"mean_per_class_error"</span>,</span>
<span id="cb192-7"><a href="#cb192-7"></a>                           <span class="at">seed =</span> <span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |========                                                              |  11%
#&gt; 11:22:40.880: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 82.0.
  |                                                                            
  |=================                                                     |  24%
  |                                                                            
  |==================                                                    |  26%
  |                                                                            
  |===================                                                   |  27%
  |                                                                            
  |====================                                                  |  28%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |=====================                                                 |  31%
  |                                                                            
  |======================                                                |  32%
  |                                                                            
  |=======================                                               |  33%
  |                                                                            
  |========================                                              |  34%
  |                                                                            
  |=========================                                             |  36%
  |                                                                            
  |===========================                                           |  39%
  |                                                                            
  |==============================                                        |  43%
  |                                                                            
  |================================                                      |  46%
  |                                                                            
  |===================================                                   |  49%
  |                                                                            
  |=====================================                                 |  53%
  |                                                                            
  |=======================================                               |  56%
  |                                                                            
  |==========================================                            |  60%
  |                                                                            
  |============================================                          |  63%
  |                                                                            
  |===============================================                       |  67%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |===================================================                   |  73%
  |                                                                            
  |======================================================                |  77%
  |                                                                            
  |========================================================              |  80%
  |                                                                            
  |===========================================================           |  84%
  |                                                                            
  |=============================================================         |  87%
  |                                                                            
  |================================================================      |  91%
  |                                                                            
  |==================================================================    |  94%
  |                                                                            
  |===================================================================== |  98%
  |                                                                            
  |======================================================================| 100%</code></pre>
</div>
</div>
<p><strong>Steps</strong></p>
<ol type="1">
<li>Extract the <strong>leaderboard</strong> from the AutoML output. Note, that you can extract subsets of objects not only with <code>$</code> but also with <code>@</code>.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb194"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb194-1"><a href="#cb194-1"></a><span class="co"># Extract the leaderboard</span></span>
<span id="cb194-2"><a href="#cb194-2"></a>lb <span class="ot">&lt;-</span> automl_model<span class="sc">@</span>leaderboard</span>
<span id="cb194-3"><a href="#cb194-3"></a><span class="fu">head</span>(lb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">

<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["model_id"],"name":[1],"type":["chr"],"align":["left"]},{"label":["mean_per_class_error"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["logloss"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["rmse"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["mse"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"DeepLearning_grid_2_AutoML_5_20221209_112240_model_1","2":"0.02564103","3":"0.1528640","4":"0.1597345","5":"0.02551511","_rn_":"1"},{"1":"XGBoost_grid_1_AutoML_5_20221209_112240_model_234","2":"0.02564103","3":"0.0995590","4":"0.1477110","5":"0.02181855","_rn_":"2"},{"1":"XGBoost_grid_1_AutoML_5_20221209_112240_model_140","2":"0.02564103","3":"0.1931445","4":"0.2004725","5":"0.04018923","_rn_":"3"},{"1":"XGBoost_grid_1_AutoML_5_20221209_112240_model_108","2":"0.02564103","3":"0.2116388","4":"0.2193494","5":"0.04811416","_rn_":"4"},{"1":"GBM_grid_1_AutoML_5_20221209_112240_model_4","2":"0.02564103","3":"0.0760121","4":"0.1459501","5":"0.02130143","_rn_":"5"},{"1":"StackedEnsemble_BestOfFamily_5_AutoML_5_20221209_112240","2":"0.02564103","3":"0.2734855","4":"0.2646685","5":"0.07004942","_rn_":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Assign the <strong>best model</strong> from the leaderboard the name <code>aml_leader</code> and examine it with the <code>summary()</code> function. Remember, that you can also use the @ sign to access parts of the <code>automl_model</code> object.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb195-1"><a href="#cb195-1"></a><span class="co"># Assign best model new object name</span></span>
<span id="cb195-2"><a href="#cb195-2"></a>aml_leader <span class="ot">&lt;-</span> automl_model<span class="sc">@</span>leader</span>
<span id="cb195-3"><a href="#cb195-3"></a></span>
<span id="cb195-4"><a href="#cb195-4"></a><span class="co"># Look at best model</span></span>
<span id="cb195-5"><a href="#cb195-5"></a><span class="fu">summary</span>(aml_leader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Model Details:
#&gt; ==============
#&gt; 
#&gt; H2OMultinomialModel: deeplearning
#&gt; Model Key:  DeepLearning_grid_2_AutoML_5_20221209_112240_model_1 
#&gt; Status of Neuron Layers: predicting seed_type, 3-class classification, multinomial distribution, CrossEntropy loss, 643 weights/biases, 12,2 KB, 597 training samples, mini-batch size 1
#&gt;   layer units             type dropout       l1       l2 mean_rate rate_rms
#&gt; 1     1     7            Input 15.00 %       NA       NA        NA       NA
#&gt; 2     2    20 RectifierDropout  0.00 % 0.000000 0.000000  0.001876 0.001772
#&gt; 3     3    20 RectifierDropout  0.00 % 0.000000 0.000000  0.001972 0.001963
#&gt; 4     4     3          Softmax      NA 0.000000 0.000000  0.006924 0.028369
#&gt;   momentum mean_weight weight_rms mean_bias bias_rms
#&gt; 1       NA          NA         NA        NA       NA
#&gt; 2 0.000000   -0.057683   0.272770  0.520670 0.077093
#&gt; 3 0.000000   -0.003009   0.222437  0.987687 0.051666
#&gt; 4 0.000000    0.003565   1.115716  0.011791 0.036845
#&gt; 
#&gt; H2OMultinomialMetrics: deeplearning
#&gt; ** Reported on training data. **
#&gt; ** Metrics reported on full training frame **
#&gt; 
#&gt; Training Set Metrics: 
#&gt; =====================
#&gt; 
#&gt; Extract training frame with `h2o.getFrame("AutoML_5_20221209_112240_training_RTMP_sid_9c1e_6")`
#&gt; MSE: (Extract with `h2o.mse`) 0.01028865
#&gt; RMSE: (Extract with `h2o.rmse`) 0.101433
#&gt; Logloss: (Extract with `h2o.logloss`) 0.05975107
#&gt; Mean Per-Class Error: 0
#&gt; AUC: (Extract with `h2o.auc`) NaN
#&gt; AUCPR: (Extract with `h2o.aucpr`) NaN
#&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`)
#&gt; =========================================================================
#&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
#&gt;         1  2  3  Error     Rate
#&gt; 1      26  0  0 0.0000 = 0 / 26
#&gt; 2       0 30  0 0.0000 = 0 / 30
#&gt; 3       0  0 26 0.0000 = 0 / 26
#&gt; Totals 26 30 26 0.0000 = 0 / 82
#&gt; 
#&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)`
#&gt; =======================================================================
#&gt; Top-3 Hit Ratios: 
#&gt;   k hit_ratio
#&gt; 1 1  1.000000
#&gt; 2 2  1.000000
#&gt; 3 3  1.000000
#&gt; 
#&gt; 
#&gt; 
#&gt; 
#&gt; 
#&gt; H2OMultinomialMetrics: deeplearning
#&gt; ** Reported on cross-validation data. **
#&gt; ** 3-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
#&gt; 
#&gt; Cross-Validation Set Metrics: 
#&gt; =====================
#&gt; 
#&gt; Extract cross-validation frame with `h2o.getFrame("AutoML_5_20221209_112240_training_RTMP_sid_9c1e_6")`
#&gt; MSE: (Extract with `h2o.mse`) 0.02551511
#&gt; RMSE: (Extract with `h2o.rmse`) 0.1597345
#&gt; Logloss: (Extract with `h2o.logloss`) 0.152864
#&gt; Mean Per-Class Error: 0.02564103
#&gt; AUC: (Extract with `h2o.auc`) NaN
#&gt; AUCPR: (Extract with `h2o.aucpr`) NaN
#&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,xval = TRUE)`
#&gt; =======================================================================
#&gt; Top-3 Hit Ratios: 
#&gt;   k hit_ratio
#&gt; 1 1  0.975610
#&gt; 2 2  1.000000
#&gt; 3 3  1.000000
#&gt; 
#&gt; 
#&gt; 
#&gt; 
#&gt; Cross-Validation Metrics Summary: 
#&gt;                             mean       sd cv_1_valid cv_2_valid cv_3_valid
#&gt; accuracy                0.817460 0.096588   0.785714   0.925926   0.740741
#&gt; auc                           NA 0.000000         NA         NA         NA
#&gt; err                     0.182540 0.096588   0.214286   0.074074   0.259259
#&gt; err_count               5.000000 2.645751   6.000000   2.000000   7.000000
#&gt; logloss                 0.320788 0.121066   0.254927   0.246932   0.460507
#&gt; max_per_class_error     0.486111 0.312731   0.666667   0.125000   0.666667
#&gt; mean_per_class_accuracy 0.815741 0.096078   0.777778   0.925000   0.744444
#&gt; mean_per_class_error    0.184259 0.096078   0.222222   0.075000   0.255556
#&gt; mse                     0.112201 0.046437   0.092753   0.078651   0.165200
#&gt; pr_auc                        NA 0.000000         NA         NA         NA
#&gt; r2                      0.822526 0.074637   0.855717   0.874811   0.737050
#&gt; rmse                    0.330483 0.066883   0.304554   0.280447   0.406448
#&gt; 
#&gt; Scoring History: 
#&gt;             timestamp   duration training_speed  epochs iterations    samples
#&gt; 1 2022-12-09 11:23:38  0.000 sec             NA 0.00000          0   0.000000
#&gt; 2 2022-12-09 11:23:38  0.552 sec  54000 obs/sec 0.65854          1  54.000000
#&gt; 3 2022-12-09 11:23:38  0.558 sec  85285 obs/sec 7.28049         11 597.000000
#&gt;   training_rmse training_logloss training_r2 training_classification_error
#&gt; 1            NA               NA          NA                            NA
#&gt; 2       0.64401          1.90182     0.34597                       0.47561
#&gt; 3       0.10143          0.05975     0.98378                       0.00000
#&gt;   training_auc training_pr_auc
#&gt; 1           NA              NA
#&gt; 2           NA              NA
#&gt; 3           NA              NA
#&gt; 
#&gt; Variable Importances: (Extract with `h2o.varimp`) 
#&gt; =================================================
#&gt; 
#&gt; Variable Importances: 
#&gt;        variable relative_importance scaled_importance percentage
#&gt; 1          area            1.000000          1.000000   0.171565
#&gt; 2     perimeter            0.939022          0.939022   0.161104
#&gt; 3     asymmetry            0.936690          0.936690   0.160703
#&gt; 4 kernel_length            0.807065          0.807065   0.138464
#&gt; 5 kernel_groove            0.778050          0.778050   0.133486
#&gt; 6   compactness            0.713528          0.713528   0.122417
#&gt; 7  kernel_width            0.654331          0.654331   0.112260</code></pre>
</div>
</div>
<p>–&gt; NO VALIDATION DATA SET</p>
<blockquote class="blockquote">
<h2 id="question-14" data-number="4.14" class="anchored"><span class="header-section-number">4.14</span> <em>Question</em></h2>
<p>Look at the <strong>summary</strong> of the best model: How high is the <strong>residual deviance</strong> of the validation data?<br> <br> ⬜ 14.70705<br> ⬜ 4.039518<br> ⬜ 22.54199<br> ✅ [8.085209]<br></p>
</blockquote>
<blockquote class="blockquote">
<h2 id="question-15" data-number="4.15" class="anchored"><span class="header-section-number">4.15</span> <em>Question</em></h2>
<p>The <code>aml_leader</code> object can now be treated as you would any H2O model object. How would you calculate the <strong>model performance</strong> of this best model on <code>test</code> data?<br> <br> ⬜ <code>h2o.aggregated_frame(aml_leader, test)</code><br> ⬜ <code>h2o.varimp(aml_leader, test)</code><br> ✅ <code>h2o.performance(aml_leader, test)</code><br> ⬜ <code>h2o.cross_validation_models(aml_leader, test)</code><br></p>
</blockquote>
<p>Correct! You know how to extract the best model from an AutoML leaderboard and how to evaluate it on test data.</p>
</section>
<section id="wrap-up" class="level2" data-number="4.16">
<h2 data-number="4.16" class="anchored" data-anchor-id="wrap-up"><span class="header-section-number">4.16</span> Wrap-up</h2>
<p>Theory. Coming soon …</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../../content/R/topics/07_machine_learning/10_topic_modeling/10_topic_modeling.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">7.10: Topic Modeling</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../content/R/topics/07_machine_learning/12_bayesian_regression_modeling_with_rstanarm/12_bayesian_regression_modeling_with_rstanarm.html" class="pagination-link">
        <span class="nav-page-text">7.12: Bayesian Regression Modeling</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Content 2022 by <a href="https://www.startupengineer.io/authors/schwarz/">Joschka Schwarz</a> <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with and <a href="https://quarto.org/">Quarto</a><br> <a href="https://www.github.com/jwarz/jwarz.github.io">View the source at GitHub</a></div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"openEffect":"zoom","selector":".lightbox","closeEffect":"zoom","descPosition":"bottom","loop":true});</script>



<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>