---
title: "Introduction to Importing Data in R"
author: "Joschka Schwarz"
toc-depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = F)
```

Importing data into R should be the easiest step in your analysis. Unfortunately, that is almost never the case. Data can come in many formats, ranging from .csv and text files, to statistical software files, to databases and HTML data. Knowing which approach to use is key to getting started with the actual analysis.\nIn this course, you’ll start by learning how to read .csv and text files in R. You will then cover the readr and data.table packages to easily and efficiently import flat file data. After that, you will learn how to read .xls files in R using readxl and gdata.

# 1. Importing data from flat files with utils

A lot of data comes in the form of flat files: simple tabular text files. Learn how to import the common formats of flat file data with base R functions.

## Introduction & read.csv

Theory. Coming soon ...

**1. Introduction & read.csv**

Hi, and welcome to the first importing data in R course.

**2. Importing data in R**

Imagine this situation: A colleague of yours is still doing his analyses in Excel and finally decided to transition to R. He needs an easy way to convert the Excel spreadsheets into R data frames,

**3. Importing data in R**

but he can't seem to find the tools to do so. Well, getting to know these tools is exactly what we'll do here.

**4. 5 types**

In this two-part course, we will focus on 5 types of data:

**5. 5 types**

data from flat files,

**6. 5 types**

data from Excel,

**7. 5 types**

data from databases,

**8. 5 types**

data from the web,

**9. 5 types**

and finally data from other statistical software like SAS, SPSS, and Stata.

**10. 5 types**

You'll learn to convert each data format, one after the other, into an R data frame, ready to do some fancy analyses.

**11. Flat files**

Let's start off with flat files. Flat files are typically simple text files that display data as tables. Have a look at this example, states-dot-csv, a flat file where CSV stands for comma-separated values. The data lists basic information on some US states. The first line here gives the names of the different columns or fields. After that, each line is a record, and the fields are separated by a comma, hence the extension CSV. For example, there's the state Hawaii with the capital Honolulu and a total population of 1-point-42 million. What would this data look like in R? Well, actually, the structure nicely corresponds to a data frame in R, that ideally looks like this: the rows in the data frame correspond to the records and the columns of the data frame correspond to the fields. The field names are used to name the columns of the data frame. But how to go from the CSV file to this data frame? We're in luck, because the standard distribution of R provides functionality to import these flat files into R as a data frame.

**12. utils - read.csv**

These functions belong to the utils package that is loaded by default when you start R. More specifically, we'll need the read-dot-csv function, as follows: The first argument of read-dot-csv is the path to the file you want to import in R. If the file is in your current working directory, simply passing the filename as a character string works. If your file is located somewhere else, things get tricky. Depending on the platform you're working on, Linux, Microsoft, Mac, whatever, file paths are specified differently. To build a path to a file in a platform-independent way, you can use the file-dot-path function. Suppose our states-dot-csv file is located in the datasets folder of the home directory. You can use file-dot-path like this: Because I'm working on a Mac, this is the resulting path. I can now use this path inside read-dot-csv to point to the correct file, like this. The second argument, stringsAsFactors, is pretty important. Say we have columns that are strings. We can choose to import these columns as actual strings, or as factors, which R uses to store categorical variables. By default, the stringsAsFactors argument is TRUE, so it would convert strings into factors. In our case, however, the state and capital names shouldn't be categorical variables, so we set stringsAsFactors to FALSE.

**13. read.csv()**

If we run this call now, we can see we get a data frame with 5 observations and 4 variables. This corresponds nicely to the CSV file we started with. If we print out the structure of this data frame, with str, we see that indeed, the two first columns are strings, and not factors, exactly like we wanted it! 

## read.csv

The `utils` package, which is automatically loaded in your R session on startup, can import CSV files with the <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.csv()`</a> function.

In this exercise, you'll be working with <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/swimming_pools.csv" target="_blank" rel="noopener noreferrer">`swimming_pools.csv`</a>; it contains data on swimming pools in Brisbane, Australia (Source: <a href="https://data.gov.au/dataset/swimming-pools-brisbane-city-council" target="_blank" rel="noopener noreferrer">data.gov.au</a>). The file contains the column names in the first row. It uses a comma to separate values within rows. 

Type <a href="http://www.rdocumentation.org/packages/base/functions/list.files" target="_blank" rel="noopener noreferrer">`dir()`</a> in the console to list the files in your working directory. You'll see that it contains `swimming_pools.csv`, so you can start straight away.

**Steps**

1. Use <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.csv()`</a> to import `"swimming_pools.csv"` as a data frame with the name `pools`.
2. Print the structure of `pools` using <a href="http://www.rdocumentation.org/packages/utils/functions/str" target="_blank" rel="noopener noreferrer">`str()`</a>.

```{r,eval=TRUE}
# Import swimming_pools.csv: pools
pools <- read.csv("data/swimming_pools.csv")

# Print the structure of pools
str(pools)
```

## stringsAsFactors

With `stringsAsFactors`, you can tell R whether it should convert strings in the flat file to factors. 

For all importing functions in the `utils` package, this argument is `TRUE`, which means that you import strings as factors. This only makes sense if the strings you import represent categorical variables in R. If you set `stringsAsFactors` to `FALSE`, the data frame columns corresponding to strings in your text file will be `character`.

You'll again be working with the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/swimming_pools.csv" target="_blank" rel="noopener noreferrer">`swimming_pools.csv`</a> file. It contains two columns (`Name` and `Address`), which shouldn't be factors.


**Steps**

1. Use `read.csv()` to import the data in <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/swimming_pools.csv" target="_blank" rel="noopener noreferrer">`"swimming_pools.csv"`</a> as a data frame called `pools`; make sure that strings are imported as characters, not as factors.
2. Using <a href="http://www.rdocumentation.org/packages/utils/functions/str" target="_blank" rel="noopener noreferrer">`str()`</a>, display the structure of the dataset and check that you indeed get character vectors instead of factors.

```{r,eval=TRUE}
# Import swimming_pools.csv correctly: pools
pools <- read.csv("data/swimming_pools.csv", stringsAsFactors = F)

# Check the structure of pools
str(pools)
```

## Any changes?

Consider the code below that loads data from `swimming_pools.csv` in two distinct ways:

```{r}
# Option A
pools <- read.csv("swimming_pools.csv", stringsAsFactors = TRUE)

# Option B
pools <- read.csv("swimming_pools.csv", stringsAsFactors = FALSE)
```

> *Question*
> ---
> How many variables in the resulting `pools` data frame have different types if you specify the `stringsAsFactors` argument differently?<br>
> <br>
> ⬜ Just one: `Name`.<br>
> ✅ Two variables: `Name` and `Address`.<br>
> ⬜ Three columns: all but `Longitude`.<br>
> ⬜ All four of them!<br>

## read.delim & read.table

Theory. Coming soon ...

**1. read.delim &amp; read.table**

In the previous video you learned about read-dot-csv, which is an R function that specifically exists for importing CSV data. However, flat file data doesn't come as comma separated values alone.

**2. Tab-delimited file**

Another common format of flat file data is the tab-delimited file, like this states-dot-txt, with the same data as before:To import it, we'll need read-dot-delim this time. As usual, you simply specify the path to the file, and also that we want to import strings as strings, and not as factors. Works like a charm!If your data comes in this typical comma separated or tab-delimited format, your life is easy and importing the data is a walk in the park. If it's not, you'll have to some more customization work.

**3. Exotic file format**

Say we have the same states data again, but this time, the values are separated by a forward slash instead of commas or tabs, in a file states2.txt. How to go about this? read-dot-csv and read-dot-delim won't do you much good in this case.

**4. read.table()**

Here, you'll want to use read-dot-table. It's the main importing function in the utils package, allowing you to read in any file in table format and create a data frame from it.The number of arguments you can specify for this function is huge, so I won't go through each and every one of these arguments. Instead, let's have a look at the read-dot-table call that imports states2.txt and try to understand what happens.As usual, the first argument is the path to the file you want to import.The header argument is something we haven't seen before. If you set this to TRUE, you tell R that the first row of the text file contains the variable names, which is the case here. read-dot-table sets this argument to FALSE by default, which would mean that the first row is always read as an observation, even if it's a row of variable names.Next, sep is the argument that specifies how fields in a record are separated. For our file here, the field separator is a forward slash, like this.Finally, there's again the stringsAsFactors argument, which we set to FALSE again because we want to import strings as strings. The result looks just as we'd want, nice!Apart from the arguments we discussed here, there are also ways to specify column names and column types.


## read.delim

Aside from `.csv` files, there are also the `.txt` files which are basically text files. You can import these functions with <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.delim()`</a>. By default, it sets the `sep` argument to `"\t"` (fields in a record are delimited by tabs) and the `header` argument to `TRUE` (the first row contains the field names). 

In this exercise, you will import <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/hotdogs.txt" target="_blank" rel="noopener noreferrer">`hotdogs.txt`</a>, containing information on sodium and calorie levels in different hotdogs (Source: <a href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_012708_ID_Data_HotDogs" target="_blank" rel="noopener noreferrer">UCLA</a>). The dataset has 3 variables, but the variable names are *not* available in the first line of the file. The file uses tabs as field separators.

**Steps**

1. Import the data in `"hotdogs.txt"` with <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.delim()`</a>. Call the resulting data frame `hotdogs`. The variable names are **not** on the first line, so make sure to set the `header` argument appropriately.
2. Call <a href="http://www.rdocumentation.org/packages/base/functions/summary" target="_blank" rel="noopener noreferrer">`summary()`</a> on `hotdogs`. This will print out some summary statistics about all variables in the data frame.

```{r,eval=TRUE}
# Import hotdogs.txt: hotdogs
hotdogs <- read.delim("data/hotdogs.txt", header = F)

# Summarize hotdogs
summary(hotdogs)
```

## read.table

If you're dealing with more exotic flat file formats, you'll want to use <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.table()`</a>. It's the most basic importing function; you can specify tons of different arguments in this function. Unlike <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.csv()`</a> and <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.delim()`</a>, the `header` argument defaults to `FALSE` and the `sep` argument is `""` by default.

Up to you again! The data is still <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/hotdogs.txt" target="_blank" rel="noopener noreferrer">`hotdogs.txt`</a>. It has no column names in the first row, and the field separators are tabs. This time, though, the file is in the `data` folder inside your current working directory. A variable `path` with the location of this file is already coded for you.

**Steps**

1. Finish the `read.table()` call that's been prepared for you. Use the `path` variable, and make sure to set `sep` correctly.
2. Call `head()` on `hotdogs`; this will print the first 6 observations in the data frame.

```{r,eval=TRUE}
# Path to the hotdogs.txt file: path
path <- file.path("data", "hotdogs.txt")

# Import the hotdogs.txt file: hotdogs
hotdogs <- read.table(path, 
                      sep = "\t", 
                      col.names = c("type", "calories", "sodium"))

# Call head() on hotdogs
head(hotdogs)
```

Great! No need to specify the `header` argument: it is `FALSE` by default for `read.table()`, which is exactly what you want here.

## Arguments

Lily and Tom are having an argument because they want to share a hot dog but they can't seem to agree on which one to choose. After some time, they simply decide that they will have one each. Lily wants to have the one with the fewest calories while Tom wants to have the one with the most sodium.

Next to `calories` and `sodium`, the hotdogs have one more variable: `type`. This can be one of three things: `Beef`, `Meat`, or `Poultry`, so a categorical variable: a factor is fine.


**Steps**

1. Finish the `read.delim()` call to import the data in <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/hotdogs.txt" target="_blank" rel="noopener noreferrer">`"hotdogs.txt"`</a>. It's a tab-delimited file without names in the first row.
2. The code that selects the observation with the lowest calorie count and stores it in the variable `lily` is already available. It uses the function <a href="http://www.rdocumentation.org/packages/base/functions/which.min" target="_blank" rel="noopener noreferrer">`which.min()`</a>, that returns the index the smallest value in a vector.
3. Do a similar thing for Tom: select the observation with the *most sodium* and store it in `tom`. Use <a href="http://www.rdocumentation.org/packages/base/functions/which.min" target="_blank" rel="noopener noreferrer">`which.max()`</a> this time.
4. Finally, print both the observations `lily` and `tom`.

```{r,eval=TRUE}
# Finish the read.delim() call
hotdogs <- read.delim("data/hotdogs.txt", header = F, col.names = c("type", "calories", "sodium"))

# Select the hot dog with the least calories: lily
lily <- hotdogs[which.min(hotdogs$calories), ]

# Select the observation with the most sodium: tom
tom <- hotdogs[which.max(hotdogs$sodium),]

# Print lily and tom
lily
tom
```

## Column classes

Next to column names, you can also specify the column types or column classes of the resulting data frame. You can do this by setting the `colClasses` argument to a vector of strings representing classes:

```{r}
read.delim("data/my_file.txt", 
           colClasses = c("character",
                          "numeric",
                          "logical"))
```

This approach can be useful if you have some columns that should be factors and others that should be characters. You don't have to bother with `stringsAsFactors` anymore; just state for each column what the class should be.

If a column is set to `"NULL"` in the `colClasses` vector, this column will be skipped and will not be loaded into the data frame.

**Steps**

1. The `read.delim()` call from before is already included and creates the `hotdogs` data frame. Go ahead and display the structure of `hotdogs`.
2. **Edit** the second <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.delim()`</a> call. Assign the correct vector to the `colClasses` argument. `NA` should be replaced with a character vector: `c("factor", "NULL", "numeric")`.
3. Display the structure of `hotdogs2` and look for the difference.

```{r,eval=TRUE}
# Previous call to import hotdogs.txt
hotdogs <- read.delim("data/hotdogs.txt", header = FALSE, col.names = c("type", "calories", "sodium"))

# Display structure of hotdogs
str(hotdogs)
```

```{r}
# Edit the colClasses argument to import the data correctly: hotdogs2
hotdogs2 <- read.delim("hotdogs.txt", header = FALSE, 
                       col.names = c("type", "calories", "sodium"),
                       colClasses = c("factor", "NULL", "numeric"))


# Display structure of hotdogs2
str(hotdogs2)
```

## Final Thoughts

Theory. Coming soon ...


**1. Final Thoughts**

There's something I haven't told you yet. When I said that read-dot-table was utils' main importing functions, I was serious.

**2. Wrappers**

Actually, read-dot-csv and read-dot-delim, the functions to import comma-separated values and tab-delimited files, are so-called wrapper functions around read-dot-table. They call read-dot-table behind the scenes, but with different default arguments to match the specific formats.

**3. read.csv**

For read-dot-csv, the default for header is TRUE and for sep is a comma, so you don't have to manually specify these anymore. This means that this read-dot-table call to import the CSV version of states, is exactly the same as this read-dot-csv call. Shorter and easier to read, if you ask me.

**4. read.delim**

Likewise, read-dot-delim sets the header and sep argument, among some others. This call to import the tab-delimited version of states, is exactly the same as this read-dot-delim call.

**5. Documentation**

If you have a look at the documentation of read-dot-table, you'll see that there are two more functions in there that we haven't discussed yet. read-dot-csv2 and read-dot-delim2. These functions exist to deal with regional differences in representing numbers.

**6. Locale differences**

Have a look at this csv file, states_aye-dot-csv, typical for the US and Great Britain, and its counterpart, states_nay-dot-csv.You'll notice that the states_nay use commas for decimal points, as opposed to the dot for states_aye-dot-csv. This means that they can't use the comma as the field-delimiter anymore, they need a semicolon.

**7. Locale differences**

That's why the read-dot-csv2 and read-dot-delim2 functions exist. Can you spot the difference in default arguments again?

**8. states_nay.csv**

Let's try to import the states_nay-dot-csv file with the basic read-dot-csv function.R gives a result, but it clearly is not the result we want. It's a dataset with 5 observations but a single variable.If we try again with read-dot-csv2, it works perfectly this time!These were just some side notes to wrap up on this chapter. By now, you now how to import comma-separated, tab-delimited and even more exotic data formats. But there's much more to learn!

# 2. readr & data.table

In addition to base R, there are dedicated packages to easily and efficiently import flat file data. We'll talk about two such packages: readr and data.table.

## readr: read_csv & read_tsv

Theory. Coming soon ...

**1. readr: read_csv &amp; read_tsv**

By now, you already now how to import flat files using

**2. Overview**

the utils package. The utils package is loaded by default when you start R. Of course, R wouldn't be R if there aren't specialized packages to import your data. In this chapter, I'm going to talk about two such packages: readr and data-dot-table.

**3. readr**

First, let's dive into the "readr" package, written by Hadley Wickham! It's a fast and very easy to use package with very consistent naming, while utils is more verbose and multiple times slower.We'll start with installing and loading the readr package, like this.

**4. CSV files**

Before, you used read dot csv to import CSV files as a data frame, like this call, to import the states dot csv file. To do this the readr way, you'll want to use read underscore csv. This will do the trick:The result is pretty much the same. The only difference is that read_delim outputs a tibble, which is a supercharged version of a dataframe that Hadley Wickham introduced; you can work with it as a normal data frame, but can do additional stuff with it. The printout conveniently shows the column classes. In general, all of Hadley's packages work with tibbles; whether or not they show this convienent printout depends on which packages you have loaded in.In both calls, the first argument is the path to the file you want to import. Notice that in readr, strings are not imported as factors by default, so an equivalent of the stringsAsFactors argument is not required.Remember that utils also featured the read-dot-delim function to import tab-delimited files.

**5. TSV files**

This call imported states-dot-txt. readr also provides a similar function, but it's called read underscore tsv, short for tab separated value. This is the call you need. Again, no need to specify stringsAsFactors explicitly, which is practical.

**6. Wrapping in utils and readr**

Just like in utils, both the read_csv and read_tsv functions are wrappers around a 'mother import function',

**7. Wrapping in utils and readr**

called read_delim. This table summarizes how the wrapping works for utils and readr, make sure not to mix things up.

**8. Let's practice!**

I suggest you already head over to the exercises for some practice before we dive into some more customization possibilities for read_delim. I'll see you again in the next video!

## read_csv

CSV files can be imported with <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_csv()`</a>. It's a wrapper function around <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_delim()`</a> that handles all the details for you. For example, it will assume that the first row contains the column names.

The dataset you'll be working with here is <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.csv" target="_blank" rel="noopener noreferrer">`potatoes.csv`</a>. It gives information on the impact of storage period and cooking on potatoes' flavor. It uses commas to delimit fields in a record, and contains column names in the first row. The file is available in your workspace. Remember that you can inspect your workspace with `dir()`.

**Steps**

1. Load the `readr` package with <a href="http://www.rdocumentation.org/packages/base/functions/library" target="_blank" rel="noopener noreferrer">`library()`</a>. You **do not** need to install the package, it is already installed on DataCamp's servers. 
2. Import `"potatoes.csv"` using `read_csv()`. Assign the resulting data frame to the variable `potatoes`.

```{r,eval=TRUE}
# Load the readr package
library(readr)

# Import potatoes.csv with read_csv(): potatoes
potatoes <- read_csv("data/potatoes.csv")
```

## read_tsv

Where you use `read_csv()` to easily read in CSV files, you use <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_tsv()`</a> to easily read in TSV files. TSV is short for tab-separated values.

This time, the potatoes data comes in the form of a tab-separated values file; <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.txt" target="_blank" rel="noopener noreferrer">`potatoes.txt`</a> is available in your workspace. In contrast to `potatoes.csv`, this file does **not** contain columns names in the first row, though.

There's a vector `properties` that you can use to specify these column names manually.

**Steps**

1. Use `read_tsv()` to import the potatoes data from `potatoes.txt` and store it in the data frame `potatoes`. In addition to the path to the file, you'll also have to specify the `col_names` argument; you can use the `properties` vector for this.
2. Call `head()` on `potatoes` to show the first observations of your dataset.

```{r,eval=TRUE}
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import potatoes.txt: potatoes
potatoes <- read_tsv("data/potatoes.txt" , col_names = properties)

# Call head() on potatoes
head(potatoes)
```

## readr: read_delim

Theory. Coming soon ...


**1. readr: read_delim**

Remember the states2.txt file from before,

**2. states2.txt**

that uses forward slashes as separators? Before, we've already written

**3. states2.txt**

this customized read-dot-table call for it. Let's now use readr's more low-level read_delim function to do the same thing:As usual, the first argument is the path the file. Next, the delim argument specifies the character that is used to separate fields within a record; it's the equivalent of the sep argument in read-dot-table.The output corresponds to the output fo the read-dot-table call, but the readr version outputs a tibble again.Let's compare the utils and the readr calls here. First off, we didn't have to specify something like header is TRUE, because by default, read_delim expects the first row to contain the column names. It does this with the col_names argument. Also, strings are not imported as factors by default, so a stringsAsFactors equivalent is not necessary. To control the types of the columns, readr uses the col_types argument, similar to the colClasses argument from utils. Let me dive into col_names first, and then talk some more about col_types.

**4. col_names**

col_names is TRUE by default. Suppose you have another version of the states file, without column names this time, states3.txt. The first line is already a record now.

**5. col_names**

Setting col_names to FALSE, leads to automatic generation of column names, like in this example. You can also manually set col_names to a character vector. The names you pass will be used as the names of the columns, and the first line is read as a record, like here:

**6. col_types**

Next, there's also col_types, to control the column classes. If we just import states2.txt, the file with header names, like before, without specifying col_types, the column types will be guessed from the first 30 rows on the input. The printout of the tibble shows us the class of each column, which is very practical. The first two columns are character, the third is double, and the fourth is integer.You can also specify the column classes manually. In this call, we enforce the state and city to be a character and the population and area to be both numeric. I used short string representations here: c stands for character, d for double or numeric, i for integer, and l for logical. The result is what we'd expect: the fourth column is a double now.Instead of c, d, i and l, you can also use an underscore, to skip a column. A totally different way to control the types of the columns is through collector functions. Although more complicated, they are more versatile. You'll learn more about this in the exercises.

**7. skip and n_max**

If you're working on huge flat files, say one million lines, you might be interested in handling the data in chunks of 50-point-000 lines for example. This keeps your work tractable and you can easily follow up on the progress of your algorithms. In readr, You can do this with a combination of the skip and n_max arguments. Have a look at the output of this call:We skipped 2 rows, and then read in three records. There's a problem though! Because col_names is TRUE by default, the first row that's read is used for the column names, but this information has been skipped! We'll have to manually specify some column names this time, by setting col_names to a character vector. This time, the two first rows, so the column names and the first observation are skipped, and the next three observations are read in. Perfect.

**8. Let's practice!**

Let's see your importing skills progress in the exercises. In the last video of this chapter, we'll talk about the amazing fread function from the data-dot-table package!

## read_delim

Just as <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.table()`</a> was the main `utils` function, <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_delim()`</a> is the main `readr` function.

<a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_delim()`</a> takes two mandatory arguments:

* `file`: the file that contains the data
* `delim`: the character that separates the values in the data file

You'll again be working <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.txt" target="_blank" rel="noopener noreferrer">`potatoes.txt`</a>; the file uses tabs (`"\t"`) to delimit values and does **not** contain column names in its first line. It's available in your working directory so you can start right away. As before, the vector `properties` is available to set the `col_names`.

**Steps**

1. Import all the data in <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.txt" target="_blank" rel="noopener noreferrer">`"potatoes.txt"`</a> using <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_delim()`</a>; store the resulting data frame in `potatoes`.
2. Print out `potatoes`.

```{r}
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import potatoes.txt using read_delim(): potatoes
potatoes <- read_delim("data/potatoes.txt", delim = "\t", col_names = properties)

# Print out potatoes
potatoes
```

Good job! Notice that you could just as well have used [`read_tsv()`](http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim) here.

## skip and n_max

Through `skip` and `n_max` you can control *which part* of your flat file you're actually importing into R.

* `skip` specifies the number of lines you're ignoring in the flat file before actually starting to import data.
* `n_max` specifies the number of lines you're actually importing.

Say for example you have a CSV file with 20 lines, and set `skip = 2` and `n_max = 3`, you're only reading in lines 3, 4 and 5 of the file.

Watch out: Once you `skip` some lines, you also skip the first line that can contain column names!

<a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.txt" target="_blank" rel="noopener noreferrer">`potatoes.txt`</a>, a flat file with tab-delimited records and without column names, is available in your workspace.

**Steps**

1. Finish the first <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_tsv()`</a> call to import observations 7, 8, 9, 10 and 11 from `potatoes.txt`.

```{r,eval=TRUE}
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import 5 observations from potatoes.txt: potatoes_fragment
potatoes_fragment <- read_tsv("data/potatoes.txt", skip = 6, n_max = 5, col_names = properties)
```

## col_types

You can also specify which types the columns in your imported data frame should have. You can do this with `col_types`. If set to `NULL`, the default, functions from the `readr` package will try to find the correct types themselves. You can manually set the types with a string, where each character denotes the class of the column: `c`haracter, `d`ouble, `i`nteger and `l`ogical. `_` skips the column as a whole.

<a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.txt" target="_blank" rel="noopener noreferrer">`potatoes.txt`</a>, a flat file with tab-delimited records and without column names, is again available in your workspace.


**Steps**

1. In the second <a href="http://www.rdocumentation.org/packages/readr/versions/1.0.0/topics/read_delim" target="_blank" rel="noopener noreferrer">`read_tsv()`</a> call, edit the `col_types` argument to import *all* columns as characters (`c`). Store the resulting data frame in `potatoes_char`.
2. Print out the structure of `potatoes_char` and verify whether all column types are `chr`, short for `character`.

```{r,eval=TRUE}
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import all data, but force all columns to be character: potatoes_char
potatoes_char <- read_tsv("data/potatoes.txt", col_types = "cccccccc", col_names = properties)

# Print out structure of potatoes_char
str(potatoes_char)
```

## col_types with collectors

Another way of setting the types of the imported columns is using **collectors**. Collector functions can be passed in a <a href="http://www.rdocumentation.org/packages/base/functions/list" target="_blank" rel="noopener noreferrer">`list()`</a> to the `col_types` argument of `read_` functions to tell them how to interpret values in a column.

For a complete list of collector functions, you can take a look at the <a href="https://www.rdocumentation.org/packages/readr/topics/collector" target="_blank" rel="noopener noreferrer">`collector`</a> documentation. For this exercise you will need two collector functions:


* `col_integer()`: the column should be interpreted as an integer.
* `col_factor(levels, ordered = FALSE)`: the column should be interpreted as a factor with `levels`.

In this exercise, you will work with <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/hotdogs.txt" target="_blank" rel="noopener noreferrer">`hotdogs.txt`</a>, which is a tab-delimited file without column names in the first row.


**Steps**

1. `hotdogs` is created for you without setting the column types. Inspect its summary using the <a href="http://www.rdocumentation.org/packages/base/functions/summary" target="_blank" rel="noopener noreferrer">`summary()`</a> function.
2. Two collector functions are defined for you: `fac` and `int`. Have a look at them, do you understand what they're collecting?
3. In the second `read_tsv()` call, edit the `col_types` argument: Pass a `list()` with the elements `fac`, `int` and `int`, so the first column is imported as a factor, and the second and third column as integers.
4. Create a <a href="http://www.rdocumentation.org/packages/base/functions/summary" target="_blank" rel="noopener noreferrer">`summary()`</a> of `hotdogs_factor`. Compare this to the summary of `hotdogs`.

```{r,eval=TRUE}
# Import without col_types
hotdogs <- read_tsv("data/hotdogs.txt", col_names = c("type", "calories", "sodium"))

# Display the summary of hotdogs
summary(hotdogs)
```

```{r,eval=TRUE}
# The collectors you will need to import the data
fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))
int <- col_integer()

# Edit the col_types argument to import the data correctly: hotdogs_factor
hotdogs_factor <- read_tsv("data/hotdogs.txt",
                           col_names = c("type", "calories", "sodium"),
                           col_types = list(fac, int, int))

# Display the summary of hotdogs_factor
summary(hotdogs_factor)
```

Awesome! The summary of `hotdogs_factor` clearly contains more interesting information for the `type` column, right?

## data.table: fread

Theory. Coming soon ...


**1. data.table: fread**

The other package I wanted to discuss is data-dot-table.

**2. data.table**

The key performance metric for the authors of data table, Matt Dowle and Arun Srinivasan, is speed. The package is mainly about data manipulation in R, but also features a super powerful function to read your data into R: fread. If you have huge files you have to import and munge in R, you'll want to go with the data-dot-table package. Let's start with installing and loading the library:If you want to learn everything about data-dot-table, DataCamp offers course material dedicated to this package. In this course, we'll stick to the fread function. It is pretty similar to the "read-dot-table" function you've seen before but it is more convenient to use. Let me introduce it with an example.

**3. fread()**

Suppose we have two versions of the states csv file, one with and one without column names in the first row.

**4. fread()**

Let's call fread on both these csv files, without additional arguments. fread automatically deals with these two different cases. In the first case, the column names are correctly transfered to the data frame, while in the second case, fread made up some column names itself.

**5. fread()**

Next to automatically handling the names, fread can also infer the column types and the field separators without having to specify these. That's the cool thing about fread: it just works. On top of that, it's ridiculously fast.Of course, the fread function does much more than automatically finding out all column types and conventions in the files you want to read. You can also manually specify the separator, the colClasses, the number of lines to skip and the number of lines to read in manually. Think of fread as an improved version of read-dot-table which is faster, more convenient and adds functionality.

**6. Let's practice!**

With these pointers on data-dot-table it's time for you to get some hands-on practice yourself. Enjoy!

## fread

You still remember how to use <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.table()`</a>, right? Well, <a href="http://www.rdocumentation.org/packages/data.table/functions/fread" target="_blank" rel="noopener noreferrer">`fread()`</a> is a function that does the same job with very similar arguments. It is extremely easy to use and blazingly fast! Often, simply specifying the path to the file is enough to successfully import your data.

Don't take our word for it, try it yourself! You'll be working with the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.csv" target="_blank" rel="noopener noreferrer">`potatoes.csv`</a> file, that's available in your workspace. Fields are delimited by commas, and the first line contains the column names.


**Steps**

1. Use <a href="http://www.rdocumentation.org/packages/base/functions/library" target="_blank" rel="noopener noreferrer">`library()`</a> to load (NOT install) the `data.table` package. You **do not** need to install the package, it is already installed on DataCamp's servers. 
2. Import `"potatoes.csv"` with <a href="http://www.rdocumentation.org/packages/data.table/functions/fread" target="_blank" rel="noopener noreferrer">`fread()`</a>. Simply pass it the file path and see if it worked. Store the result in a variable `potatoes`.
3. Print out `potatoes`.

```{r,eval=TRUE}
# load the data.table package
library(data.table)

# Import potatoes.csv with fread(): potatoes
potatoes <- fread("data/potatoes.csv")

# Print out potatoes
potatoes
```

## fread: more advanced use

Now that you know the basics about <a href="http://www.rdocumentation.org/packages/data.table/functions/fread" target="_blank" rel="noopener noreferrer">`fread()`</a>, you should know about two arguments of the function: `drop` and `select`, to drop or select variables of interest.

Suppose you have a dataset that contains 5 variables and you want to keep the first and fifth variable, named "a" and "e". The following options will all do the trick:

```{r}
fread("path/to/file.txt", drop = 2:4)
fread("path/to/file.txt", select = c(1, 5))
fread("path/to/file.txt", drop = c("b", "c", "d"))
fread("path/to/file.txt", select = c("a", "e"))
```

Let's stick with potatoes since we're particularly fond of them here at DataCamp. The data is again available in the file <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.csv" target="_blank" rel="noopener noreferrer">`potatoes.csv`</a>, containing comma-separated records.


**Steps**

1. Using <a href="http://www.rdocumentation.org/packages/data.table/functions/fread" target="_blank" rel="noopener noreferrer">`fread()`</a> and `select` or `drop` as arguments, only import the `texture` and `moistness` columns of the flat file. They correspond to the columns 6 and 8 in `"potatoes.csv"`. Store the result in a variable `potatoes`.
2. <a href="http://www.rdocumentation.org/packages/graphics/functions/plot" target="_blank" rel="noopener noreferrer">`plot()`</a> 2 columns of the `potatoes` data frame: `texture` on the x-axis, `moistness` on the y-axis. Use the dollar sign notation twice. Feel free to name your axes and plot.

```{r,eval=TRUE}
# Import columns 6 and 8 of potatoes.csv: potatoes
potatoes <- fread("data/potatoes.csv", select = c("texture", "moistness"))

# Plot texture (x) and moistness (y) of potatoes
plot(potatoes$texture, potatoes$moistness)
```

Congratulations! We can see that moistness and texture are positively correlated.

## Dedicated classes

You might have noticed that the <a href="http://www.rdocumentation.org/packages/data.table/functions/fread" target="_blank" rel="noopener noreferrer">`fread()`</a> function produces data frames that look slightly different when you print them out. That's because another class named `data.table` is assigned to the resulting data frames. The printout of such `data.table` objects is different. Does something similar happen with the data frames generated by `readr`?

In your current working directory, we prepared the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/potatoes.csv" target="_blank" rel="noopener noreferrer">`potatoes.csv`</a> file. The packages `data.table` and `readr` are both loaded, so you can experiment straight away.

> *Question*
> ---
> Which of the following statements is true?<br>
> <br>
> ⬜ <a href="http://www.rdocumentation.org/packages/data.table/functions/fread">`fread()`</a> creates an object whose only class is `data.table` class. <a href="http://www.rdocumentation.org/packages/readr/topics/read_delim">`read_csv()`</a> creates an object with class `tbl_df`.<br>
> ⬜ The class of the result of <a href="http://www.rdocumentation.org/packages/data.table/functions/fread">`fread()`</a> is only `data.table`. That of the result of <a href="http://www.rdocumentation.org/packages/readr/topics/read_delim">`read_csv()`</a> is both `tbl_df` and `tbl`.<br>
> ✅ The class of the result of <a href="http://www.rdocumentation.org/packages/data.table/functions/fread">`fread()`</a> is both `data.table` and `data.frame`. <a href="http://www.rdocumentation.org/packages/readr/topics/read_delim">`read_csv()`</a> creates an object with three classes: `tbl_df`, `tbl` and `data.frame`.<br>
> ⬜ <a href="http://www.rdocumentation.org/packages/data.table/functions/fread">`fread()`</a> creates an object of the `data.table` class, while <a href="http://www.rdocumentation.org/packages/readr/topics/read_delim">`read_csv()`</a> simply generates a `data.frame`, nothing more.<br>

Correct! What's the benefit of these additional classes? Well, it allows for a different treatment of printouts, for example. To learn all about it, you can check out the DataCamp courses dedicated to dplyr and data.table. For now, you can proceed to the next chapter in this course! 

# 3. Importing Excel data

Excel is a widely used data analysis tool. If you prefer to do your analyses in R, though, you'll need an understanding of how to import  .csv data into R. This chapter will show you how to use readxl and gdata to do so.

## readxl (1)

Theory. Coming soon ...


**1. readxl (1)**

Another tool that is extremely common in data analysis is

**2. Microsoft Excel**

Microsoft Excel. It shouldn't be a surprise that there are a lot of packages out there that interact with Excel so that you can work with the data inside R. In this chapter, we'll cover everything you need to know to get started with excel files in R with practically no extra work. In this video, I'll be taking about Hadley Wickham's readxl package.

**3. Typical Structure Excel Data**

Before we dive into the R side of things, it's a good idea to quickly recap on what an excel file typically is. For most data related tasks, an excel file contains different sheets that contain tabular data. Take this excel file for example, cities-dot-xlsx, that contains two sheets containing data of the total population of some capitals for two different years.When you're working in R, you'll typically want to explore your excel file first and then import some data from it. But how do you go about this?

**4. readxl**

This is where the readxl package comes in. It basically contains two main functions: excel_sheets and read_excel. The first one is used to list the different sheets in your excel file, while the second one is used to actually import a sheet into your R session. readxl is able to handle both dot-xls as dot-xlsx files.So, let's try it out! Let's first install and load the readxl package and then

**5. excel_sheets()**

start with the excel_sheets function. You simply pass it the file path, which is the location of the dot-xls file on your own system. The file is already in our working directory, as dir reveals, so we can simply use the following call.The result is a simple character vector, that contains the names of the different sheets. Indeed, you saw before that the two sheets in the excel file are named year_1990 and year_2000. Great.We already know about the sheet names now, but that's just the names, not the actual population data.

**6. read_excel()**

Fortunately, readxl also features the read_excel function to actually import the sheet data into your R session.In its most basic use, you simply specify the path to the excel file again.By default, the first sheet, year_1990 in this case, is imported as a tibble. You can also explicitly tell read_excel which sheet to import, by setting the sheet argument. You can use both an index or the sheet name. Suppose you want to load in the second sheet, named year_2000. The following two calls both do that.In all of these read_excel calls, an R data frame results that contains the Excel data. You can start your analyses right away!

**7. Let's practice!**

Give it a first try in the exercises. In the next video, I'll dive a little deeper into the read_excel function!

## List the sheets of an Excel file

Before you can start importing from Excel, you should find out which sheets are available in the workbook. You can use the <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`excel_sheets()`</a> function for this.

You will find the Excel file <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a> in your working directory (type <a href="http://www.rdocumentation.org/packages/base/functions/list.files" target="_blank" rel="noopener noreferrer">`dir()`</a> to see it). This dataset contains urban population metrics for practically all countries in the world throughout time (Source: <a href="http://www.gapminder.org/" target="_blank" rel="noopener noreferrer">Gapminder</a>). It contains three sheets for three different time periods. In each sheet, the first row contains the column names.

**Steps**

1. Load the `readxl` package using <a href="http://www.rdocumentation.org/packages/base/functions/library" target="_blank" rel="noopener noreferrer">`library()`</a>. It's already installed on DataCamp's servers.
2. Use <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`excel_sheets()`</a> to print out the names of the sheets in `urbanpop.xlsx`.

```{r,eval=TRUE}
# Load the readxl package
library("readxl")

# Print the names of all worksheets
excel_sheets("data/urbanpop.xlsx")
```

Congratulations! As you can see, the result of [`excel_sheets()`](https://cran.r-project.org/web/packages/readxl/readxl.pdf) is simply a character vector; you haven't imported anything yet. That's something for the [`read_excel()`](https://cran.r-project.org/web/packages/readxl/readxl.pdf) function. Learn all about it in the next exercise!

## Import an Excel sheet

Now that you know the names of the sheets in the Excel file you want to import, it is time to import those sheets into R. You can do this with the <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`read_excel()`</a> function. Have a look at this recipe:


```{r}
data <- read_excel("data.xlsx", sheet = "my_sheet")
```

This call simply imports the sheet with the name `"my_sheet"` from the `"data.xlsx"` file. You can also pass a number to the `sheet` argument; this will cause <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`read_excel()`</a> to import the sheet with the given sheet number. `sheet = 1` will import the first sheet, `sheet = 2` will import the second sheet, and so on.

In this exercise, you'll continue working with the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a> file.


**Steps**

1. The code to import the first and second sheets is already included. Can you add a command to also import the third sheet, and store the resulting data frame in `pop_3`?
2. Store the data frames `pop_1`, `pop_2` and `pop_3` in a list, that you call `pop_list`.
3. Display the structure of `pop_list`.

```{r,eval=TRUE}
# Read the sheets, one by one
pop_1 <- read_excel("data/urbanpop.xlsx", sheet = 1)
pop_2 <- read_excel("data/urbanpop.xlsx", sheet = 2)
pop_3 <- read_excel("data/urbanpop.xlsx", sheet = 3)

# Put pop_1, pop_2 and pop_3 in a list: pop_list
pop_list <- list(pop_1, pop_2, pop_3)

# Display the structure of pop_list
str(pop_list)
```

Great! Now you imported the sheets from `urbanpop.xlsx` correctly. From here on you are able to read and to operate on the imported file. In the next exercise you will learn how to use both the [`excel_sheets()`](https://cran.r-project.org/web/packages/readxl/readxl.pdf) and the [`read_excel()`](https://cran.r-project.org/web/packages/readxl/readxl.pdf) function in combination with [`lapply()`](http://www.rdocumentation.org/packages/base/functions/lapply) to read multiple sheets at once.

## Reading a workbook

In the previous exercise you generated a list of three Excel sheets that you imported. However, loading in every sheet manually and then merging them in a list can be quite tedious. Luckily, you can automate this with <a href="http://www.rdocumentation.org/packages/base/functions/lapply" target="_blank" rel="noopener noreferrer">`lapply()`</a>. If you have no experience with <a href="http://www.rdocumentation.org/packages/base/functions/lapply" target="_blank" rel="noopener noreferrer">`lapply()`</a>, feel free to take <a href="https://campus.datacamp.com/courses/intermediate-r/chapter-4-the-apply-family?ex=1" target="_blank" rel="noopener noreferrer">Chapter 4 of the Intermediate R course</a>.

Have a look at the example code below:

```{r}
my_workbook <- lapply(excel_sheets("data.xlsx"),
                      read_excel,
                      path = "data.xlsx")
```

The <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`read_excel()`</a> function is called multiple times on the `"data.xlsx"` file and each sheet is loaded in one after the other. The result is a list of data frames, each data frame representing one of the sheets in `data.xlsx`.

You're still working with the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a> file.

**Steps**

1. Use <a href="http://www.rdocumentation.org/packages/base/functions/lapply" target="_blank" rel="noopener noreferrer">`lapply()`</a> in combination with <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`excel_sheets()`</a> and <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`read_excel()`</a> to read all the Excel sheets in `"urbanpop.xlsx"`. Name the resulting list `pop_list`.
2. Print the structure of `pop_list`.

```{r}
# Read all Excel sheets with lapply(): pop_list
pop_list <- lapply(excel_sheets("data/urbanpop.xlsx"), read_excel, path = "data/urbanpop.xlsx")

# Display the structure of pop_list
str(pop_list)
```

Congratulations! If you're clever, reading multiple Excel sheets doesn't require a lot of coding at all!

## readxl (2)

Theory. Coming soon ...

**1. readxl (2)**

Next to the path and sheet arguments that we discussed in the previous video,

**2. read_excel()**

there are also other arguments you can specify. Have a look at the usage of the read_excel function as it appears in the documentation; can you tell the meaning of these arguments? path and sheets have no secrets for you anymore.

**3. read_excel() - col_names**

Then, there's col_names, which can be three things: TRUE, the default, which means what the first row of the Excel sheet contains the column names. It can also be FALSE, in which case R comes up with its own names. Finally, you can also set it a character vector specifying the column names. Excel can contain different data types, such as text, numerics and dates. How this converts to R classes is controlled in the col_types argument:

**4. read_excel() - col_types**

By default it is NULL, which has R guess the data types of the different columns. But you can also manually specify the col_types. Suppose you want to import all the columns of the first sheet as text. This command will do the trick: The column classes we can see in the printout reveal that indeed, the pop_data variable contains two columns that two are character now, perfect. Other keywords to enforce a type are "numeric", "date" and "blank". "numeric" and "date" are straightforward, but what about "blank"?

**5. read_excel() - col_types**

Well, if you use blank, you're simply ignoring that column. If we adapt the previous call like this: You'll see that the population column is missing. This "blank" keyword can come in handy when you have an Excel file with a lot of columns and you only need some of them in R.

**6. read_excel() - skip**

Next, there's the skip argument. It specifies the number of rows in your excel sheet R has to skip before actually importing the data. Let's say the first two rows in the first sheet of cities-dot-xlsx are not necessary for our analysis. To ignore them, we can simply set the skip argument to 2 and read_excel will not import these lines. we'll also have to set the col_names argument, because the first row with the column names is skipped as well. if we try out this code, indeed 2 rows were skipped: the row containing the column names and the first observations. That leaves us with the last three observations. In readr, there was also the n_max argument, to specify the number of records to read. Currently this functionality is not available in readxl, but this might be added in the future. The package is still under development as we speak.

**7. Wrap-up**

Actually, the excel_sheets and read_excel are the only functions that are available in the readxl package. These two functions and the customization possibilities are all you need to get started with your excel data in R. On top of all that, readxl is extremely fast. You might have recognized many of the arguments of the readr package of the previous chapter. Hadley Wickham made reading data from different data sources pretty consistent. This can only make your job of importing data easier, right? 

## The col_names argument

Apart from `path` and `sheet`, there are several other arguments you can specify in <a href="https://cran.r-project.org/web/packages/readxl/readxl.pdf" target="_blank" rel="noopener noreferrer">`read_excel()`</a>. One of these arguments is called `col_names`.

By default it is `TRUE`, denoting whether the first row in the Excel sheets contains the column names. If this is not the case, you can set `col_names` to `FALSE`. In this case, R will choose column names for you. You can also choose to set `col_names` to a character vector with names for each column. It works exactly the same as in the `readr` package.

You'll be working with the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop_nonames.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop_nonames.xlsx`</a> file. It contains the same data as <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a> but has no column names in the first row of the excel sheets.


**Steps**

1. Import the *first* Excel sheet of `"urbanpop_nonames.xlsx"` and store the result in `pop_a`. Have R set the column names of the resulting data frame itself.
2. Import the first Excel sheet of `urbanpop_nonames.xlsx`; this time, use the `cols` vector that has already been prepared for you to specify the column names. Store the resulting data frame in `pop_b`.
3. Print out the summary of `pop_a`.
4. Print out the summary of `pop_b`. Can you spot the difference with the other summary?

```{r,eval=TRUE}
# Import the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a
pop_a <- read_excel("data/urbanpop_nonames.xlsx", col_names = F)

# Import the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b
cols <- c("country", paste0("year_", 1960:1966))
pop_b <- read_excel("data/urbanpop_nonames.xlsx", col_names = cols)

# Print the summary of pop_a
summary(pop_a)

# Print the summary of pop_b
summary(pop_b)
```

Well done! Did you spot the difference between the summaries? It's really crucial to correctly tell R whether your Excel data contains column names. If you don't, the head of the data frame you end up with will contain incorrect information...

## The skip argument

Another argument that can be very useful when reading in Excel files that are less tidy, is `skip`. With `skip`, you can tell R to ignore a specified number of rows inside the Excel sheets you're trying to pull data from. Have a look at this example:


```{r}
read_excel("data.xlsx", skip = 15)
```

In this case, the first 15 rows in the first sheet of `"data.xlsx"` are ignored.

If the first row of this sheet contained the column names, this information will also be ignored by `readxl`. Make sure to set `col_names` to `FALSE` or manually specify column names in this case!

The file <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a> is available in your directory; it has column names in the first rows.

**Steps**

1. Import the *second* sheet of `"urbanpop.xlsx"`, but skip the first 21 rows. Make sure to set `col_names = FALSE`. Store the resulting data frame in a variable `urbanpop_sel`.
2. Select the first observation from `urbanpop_sel` and print it out.

```{r,eval=TRUE}
# Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel
urbanpop_sel <- read_excel("data/urbanpop.xlsx", sheet = 2, col_names = F, skip = 21)

# Print out the first observation from urbanpop_sel
urbanpop_sel[1,]
```

Nice job! This is about as complicated as the [`read_excel()`](https://cran.r-project.org/web/packages/readxl/readxl.pdf) call can get! Time to learn about another package to import data from Excel: gdata.

## gdata

Theory. Coming soon ...

**1. gdata**

Before, I introduced the readxl package, which is a very efficient package to import data from excel to R. But there are of course alternatives to readxl. In this video, I'll be discussing one of those alternatives:

**2. gdata**

the gdata package, currently maintained by Gregory Warnes.Actually, the gdata package is an entire suite of tools for performing data manipulation in all sorts of fields. It kind of supercharges your basic R distribution to make handling your data less painful. Among these tools, there is also a function that allows you to import Excel data: read dot xls. Out of the box, gdata can only handle the xls format, but you can easily install a driver to support the newer xlsx format as well. There's no function to list the different sheets in an excel file, like excel_sheets in readxl.

**3. gdata**

To import data from Excel, gdata uses Perl, another interpreted programming language. The Perl code converts the data in the excel sheet to a csv file.

**4. gdata**

Next, this csv file is read into R using the default read-dot-csv function from the utils package.

**5. gdata**

read-dot-csv itself is a wrapper around read-dot-table, remember?

**6. gdata**

This function can be customized in millions of ways; there are more than 15 arguments you can specify. All these read-dot-table arguments are also available for gdata read-dot-xls function. In this respect, the read-dot-xls function is an extension of the data input functions from the utils package to Excel files. This makes it easy to use for people who are familiar with the import functions of the utils package and their arguments.On the other hand, first converting an entire xls file to a csv file, to next import it into R with the slow read dot csv function is extremely inefficient. What if you're dealing with huge Excel files? You'd have to convert the entire file, and then read it. That's quite some extra work you're doing. Wickham's readxl package is way faster here.So why use gdata in the first place then? Well, we believe that readxl will become the standard package to import Excel data. However, as we speak, the readxl package is still under development: the version we're working with is pre-V1. If you don't want to rely on packages that are still under heavy development and whose syntax can still change, you can stick to more established packages like gdata.Now, let's get practical. Still remember the cities dot xlsx file?

**7. cities.xls**

Here it is as a xls file this time, cities dot xls. Two sheets, containing the population of some capitals in 1990 and 2000.

**8. read.xls()**

Let's first install and load gdata, and then try gdata's read-dot-xls function to import the excel data.As before, the first argument you pass it is the path to the file; in our case, cities-dot-xls is still in the working directory. If your Excel file contains multiple sheets, only the first sheet is imported. You can again choose to specify a different sheet by sheet number or by sheet name, like this example.

## Import a local file

In this part of the chapter you'll learn how to import `.xls` files using the `gdata` package. Similar to the `readxl` package, you can import single Excel sheets from Excel sheets to start your analysis in R.

You'll be working with the <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xls" target="_blank" rel="noopener noreferrer">`urbanpop.xls`</a> dataset, the `.xls` version of the Excel file you've been working with before. It's available in your current working directory.


**Steps**

1. Load the `gdata` package with <a href="http://www.rdocumentation.org/packages/base/functions/library" target="_blank" rel="noopener noreferrer">`library()`</a>. `gdata` and Perl are already installed on DataCamp's Servers.
2. Import the second sheet, named `"1967-1974"`, of `"urbanpop.xls"` with <a href="http://www.rdocumentation.org/packages/gdata/functions/read.xls" target="_blank" rel="noopener noreferrer">`read.xls()`</a>. Store the resulting data frame as `urban_pop`.
3. Print the first 11 observations of `urban_pop` with <a href="http://www.rdocumentation.org/packages/utils/functions/head" target="_blank" rel="noopener noreferrer">`head()`</a>.

```{r,eval=TRUE}
# Load the gdata package
library("gdata")

# Import the second sheet of urbanpop.xls: urban_pop
urban_pop <- read.xls("data/urbanpop.xls", sheet = "1967-1974")

# Print the first 11 observations using head()
head(urban_pop, n = 11)
```

Congratulations! There seems to be a lot of missing data, but [`read.xls()`](http://www.rdocumentation.org/packages/gdata/functions/read.xls) knows how to handle it. In the next exercise you will learn which arguments you can use in `read.xls()`.

## read.xls() wraps around read.table()

Remember how <a href="http://www.rdocumentation.org/packages/gdata/functions/read.xls" target="_blank" rel="noopener noreferrer">`read.xls()`</a> actually works? It basically comes down to two steps: converting the Excel file to a `.csv` file using a Perl script, and then reading that `.csv` file with the <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.csv()`</a> function that is loaded by default in R, through the `utils` package.

This means that all the options that you can specify in <a href="http://www.rdocumentation.org/packages/utils/functions/read.table" target="_blank" rel="noopener noreferrer">`read.csv()`</a>, can also be specified in <a href="http://www.rdocumentation.org/packages/gdata/functions/read.xls" target="_blank" rel="noopener noreferrer">`read.xls()`</a>.

The <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xls" target="_blank" rel="noopener noreferrer">`urbanpop.xls`</a> dataset is already available in your workspace. It's still comprised of three sheets, and has column names in the first row of each sheet.

**Steps**

1. Finish the `read.xls()` call that reads data from the second sheet of `urbanpop.xls`: skip the first 50 rows of the sheet. Make sure to set `header` appropriately and that the country names are not imported as factors.
2. Print the first 10 observations of `urban_pop` with `head()`.

```{r,eval=TRUE}
# Column names for urban_pop
columns <- c("country", paste0("year_", 1967:1974))

# Finish the read.xls call
urban_pop <- read.xls("data/urbanpop.xls", sheet = 2,
                      skip = 50, header = F, stringsAsFactors = F,
                      col.names = columns)

# Print first 10 observation of urban_pop
head(urban_pop, n = 10)
```

## Work that Excel data!

Now that you can read in Excel data, let's try to clean and merge it. You already used the <a href="http://www.rdocumentation.org/packages/base/functions/cbind" target="_blank" rel="noopener noreferrer">`cbind()`</a> function some exercises ago. Let's take it one step further now.

The <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xls" target="_blank" rel="noopener noreferrer">`urbanpop.xls`</a> dataset is available in your working directory. The file still contains three sheets, and has column names in the first row of each sheet.


**Steps**

1. Add code to read the data from the third sheet in `"urbanpop.xls"`. You want to end up with three data frames: `urban_sheet1`, `urban_sheet2` and `urban_sheet3`.
2. Extend the `cbind()` call so that it also includes `urban_sheet3`. Make sure the first column of `urban_sheet2` and `urban_sheet3` are removed, so you don't have duplicate columns. Store the result in `urban`.
3. Use <a href="http://www.rdocumentation.org/packages/stats/functions/na.fail" target="_blank" rel="noopener noreferrer">`na.omit()`</a> on the `urban` data frame to remove all rows that contain `NA` values. Store the cleaned data frame as `urban_clean`.
4. Print a summary of `urban_clean` and assert that there are no more `NA` values.

```{r,eval=TRUE}
# Add code to import data from all three sheets in urbanpop.xls
path <- "data/urbanpop.xls"
urban_sheet1 <- read.xls(path, sheet = 1, stringsAsFactors = FALSE)
urban_sheet2 <- read.xls(path, sheet = 2, stringsAsFactors = FALSE)
urban_sheet3 <- read.xls(path, sheet = 3, stringsAsFactors = FALSE)

# Extend the cbind() call to include urban_sheet3: urban
urban <- cbind(urban_sheet1, urban_sheet2[-1], urban_sheet3[-1])

# Remove all rows with NAs from urban: urban_clean
urban_clean <- na.omit(urban)

# Print out a summary of urban_clean
summary(urban_clean)
```

Awesome! Time for something totally different: `XLConnect`.

# 4. Reproducible Excel work with XLConnect

Beyond importing data from Excel, you can take things one step further with XLConnect. Learn all about it and bridge the gap between R and Excel.

## Reading sheets

Theory. Coming soon ...

**1. Reading sheets**

You already got to know two very useful packages to work with Excel data - the readxl package and the gdata package. Now imagine this situation.

**2. Insert title here...**

You're working in a big company that uses Excel for all there analysis work. You are forced to work on and deliver these Excel files, but you want to work on the data through R, so that you can modify the files in a reproducible way? I'm happy to tell you that there is a package that does just that:

**3. XLConnect**

XLConnect, written and maintained by Martin Studer. He created one of the most comprehensive packages for working with Excel files through R.You can think of XLConnect as a bridge between Excel and R. That means you can do practically any action you could do within Excel but you do it from inside R. Editing Excel sheets, formatting data and adapting entire calculation sheets, you name it. XLConnect has a function for it. XLConnect works with xls and xlsx files, and has easy-to-use functions.To get this all working so smoothly, XLConnect depends on Java. This is totally abstracted away for us end-users, but installing the package can have its difficulties.

**4. Installation**

If you're starting from a reasonably clean computing environment, this traditional install-dot-packages command will work fine. From the messaging, you'll see that it also installs the XLConnectJars package containing Java files and class definitions that XLConnect depends on. If it wasn't installed already, the rJava package will also be installed, providing a low-level R to Java interface that XLConnect uses.If something goes wrong during installation, it's possible that you first have to install the Java Development Kit, or JDK, from Oracle's web site. If things still don't work out, I suggest you google the errors you're getting: there's quite some people using this package so help is never far away.With our package installed and not to forget, loaded,

**5. loadWorkbook()**

let's take the first step: loading a workbook into R. You do this with the loadWorkbook function, by simply passing the name of the excel file you want to interface to. Assuming that our cities-dot-xlsx file is still in the current working directory, this call works.If you have a look at the structure of book, we see that it is a so-called workbook object. This object is the actual "bridge" between R and Excel I talked about earlier.After building a workbook object in R, you can use it to get information on the Excel file it links to. To get the names of the different sheets,

**6. getSheets()**

for example, you can use getSheets.The result is exactly the same to the excel_sheets function from readxl: a character vector containing the two sheet names.Apart from sheet information,

**7. readWorksheet()**

you can also read the actual data from the sheets, like readxl's read_excel function and gdata's read dot xls function. Suppose we want to import the data from the year_2000 sheet as a data frame. As the first argument to readWorksheet, we pass the workbook object, book in our case. The second argument, sheet, is the name or index of the sheet you want to import from.Works just like before. The cool thing here is, that you can easily specify from which row and which column to start reading information.

**8. readWorksheet()**

Say you only want the population information for Berlin and Madrid. You can simply set startRow to 3, endRow to 4 and startCol to 2. Because you skipped the first row, the column names are also skipped, so you should set header to FALSE.

## Connect to a workbook

When working with `XLConnect`, the first step will be to load a workbook in your R session with <a href="http://www.rdocumentation.org/packages/XLConnect/functions/loadWorkbook" target="_blank" rel="noopener noreferrer">`loadWorkbook()`</a>; this function will build a "bridge" between your Excel file and your R session.

In this and the following exercises, you will continue to work with <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a>, containing urban population data throughout time. The Excel file is available in your current working directory.

**Steps**

1. Load the `XLConnect` package using <a href="http://www.rdocumentation.org/packages/base/functions/library" target="_blank" rel="noopener noreferrer">`library()`</a>; it is already installed on DataCamp's servers.
2. Use <a href="http://www.rdocumentation.org/packages/XLConnect/functions/loadWorkbook" target="_blank" rel="noopener noreferrer">`loadWorkbook()`</a> to build a connection to the `"urbanpop.xlsx"` file in R. Call the workbook `my_book`.
3. Print out the class of `my_book`. What does this tell you?

```{r,eval=TRUE}
# Load the XLConnect package
library("XLConnect")

# Build connection to urbanpop.xlsx: my_book
my_book <- loadWorkbook("data/urbanpop.xlsx")

# Print out the class of my_book
class(my_book)
```


## List and read Excel sheets

Just as `readxl` and `gdata`, you can use `XLConnect` to import data from Excel file into R.

To list the sheets in an Excel file, use <a href="http://www.rdocumentation.org/packages/XLConnect/functions/getSheets-methods" target="_blank" rel="noopener noreferrer">`getSheets()`</a>. To actually import data from a sheet, you can use <a href="http://www.rdocumentation.org/packages/XLConnect/functions/readWorksheet-methods" target="_blank" rel="noopener noreferrer">`readWorksheet()`</a>. Both functions require an XLConnect workbook object as the first argument.

You'll again be working with <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a>. The `my_book` object that links to this Excel file has already been created.


**Steps**

1. Print out the sheets of the Excel file that `my_book` links to.
2. Import the second sheet in `my_book` as a data frame. Print it out.

```{r,eval=TRUE}
# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("data/urbanpop.xlsx")

# List the sheets in my_book
getSheets(my_book)

# Import the second sheet in my_book
readWorksheet(my_book, "1967-1974")
```

## Customize readWorksheet

To get a clear overview about <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a> without having to open up the Excel file, you can execute the following code:


```{r}
my_book <- loadWorkbook("urbanpop.xlsx")
sheets <- getSheets(my_book)
all <- lapply(sheets, readWorksheet, object = my_book)
str(all)
```

Suppose we're only interested in urban population data of the years 1968, 1969 and 1970. The data for these years is in the columns 3, 4, and 5 of the second sheet. Only selecting these columns will leave us in the dark about the actual countries the figures belong to.


**Steps**

1. Extend the `readWorksheet()` command with the `startCol` and `endCol` arguments to only import the columns 3, 4, and 5 of the second sheet.
2. `urbanpop_sel` no longer contains information about the countries now. Can you write another `readWorksheet()` command that imports only the first column from the second sheet? Store the resulting data frame as `countries`.
3. Use `cbind()` to paste together `countries` and `urbanpop_sel`, in this order. Store the result as `selection`.

```{r,eval=TRUE}
# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("data/urbanpop.xlsx")

# Import columns 3, 4, and 5 from second sheet in my_book: urbanpop_sel
urbanpop_sel <- readWorksheet(my_book, sheet = 2, startCol = 3, endCol = 5)

# Import first column from second sheet in my_book: countries
countries <- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1)

# cbind() urbanpop_sel and countries together: selection
selection <- cbind(countries, urbanpop_sel)
```

## Adapting sheets

Theory. Coming soon ...

**1. Adapting sheets**

Up to now, XLConnect isn't blowing your mind, is it? Just listing sheets and importing data from them into R is nothing new. But XLConnect has more to offer than just importing excel data into R. This package is an easy tool to modify the content of your workbook comfortably through R.

**2. New data!**

Say you managed to lay your hands om some fresh population data from 2010, that is stored in a data frame, pop_2010.

**3. createSheet()**

To store this info a new sheet, we start with loading XLConnect, and making a connection to the workbook. After that, we can use createSheet, and pass the workbook and the name of the new sheet, like this.

**4. createSheet()**

Now we can actually populate our new sheet with the data, for which we'll use

**5. writeWorksheet()**

writeWorksheet. The first argument, as always, is the workbook, followed by the data we want to add, so pop_2010 and finally the sheet we want to add it to. Let's use the sheet name that was specified in createSheet, but the number of the sheet, 3, would work fine as well.If you open the excel file, though, you won't see the new sheet. You'll have to explicitly save the entire workbook to a file for the changes to take effect.

**6. saveWorkbook()**

You do this with saveWorkbook, like this.

**7. saveWorkbook()**

I suggest you specify a new filename, cities2.xlsx for example, so you don't overwrite the file you started with.If you now check out the new Excel file, you see that the additional data is in there. Awesome!Suppose that after creating this additional worksheet, you don't feel comfortable with the name you chose. In fact, you want to rename all sheets. Piece of cake:

**8. renameSheet()**

just use the renameSheet function. As uaual, the first argument is the workbook, and then you pass the old name and the new name. We'll use this command three times, the change year with y for the three different sheets.

**9. renameSheet()**

Finally, we save the result again to a new file with saveWorksheet: cities3.xlsx.A quick peek at the new Excel file reveals that we successfully renamed the sheets. Perfect.

**10. removeSheet()**

Another Excel job would be to remove a sheet altogether. To remove the third sheet here, for example, simply use removeSheet with the workbook and the sheet name or sheet number as arguments.

**11. removeSheet()**

If you save the workbook to a file again and open up the file, our third sheet is gone.

**12. Wrap-up**

Of course these are pretty basic operations, that you can easily do in Excel as well, but the cool thing is that you can program all of these tasks in R in a reproducible way. If you update the commands that we've used here, and run them all again, one after the other, it should all still work fine.Apart from the functions I discussed here, there are also methods to style cells, getting, setting and recalculate formulas, merging and splitting up cells, the whole shebang. But let's not dive into those here and start small.

## Add worksheet

Where `readxl` and `gdata` were only able to import Excel data, `XLConnect`'s approach of providing an actual interface to an Excel file makes it able to edit your Excel files from inside R. In this exercise, you'll create a new sheet. In the next exercise, you'll populate the sheet with data, and save the results in a new Excel file.

You'll continue to work with <a href="http://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/urbanpop.xlsx" target="_blank" rel="noopener noreferrer">`urbanpop.xlsx`</a>. The `my_book` object that links to this Excel file is already available.

**Steps**

1. Use <a href="http://www.rdocumentation.org/packages/XLConnect/functions/createSheet-methods" target="_blank" rel="noopener noreferrer">`createSheet()`</a>, to create a new sheet in `my_book`, named `"data_summary"`.
2. Use <a href="https://www.rdocumentation.org/packages/XLConnect/topics/getSheets-methods" target="_blank" rel="noopener noreferrer">`getSheets()`</a> to verify that `my_book` now represents an Excel file with four sheets.

```{r,eval=TRUE}
# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("data/urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Use getSheets() on my_book
getSheets(my_book)
```

Great! It's time to populate your newly created worksheet!

## Populate worksheet

The first step of creating a sheet is done; let's populate it with some data now! `summ`, a data frame with some summary statistics on the two Excel sheets is already coded so you can take it from there.

**Steps**

1. Use <a href="http://www.rdocumentation.org/packages/XLConnect/functions/writeWorksheet-methods" target="_blank" rel="noopener noreferrer">`writeWorksheet()`</a> to populate the `"data_summary"` sheet with the `summ` data frame.
2. Call <a href="http://www.rdocumentation.org/packages/XLConnect/functions/saveWorkbook-methods" target="_blank" rel="noopener noreferrer">`saveWorkbook()`</a> to store the adapted Excel workbook as a new file, `"summary.xlsx"`.

```{r,eval=TRUE}
# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("data/urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Create data frame: summ
sheets <- getSheets(my_book)[1:3]
dims <- sapply(sheets, function(x) dim(readWorksheet(my_book, sheet = x)), USE.NAMES = FALSE)
summ <- data.frame(sheets = sheets,
                   nrows = dims[1, ],
                   ncols = dims[2, ])

# Add data in summ to "data_summary" sheet
writeWorksheet(my_book, data = summ, sheet = "data_summary")

# Save workbook as summary.xlsx
saveWorkbook(my_book, "data/summary.xlsx")
```

Great! If you took the correct steps, the resulting Excel file looks like [this file](https://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/summary.xlsx).

## Renaming sheets

Come to think of it, `"data_summary"` is not an ideal name. As the summary of these excel sheets is always data-related, you simply want to name the sheet `"summary"`.

The code to build a connection to `"urbanpop.xlsx"` and create `my_book` is already provided for you. It refers to an Excel file with 4 sheets: the three data sheets, and the `"data_summary"` sheet.


**Steps**

1. Use `renameSheet()` to rename the fourth sheet to `"summary"`.
2. Next, call `getSheets()` on `my_book` to print out the sheet names.
3. Finally, make sure to actually save the `my_book` object to a new Excel file, `"renamed.xlsx"`.

```{r,eval=TRUE}
# Rename "data_summary" sheet to "summary"
renameSheet(my_book, "data_summary", "summary")

# Print out sheets of my_book
getSheets(my_book)

# Save workbook to "renamed.xlsx"
saveWorkbook(my_book, "data/renamed.xlsx")
```

Nice one! You can find the file you just created [here](https://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/renamed.xlsx).

## Removing sheets

After presenting the new Excel sheet to your peers, it appears not everybody is a big fan. Why summarize sheets and store the info in Excel if all the information is implicitly available? To hell with it, just remove the entire fourth sheet!

**Steps**

1. Load the `XLConnect` package.
2. Build a connection to `"renamed.xlsx"`, the Excel file that you've built in the previous exercise; it's available in your working directory. Store this connection as `my_book`.
3. Use `removeSheet()` to remove the fourth sheet from `my_book`. The sheet name is `"summary"`. Recall that `removeSheet()` accepts either the index or the name of the sheet as the second argument.
4. Save the resulting workbook, `my_book`, to a file `"clean.xlsx"`.

```{r,eval=TRUE}
# Load the XLConnect package
library("XLConnect")

# Build connection to renamed.xlsx: my_book
my_book <- loadWorkbook("data/renamed.xlsx")

# Remove the fourth sheet
removeSheet(my_book, 4)

# Save workbook to "clean.xlsx"
saveWorkbook(my_book, "data/clean.xlsx")
```

Nice one! The file you've created in this exercise is available [here](https://s3.amazonaws.com/assets.datacamp.com/production/course_1477/datasets/clean.xlsx).
