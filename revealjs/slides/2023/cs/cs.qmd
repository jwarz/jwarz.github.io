---
# TITLE & AUTHOR
title: "CoreSignal Analysis"
subtitle: "Current Status"
author: "Joschka Schwarz"
institute: "Hamburg University of Technology"
date: today
date-format: "dddd, D[<sup style='font-size:65%;font-style:italic;'>th</sup>] [of] MMMM YYYY"
section-divs: true
filters:
   - lightbox
lightbox: auto
engine: knitr
knitr:
  opts_chunk: 
    class-output: hscroll
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
  comment = '#>', fig.width = 6, fig.height = 6
)
library(pacman)
source("../../../../R/setup-ggplot2-tie-bright.R")
```

## Table of Contents {data-state="hide-menubar"}
<ul class="menu"><ul>

```{css, echo=FALSE}
.hscroll {
  overflow-x: auto;
  white-space: nowrap;
}
```

# Preperation {data-stack-name="Prep"}
Input for CoreSignal

# Data sources I {data-stack-name="Data I"}
1^st^ objective: Prepare `CrunchBase` / `PitchBook` and `CoreSignal` data

## Load & Init Companies (CrunchBase, PitchBook & CoreSignal)

::::::: {.panel-tabset}

### CBPB: SELECT

Crunchbase data contains **150,838** startups with a valid funding trajectory.

```{r}
#| label: pillar-options1
#| echo: false
p_width <- getOption("width")
options(
    pillar.print_max = 3,
    pillar.print_min = 3
)
```

```{r}
#| eval: false
p_load(arrow, dplyr, tidyr)

funded_companies_prqt <- open_dataset("funded_companies_identifiers.parquet") 
funded_companies_prqt
```

```{r}
#| echo: false
#| message: false
p_load(arrow, dplyr, tidyr)
funded_companies_prqt <- open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/funded_companies_identifiers.parquet")
funded_companies_prqt |> collect()
```

::: {.fragment}

Multiple domains (Unnesting via `Arrow` not possible. Options: `Spark` & `sparklyr.nested`):

```{r}
#| eval: true
fc_unnested_tbl <- funded_companies_prqt |> collect() |> 
                      # 1. Allow multiple domains per company. No multiple linkedin handles.
                      unnest(domain) 
fc_unnested_tbl
```

:::


### CBPB: WRANGLE

1. Must have identifier (domain, linkedin)
2. Clean identifiers
3. Remove duplicates

```{r}
#| eval: false
library(stringr)
fc_unnested_tbl |> 
  
  # 1. At least 1 identifier: 4.518 observations are filtered out
  filter(if_any(c(domain, linkedin_url), ~!is.na(.))) |>
  
  # 2. Extract linkedin handle & clean domains
  mutate(linkedin_handle = linkedin_url |> str_extract("(?<=linkedin\\.com/company/).*?(?=(?:\\?|$|/))")) |>
  mutate(domain          = domain |> clean_domain()) |>

  # 3. Remove 532 duplicates
  distinct()
```

```{r}
#| echo: false
remove_path <- function(x) {
  urltools::parameters(x) <- NULL
  urltools::path(x)       <- NULL
  x
  suffixes <- x |> urltools::suffix_extract()
  res <- str_c(suffixes$domain, ".", suffixes$suffix)
  res
}
library(stringr)
fc_wrangled_tbl <- fc_unnested_tbl |>

  # Only with domain or linkedin_url --> 4.518 observations are filtered out
  filter(if_any(c(domain, linkedin_url), ~!is.na(.))) |>

  # Extract linkedin handle & clean domains
  mutate(linkedin_handle = linkedin_url |> str_extract("(?<=linkedin\\.com/company/).*?(?=(?:\\?|$|/))")) |>
  mutate(domain = domain |> remove_path() |> str_remove("^www\\.")) |>
  select(-linkedin_url) |>

  # Distinct (Remove 532 duplicates)
  distinct()
fc_wrangled_tbl
```

--> [145.991]{style="color:#cc0000; font-weight: bold;"} distinct examineable companies.

### CBPB: CLEAN

`Issue:` Some extracted domains are not unique and associated with multiple companies.<br> `Manual Cleaning:` Domains with a count exceeding two were analyzed and set to NA if they do not correspond to the actual one.


```{r}
# ANALYZE
# fc_wrangled_tbl |> 
#   distinct(company_id, domain) |> 
#   count(domain, sort = T) |> 
#   filter(n>2)`

unwanted_domains_cb <- c("webflow.io", "angel.co", "weebly.com", "wordpress.com", "wixsite.com", "squarespace.com", 
                         "webflow.io", "crypt2esports.com", "myshopify.com", "business.site", "mystrikingly.com", 
                         "launchrock.com", "square.site", "google.com", "sites.google.com", "t.co", "linktr.ee",
                         "netlify.app", "itunes.apple.com", "apple.com", "crunchb.com", "tumblr.com", "linkedin.com",
                         "godaddysites.com", "mit.edu", "paloaltonetworks.com", " wpengine.com", "facebook.com",
                         "intuit.com", "medium.com", "salesforce.com", "strikingly.com", "wix.com", "cisco.com",
                         "digi.me", "apps.apple.com", "bit.ly", "fleek.co", "harvard.edu", "ibm.com", "jimdo.com",
                         "myftpupload.com", "odoo.com", "storenvy.com", "twitter.com", "umd.edu", "umich.edu", "vmware.com", "webs.com")

# Not all observations with unwanted domains are bad per se:
wanted_ids_cb <- c(angel = 128006, `catapult-centres-uk` = 115854, digime1 = 140904, digimi2 = 95430, fleek = 50738, 
                   jimdo = 108655, medium = 113415, storenvy = 85742, strikingly = 95831, substack = 34304, 
                   tumblr = 84838, twitter = 53139, weebly = 91365, wpengine = 91720)

# Set misleading domains to NA
funded_companies_clnd <- fc_wrangled_tbl |> 
                              
  mutate(domain = if_else(
    domain %in% unwanted_domains_cb & !(company_id %in% wanted_ids_cb), 
    NA_character_, domain))
```

### CS: SELECT

```{r}
#| echo: false
options(
    pillar.print_max = 3,
    pillar.print_min = 3
)
```

It appears that CoreSignal has been able to locate **45.026** companies within our gathered data.

```{r}
#| eval: false
# Selection & Wrangle has been done already
cs_companies_base_slct <- readRDS("cs_companies_base_slct.rds") 
cs_companies_base_slct
```

```{r}
#| echo: false
cs_companies_base_slct <- readRDS("~/Library/Mobile Documents/com~apple~CloudDocs/01_tuhh/02_diss/01_data_sources/06_core_signal/01_data/04_wrangled/cs_companies_base_slct.rds") 
cs_companies_base_slct
```

::: {.fragment}

```{r}
cs_companies_base_slct$id |> n_distinct()
```

:::
::: {.fragment}

```{r}
cs_companies_base_slct |> janitor::get_dupes(id)
```

:::

### CS: WRANGLE

Nothing to wrangle ...

```{r}
cs_companies_base_wrangled <- cs_companies_base_slct |> select(-name) |> 
  
                                        # Add suffixes to col names
                                        rename_with(~ paste(., "cs", sep = "_"))
```

### CS: CLEAN

::: {.callout-important}
More cleaning necessary (same as CBPB)! The task was undertaken with a limited degree of enthusiasm.
:::

```{r}
unwanted_domains_cs    <- c("bit.ly", "linktr.ee", "facebook.com", "linkedin.com", "twitter.com", "crunchbase.com")
wanted_ids_cs          <- c(crunchbase = 1634413, linkedin = 8568581, twitter = 24745469)

cs_companies_base_clnd <- cs_companies_base_wrangled |> 
  
  mutate(domain_cs = if_else(
    domain_cs %in% unwanted_domains_cs & !(id_cs %in% wanted_ids_cs), 
    NA_character_, 
    domain_cs)
    )
```


:::::::

# Data sources II {data-stack-name="Data II"}
2^nd^ objective: Match `CrunchBase` / `PitchBook` with `CoreSignal` data

## Join *companies*, *member experiences* and *funding* information

::::::: {.panel-tabset}

### Companies

```{r}
#| echo: false
options(
    pillar.print_max = 5,
    pillar.print_min = 5
)
```

We were able to match [37.287]{.fragment fragment-index=1 style="color:#cc0000; font-style: italic;"} CS & CB/PB companies.

```{r}
#| eval: false
cb_cs_joined <- funded_companies_clnd |> 

    # Leftjoins
    left_join(cs_companies_base_clnd |> select(id_cs, domain_cs),          by = c(domain          = "domain_cs"),          na_matches = "never") |> 
    left_join(cs_companies_base_clnd |> select(id_cs, linkedin_handle_cs), by = c(linkedin_handle = "linkedin_handle_cs"), na_matches = "never") |> 

    # Remove obs with no cs_id
    filter(!is.na(id_cs)) |>
    
    # Remove matches, that matched different domains, but same company (e.g. company_id: 83060, id_cs: 4507928) block.xyz & squareup.com
    select(company_id, id_cs) |> 
    distinct()
    
cb_cs_joined
```

```{r}
#| echo: false
library(purrr)
cb_cs_joined <- funded_companies_clnd |> 

    # LeftJoins
    left_join(cs_companies_base_clnd |> select(id_cs, domain_cs),          by = c(domain          = "domain_cs"),          na_matches = "never") |> 
    left_join(cs_companies_base_clnd |> select(id_cs, linkedin_handle_cs), by = c(linkedin_handle = "linkedin_handle_cs"), na_matches = "never") |> 

    # Allow multiple matches: Merge ids (id_cs.x, id_cs.y) into one column (due to multiple left_joins) and unnest()
    rowwise() |> 
    mutate(id_cs = list(c(id_cs.x, id_cs.y) |> na.omit() |> unique()))

    cb_cs_joined$id_cs <- modify_if(cb_cs_joined$id_cs, ~length(.) == 0, ~NA_integer_)
    cb_cs_joined <- cb_cs_joined |> 
          
          select(-c(id_cs.x,id_cs.y)) |> 
          unnest(id_cs) |>
          
          # Remove obs with no cs_id
          filter(!is.na(id_cs)) |>
          
          # Remove matches, that matched different domains, but same company (e.g. company_id: 83060, id_cs: 4507928) block.xyz & squareup.com
          select(company_id, id_cs) |> 
          distinct()
    
cb_cs_joined
```

::: {.fragment fragment-index=1}

```{r}
cb_cs_joined |> distinct(company_id) |> nrow()
```

:::

### Jobs (all)

We got over 460 million employment observations from CoreSignal.

```{r}
#| eval: false
# Other data versions
# 1. Complete: 
member_experience_dt 
#> {462.711.794}  

# 2. Distinct1: 
member_experience_dist_dt <- unique(member_experience_dt) 
#> {432.368.479}

# 3. Distinct2: 
unique(member_experience_dist_dt[order(id)], by = setdiff(names(member_experience_dist_dt), "id")) 
#> {431.899.547}
```

### Jobs (dist)

But only ~50 Mil distinct employments

```{r}
#| eval: false
# Load distinct member experiences
me_dist8_prqt <- arrow::open_dataset("cs_me_dist8_unest_empl_hist.parquet") 
me_dist8_prqt |> glimpse()
```

```{r}
#| echo: false
# Load distinct member experiences
me_dist8_prqt <- arrow::open_dataset("~/02_diss/01_data_sources/01_coresignal/02_data/03_wrangled/cs_me_dist8_unest_empl_hist.parquet") 
me_dist8_prqt |> 
  glimpse()
```

Example

```{r}
#| eval: false
me_orig <- open_dataset("~/02_diss/01_data_sources/01_coresignal/02_data/member_experience/me_orig/")
me_dist <- open_dataset("~/02_diss/01_data_sources/01_coresignal/02_data/member_experience/me_dist/")

me_orig |> filter(member_id == 4257, company_id == 9007053) |> collect() |> as_tibble() |> arrange(date_from_parsed) |> print(n=19)
me_dist |> filter(member_id == 4257, company_id == 9007053) |> collect() |> as_tibble() |> arrange(date_from_parsed)
```

### Jobs (focal)

Over 10 million [(valid: must have starting date)]{style="color:#cc0000; font-style: italic;"} employments at our crunchbase / pitchbook data set companies. 385.100 with a title containing the string [founder]{style="color:rgb(0, 94, 115); font-style: italic;"}.

```{r}
# Distinct company ids
cb_cs_joined_cs_ids <- cb_cs_joined |> distinct(id_cs) |> pull(id_cs)
me_wrangled_prqt    <- me_dist8_prqt |> 
  
                          # Select features
                          select(member_id, company_id, exp_id = "id", date_from_parsed) |> 
                          
                          # Select observations
                          filter(company_id %in% cb_cs_joined_cs_ids) |> 
                          # - 967.080 observations (date_to not considered yet)
                          filter(!is.na(date_from_parsed)) |> 

                          # Add suffix to col names
                          rename_with(~ paste(., "cs", sep = "_")) |> 
                          compute()

me_wrangled_prqt |> 
  glimpse()
```

### Funding

Multiple Funding Dates --> [Take oldest]{style="color:#cc0000;"}

```{r}
#| eval: false
fc_wrangled_tbl <- funded_companies_tbl |> 
  
  # Consider multiple founding dates: Take oldest founding date
  unnest(founded_on) |> 
  arrange(company_id, founded_on) |> 
  group_by(company_id) |> slice(1) |> ungroup()
```

```{css, echo=FALSE}
.cell-top-margin {
    margin-top: 0.5rem;
}
```

Example of funding round data:

```{r}
#| eval: false
fc_wrangled_tbl$funding_rounds[[1]] |>  
  glimpse()
```

```{r}
#| echo: false
funded_companies_wrangled_sliced_tbl <- readRDS("~/Library/Mobile Documents/com~apple~CloudDocs/01_tuhh/02_diss/02_data_mapping/10_cbpb_cs/02_data/funded_companies_wrangled_sliced_tbl.rds")
funded_companies_wrangled_sliced_tbl$funding_rounds[[1]] |> 
  glimpse()
```

### Conversion

* Joining via `dplyr` due to memory constraint not possible.<br>
* Joining via `Arrow` due to structure constraints not possible.<br>
* --> Joining via `data.table` most efficient.<br>

Conversion to `data.tables` necessary:

```{r}
#| eval: false
# 1.  Funding Data
# 1.1 Level 1
fc_wrangled_dt |> setDT()

# 1.2 Funding Data Level 2 (funding_rounds)
purrr::walk(fc_wrangled_dt$funding_rounds, setDT)

# 1.3 Remove unnecessary columns + initialize dummy for before_join
purrr::walk(fc_wrangled_dt$funding_rounds, ~ .x[, 
          `:=`(round_uuid_pb = NULL, round_uuid_cb        = NULL, round_new          = NULL, round          = NULL,
               exit_cycle    = NULL, last                 = NULL, round_type         = NULL, round_type_new = NULL, 
               round_types   = NULL, post_money_valuation = NULL, investors_in_round = NULL, before_join    = NA)
          ]
        )

# 2. Matching Table
cb_cs_joined_slct_dt |> setDT()

# 3. Member experiences
me_wrangled_dt <- me_wrangled_prqt |> collect()
```

### Join

Working `data.table` solution (efficiency increase through join `by reference` possible).

```{r}
#| eval: false
# 1. Add company_id from funded_companies to member experiences
me_joined_dt <- cb_cs_joined_slct_dt[me_wrangled_dt, on = .(id_cs = company_id_cs), allow.cartesian = TRUE]
#> 12.978.226

# 2. Add funding data from funded_companies
me_joined_dt <- fc_wrangled_dt[me_joined_dt, on = .(company_id)]
#> 12.270.572

# 3. Remove duplicates (why are there any?)
me_joined_dt <- unique(me_joined_dt, by = setdiff(names(me_joined_dt), "funding_rounds"))
#> 12.270.572 .... No duplicates anymore. Removed from cb_cs_joined_slct_dt
```

Not working `dplyr` solution

```{r}
#| eval: false
me_joined_dt_dplyr <- me_wrangled_dt |>

  # Add company_id from funded_companies
  left_join(cb_cs_joined_slct_dt,
            by = c(company_id_cs = "id_cs")) |>

  # Add data from funded_companies
  left_join(funded_companies_wrangled_dt,
            by = "company_id")  |>
  distinct()
```

`Arrow` because of nested funding data not possible.

:::::::

# Analysis {data-stack-name="Features"}
Using domain knowledge to extract features

## Feature Engineering

::::::: {.panel-tabset}

### F1: T~join~ - T~found~

How many month have passed since the company was founded and before the person joined the company (in months)?

```{r}
#| eval: false
library(lubridate)
me_joined_dt[, tjoin_tfound := (interval(founded_on, date_from_parsed_cs) %/% months(1))]
```

### Prep1

Unnesting necessary due to memory constraints (takes multiple hours ... to be measured).

```{r}
#| eval: false
# Working: data.table
me_joined_unnested_dt <- me_joined_dt[,rbindlist(funding_rounds), by = setdiff(names(me_joined_dt), "funding_rounds")]
# Not working: dplyr
me_joined_unnested_tbl <- me_joined_dt |> unnest(funding_rounds)
```

Add feature whether or not member joined before Announcement of a funding round:

```{r}
#| eval: false
# Add feature whether or not member joined before Announcement of a funding round
me_joined_unnested_dt[,before_join := date_from_parsed_cs >= announced_on]

# Inspect
open_dataset("me_joined_unnested1.parquet") |> 
  glimpse()
```

```{r}
#| echo: false
# Unnested1 without features 2 dnd 3
me_joined_unnested_prqt <- arrow::open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/me_joined_unnested1.parquet")
me_joined_unnested_prqt |> 
  dplyr::glimpse()
```

### F2, F3

F2. How much capital has been acquired by the time the person joins?<br>
F3. How many funding rounds have been acquired by the time the person joins?

```{r}
#| eval: false
# Initialize empty columns (not sure yet if that increases performance)
me_joined_unnested_dt[, `:=` (raised_amount_before_join = NA_real_, 
                              num_rounds_before_join    = NA_real_)]

# Add features
me_joined_unnested_dt[, `:=` (raised_amount_before_join = sum(raised_amount[before_join == T], na.rm = T),
                              num_rounds_before_join    = sum(  before_join[before_join == T])), 
                      by = .(company_id, exp_id_cs)]

open_dataset("me_joined_unnested2.parquet") |> 
  glimpse()
```

```{r}
#| echo: false
me_joined_unnested_prqt <- arrow::open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/me_joined_unnested2.parquet")
me_joined_unnested_prqt |> 
  dplyr::glimpse()
```

### Prep2

Nest again

```{r}
#| eval: false
# data.table
excluded_cols       <- setdiff(names(me_joined_unnested_dt), c("round_id", "announced_on", "raised_amount", "before_join"))
me_joined_nested_dt <- me_joined_unnested_dt[, list(funding_rounds=list(.SD)), by=excluded_cols]

# Dplyr (not working)
# me_joined_nested_dt <- me_joined_unnested_dt |> 
#         nest(funding_rounds = c("round_id", "announced_on", "raised_amount", "before_join"))

open_dataset("me_joined_nested.parquet") |> 
  glimpse()
```

```{r}
#| echo: false
me_joined_nested_prqt <- open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/me_joined_nested.parquet")
me_joined_nested_prqt |> 
  glimpse()
```

### Titles

To differentiate between founder and non-founder CS titles are needed

```{r}
#| eval: false
# Prep data (shrink / remove unnecessary data)
me_joined_nested_foc_dt[, funding_rounds := NULL]

# Prep titles
me_wrangled_wt_dt <-  me_dist_prqt |> 
                          filter(company_id %in% cb_cs_joined_cs_ids, !is.na(date_from_parsed)) |>  
                          select(exp_id_cs, title_cs) |> 
                          collect() |> 
                          setDT()

# Join
me_joined_nested_foc_dt[me_wrangled_wt_dt, on = .(exp_id_cs), title_cs := i.title_cs]

# Inspect
open_dataset("me_joined_nested_foc.parquet") |> 
  glimpse()
```

```{r}
#| echo: false
me_joined_nested_foc_prqt <- open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/me_joined_nested_foc.parquet")
me_joined_nested_foc_prqt |> 
  glimpse()
```

:::::::


# Univariate summary statistics {data-stack-name="Plots"}
Describing patterns found in univariate data

## Plots

::::::: {.panel-tabset}

### Data

Separate between "Founder" & "Non-Founder" and calculate summary statistics necessary for plotting.

```{r}
library(ggplot2)

lookup_term <- "founder"
data        <- me_joined_nested_foc_prqt |> 
                  filter(!is.na(title_cs)) |> 
                  mutate(Role = title_cs |> tolower() |> str_detect(lookup_term)) |> 
                  collect() |> 
                  mutate(
                    Role = Role |> factor(levels = c(TRUE, FALSE), 
                                                labels = c('Founder', 'Non-Founder'))
                    ) 

# Summary Statistics (Mean & Median)
df_vline_long <- data |> 
  group_by(Role) |> 
  summarise(Mean = mean(tjoin_tfound), Median = median(tjoin_tfound), n = n()) |> 
  pivot_longer(c(Mean, Median,n ), names_to = "Statistic", values_to = "Value") |> 
  mutate(Value_chr    = format(round(Value, 1), big.mark=".", decimal.mark = ",", drop0trailing = T),
         gg_pos_y = rep(c(0.07,0.06, 0.05),2),
         gg_color = rep(c("#8F8470", "#BF9240", "#FEA400"), 2))
```

```{css, echo=FALSE}
.reveal .column-output-location .column:first-of-type div.sourceCode {
    height: auto;
}
```

### F1

How many month have passed since the company was founded and before the person joined the company (binwidth: 3 months)?

```{r}
#| label: plot1
#| output-location: column
data |> 
  
  # Plot
  ggplot(aes(x = tjoin_tfound, fill = Role, color = Role)) +
  geom_histogram(aes(y =..density..), size = .2, binwidth = 3, alpha = 0.5) +
  facet_wrap(~Role, nrow=2) +
  
  # Statistics & Design
  ggnewscale::new_scale_color() +
  geom_vline(data = df_vline_long |> filter(Statistic != "n"), aes(xintercept = Value, linetype = Statistic, color = Statistic), key_glyph = "path") +
  scale_linetype_manual(values = c(2,3)) +
  scale_color_manual(values = c("#8F8470", "#BF9240", "#FEA400")) +
  geom_label(data = df_vline_long, aes(x = 100, y = gg_pos_y, label = paste0(Statistic, ' = ', Value_chr)), 
             color = df_vline_long$gg_color, fill = "transparent", alpha = 0.5, size = 3, hjust = "left") +
  xlim(-250, 250) +
  labs(x = "Î” T_join, T_foundation (in month)", y = "Density") + 
  theme(legend.key=element_blank())
```

### F2

How much capital has been acquired by the time the person joins?

```{r}
#| label: plot2
#| output-location: column
data |> 
  
  # Plot
  ggplot(aes(x = raised_amount_before_join, color = Role, fill = Role)) + 
  geom_histogram(aes(y =..density..), alpha=0.5) +
  facet_wrap(~Role, nrow=2) +
  
  # Design
  scale_x_continuous(labels = scales::label_number(prefix = "$", accuracy = 0.1, scale_cut = scales::cut_short_scale()), limits = c(NA,1e+09)) +
  labs(x = "Raised amount before join", y = "Density", fill="", color = "")
```

### F3

How many funding rounds have been acquired by the time the person joins?

```{r}
#| label: plot3
#| output-location: column
data |> 
  
  ggplot(aes(x = num_rounds_before_join, color = Role, fill = Role)) + 
  geom_histogram(aes(y =..density..), binwidth = 1, alpha=0.5) +
  facet_wrap(~Role, nrow=2) +
  
  # Design
  xlim(NA, 20) +
  labs(x = "# Rounds before join", y = "Density", fill="", color = "")
```

:::::::

## Fortune500

```{r}
#| eval: false
me_matched_members_dt <- me_matched_members_prqt |> collect()

# 46 Batches (Chunks with 5 Million rows)
slice_ids <- tibble(from = seq(1, 230000000, 5000000),to = c(seq(5000000, 225000000, 5000000), 229065592))
for (i in 1:46) {
  # Build Batch
  x <- slice_ids$from[i]; y <- slice_ids$to[i]
  me_matched_members_slice_dt <- me_matched_members_dt[x:y,]
  # Create Features
  me_matched_members_slice_dt[, `:=` (f500 = (purrr::pmap_lgl(list(company_name, date_from_parsed, date_to_parsed), check_f500, .progress = TRUE)),
                                      role = title |> tolower() |> stringr::str_detect("founder"))]
  # Save
  me_matched_members_slice_dt |> write_parquet(paste0("/media/tie/ssd2/joschka/me_f500/me_f500_", cur_id, ".parquet"))
}
```

```{r}
#| eval: false
check_f500 <- function(name,year_from,year_to) {
  
  if (is.na(year_to))   {year_to <- 2023}
  if (is.na(year_from)) {return(NA)}
  
  data <- fortune500 |> 
    filter(year |> between(year_from, year_to)) |> 
    pull(company)
  
  name |> tolower() %in% data
}
```
 
# Employment History {data-stack-name="Employment History"}
Match current and past employments to corresponding Fortune500 companies
 
## Employment History (Worked for Fortune 500 company?)

**Content-related problems**

* Matching problems:
  - amd <> advanced micro devices
  - intel <> intel corporation
  
* Geographical issues:
  - Just US companies (use Fortune Global data)
  
**Technical issues**

Takes loooooooong time to calculate...

## Code I (Data + Function)

Function

```{r}
# 1. Function
check_f500 <- function(title, year_from, year_to) {
  
  # Handle NA inputs
  if (is.na(year_to))   {year_to <- 2023}
  if (is.na(year_from)) {return(NA)}
  
  # Filter time frame
  data <- fortune500 |> 
    
    filter(year |> between(year_from, year_to)) |> 
    pull(company)
  
  # Check match and return bool
  title |> tolower() %in% data
}
```

:::: {.columns}

```{r}
#| echo: false
options(
    pillar.print_max = 3,
    pillar.print_min = 3
)
```

::: {.column width="50%"}

CoreSignal data (n = 229.065.592)

```{r}
#| eval: false
# 1. Data
me_matched_members_prqt <- open_dataset("me_matched_members.parquet") 
me_matched_members_prqt |> 
  
  head() |> collect()
```

```{r}
#| echo: false
me_matched_members_prqt <- open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/me_matched_members.parquet") 
me_matched_members_prqt |> 
  
  head() |> collect()
```
:::

::: {.column width="50%"}

US Fortune 500 data

```{r}
#| eval: false
fortune500 <- readRDS("fortune_500_1955-2022.rds") |> 
                
                select(year, company) |> 
                mutate(company = company |> tolower())
fortune500
```

```{r}
#| echo: false
fortune500 <- readRDS("/Users/jschwarz/02_diss/01_data_sources/02_fortune500/02_data/fortune_500_1955-2022.rds") |> 
                select(year, company) |> 
                mutate(company = company |> tolower())
fortune500
```
:::
::::

## Code II (Chunkwise Execution)

```{r}
#| eval: false
# 1. Collect data
me_matched_members_dt <- me_matched_members_prqt |> collect()
# 2.Build batches
slice_ids <- tibble(
  from  = seq(1, 230000000, 5000000),
  to    = c(seq(5000000, 225000000, 5000000), 229065592),
)

for (i in 1:46) {

  # Create current batch  
  x <- slice_ids$from[i]
  y <- slice_ids$to[i]
  me_matched_members_slice_dt <- me_matched_members_dt[x:y,]

  # Add features
  me_matched_members_slice_dt[, `:=` (f500 = (purrr::pmap_lgl(list(company_name, date_from_parsed, date_to_parsed), check_f500, .progress = TRUE)),
                                      role = title |> tolower() |> stringr::str_detect("founder"))]
  
  # Save
  me_matched_members_slice_dt |> write_parquet(paste0("me_f500/me_f500_", cur_id, ".parquet"))
}
```

## Code III (Build feature)

```{r}
#| eval: false
# 1. Load data 
me_f500 <- open_dataset("me_f500/") |> 
  collect()

# 2. Add Ids
me_dist_ids_prqt <- arrow::open_dataset("me_dist4.parquet") |> select(id, member_id) |> collect()
me_f500_id       <-  me_dist_ids_prqt[me_f500, on = .(id)] 

# 3. Add features
# 3.1 Earliest founding date & earliest f500 date (if founded)
me_f500_id[,`:=` (founding_min      = (ifelse(any(role == T),                  min(date_from_parsed[role==T]),   NA_real_)),
                  f500_min_founding = (ifelse(any(role == T) & any(f500 == T), min(date_from_parsed[f500 == T]), NA_real_))),
           by = .(member_id)]

# 3.2 Compare
me_f500_id[, f500_before_founding := f500_min_founding <= founding_min]
```

## Plot

Constraints: 

1. Analysis takes place on an annual level.
2. First funding event is being considered
3. Exact matches only

```{r}
#| output-location: column
me_f500_id <- arrow::open_dataset("~/02_diss/01_data_sources/01_coresignal/02_data/me_f500_id.parquet")
data <- me_f500_id |> 
  
          # Filter "Founder"
          filter(role == T) |> collect() 
  
data |> 
  ggplot(aes(x = f500_before_founding)) + 
    geom_bar() +
    scale_x_discrete(labels=c("Employment at Fortune500\nAFTER\nfounding", "Employment at Fortune500\nBEFORE\nfounding", "Neither case")) +
    scale_y_continuous(labels = scales::unit_format(unit = "M", scale = 1e-6, accuracy = 0.1)) + 
    labs(x = "", y = "Count")
```



# Employment Skills {data-stack-name="Skills"}
Inferences from semantic similarity of LinkedIn users' skills
 
## Cluster skills

tbd

```{r}
#| eval: false
member_skills_prqt <- arrow::open_dataset("member_skills.parquet")
member_skills_prqt |> 
  glimpse()
```

```{r}
#| echo: false
member_skills_prqt <- arrow::open_dataset("/Users/jschwarz/Downloads/member_skills.parquet")
member_skills_prqt |> 
  glimpse()
```

```{r}
#| eval: false
skill_names_tbl <- readRDS("skill_names_tbl.rds")
skill_names_tbl
```

```{r}
#| echo: false
skill_names_tbl <- readRDS("~/Library/Mobile Documents/com~apple~CloudDocs/01_tuhh/02_diss/01_data_sources/06_core_signal/01_data/03_extracted/02_member/03_skills/skill_names_tbl.rds")
skill_names_tbl
```

# Current variables {data-stack-name="Current"}
Variables built from Experience, Education and funding data

## Built variables

::::::: {.panel-tabset}

### All

```{r}
#| echo: false
cs_me_dist8_unest_wedu_dt <-  open_dataset("/Users/jschwarz/02_diss/01_data_sources/01_coresignal/02_data/03_wrangled/cs_me_dist8_unest_fin4.parquet") |> collect()
```

```{r}
cs_me_dist8_unest_wedu_dt |> 
  glimpse()
```

### General

```{r}
cs_me_dist8_unest_wedu_dt |> 
  select(id_tie, member_id, exp_id_cs, company_id_cbpb, company_name_cbpb, company_id_cs, company_name_cs, 
         founded_on_cbpb, closed_on_cbpb,
         title_cs) |> 
  glimpse()
```

### EDA

```{r}
cs_me_dist8_unest_wedu_dt |> 
  select(date_from_parsed_cs, date_to_parsed_cs, 
         tjoin_tfound, raised_amount_before_join_company, num_rounds_before_join) |> 
  glimpse()
```

### Exp (dummy)

```{r}
cs_me_dist8_unest_wedu_dt |> 
  select(starts_with("is_"),
         starts_with("was_")) |> 
  glimpse()
```

### Exp (quant)

```{r}
cs_me_dist8_unest_wedu_dt |> 
  select(
    starts_with("date_1st_"),
    starts_with("time_since_1st_"),
    starts_with("exp_"), -exp_id_cs) |> 
  glimpse()
```

### Edu

```{r}
cs_me_dist8_unest_wedu_dt |> 
  select(score_global_2023_best,
         starts_with("rank"),
         starts_with("degree")) |> 
  glimpse()
```

### Fund

```{r}
cs_me_dist8_unest_wedu_dt |> 
  select(date_from_stage, company_start_mid, company_start_late,
         raised_amount_before_founder_member, raised_amount_before_all_member,
         funding_after_mid, funding_after_early) |> 
  glimpse()
```

::::::: 



# Next Steps {data-stack-name="Next Steps"}

## Further analysis is necessary

* Match employment history with Fortune500 (revenue based selection)
* Cluster job titles
* Cluster skills

. . .

![](https://c.tenor.com/3EYd9ID79vcAAAAd/mic-drop-the-voice.gif){fig-align="center" width=50%}
