[
  {
    "objectID": "slides/synthetic-data.html",
    "href": "slides/synthetic-data.html",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "",
    "text": "In the example guide for generating random numbers, we explored how to use a bunch of different statistical distributions to create variables that had reasonable values. However, each of the columns that we generated there were completely independent of each other. In the final example, we made some data with columns like age, education, and income, but none of those were related, though in real life they would be.\nGenerating random variables is fairly easy: choose some sort of distributional shape, set parameters like a mean and standard deviation, and let randomness take over. Forcing variables to be related is a little trickier and involves a little math. But don’t worry! That math is all just regression stuff!\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(scales)\nlibrary(ggdag)",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#basic-example",
    "href": "slides/synthetic-data.html#basic-example",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "1 Basic example",
    "text": "1 Basic example\n\nRelationships and regression\nLet’s pretend we want to predict someone’s happiness on a 10-point scale based on the number of cookies they’ve eaten and whether or not their favorite color is blue.\n\\[\n\\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon\n\\]\nWe can generate a fake dataset with columns for happiness (Beta distribution clustered around 7ish), cookies (Poisson distribution), and favorite color (binomial distribution for blue/not blue):\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_simple &lt;- tibble(\n  id = 1:n_people,\n  happiness = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness = round(happiness * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\"))\n\nhead(happiness_simple)\n\nWe have a neat dataset now, so let’s run a regression. Is eating more cookies or liking blue associated with greater happiness?\n\nmodel_happiness1 &lt;- lm(happiness ~ cookies + color_blue, data = happiness_simple)\ntidy(model_happiness1)\n\nNot really. The coefficients for both cookies and color_blueBlue are basically 0 and not statistically significant. That makes sense since the three columns are completely independent of each other. If there were any significant effects, that’d be strange and solely because of random chance.\nFor the sake of your final project, you can just leave all the columns completely independent of each other if you want. None of your results will be significant and you won’t see any effects anywhere, but you can still build models, run all the pre-model diagnostics, and create graphs and tables based on this data.\nHOWEVER, it will be far more useful to you if you generate relationships. The whole goal of this class is to find causal effects in observational, non-experimental data. If you can generate synthetic non-experimental data and bake in a known causal effect, you can know if your different methods for recovering that effect are working.\nSo how do we bake in correlations and causal effects?\n\n\nExplanatory variables linked to outcome; no connection between explanatory variables\nTo help with the intuition of how to link these columns, think about the model we’re building:\n\\[\n\\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon\n\\]\nThis model provides estimates for all those betas. Throughout the semester, we’ve used the analogy of sliders and switches to describe regression coefficients. Here we have both:\n\n\\(\\beta_0\\): The average baseline happiness.\n\\(\\beta_1\\): The additional change in happiness that comes from eating one cookie. This is a slider: move cookies up by one and happiness changes by \\(\\beta_1\\).\n\\(\\beta_2\\): The change in happiness that comes from having your favorite color be blue. This is a switch: turn on “blue” for someone and their happiness changes by \\(\\beta_2\\).\n\nWe can invent our own coefficients and use some math to build them into the dataset. Let’s use these numbers as our targets:\n\n\\(\\beta_0\\): Average happiness is 7\n\\(\\beta_1\\): Eating one more cookie boosts happiness by 0.25 points\n\\(\\beta_2\\): People who like blue have 0.75 higher happiness\n\nWhen generating the data, we can’t just use rbeta() by itself to generate happiness, since happiness depends on both cookies and favorite color (that’s why we call it a dependent variable). To build in this effect, we can add a new column that uses math and modifies the underlying rbeta()-based happiness score:\n\nhappiness_with_effect &lt;- happiness_simple %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite color\n  mutate(happiness_modified = happiness + (0.25 * cookies) + (0.75 * color_blue_binary))\nhead(happiness_with_effect)\n\nNow that we have a new happiness_modified column we can run a model using it as the outcome:\n\nmodel_happiness2 &lt;- lm(happiness_modified ~ cookies + color_blue, data = happiness_with_effect)\ntidy(model_happiness2)\n\nWhoa! Look at those coefficients! They’re exactly what we tried to build in! The baseline happiness (intercept) is ≈7, eating one cookie is associated with a ≈0.25 increase in happiness, and liking blue is associated with a ≈0.75 increase in happiness.\nHowever, we originally said that happiness was a 0-10 point scale. After boosting it with extra happiness for cookies and liking blue, there are some people who score higher than 10:\n\n# Original scale\nggplot(happiness_with_effect, aes(x = happiness)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:11) +\n  coord_cartesian(xlim = c(0, 11))\n\n\n# Scaled up\nggplot(happiness_with_effect, aes(x = happiness_modified)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:11) +\n  coord_cartesian(xlim = c(0, 11))\n\nTo fix that, we can use the rescale() function from the scales package to force the new happiness_modified variable to fit back in its original range:\n\nhappiness_with_effect &lt;- happiness_with_effect %&gt;%\n  mutate(happiness_rescaled = rescale(happiness_modified, to = c(3, 10)))\n\nggplot(happiness_with_effect, aes(x = happiness_rescaled)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:11) +\n  coord_cartesian(xlim = c(0, 11))\n\nEverything is back in the 3–10 range now. However, the rescaling also rescaled our built-in effects. Look what happens if we use the happiness_rescaled in the model:\n\nmodel_happiness3 &lt;- lm(happiness_rescaled ~ cookies + color_blue, data = happiness_with_effect)\ntidy(model_happiness3)\n\nNow the baseline happiness is 6.3, the cookies effect is 0.2, and the blue effect is 0.63. These effects shrunk because we shrunk the data back down to have a maximum of 10.\nThere are probably fancy mathy ways to rescale data and keep the coefficients the same size, but rather than figure that out (who even wants to do that?!), my strategy is just to play with numbers until the results look good. Instead of using a 0.25 cookie effect and 0.75 blue effect, I make those effects bigger so that the rescaled version is roughly what I really want. There’s no systematic way to do this—I ran this chunk below a bunch of times until the numbers worked.\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_real_effect &lt;- tibble(\n  id = 1:n_people,\n  happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness_baseline = round(happiness_baseline * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\")) %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite color\n  mutate(happiness_effect = happiness_baseline +\n           (0.31 * cookies) +  # Cookie effect\n           (0.91 * color_blue_binary)) %&gt;%  # Blue effect\n  # Rescale to 3-10, since that's what the original happiness column looked like\n  mutate(happiness = rescale(happiness_effect, to = c(3, 10)))\n\nmodel_does_this_work_yet &lt;- lm(happiness ~ cookies + color_blue, data = happiness_real_effect)\ntidy(model_does_this_work_yet)\n\nThere’s nothing magical about the 0.31 and 0.91 numbers I used here; I just kept changing those to different things until the regression coefficients ended up at ≈0.25 and ≈0.75. Also, I gave up on trying to make the baseline happiness 7. It’s possible to do—you’d just need to also shift the underlying Beta distribution up (like shape1 = 9, shape2 = 2 or something). But then you’d also need to change the coefficients more. You’ll end up with 3 moving parts and it can get complicated, so I don’t worry too much about it, since what we care about the most here is the effect of cookies and favorite color, not baseline levels of happiness.\nPhew. We successfully connected cookies and favorite color to happiness and we have effects that are measurable with regression! One last thing that I would do is get rid of some of the intermediate columns like color_blue_binary or happiness_effect—we only used those for the behind-the-scenes math of creating the effect. Here’s our final synthetic dataset:\n\nhappiness &lt;- happiness_real_effect %&gt;%\n  select(id, happiness, cookies, color_blue)\nhead(happiness)\n\nWe can save that as a CSV file with write_csv():\n\nwrite_csv(happiness, \"data/happiness_fake_data.csv\")\n\n\n\nExplanatory variables linked to outcome; connection between explanatory variables\nIn that cookie example, we assumed that both cookie consumption and favorite color are associated with happiness. We also assumed that cookie consumption and favorite color are not related to each other. But what if they are? What if people who like blue eat more cookies?\nWe’ve already used regression-based math to connect explanatory variables to outcome variables. We can use that same intuition to connect explanatory variables to each other.\nThe easiest way to think about this is with DAGs. Here’s the DAG for the model we just ran:\n\nhappiness_dag1 &lt;- dagify(hap ~ cook + blue,\n                         coords = list(x = c(hap = 3, cook = 1, blue = 2),\n                                       y = c(hap = 1, cook = 1, blue = 2)))\n\nggdag(happiness_dag1) +\n  theme_dag()\n\nBoth cookies and favorite color cause happiness, but there’s no link between them. Notice that dagify() uses the same model syntax that lm() does: hap ~ cook + blue. If we think of this just like a regression model, we can pretend that there are coefficients there too: hap ~ 0.25*cook + 0.75*blue. We don’t actually include any coefficients in the DAG or anything, but it helps with the intuition.\nBut what if people who like blue eat more cookies on average? For whatever reason, let’s pretend that liking blue causes you to eat 0.5 more cookies, on average. Here’s the new DAG:\n\nhappiness_dag2 &lt;- dagify(hap ~ cook + blue,\n                         cook ~ blue,\n                         coords = list(x = c(hap = 3, cook = 1, blue = 2),\n                                       y = c(hap = 1, cook = 1, blue = 2)))\n\nggdag(happiness_dag2) +\n  theme_dag()\n\nNow we have two different equations: hap ~ cook + blue and cook ~ blue. Conveniently, these both translate to models, and we know the coefficients we want!\n\nhap ~ 0.25*cook + 0.75*blue: This is what we built before—cookies boost happiness by 0.25 and liking blue boosts happiness by 0.75\ncook ~ 0.3*blue: This is what we just proposed—liking blue boosts cookies by 0.5\n\nWe can follow the same process we did when building the cookie and blue effects into happiness to also build a blue effect into cookies!\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_cookies_blue &lt;- tibble(\n  id = 1:n_people,\n  happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness_baseline = round(happiness_baseline * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\")) %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make blue have an effect on cookie consumption\n  mutate(cookies = cookies + (0.5 * color_blue_binary)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite color\n  mutate(happiness_effect = happiness_baseline +\n           (0.31 * cookies) +  # Cookie effect\n           (0.91 * color_blue_binary)) %&gt;%  # Blue effect\n  # Rescale to 3-10, since that's what the original happiness column looked like\n  mutate(happiness = rescale(happiness_effect, to = c(3, 10)))\nhead(happiness_cookies_blue)\n\nNotice now that people who like blue eat partial cookies, as expected. We can verify that there’s a relationship between liking blue and cookies by running a model:\n\nlm(cookies ~ color_blue, data = happiness_cookies_blue) %&gt;%\n  tidy()\n\nYep. Liking blue is associated with 0.46 more cookies on average (it’s not quite 0.5, but that’s because of randomness).\nNow let’s do some neat DAG magic. Let’s say we’re interested in the causal effect of cookies on happiness. We could run a naive model:\n\nmodel_happiness_naive &lt;- lm(happiness ~ cookies, data = happiness_cookies_blue)\ntidy(model_happiness_naive)\n\nBased on this, eating a cookie causes you to have 0.325 more happiness points. But that’s wrong! Liking the color blue is a confounder and opens a path between cookies and happiness. You can see the confounding both in the DAG (since blue points to both the cookie node and the happiness node) and in the math (liking blue boosts happiness + liking blue boosts cookie consumption, which boosts happiness).\nTo fix this confounding, we need to statistically adjust for liking blue and close the backdoor path. Ordinarily we’d do this with something like matching or inverse probability weighting, but here we can just include liking blue as a control variable (since it’s linearly related to both cookies and happiness):\n\nmodel_happiness_ate &lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_blue)\ntidy(model_happiness_ate)\n\nAfter adjusting for backdoor confounding, eating one additional cookie causes a 0.249 point increase in happiness. This is the effect we originally built into the data!\nIf you wanted, we could rescale the number of cookies just like we rescaled happiness before, since sometimes adding effects to columns changes their reasonable ranges.\nNow that we have a good working dataset, we can keep the columns we care about and save it as a CSV file for later use:\n\nhappiness &lt;- happiness_cookies_blue %&gt;%\n  select(id, happiness, cookies, color_blue)\nhead(happiness)\n\n\nwrite_csv(happiness, \"data/happiness_fake_data.csv\")\n\n\n\nAdding extra noise\nWe’ve got columns that follow specific distributions, and we’ve got columns that are statistically related to each other. We can add one more wrinkle to make our fake data even more fun (and even more reflective of real life). We can add some noise.\nRight now, the effects we’re finding are too perfect. When we used mutate() to add a 0.25 boost in happiness for every cookie people ate, we added exactly 0.25 happiness points. If someone ate 2 cookies, they got 0.5 more happiness; if they ate 5, they got 1.25 more.\nWhat if the cookie effect isn’t exactly 0.25, but somewhere around 0.25? For some people it’s 0.1, for others it’s 0.3, for others it’s 0.22. We can use the same ideas we talked about in the random numbers example to generate a distribution of an effect. For instance, let’s say that the average cookie effect is 0.25, but it can vary somewhat with a standard deviation of 0.15:\n\ntemp_data &lt;- tibble(x = rnorm(10000, mean = 0.25, sd = 0.15))\n\nggplot(temp_data, aes(x = x)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, color = \"white\")\n\nSometimes it can go as low as −0.25; sometimes it can go as high as 0.75; normally it’s around 0.25.\nNothing in the model explains why it’s higher or lower for some people—it’s just random noise. Remember that the model accounts for that! This random variation is what the \\(\\varepsilon\\) is for in this model equation:\n\\[\n\\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon\n\\]\nWe can build that uncertainty into the fake column! Instead of using 0.31 * cookies when generating happiness (which is technically 0.25, but shifted up to account for rescaling happiness back down after), we’ll make a column for the cookie effect and then multiply that by the number of cookies.\n\nset.seed(1234)\n\nn_people &lt;- 1000\nhappiness_cookies_noisier &lt;- tibble(\n  id = 1:n_people,\n  happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3),\n  cookies = rpois(n_people, lambda = 1),\n  cookie_effect = rnorm(n_people, mean = 0.31, sd = 0.2),\n  color_blue = sample(c(\"Blue\", \"Not blue\"), n_people, replace = TRUE)\n) %&gt;%\n  # Adjust some of the columns\n  mutate(happiness_baseline = round(happiness_baseline * 10, 1),\n         cookies = cookies + 1,\n         color_blue = fct_relevel(factor(color_blue), \"Not blue\")) %&gt;%\n  # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it\n  mutate(color_blue_binary = ifelse(color_blue == \"Blue\", TRUE, FALSE)) %&gt;%\n  # Make blue have an effect on cookie consumption\n  mutate(cookies = cookies + (0.5 * color_blue_binary)) %&gt;%\n  # Make a new happiness column that uses coefficients for cookies and favorite\n  # color. Importantly, instead of using 0.31 * cookies, we'll use the random\n  # cookie effect we generated earlier\n  mutate(happiness_effect = happiness_baseline +\n           (cookie_effect * cookies) +\n           (0.91 * color_blue_binary)) %&gt;%\n  # Rescale to 3-10, since that's what the original happiness column looked like\n  mutate(happiness = rescale(happiness_effect, to = c(3, 10)))\nhead(happiness_cookies_noisier)\n\nNow let’s look at the cookie effect in this noisier data:\n\nmodel_noisier &lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_noisier)\ntidy(model_noisier)\n\nThe effect is a little smaller now because of the extra noise, so we’d need to mess with the 0.31 and 0.91 coefficients more to get those numbers back up to 0.25 and 0.75.\nWhile this didn’t influence the findings too much here, it can have consequences for other variables. For instance, in the previous section we said that the color blue influences cookie consumption. If the blue effect on cookies isn’t precisely 0.5 but follows some sort of distribution (sometimes small, sometimes big, sometimes negative, sometimes zero), that will influence cookies differently. That random effect on cookie consumption will then work together with the random effect of cookies on happiness, resulting in multiple varied values.\nFor instance, imagine the average effect of liking blue on cookies is 0.5, and the average effect of cookies on happiness is 0.25. For one person, their blue-on-cookie effect might be 0.392, which changes the number of cookies they eat. Their cookie-on-happiness effect is 0.573, which changes their happiness. Both of those random effects work together to generate the final happiness.\nIf you want more realistic-looking synthetic data, it’s a good idea to add some random noise wherever you can.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#visualizing-variables-and-relationships",
    "href": "slides/synthetic-data.html#visualizing-variables-and-relationships",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "2 Visualizing variables and relationships",
    "text": "2 Visualizing variables and relationships\nGoing through this process requires a ton of trial and error. You will change all sorts of numbers to make sure the relationships you’re building work. This is especially the case if you rescale things, since that rescales your effects. There are a lot of moving parts and this is a complicated process.\nYou’ll run your data generation chunks lots and lots and lots of times, tinkering with the numbers as you go. This example makes it look easy, since it’s the final product, but I ran all these chunks over and over again until I got the causal effect and relationships just right.\nIt’s best if you also create plots and models to see what the relationships look like\n\nVisualizing one variable\nWe covered a bunch of distributions in the random number generation example, but it’s hard to think about what a standard deviation of 2 vs 10 looks like, or what happens when you mess with the shape parameters in a Beta distribution.\nIt’s best to visualize these variables. You could build the variable into your official dataset and then look at it, but I find it’s often faster to just look at what a general distribution looks like first. The easiest way to do this is generate a dataset with just one column in it and look at it, either with a histogram or a density plot.\nFor instance, what does a Beta distribution with shape1 = 3 and shape2 = 16 look like? The math says it should peak around 0.15ish (\\(\\frac{3}{3 + 16}\\)), and that looks like the case:\n\ntemp_data &lt;- tibble(x = rbeta(10000, shape1 = 3, shape2 = 16))\n\nplot1 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_histogram(binwidth = 0.05, boundary = 0, color = \"white\")\n\nplot2 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_density()\n\nplot1 + plot2\n\nWhat if we want a normal distribution centered around 100, with most values range from 50 to 150. That’s range of ±50, but that doesn’t mean the sd will be 50—it’ll be much smaller than that, like 25ish. Tinker with the numbers until it looks right.\n\ntemp_data &lt;- tibble(x = rnorm(10000, mean = 100, sd = 25))\n\nplot1 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_histogram(binwidth = 10, boundary = 0, color = \"white\")\n\nplot2 &lt;- ggplot(temp_data, aes(x = x)) +\n  geom_density()\n\nplot1 + plot2\n\n\n\nVisualizing two continuous variables\nIf you have two continuous/numeric columns, it’s best to use a scatterplot. For instance, let’s make two columns based on the Beta and normal distributions above, and we’ll make it so that y goes up by 0.25 for every increase in x, along with some noise:\n\nset.seed(1234)\n\ntemp_data &lt;- tibble(\n  x = rnorm(1000, mean = 100, sd = 25)\n) %&gt;%\n  mutate(y = rbeta(1000, shape1 = 3, shape2 = 16) +  # Baseline distribution\n           (0.25 * x) +  # Effect of x\n           rnorm(1000, mean = 0, sd = 10))  # Add some noise\n\nggplot(temp_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe can confirm the effect with a model:\n\nlm(y ~ x, data = temp_data) %&gt;%\n  tidy()\n\n\n\nVisualizing a binary variable and a continuous variable\nIf you have one binary column and one continuous/numeric column, it’s generally best to not use a scatterplot. Instead, either look at the distribution of the continuous variable across the binary variable with a faceted histogram or overlaid density plot, or look at the average of the continuous variable across the different values of the binary variable with a point range.\nLet’s make two columns: a continuous outcome (y) and a binary treatment (x). Being in the treatment group causes an increase of 20 points, on average.\n\nset.seed(1234)\n\ntemp_data &lt;- tibble(\n  treatment = rbinom(1000, size = 1, prob = 0.5)  # Make 1000 0/1 values with 50% chance of each\n) %&gt;%\n  mutate(outcome = rbeta(1000, shape1 = 3, shape2 = 16) +  # Baseline distribution\n           (20 * treatment) +  # Effect of treatment\n           rnorm(1000, mean = 0, sd = 20)) %&gt;%   # Add some noise\n  mutate(treatment = factor(treatment))  # Make treatment a factor/categorical variable\n\nWe can check the numbers with a model:\n\nlm(outcome ~ treatment, data = temp_data) %&gt;% tidy()\n\nHere’s what that looks like as a histogram:\n\nggplot(temp_data, aes(x = outcome, fill = treatment)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0) +\n  guides(fill = \"none\") +  # Turn off the fill legend since it's redundant\n  facet_wrap(vars(treatment), ncol = 1)\n\nAnd as overlapping densities:\n\nggplot(temp_data, aes(x = outcome, fill = treatment)) +\n  geom_density(alpha = 0.5)\n\nAnd with a point range:\n\n# hahaha these error bars are tiny\nggplot(temp_data, aes(x = treatment, y = outcome, color = treatment)) +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\") +\n  guides(color = \"none\")  # Turn off the color legend since it's redundant",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#specific-examples",
    "href": "slides/synthetic-data.html#specific-examples",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "3 Specific examples",
    "text": "3 Specific examples\n\ntl;dr: The general process\nThose previous sections go into a lot of detail. In general, here’s the process you should follow when building relationships in synthetic data:\n\nDraw a DAG that maps out how all the columns you care about are related.\nSpecify how those nodes are measured.\nSpecify the relationships between the nodes based on the DAG equations.\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nVerify all relationships with plots and models.\nTry it out!\nSave the data.\n\n\n\nCreating an effect in an observational DAG\n\nDraw a DAG that maps out how all the columns you care about are related.\nHere’s a simple DAG that shows the causal effect of mosquito net usage on malaria risk. Income and health both influence and confound net use and malaria risk, and income also influences health.\n\nmosquito_dag &lt;- dagify(mal ~ net + inc + hlth,\n                       net ~ inc + hlth,\n                       hlth ~ inc,\n                       coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3),\n                                     y = c(mal = 1, net = 1, inc = 2, hlth = 2)))\nggdag(mosquito_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nFor the sake of this example, we’ll measure these nodes like so. See the random number example for more details about the distributions.\n\nMalaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution. However, since we want to use other variables that increase the likelihood of using a net, we’ll do some cool tricky stuff, explained later.\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\nHealth: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are three models in this DAG:\n\nhlth ~ inc: Income influences health. We’ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\nnet ~ inc + hlth: Income and health both increase the probability of net usage. This is where we do some cool tricky stuff.\nBoth income and health have an effect on the probability of bed net use, but bed net use is measured as a 0/1, TRUE/FALSE variable. If we run a regression with net as the outcome, we can’t really interpret the coefficients like “a 1 point increase in health is associated with a 0.42 point increase in bed net being TRUE.” That doesn’t even make sense.\nOrdinarily, when working with binary outcome variables, you use logistic regression models (see the crash course we had when talking about propensity scores here). In this kind of regression, the coefficients in the model represent changes in the log odds of using a net. As we discuss in the crash course section, log odds are typically impossible to interpet. If you exponentiate them, you get odds ratios, which let you say things like “a 1 point increase in health is associated with a 15% increase in the likelihood of using a net.” Technically we could include coefficients for a logistic regression model and simulate probabilities of using a net or not using log odds and odds ratios (and that’s what I do in the rain barrel data from Problem Set 3 (see code here)), but that’s really hard to wrap your head around since you’re dealing with strange uninterpretable coefficients. So we won’t do that here.\nInstead, we’ll do some fun trickery. We’ll create something called a “bed net score” that gets bigger as income and health increase. We’ll say that a 1 point increase in health score is associated with a 1.5 point increase in bed net score, and a 1 dollar increase in income is associated with a 0.5 point increase in bed net score. This results in a column that ranges all over the place, from 200 to 500 (in this case; that won’t always be true). This column definitely doesn’t look like a TRUE/FALSE binary column—it’s just a bunch of numbers. That’s okay!\nWe’ll then use the rescale() function from the scales package to take this bed net score and scale it down so that it goes from 0.05 to 0.95. This represents a person’s probability of using a bed net.\nFinally, we’ll use that probability in the rbinom() function to generate a 0 or 1 for each person. Some people will have a high probability because of their income and health, like 0.9, and will most likely use a net. Some people might have a 0.15 probability and will likely not use a net.\nWhen you generate binary variables like this, it’s hard to know the exact effect you’ll get, so it’s best to tinker with the numbers until you see relationships that you want.\nmal ~ net + inc + hlth: Finally net use, income, and health all have an effect on the risk of malaria. Building this relationship is easy since it’s just a regular linear regression model (since malaria risk is not binary). We’ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That’s the casual effect we’re building in to the DAG.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nHere we go! Let’s make some data. I’ll comment the code below so you can see what’s happening at each step.\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1138 people (just for fun)\nn_people &lt;- 1138\n\nnet_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %&gt;%\n  # Generate net variable based on income, health, and random noise\n  mutate(net_score = (0.5 * income) + (1.5 * health) + rnorm(n_people, mean = 0, sd = 15),\n         # Scale net score down to 0.05 to 0.95 to create a probability of using a net\n         net_probability = rescale(net_score, to = c(0.05, 0.95)),\n         # Randomly generate a 0/1 variable using that probability\n         net = rbinom(n_people, 1, net_probability)) %&gt;%\n  # Finally generate a malaria risk variable based on income, health, net use,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down by 10 when using a net. Because we rescale things,\n         # though, we have to make the effect a lot bigger here so it scales\n         # down to -10. Risk also decreases as health and income go up. I played\n         # with these numbers until they created reasonable coefficients.\n         malaria_effect = (-30 * net) + (-1.9 * health) + (-0.1 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70)))\n\n# Look at all these columns!\nhead(net_data)\n\nVerify all relationships with plots and models.\nLet’s see if we have the relationships we want. Income looks like it’s associated with health:\n\nggplot(net_data, aes(x = income, y = health)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nlm(health ~ income, data = net_data) %&gt;% tidy()\n\nIt looks like richer and healthier people are more likely to use nets:\n\nnet_income &lt;- ggplot(net_data, aes(x = income, fill = as.factor(net))) +\n  geom_density(alpha = 0.7) +\n  theme(legend.position = \"bottom\")\n\nnet_health &lt;- ggplot(net_data, aes(x = health, fill = as.factor(net))) +\n  geom_density(alpha = 0.7) +\n  theme(legend.position = \"bottom\")\n\nnet_income + net_health\n\nIncome increasing makes it 1% more likely to use a net; health increasing make it 2% more likely to use a net:\n\nglm(net ~ income + health, family = binomial(link = \"logit\"), data = net_data) %&gt;%\n  tidy(exponentiate = TRUE)\n\nTry it out!\nIs the effect in there? Let’s try finding it by controlling for our two backdoors: health and income. Ordinarily we should do something like matching or inverse probability weighting, but we’ll just do regular regression here (which is okay-ish, since all these variables are indeed linearly related with each other—we made them that way!)\nIf we just look at the effect of nets on malaria risk without any statistical adjustment, we see that net cause a decrease of 13 points in malaria risk. This is wrong though becuase there’s confounding.\n\n# Wrong correlation-is-not-causation effect\nmodel_net_naive &lt;- lm(malaria_risk ~ net, data = net_data)\ntidy(model_net_naive)\n\nIf we control for the confounders, we get the 10 point ATE. It works!\n\n# Correctly adjusted ATE effect\nmodel_net_ate &lt;- lm(malaria_risk ~ net + health + income, data = net_data)\ntidy(model_net_ate)\n\nSave the data.\nSince it works, let’s save it:\n\n# In the end, all we need is id, income, health, net, and malaria risk:\nnet_data_final &lt;- net_data %&gt;%\n  select(id, income, health, net, malaria_risk)\nhead(net_data_final)\n\n\n# Save it as a CSV file\nwrite_csv(net_data_final, \"data/bed_nets.csv\")\n\n\n\n\nBrief pep talk intermission\nGenerating data for a full complete observational DAG like the example above is complicated and hard. These other forms of causal inference are design-based (i.e. tied to specific contexts like before/after treatment/control or arbitrary cutoffs) instead of model-based, so they’re actually a lot easier to simulate! So don’t be scared away yet!\n\n\nCreating an effect for RCTs\n\nDraw a DAG that maps out how all the columns you care about are related.\nRCTs are great because they make DAGs really easy! In a well-randomized RCT, you get to delete all arrows going into the treatment node in a DAG. We’ll stick with the same mosquito net situation we just used, but make it randomized:\n\nrct_dag &lt;- dagify(mal ~ net + inc + hlth,\n                  hlth ~ inc,\n                  coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3),\n                                y = c(mal = 1, net = 1, inc = 2, hlth = 2)))\nggdag(rct_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nWe’ll measure these nodes the same way as before:\n\nMalaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution.\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\nHealth: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThis is where RCTs are great. Because we removed all the arrows going into net, we don’t need to build any relationships that influence net use. Net use is randomized! We don’t need to make strange “bed net scores” and give people boosts according to income or health or anything. There are only two models in this DAG:\n\nhlth ~ inc: Income influences health. We’ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\nmal ~ net + inc + hlth: Net use, income, and health all have an effect on the risk of malaria. We’ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That’s the casual effect we’re building in to the DAG.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet’s make this data. It’ll be a lot easier than the full DAG we did before. Again, I’ll comment the code below so you can see what’s happening at each step.\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 793 people (just for fun)\nn_people &lt;- 793\n\nrct_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %&gt;%\n  # Randomly assign people to use a net (this is nice and easy!)\n  mutate(net = rbinom(n_people, 1, 0.5)) %&gt;%\n  # Finally generate a malaria risk variable based on income, health, net use,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down by 10 when using a net. Because we rescale things,\n         # though, we have to make the effect a lot bigger here so it scales\n         # down to -10. Risk also decreases as health and income go up. I played\n         # with these numbers until they created reasonable coefficients.\n         malaria_effect = (-35 * net) + (-1.9 * health) + (-0.1 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70)))\n\n# Look at all these columns!\nhead(rct_data)\n\nVerify all relationships with plots and models.\nIncome still looks like it’s associated with health (which isn’t surprising, since it’s the same code we used for the full DAG earlier):\n\nggplot(net_data, aes(x = income, y = health)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nlm(health ~ income, data = net_data) %&gt;% tidy()\n\nTry it out!\nIs the effect in there? With an RCT, all we really need to do is compare the outcome across treatment and control groups—because there’s no confounding, we don’t need to control for anything. Ordinarily we should check for balance across characteristics like health and income (and maybe generate other demographic columns) like we did in the RCT example, but we’ll skip all that here since we’re just checking to see if the effect is there.\nIt looks like using nets causes an average decrease of 10 risk points. Great!\n\n# Correct RCT-based ATE\nmodel_rct &lt;- lm(malaria_risk ~ net, data = rct_data)\ntidy(model_rct)\n\nJust for fun, if we control for health and income, we’ll get basically the same effect, since they don’t actualy confound the relationship and don’t really explain anything useful.\n\n# Controlling for stuff even though we don't need to\nmodel_rct_controls &lt;- lm(malaria_risk ~ net + health + income, data = rct_data)\ntidy(model_rct_controls)\n\nSave the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file.\n\n# In the end, all we need is id, income, health, net, and malaria risk:\nrct_data_final &lt;- rct_data %&gt;%\n  select(id, income, health, net, malaria_risk)\nhead(rct_data_final)\n\n\n# Save it as a CSV file\nwrite_csv(rct_data_final, \"data/bed_nets_rct.csv\")\n\n\n\n\nCreating an effect for diff-in-diff\n\nDraw a DAG that maps out how all the columns you care about are related.\nDifference-in-differences approaches to causal inference are not based on models but on context or research design. You need comparable treatment and control groups before and after some policy or program is implemented.\nWe’ll keep with our mosquito net example and pretend that two cities in some country are dealing with malaria infections. City B rolls out a free net program in 2017; City A does not. Here’s what the DAG looks like:\n\ndid_dag &lt;- dagify(mal ~ net + year + city,\n                  net ~ year + city,\n                  coords = list(x = c(mal = 3, net = 1, year = 2, city = 2),\n                                y = c(mal = 2, net = 2, year = 3, city = 1)))\nggdag(did_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n\nMalaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don’t need to simulate it since it will only happen for people who are in the treatment city after the universal net rollout.\nYear: year ranging from 2013 to 2020. Best to use a uniform distribution.\nCity: binary 0/1, City A/City B variable. Best to use a binomial distribution.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n\nnet ~ year + city: Net use is determined by being in City B and being after 2017. We’ll assume perfect compliance here (but it’s fairly easy to simulate non-compliance and have some people in City A use nets after 2017, and some people in both cities use nets before 2017).\nmal ~ net + year + city: Malaria risk is determined by net use, year, and city. It’s determined by lots of other things too (like we saw in the previous DAGs), but since we’re assuming that the two cities are comparable treatment and control groups, we don’t need to worry about things like health, income, age, etc.\nWe’ll pretend that in general, City B has historicallly had a problem with malaria and people there have had higher risk: being in City B increases malaria risk by 5 points, on average. Over time, both cities have worked on mosquito abatement, so average malaria risk has decreased by 2 points per year (in both cities, because we believe in parallel trends). Using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nGeneration time! Heavily annotated code below:\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 2567 people (just for fun)\nn_people &lt;- 2567\n\ndid_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate year variable: uniform, between 2013 and 2020. Round so it's whole.\n  year = round(runif(n_people, min = 2013, max = 2020), 0),\n  # Generate city variable: binomial, 50% chance of being in a city. We'll use\n  # sample() instead of rbinom()\n  city = sample(c(\"City A\", \"City B\"), n_people, replace = TRUE)\n) %&gt;%\n  # Generate net variable. We're assuming perfect compliance, so this will only\n  # be TRUE for people in City B after 2017\n  mutate(net = ifelse(city == \"City B\" & year &gt; 2017, TRUE, FALSE)) %&gt;%\n  # Generate a malaria risk variable based on year, city, net use, and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 6, shape2 = 3) * 100,\n         # Risk goes up if you're in City B because they have a worse problem.\n         # We could just say \"city_effect = 5\" and give everyone in City A an\n         # exact 5-point boost, but to add some noise, we'll give people an\n         # average boost using rnorm(). Some people might go up 7, some might go\n         # up 1, some might go down 2\n         city_effect = ifelse(city == \"City B\", rnorm(n_people, mean = 5, sd = 2), 0),\n         # Risk goes down by 2 points on average every year. Creating this\n         # effect with regression would work fine (-2 * year), except the years\n         # are huge here (-2 * 2013 and -2 * 2020, etc.) So first we create a\n         # smaller year column where 2013 is year 1, 2014 is year 2, and so on,\n         # that way we can say -2 * 1 and -2 * 6, or whatever.\n         # Also, rather than make risk go down by *exactly* 2 every year, we'll\n         # add some noise with rnorm(), so for some people it'll go down by 1 or\n         # 4 or up by 1, and so on\n         year_smaller = year - 2012,\n         year_effect = rnorm(n_people, mean = -2, sd = 0.1) * year_smaller,\n         # Using a net causes a decrease of 10 points, on average. Again, rather\n         # than use exactly 10, we'll use a distribution around 10. People only\n         # get a net boost if they're in City B after 2017.\n         net_effect = ifelse(city == \"City B\" & year &gt; 2017,\n                             rnorm(n_people, mean = -10, sd = 1.5),\n                             0),\n         # Finally combine all these effects to create the malaria risk variable\n         malaria_risk = malaria_risk_base + city_effect + year_effect + net_effect,\n         # Rescale so it doesn't go below 0 or above 100\n         malaria_risk = rescale(malaria_risk, to = c(0, 100))) %&gt;%\n  # Make an indicator variable showing if the row is after 2017\n  mutate(after = year &gt; 2017)\n\nhead(did_data)\n\nVerify all relationships with plots and models.\nIs risk higher in City B? Yep.\n\nggplot(did_data, aes(x = city, y = malaria_risk, color = city)) +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\") +\n  guides(color = \"none\")\n\nDoes risk decrease over time? And are the trends parallel? There was a weird random spike in City B in 2017 for whatever reason, but in general, the trends in the two cities are pretty parallel from 2013 to 2017.\n\nplot_data &lt;- did_data %&gt;%\n  group_by(year, city) %&gt;%\n  summarize(mean_risk = mean(malaria_risk),\n            se_risk = sd(malaria_risk) / sqrt(n()),\n            upper = mean_risk + (1.96 * se_risk),\n            lower = mean_risk + (-1.96 * se_risk))\n\nggplot(plot_data, aes(x = year, y = mean_risk, color = city)) +\n  geom_vline(xintercept = 2017.5) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = city), alpha = 0.3, color = FALSE) +\n  geom_line() +\n  theme(legend.position = \"bottom\")\n\nTry it out!\nLet’s see if it works! For diff-in-diff we need to use this model:\n\\[\n\\text{Malaria risk} = \\alpha + \\beta\\ \\text{City B} + \\gamma\\ \\text{After 2017} + \\delta\\ (\\text{City B} \\times \\text{After 2017}) + \\varepsilon\n\\]\n\nmodel_did &lt;- lm(malaria_risk ~ city + after + city * after, data = did_data)\ntidy(model_did)\n\nIt works! Being in City B is associated with a 5-point higher risk on average; being after 2017 is associated with a 7.5-point lower risk on average, and being in City B after 2017 causes risk to drop by −10. The number isn’t exactly −10 here, since we rescaled the malaria_risk column a little, but still, it’s close. It’d probably be a good idea to build in some more noise and noncompliance, since the p-values are really really tiny here, but this is good enough for now.\nHere’s an obligatory diff-in-diff visualization:\n\nplot_data &lt;- did_data %&gt;%\n  group_by(after, city) %&gt;%\n  summarize(mean_risk = mean(malaria_risk),\n            se_risk = sd(malaria_risk) / sqrt(n()),\n            upper = mean_risk + (1.96 * se_risk),\n            lower = mean_risk + (-1.96 * se_risk))\n\n# Extract parts of the model results for adding annotations\nmodel_results &lt;- tidy(model_did)\nbefore_treatment &lt;- filter(model_results, term == \"(Intercept)\")$estimate +\n  filter(model_results, term == \"cityCity B\")$estimate\ndiff_diff &lt;- filter(model_results, term == \"cityCity B:afterTRUE\")$estimate\nafter_treatment &lt;- before_treatment + diff_diff +\n  filter(model_results, term == \"afterTRUE\")$estimate\n\nggplot(plot_data, aes(x = after, y = mean_risk, color = city, group = city)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper)) +\n  geom_line() +\n  annotate(geom = \"segment\", x = FALSE, xend = TRUE,\n           y = before_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dashed\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = 2.1, xend = 2.1,\n           y = after_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dotted\", color = \"blue\") +\n  theme(legend.position = \"bottom\")\n\nSave the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file.\n\ndid_data_final &lt;- did_data %&gt;%\n  select(id, year, city, net, malaria_risk)\nhead(did_data_final)\n\n\n# Save data\nwrite_csv(did_data_final, \"data/diff_diff.csv\")\n\n\n\n\nCreating an effect for regression discontinuity\n\nDraw a DAG that maps out how all the columns you care about are related.\nRegression discontinuity designs are also based on context instead of models, so the DAG is pretty simple. We’ll keep with our mosquito net example and pretend that families that earn less than $450 a week qualify for a free net. Here’s the DAG:\n\nrdd_dag &lt;- dagify(mal ~ net + inc,\n                  net ~ cut,\n                  cut ~ inc,\n                  coords = list(x = c(mal = 4, net = 1, inc = 3, cut = 2),\n                                y = c(mal = 1, net = 1, inc = 2, cut = 1.75)))\nggdag(rdd_dag) +\n  theme_dag()\n\nSpecify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n\nMalaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don’t need to simulate it since it will only happen for people who below the cutoff.\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\nCutoff: binary 0/1, below/above $450 variable. This is technically binomial, but we don’t need to simulate it since it is entirely based on income.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are three models in the DAG:\n\ncut ~ inc: Being above or below the cutpoint is determined by income. We know the cutoff is 450, so we just make an indicator showing if people are below that.\nnet ~ cut: Net usage is determined by the cutpoint. If people are below the cutpoint, they’ll use a net; if not, they won’t. We can build in noncompliance here if we want and use fuzzy regression discontinuity. For the sake of this example, we’ll do it both ways, just so you can see both sharp and fuzzy synthetic data.\nmal ~ net + inc: Malaria risk is determined by both net usage and income. It’s also determined by lots of other things (age, education, city, etc.), but we don’t need to include those in the DAG because we’re using RDD to say that we have good treatment and control groups right around the cutoff.\nWe’ll pretend that a 1 dollar increase in income is associated with a drop in risk of 0.01, and that using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet’s fake some data! Heavily annotated code below:\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 5441 people (we need a lot bc we're throwing most away)\nn_people &lt;- 5441\n\nrdd_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate cutoff variable\n  mutate(below_cutoff = ifelse(income &lt; 450, TRUE, FALSE)) %&gt;%\n  # Generate net variable. We'll make two: one that's sharp and has perfect\n  # compliance, and one that's fuzzy\n  # Here's the sharp one. It's easy. If you're below the cutoff you use a net.\n  mutate(net_sharp = ifelse(below_cutoff == TRUE, TRUE, FALSE)) %&gt;%\n  # Here's the fuzzy one, which is a little trickier. If you're far away from\n  # the cutoff, you follow what you're supposed to do (like if your income is\n  # 800, you don't use the program; if your income is 200, you definitely use\n  # the program). But if you're close to the cutoff, we'll pretend that there's\n  # an 80% chance that you'll do what you're supposed to do.\n  mutate(net_fuzzy = case_when(\n    # If your income is between 450 and 500, there's a 20% chance of using the program\n    income &gt;= 450 & income &lt;= 500 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.2, 0.8)),\n    # If your income is above 500, you definitely don't use the program\n    income &gt; 500 ~ FALSE,\n    # If your income is between 400 and 450, there's an 80% chance of using the program\n    income &lt; 450 & income &gt;= 400 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.8, 0.2)),\n    # If your income is below 400, you definitely use the program\n    income &lt; 400 ~ TRUE\n  )) %&gt;%\n  # Finally we can make the malaria risk score, based on income, net use, and\n  # random noise. We'll make two outcomes: one using the sharp net use and one\n  # using the fuzzy net use. They have the same effect built in, we just have to\n  # use net_sharp and net_fuzzy separately.\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100) %&gt;%\n  # Make the sharp version. There's really a 10 point decrease, but because of\n  # rescaling, we use 15. I only chose 15 through lots of trial and error (i.e.\n  # I used -11, ran the RDD model, and the effect was too small; I used -20, ran\n  # the model, and the effect was too big; I kept changing numbers until landing\n  # on -15). Risk also goes down as income increases.\n  mutate(malaria_effect_sharp = (-15 * net_sharp) + (-0.01 * income),\n         malaria_risk_sharp = malaria_risk_base + malaria_effect_sharp + rnorm(n_people, 0, sd = 3),\n         malaria_risk_sharp = rescale(malaria_risk_sharp, to = c(5, 70))) %&gt;%\n  # Do the same thing, but with net_fuzzy\n  mutate(malaria_effect_fuzzy = (-15 * net_fuzzy) + (-0.01 * income),\n         malaria_risk_fuzzy = malaria_risk_base + malaria_effect_fuzzy + rnorm(n_people, 0, sd = 3),\n         malaria_risk_fuzzy = rescale(malaria_risk_fuzzy, to = c(5, 70))) %&gt;%\n  # Make a version of income that's centered at the cutpoint\n  mutate(income_centered = income - 450)\n\nhead(rdd_data)\n\nVerify all relationships with plots and models.\nIs there a cutoff in the running variable when we use the sharp net variable? Yep!\n\nggplot(rdd_data, aes(x = income, y = net_sharp, color = below_cutoff)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) +\n  guides(color = \"none\")\n\nIs there a cutoff in the running variable when we use the fuzzy net variable? Yep! There are some richer people using the program and some poorer people not using it.\n\nggplot(rdd_data, aes(x = income, y = net_fuzzy, color = below_cutoff)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) +\n  guides(color = \"none\")\n\nTry it out!\nLet’s test it! For sharp RDD we need to use this model:\n\\[\n\\text{Malaria risk} = \\beta_0 + \\beta_1 \\text{Income}_\\text{centered} + \\beta_2 \\text{Net} + \\varepsilon\n\\]\nWe’ll use a bandwidth of ±$50, because why not. In real life you’d be more careful about bandwidth selection (or use rdbwselect() from the rdrobust package to find the optimal bandwidth)\n\nggplot(rdd_data, aes(x = income, y = malaria_risk_sharp, color = net_sharp)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.2, size = 0.5) +\n  # Add lines for the full range\n  geom_smooth(data = filter(rdd_data, income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  # Add lines for bandwidth = 50\n  geom_smooth(data = filter(rdd_data, income_centered &gt;= -50 & income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 2) +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0 & income_centered &lt;= 50),\n              method = \"lm\", se = FALSE, size = 2) +\n  theme(legend.position = \"bottom\")\n\n\nmodel_sharp &lt;- lm(malaria_risk_sharp ~ income_centered + net_sharp,\n                  data = filter(rdd_data,\n                                income_centered &gt;= -50 & income_centered &lt;= 50))\ntidy(model_sharp)\n\nThere’s an effect! For people in the bandwidth, the local average treatment effect of nets is a 10.6 point reduction in malaria risk.\nLet’s check if it works with the fuzzy version where there are noncompliers:\n\nggplot(rdd_data, aes(x = income, y = malaria_risk_fuzzy, color = net_fuzzy)) +\n  geom_vline(xintercept = 450) +\n  geom_point(alpha = 0.2, size = 0.5) +\n  # Add lines for the full range\n  geom_smooth(data = filter(rdd_data, income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0),\n              method = \"lm\", se = FALSE, size = 1, linetype = \"dashed\") +\n  # Add lines for bandwidth = 50\n  geom_smooth(data = filter(rdd_data, income_centered &gt;= -50 & income_centered &lt;= 0),\n              method = \"lm\", se = FALSE, size = 2) +\n  geom_smooth(data = filter(rdd_data, income_centered &gt; 0 & income_centered &lt;= 50),\n              method = \"lm\", se = FALSE, size = 2) +\n  theme(legend.position = \"bottom\")\n\nhere’s a gap, but it’s hard to measure since there are noncompliers on both sides. We can deal with the noncompliance if we use above/below the cutoff as an instrument (see the fuzzy regression discontinuity guide for a complete example). We should run this set of models:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Net}} &= \\gamma_0 + \\gamma_1 \\text{Income}_{\\text{centered}} + \\gamma_2 \\text{Below 450} + \\omega \\\\\\\\\n\\text{Malaria risk} &= \\beta_0 + \\beta_1 \\text{Income}\\_{\\text{centered}} + \\beta_2 \\widehat{\\text{Net}} + \\epsilon\n\\end{aligned}\n\\]\nInstead of doing these two stages by hand (ugh), we’ll do the 2SLS regression with the iv_robust() function from the estimatr package:\n\nlibrary(estimatr)\n\nmodel_fuzzy &lt;- iv_robust(malaria_risk_fuzzy ~ income_centered + net_fuzzy |\n                           income_centered + below_cutoff,\n                         data = filter(rdd_data,\n                                       income_centered &gt;= -50 & income_centered &lt;= 50))\ntidy(model_fuzzy)\n\nThe effect is slightly larger now (−11.2), but that’s because we are looking at a doubly local ATE: compliers in the bandwidth. But still, it’s close to −10, so that’s good. And we could probably get it closer if we did other mathy shenanigans like adding squared and cubed terms or using nonparametric estimation with rdrobust() in the rdrobust package.\nSave the data.\nThe data works, so let’s get rid of the intermediate columns we don’t need and save it as a CSV file. We’ll make two separate CSV files for fuzzy and sharp, just because.\n\nrdd_data_final_sharp &lt;- rdd_data %&gt;%\n  select(id, income, net = net_sharp, malaria_risk = malaria_risk_sharp)\nhead(rdd_data_final_sharp)\n\nrdd_data_final_fuzzy &lt;- rdd_data %&gt;%\n  select(id, income, net = net_fuzzy, malaria_risk = malaria_risk_fuzzy)\nhead(rdd_data_final_fuzzy)\n\n\n# Save data\nwrite_csv(rdd_data_final_sharp, \"data/rdd_sharp.csv\")\nwrite_csv(rdd_data_final_fuzzy, \"data/rdd_fuzzy.csv\")\n\n\n\n\nCreating an effect for instrumental variables\n\nDraw a DAG that maps out how all the columns you care about are related.\nAs with diff-in-diff and regression discontinuity, instrumental variables are a design-based approach to causal inference and thus don’t require complicated models (but you can still add control variables!), so their DAGs are simpler. Once again we’ll look at the effect of mosquito nets on malaria risk, but this time we’ll say that we cannot possibly measure all the confounding factors between net use and malaria risk, so we’ll use an instrument to extract the exogeneity from net use.\nAs we talked about in Session 11, good plausible instruments are hard to find: they have to cause bed net use and not be related to malaria risk except through bed net use.\nFor this example, we’ll pretend that free bed nets are distributed from town halls around the country. We’ll use “distance to town hall” as our instrument, since it could arguably maybe work perhaps. Being closer to a town hall makes you more likely to use a net, but being closer to a town halls doesn’t make put you at higher or lower risk for malaria on its own—it does that only because it changes your likelihood of getting a net.\nThis is where the story for the instrument falls apart, actually; in real life, if you live far away from a town hall, you probably live further from health services and live in more rural places with worse mosquito abatement policies, so you’re probably at higher risk of malaria. It’s probably a bad instrument, but just go with it.\nHere’s the DAG:\n\niv_dag &lt;- dagify(mal ~ net + U,\n                 net ~ dist + U,\n                 coords = list(x = c(mal = 4, net = 2, U = 3, dist = 1),\n                               y = c(mal = 1, net = 1, U = 2, dist = 1.5)),\n                 latent = \"U\")\n\nggdag_status(iv_dag) +\n  guides(color = \"none\") +\n  theme_dag()\n\nSpecify how those nodes are measured.\nHere’s how we’ll measure these nodes:\n\nMalaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\nNet use: binary 0/1, TRUE/FALSE variable. However, since we want to use other variables that increase the likelihood of using a net, we’ll do some cool tricky stuff with a bed net score, like we did in the observational DAG example earlier.\nDistance: distance to nearest town hall, measured in kilometers, mostly around 3, with a left skewed long tail (i.e. most people live fairly close, some people live far away). Best to use a Beta distribution (to get the skewed shape) that we then rescale.\nUnobserved: who knows?! There are a lot of unknown confounders. We could generate columns like income, age, education, and health, make them mathematically related to malaria risk and net use, and then throw those columns away in the final data so they’re unobserved. That would be fairly easy and intuitive.\nFor the sake of simplicity here, we’ll make a column called “risk factors,” kind of like we did with the “ability” column in the instrumental variables example—it’s a magical column that is unmeasurable, but it’ll open a backdoor path between net use and malaria risk and thus create endogeneity. It’ll be normally distributed around 50, with a standard deviation of 25.\n\nSpecify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n\nnet ~ dist + U: Net usage is determined by both distance and our magical unobserved risk factor column. Net use is technically binomial, but in order to change the likelihood of net use based on distance to town hall and unobserved stuff, we’ll do the fancy tricky stuff we did in the observational DAG section above: we’ll create a bed net score, increase or decrease that score based on risk factors and distance, scale that score to a 0-1 scale of probabilities, and then draw a binomial 0/1 outcome using those probabilities.\nWe’ll say that a one kilometer increase in the distance to a town halls reduces the bed net score and a one point increase in risk factors reduces the bed net score.\nmal ~ net + U: Malaria risk is determined by both net usage and unkown stuff, or the magical column we’re calling “risk factors.” We’ll say that a one point increase in risk factors increases malaria risk, and that using a mosquito net causes a decrease of 10 points on average. That’s our causal effect.\n\nGenerate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you’ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nFake data time! Here’s some heavily annotated code:\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1578 people (just for fun)\nn_people &lt;- 1578\n\niv_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate magical unobserved risk factor variable: normal, 500 ± 300\n  risk_factors = rnorm(n_people, mean = 100, sd = 25),\n  # Generate distance to town hall variable\n  distance = rbeta(n_people, shape1 = 1, shape2 = 4)\n) %&gt;%\n  # Scale up distance to be 0.1-15 instead of 0-1\n  mutate(distance = rescale(distance, to = c(0.1, 15))) %&gt;%\n  # Generate net variable based on distance, risk factors, and random noise\n  # Note: These -40 and -2 effects are entirely made up and I got them through a\n  # lot of trial and error and rerunning this stupid chunk dozens of times\n  mutate(net_score = 0 +\n           (-40 * distance) +  # Distance effect\n           (-2 * risk_factors) +  # Risk factor effect\n           rnorm(n_people, mean = 0, sd = 50),  # Random noise\n        net_probability = rescale(net_score, to = c(0.15, 1)),\n        # Randomly generate a 0/1 variable using that probability\n        net = rbinom(n_people, 1, net_probability)) %&gt;%\n  # Generate malaria risk variable based on net use, risk factors, and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 7, shape2 = 5) * 100,\n         # We're aiming for a -10 net effect, but need to boost it because of rescaling\n         malaria_effect = (-20 * net) + (0.5 * risk_factors),\n         # Make the final malaria risk score\n         malaria_risk = malaria_risk_base + malaria_effect,\n         # Rescale so it doesn't go below 0\n         malaria_risk = rescale(malaria_risk, to = c(5, 80)))\niv_data\n\nVerify all relationships with plots and models.\nIs there a relationship between unobserved risk factors and malaria risk? Yep.\n\nggplot(iv_data, aes(x = risk_factors, y = malaria_risk)) +\n  geom_point(aes(color = as.factor(net))) +\n  geom_smooth(method = \"lm\")\n\nIs there a relationship between distance to town hall and net use? Yeah, those who live further away are less likely to use a net.\n\nggplot(iv_data, aes(x = distance, fill = as.factor(net))) +\n  geom_density(alpha = 0.7)\n\nIs there a relationship between net use and malaria risk? Haha, yeah, that’s a huge highly significant effect. Probably too perfect. We could increase those error bars if we tinker with some of the numbers in the code, but for the sake of this example, we’ll leave them like this.\n\nggplot(iv_data, aes(x = as.factor(net), y = malaria_risk, color = as.factor(net))) +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\")\n\nTry it out!\nCool, let’s see if this works. Remember, we can’t actually use the risk_factors column in real life, but we will here just to make sure the effect we built in exists. Here’s the true effect, where using a net causes a decrease of 10.9 malaria risk points\n\nmodel_forbidden &lt;- lm(malaria_risk ~ net + risk_factors, data = iv_data)\ntidy(model_forbidden)\n\nSince we can’t actually use that column, we’ll use distance to town hall as an instrument. We should run this set of models:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Net}} &= \\gamma_0 + \\gamma_1 \\text{Distance to town hall} + \\omega \\\\\\\\\n\\text{Malaria risk} &= \\beta_0 + \\beta_1 \\widehat{\\text{Net}} + \\epsilon\n\\end{aligned}\n\\]\nWe’ll run this 2SLS model with the iv_robust() function from the estimatr package:\n\nlibrary(estimatr)\n\nmodel_iv &lt;- iv_robust(malaria_risk ~ net | distance, data = iv_data)\ntidy(model_iv)\n\n…and it’s relatively close, I guess, at −8.2. Getting instrumental variables to find exact causal effects is tricky, but I’m fine with this for simulated data.\nSave the data.\nThe data works well enough, so we’ll get rid of the extra intermediate columns and save it as a CSV file. We’ll keep the forbidden risk_factors column just for fun.\n\niv_data_final &lt;- iv_data %&gt;%\n  select(id, net, distance, malaria_risk, risk_factors)\n\nhead(iv_data_final)\n\n\n# Save data\nwrite_csv(iv_data_final, \"data/bed_nets_iv.csv\")",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/synthetic-data.html#use-synthetic-data-packages",
    "href": "slides/synthetic-data.html#use-synthetic-data-packages",
    "title": "The ultimate guide to generating synthetic data for causal inference",
    "section": "4 Use synthetic data packages",
    "text": "4 Use synthetic data packages\nThere are several R packages that let you generate synthetic data with built-in relationships in a more automatic way. They all work a little differently, and if you’re interested in trying them out, make sure you check the documentation for details.\n\nfabricatr\nThe fabricatr package is a very powerful package for simulating data. It was invented specifically for using in preregistered studies, so it can handle a ton of different data structures like panel data and time series data. You can build in causal effects and force columns to be correlated with each other.\nfabricatr has exceptionally well-written documentation with like a billion detailed examples (see the right sidebar here). This is a gold standard package and you should most definitely check it out.\nHere’s a simple example of simulating a bunch of voters and making older ones more likely to vote:\n\nlibrary(fabricatr)\n\nset.seed(1234)\n\nfake_voters &lt;- fabricate(\n  # Make 100 people\n  N = 100,\n  # Age uniformly distributed between 18 and 85\n  age = round(runif(N, 18, 85)),\n  # Older people more likely to vote\n  turnout = draw_binary(prob = ifelse(age &lt; 40, 0.4, 0.7), N = N)\n)\n\nhead(fake_voters)\n\nAnd here’s an example of country-year panel data where there are country-specific and year-specific effects on GDP:\n\nset.seed(1234)\n\npanel_global_data &lt;- fabricate(\n  years = add_level(\n    N = 10,\n    ts_year = 0:9,\n    year_shock = rnorm(N, 0, 0.3)\n  ),\n  countries = add_level(\n    N = 5,\n    base_gdp = runif(N, 15, 22),\n    growth_units = runif(N, 0.25, 0.5),\n    growth_error = runif(N, 0.15, 0.5),\n    nest = FALSE\n  ),\n  country_years = cross_levels(\n    by = join_using(years, countries),\n    gdp_measure = base_gdp + year_shock + (ts_year * growth_units) +\n      rnorm(N, sd = growth_error)\n  )\n) %&gt;%\n  # Scale up the years to be actual years instead of 1, 2, 3, etc.\n  mutate(year = ts_year + 2010)\n\nhead(panel_global_data)\n\n\nggplot(panel_global_data, aes(x = year, y = gdp_measure, color = countries)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Log GDP\", color = \"Countries\")\n\nThat all just scratches the surface of what fabricatr can do. Again, check the examples and documentation and play around with it to see what else it can do.\n\n\nwakefield\nThe wakefield package is jokingly named after Andrew Wakefield, the British researcher who invented fake data to show that the MMR vaccine causes autism. This package lets you quickly generate random fake datasets. It has a bunch of pre-set column possibilities, like age, color, Likert scales, political parties, religion, and so on, and you can also use standard R functions like rnorm(), rbinom(), or rbeta(). It also lets you create repeated measures (1st grade score, 2nd grade score, 3rd grade score, etc.) and build correlations between variables.\nYou should definitely look at the documentation to see a ton of examples of how it all works. Here’s a basic example:\n\nlibrary(wakefield)\n\nset.seed(1234)\n\nwakefield_data &lt;- r_data_frame(\n  n = 500,\n  id,\n  treatment = rbinom(1, 0.3),  # 30% chance of being in treatment\n  outcome = rnorm(mean = 500, sd = 100),\n  race,\n  age = age(x = 18:45),\n  sex = sex_inclusive(),\n  survey_question_1 = likert(),\n  survey_question_2 = likert()\n)\nhead(wakefield_data)\n\n\n\nfaux\nThe faux package does some really neat things. We can create data that has built-in correlations without going through all the math. For instance, let’s say we have 3 variables A, B, and C that are normally distributed with these parameters:\n\nA: mean = 10, sd = 2\nB: mean = 5, sd = 1\nC: mean = 20, sd = 5\n\nWe want A to correlate with B at r = 0.8 (highly correlated), A to correlate with C at r = 0.3 (less correlated), and B to correlate with C at r = 0.4 (moderately correlated). Here’s how to create that data with faux:\n\nlibrary(faux)\n\nset.seed(1234)\n\nfaux_data &lt;- rnorm_multi(n = 100,\n                         mu = c(10, 5, 20),\n                         sd = c(2, 1, 5),\n                         r = c(0.8, 0.3, 0.4),\n                         varnames = c(\"A\", \"B\", \"C\"),\n                         empirical = FALSE)\nhead(faux_data)\n\n# Check averages and standard deviations\nfaux_data %&gt;%\n  # Convert to long/tidy so we can group and summarize\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  group_by(variable) %&gt;%\n  summarize(mean = mean(value),\n            sd = sd(value))\n\n# Check correlations\ncor(faux_data$A, faux_data$B)\ncor(faux_data$A, faux_data$C)\ncor(faux_data$B, faux_data$C)\n\nfaux can do a ton of other things too, so make sure you check out the documentation and all the articles with examples here.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 2"
    ]
  },
  {
    "objectID": "slides/slides_2023.html",
    "href": "slides/slides_2023.html",
    "title": "2024",
    "section": "",
    "text": "Druid24\n\n\n\n\n\nDiss - Current Status\n\n\n\n\nCoreSignal Analysis\n\n\n\nCoreSignal Analysis",
    "crumbs": [
      "Presentations",
      "2023",
      "Diss"
    ]
  },
  {
    "objectID": "slides/slides_2021.html",
    "href": "slides/slides_2021.html",
    "title": "2022 - January",
    "section": "",
    "text": "First presentation\n\n\n\nSecond presentation",
    "crumbs": [
      "Presentations",
      "2022",
      "January"
    ]
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Presentations overview",
    "section": "",
    "text": "Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!",
    "crumbs": [
      "Presentations",
      "Overview"
    ]
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#table-of-contents",
    "href": "revealjs/slides/2023/cs/cs_test.html#table-of-contents",
    "title": "CoreSignal Analysis",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#input-for-cs-domain-linkedin-handle-list-from-cb",
    "href": "revealjs/slides/2023/cs/cs_test.html#input-for-cs-domain-linkedin-handle-list-from-cb",
    "title": "CoreSignal Analysis",
    "section": "1. Input for CS: Domain & LinkedIn handle list (from CB)",
    "text": "1. Input for CS: Domain & LinkedIn handle list (from CB)\nObjective: Create list with domains and/or LinkedIn handles\n\nSeperated by organizations & employees\nBased mostly on our crunchbase dataset (I have added some angellist data as well)\n\n\n\n\nScript and File locations\n\n\nData 01_data_sources/06_coresignal/01_data/01_input_for_cs/final_selection/version_3_cijs/\n\norgs_cs.csv\nusr_cs.csv\n\nScripts 01_data_sources/06_coresignal/02_scripts/01_input_for_cs/\n\ncs_org_select_v2.R"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#output-from-cs-company-personal-linkedin-profile-date",
    "href": "revealjs/slides/2023/cs/cs_test.html#output-from-cs-company-personal-linkedin-profile-date",
    "title": "CoreSignal Analysis",
    "section": "2. Output from CS: Company & Personal LinkedIn Profile Date",
    "text": "2. Output from CS: Company & Personal LinkedIn Profile Date\nInstructions:\n\nMap provided domains with company profiles\nProvide entire employed user data (including company profiles)\n\nData was proided as json data:\n\n5 files for companies (~160MB)\n1662 files for employees (~175GB)\n\n\n\n\nScript and File locations\n\n\nData 01_data_sources/06_coresignal/01_data/02_raw/\n\ncompany/202203_custom/...\nmember/202203_custom/..."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#extract-convert-relevant-data",
    "href": "revealjs/slides/2023/cs/cs_test.html#extract-convert-relevant-data",
    "title": "CoreSignal Analysis",
    "section": "3. Extract & convert relevant data",
    "text": "3. Extract & convert relevant data\nObjectives: Extract data that is relevant (variables) for our analyses and convert it to .rds / .parquet files\n\n\n\nScript and File locations\n\n\nScripts 01_data_sources/06_coresignal/02_scripts/02_build_tables/\n\n01_cs_build_table_company.R\n02_cs_build_tables_member_V1_bd_exp_skills.R\n03_cs_build_tables_member_V2_edu_exp.R\n\nData 01_data_sources/06_coresignal/01_data/03_extracted/\n\ncompany/cs_companies_base.rds\nmember/01_basic_data/...\nmember/02_experience/... (non-deduplicated)\nmember/03_skills/...\nmember/04_education/... (non-deduplicated)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#deduplicate-data",
    "href": "revealjs/slides/2023/cs/cs_test.html#deduplicate-data",
    "title": "CoreSignal Analysis",
    "section": "4. Deduplicate data",
    "text": "4. Deduplicate data\nEverytime a user changes something on their profile a new record is being created (date, typos, names, …). The column deleted is not useful.\nObjectives: Deduplicate the member data: Experiences & Education Tables\n\n\n\nScript and File locations\n\n\nScripts 01_data_sources/06_coresignal/02_scripts/02_build_tables/\n\n04_cs_build_tables_member_exp_dist.R\n05_cs_build_tables_member_edu_dist.R\n\nData 01_data_sources/06_coresignal/01_data/04_wrangled/\n\ncompanies/cs_companies_base_slct.rds (just relevant columns selected)\nmember/02_experience/me_dist8/... (deduplicated)\nmember/04_education/02_wrangled_dist_chunked/... (deduplicated)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#overview---orgs",
    "href": "revealjs/slides/2023/cs/cs_test.html#overview---orgs",
    "title": "CoreSignal Analysis",
    "section": "1. Overview - Orgs",
    "text": "1. Overview - Orgs\nCoreSignal did not provide a matching table but provided only the resulting data. Hence, backmapping to our CrunchBase / Pitchbook Data via domains is necessary:\n\nMap Companies\n\n\n\n\nScript and File locations\n\n\nScripts 02_data_mapping/10_cbpb_cs/01_scripts/\n\n06_cbpb_cs_matching_companies.R\n\nData (Input) 02_data_mapping/10_cbpb_cs/02_data/\n\nfunded_companies.rds (created by Christoph)\n\nData (Output)\n\ncbpb_cs_joined.rds (companies joined)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#overview---employees",
    "href": "revealjs/slides/2023/cs/cs_test.html#overview---employees",
    "title": "CoreSignal Analysis",
    "section": "2. Overview - Employees",
    "text": "2. Overview - Employees\nMap Crunchbase / Pitchbook data to CoreSignal profiles (via mapped companies (1))\n\nMap employees\n\n\n\n\nScript and File locations\n\n\nScripts 02_data_mapping/10_cbpb_cs/01_scripts/\n\n07_cs_cb_matching_employees.R\n\nData (Input) 01_data_sources/06_coresignal/01_data/04_wrangled/member/02_experience/me_dist8/02_unnested/ 02_data_mapping/10_cbpb_cs/02_data/\n\ncs_me_dist8_unest_prqt (CoreSignal distinct Member Experiences)\ncbpb_cs_joined.rds (Joined CoreSignal / CrunchbasePitchbook Org Data)\n\nData (Output)\n\ncs_me_dist8_unest_fc_joined.parquet (intermediate data)\n\n\n\n\nThere are some further explanations about the org join and empployee join in the next two section (click on each tabset. There are also some fragments –&gt; You have to hit enter/arrow to slide them in). You can skip to the next chapter (VAR I) though."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#details-load-init-companies-crunchbase-pitchbook-coresignal",
    "href": "revealjs/slides/2023/cs/cs_test.html#details-load-init-companies-crunchbase-pitchbook-coresignal",
    "title": "CoreSignal Analysis",
    "section": "3. Details: Load & Init Companies (CrunchBase, PitchBook & CoreSignal)",
    "text": "3. Details: Load & Init Companies (CrunchBase, PitchBook & CoreSignal)\n\nCBPB: SELECTCBPB: WRANGLECBPB: CLEANCS: SELECTCS: WRANGLECS: CLEAN\n\n\nCrunchbase data contains 150,838 startups with a valid funding trajectory.\n\np_load(arrow, dplyr, tidyr)\n\nfunded_companies_prqt &lt;- open_dataset(\"funded_companies_identifiers.parquet\") \nfunded_companies_prqt\n\n\n\n#&gt; # A tibble: 150,838 × 3\n#&gt;   company_id            domain linkedin_url                                     \n#&gt;        &lt;int&gt; &lt;list&lt;character&gt;&gt; &lt;chr&gt;                                            \n#&gt; 1          1               [1] &lt;NA&gt;                                             \n#&gt; 2          2               [1] https://www.linkedin.com/company/luna-pharmaceut…\n#&gt; 3          3               [1] http://www.linkedin.com/company/chainsync        \n#&gt; # ℹ 150,835 more rows\n\n\n\nMultiple domains (Unnesting via Arrow not possible. Options: Spark & sparklyr.nested):\n\nfc_unnested_tbl &lt;- funded_companies_prqt |&gt; collect() |&gt; \n                      # 1. Allow multiple domains per company. No multiple linkedin handles.\n                      unnest(domain) \nfc_unnested_tbl\n\n#&gt; # A tibble: 155,413 × 3\n#&gt;   company_id domain              linkedin_url                                   \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                                          \n#&gt; 1          1 zana.io             &lt;NA&gt;                                           \n#&gt; 2          2 premamawellness.com https://www.linkedin.com/company/luna-pharmace…\n#&gt; 3          3 chainsync.com       http://www.linkedin.com/company/chainsync      \n#&gt; # ℹ 155,410 more rows\n\n\n\n\n\n\nMust have identifier (domain, linkedin)\nClean identifiers\nRemove duplicates\n\n\nlibrary(stringr)\nfc_unnested_tbl |&gt; \n  \n  # 1. At least 1 identifier: 4.518 observations are filtered out\n  filter(if_any(c(domain, linkedin_url), ~!is.na(.))) |&gt;\n  \n  # 2. Extract linkedin handle & clean domains\n  mutate(linkedin_handle = linkedin_url |&gt; str_extract(\"(?&lt;=linkedin\\\\.com/company/).*?(?=(?:\\\\?|$|/))\")) |&gt;\n  mutate(domain          = domain |&gt; clean_domain()) |&gt;\n\n  # 3. Remove 532 duplicates\n  distinct()\n\n\n\n#&gt; # A tibble: 150,363 × 3\n#&gt;   company_id domain              linkedin_handle          \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                    \n#&gt; 1          1 zana.io             &lt;NA&gt;                     \n#&gt; 2          2 premamawellness.com luna-pharmaceuticals-inc-\n#&gt; 3          3 chainsync.com       chainsync                \n#&gt; # ℹ 150,360 more rows\n\n\n–&gt; 145.991 distinct examineable companies.\n\n\nIssue: Some extracted domains are not unique and associated with multiple companies. Manual Cleaning: Domains with a count exceeding two were analyzed and set to NA if they do not correspond to the actual one.\n\n# ANALYZE\n# fc_wrangled_tbl |&gt; \n#   distinct(company_id, domain) |&gt; \n#   count(domain, sort = T) |&gt; \n#   filter(n&gt;2)`\n\nunwanted_domains_cb &lt;- c(\"webflow.io\", \"angel.co\", \"weebly.com\", \"wordpress.com\", \"wixsite.com\", \"squarespace.com\", \n                         \"webflow.io\", \"crypt2esports.com\", \"myshopify.com\", \"business.site\", \"mystrikingly.com\", \n                         \"launchrock.com\", \"square.site\", \"google.com\", \"sites.google.com\", \"t.co\", \"linktr.ee\",\n                         \"netlify.app\", \"itunes.apple.com\", \"apple.com\", \"crunchb.com\", \"tumblr.com\", \"linkedin.com\",\n                         \"godaddysites.com\", \"mit.edu\", \"paloaltonetworks.com\", \" wpengine.com\", \"facebook.com\",\n                         \"intuit.com\", \"medium.com\", \"salesforce.com\", \"strikingly.com\", \"wix.com\", \"cisco.com\",\n                         \"digi.me\", \"apps.apple.com\", \"bit.ly\", \"fleek.co\", \"harvard.edu\", \"ibm.com\", \"jimdo.com\",\n                         \"myftpupload.com\", \"odoo.com\", \"storenvy.com\", \"twitter.com\", \"umd.edu\", \"umich.edu\", \"vmware.com\", \"webs.com\")\n\n# Not all observations with unwanted domains are bad per se:\nwanted_ids_cb &lt;- c(angel = 128006, `catapult-centres-uk` = 115854, digime1 = 140904, digimi2 = 95430, fleek = 50738, \n                   jimdo = 108655, medium = 113415, storenvy = 85742, strikingly = 95831, substack = 34304, \n                   tumblr = 84838, twitter = 53139, weebly = 91365, wpengine = 91720)\n\n# Set misleading domains to NA\nfunded_companies_clnd &lt;- fc_wrangled_tbl |&gt; \n                              \n  mutate(domain = if_else(\n    domain %in% unwanted_domains_cb & !(company_id %in% wanted_ids_cb), \n    NA_character_, domain))\n\n\n\nIt appears that CoreSignal has been able to locate 45.026 companies within our gathered data.\n\n# Selection & Wrangle has been done already\ncs_companies_base_slct &lt;- readRDS(\"cs_companies_base_slct.rds\") \ncs_companies_base_slct\n\n\n\n#&gt; # A tibble: 45,362 × 4\n#&gt;      id name                                domain               linkedin_handle\n#&gt;   &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;                &lt;chr&gt;          \n#&gt; 1   305 Blueprint, a David's Bridal Company blueprintregistry.c… blueprint-regi…\n#&gt; 2   793 BookingLive                         bookinglive.com      bookinglive    \n#&gt; 3  2425 Brandvee                            momentum.ai          brandvee       \n#&gt; # ℹ 45,359 more rows\n\n\n\n\ncs_companies_base_slct$id |&gt; n_distinct()\n\n#&gt; [1] 45026\n\n\n\n\n\ncs_companies_base_slct |&gt; janitor::get_dupes(id)\n\n#&gt; # A tibble: 672 × 5\n#&gt;        id dupe_count name          domain           linkedin_handle\n#&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;          \n#&gt; 1  596494          2 Vi            vi.co            vitrainer      \n#&gt; 2  596494          2 Vi            vi.co            vi             \n#&gt; 3 1324413          2 Patch Lending patchlending.com patch-of-land  \n#&gt; # ℹ 669 more rows\n\n\n\n\n\nNothing to wrangle …\n\ncs_companies_base_wrangled &lt;- cs_companies_base_slct |&gt; select(-name) |&gt; \n  \n                                        # Add suffixes to col names\n                                        rename_with(~ paste(., \"cs\", sep = \"_\"))\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMore cleaning necessary (same as CBPB)! The task was undertaken with a limited degree of enthusiasm.\n\n\n\n\nunwanted_domains_cs    &lt;- c(\"bit.ly\", \"linktr.ee\", \"facebook.com\", \"linkedin.com\", \"twitter.com\", \"crunchbase.com\")\nwanted_ids_cs          &lt;- c(crunchbase = 1634413, linkedin = 8568581, twitter = 24745469)\n\ncs_companies_base_clnd &lt;- cs_companies_base_wrangled |&gt; \n  \n  mutate(domain_cs = if_else(\n    domain_cs %in% unwanted_domains_cs & !(id_cs %in% wanted_ids_cs), \n    NA_character_, \n    domain_cs)\n    )"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#details-join-companies-member-experiences-and-funding-information",
    "href": "revealjs/slides/2023/cs/cs_test.html#details-join-companies-member-experiences-and-funding-information",
    "title": "CoreSignal Analysis",
    "section": "4. Details: Join companies, member experiences and funding information",
    "text": "4. Details: Join companies, member experiences and funding information\n\nCompaniesJobs (all)Jobs (dist)Jobs (focal)FundingConversionJoin\n\n\nWe were able to match 37.287 CS & CB/PB companies.\n\ncb_cs_joined &lt;- funded_companies_clnd |&gt; \n\n    # Leftjoins\n    left_join(cs_companies_base_clnd |&gt; select(id_cs, domain_cs),          by = c(domain          = \"domain_cs\"),          na_matches = \"never\") |&gt; \n    left_join(cs_companies_base_clnd |&gt; select(id_cs, linkedin_handle_cs), by = c(linkedin_handle = \"linkedin_handle_cs\"), na_matches = \"never\") |&gt; \n\n    # Remove obs with no cs_id\n    filter(!is.na(id_cs)) |&gt;\n    \n    # Remove matches, that matched different domains, but same company (e.g. company_id: 83060, id_cs: 4507928) block.xyz & squareup.com\n    select(company_id, id_cs) |&gt; \n    distinct()\n    \ncb_cs_joined\n\n\n\n#&gt; # A tibble: 38,118 × 2\n#&gt;   company_id    id_cs\n#&gt;        &lt;int&gt;    &lt;int&gt;\n#&gt; 1          2  8345218\n#&gt; 2          5 28149599\n#&gt; 3          8  4469271\n#&gt; 4         11  5349023\n#&gt; 5         12  9364263\n#&gt; # ℹ 38,113 more rows\n\n\n\n\ncb_cs_joined |&gt; distinct(company_id) |&gt; nrow()\n\n#&gt; [1] 37287\n\n\n\n\n\nWe got over 460 million employment observations from CoreSignal.\n\n# Other data versions\n# 1. Complete: \nmember_experience_dt \n#&gt; {462.711.794}  \n\n# 2. Distinct1: \nmember_experience_dist_dt &lt;- unique(member_experience_dt) \n#&gt; {432.368.479}\n\n# 3. Distinct2: \nunique(member_experience_dist_dt[order(id)], by = setdiff(names(member_experience_dist_dt), \"id\")) \n#&gt; {431.899.547}\n\n\n\nBut only ~50 Mil distinct employments\n\n# Load distinct member experiences\nme_dist8_prqt &lt;- arrow::open_dataset(\"cs_me_dist8_unest_empl_hist.parquet\") \nme_dist8_prqt |&gt; glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 51,621,196 rows x 10 columns\n#&gt; $ id_tie                 &lt;int32&gt; 16615559, 16615560, 16615561, 16615562, 1661556…\n#&gt; $ id                    &lt;double&gt; 2244288231, 254049663, 948937291, 254049667, 25…\n#&gt; $ member_id              &lt;int32&gt; 179313066, 179313066, 179313066, 179313066, 179…\n#&gt; $ company_id             &lt;int32&gt; 865089, 9098713, 9098713, NA, 865089, 9020540, …\n#&gt; $ company_name          &lt;string&gt; \"heritage community bank\", \"aurora bank fsb\", \"…\n#&gt; $ title                 &lt;string&gt; \"AVP Chief Compliance/BSA Officer\", \"AVP Compli…\n#&gt; $ date_from_parsed &lt;date32[day]&gt; 2010-02-01, 2012-07-01, 2011-11-01, 1997-07-01,…\n#&gt; $ date_to_parsed   &lt;date32[day]&gt; 2011-11-01, 2013-06-01, 2012-07-01, 2006-05-01,…\n#&gt; $ date_from_parsed_year  &lt;int32&gt; 2010, 2012, 2011, 1997, 2006, 2019, 2017, 2021,…\n#&gt; $ date_to_parsed_year    &lt;int32&gt; 2011, 2013, 2012, 2006, 2010, 2021, 2018, NA, 1…\n#&gt; Call `print()` for full schema details\n\n\nExample\n\nme_orig &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_orig/\")\nme_dist &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_dist/\")\n\nme_orig |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed) |&gt; print(n=19)\nme_dist |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed)\n\n\n\nOver 10 million (valid: must have starting date) employments at our crunchbase / pitchbook data set companies. 385.100 with a title containing the string founder.\n\n# Distinct company ids\ncb_cs_joined_cs_ids &lt;- cb_cs_joined |&gt; distinct(id_cs) |&gt; pull(id_cs)\nme_wrangled_prqt    &lt;- me_dist8_prqt |&gt; \n  \n                          # Select features\n                          select(member_id, company_id, exp_id = \"id\", date_from_parsed) |&gt; \n                          \n                          # Select observations\n                          filter(company_id %in% cb_cs_joined_cs_ids) |&gt; \n                          # - 967.080 observations (date_to not considered yet)\n                          filter(!is.na(date_from_parsed)) |&gt; \n\n                          # Add suffix to col names\n                          rename_with(~ paste(., \"cs\", sep = \"_\")) |&gt; \n                          compute()\n\nme_wrangled_prqt |&gt; \n  glimpse()\n\n#&gt; Table\n#&gt; 11,050,164 rows x 4 columns\n#&gt; $ member_id_cs              &lt;int32&gt; 9897605, 9897605, 9897605, 9897605, 9897928,…\n#&gt; $ company_id_cs             &lt;int32&gt; 1105483, 5181133, 5181133, 5181133, 5025265,…\n#&gt; $ exp_id_cs                &lt;double&gt; 1665233144, 12744849, 995032176, 1665233146,…\n#&gt; $ date_from_parsed_cs &lt;date32[day]&gt; 2018-03-01, 2010-06-01, 2014-09-01, 2011-01-…\n#&gt; Call `print()` for full schema details\n\n\n\n\nMultiple Funding Dates –&gt; Take oldest\n\nfc_wrangled_tbl &lt;- funded_companies_tbl |&gt; \n  \n  # Consider multiple founding dates: Take oldest founding date\n  unnest(founded_on) |&gt; \n  arrange(company_id, founded_on) |&gt; \n  group_by(company_id) |&gt; slice(1) |&gt; ungroup()\n\n\n\n\nExample of funding round data:\n\nfc_wrangled_tbl$funding_rounds[[1]] |&gt;  \n  glimpse()\n\n\n\n#&gt; Rows: 15\n#&gt; Columns: 14\n#&gt; $ round_id             &lt;int&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n#&gt; $ round_uuid_pb        &lt;chr&gt; NA, \"47208-70T\", NA, \"58843-18T\", NA, NA, NA, \"78…\n#&gt; $ round_uuid_cb        &lt;chr&gt; \"a6d3bfd9-5afa-47ce-86de-30a3abad6c9b\", NA, \"ea3b…\n#&gt; $ announced_on         &lt;date&gt; 2013-01-01, 2014-04-01, 2015-06-01, 2015-10-07, …\n#&gt; $ round_new            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 12, 13\n#&gt; $ round                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n#&gt; $ exit_cycle           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n#&gt; $ last                 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1…\n#&gt; $ round_type_new       &lt;fct&gt; Seed, Series A, Series B, Series C, Series D, Ser…\n#&gt; $ round_type           &lt;list&gt; \"angel\", \"angel\", \"early_vc\", \"early_vc\", \"conver…\n#&gt; $ round_types          &lt;list&gt; &lt;\"angel\", \"angel_group\", \"investor\", \"company\", \"…\n#&gt; $ raised_amount        &lt;dbl&gt; NA, 520000, NA, 1399999, NA, NA, NA, 3250000, NA,…\n#&gt; $ post_money_valuation &lt;dbl&gt; NA, NA, NA, 3399998, NA, NA, NA, 10249998, NA, N…\n#&gt; $ investors_in_round   &lt;list&gt; [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df…\n\n\n\n\n\nJoining via dplyr due to memory constraint not possible.\nJoining via Arrow due to structure constraints not possible.\n–&gt; Joining via data.table most efficient.\n\nConversion to data.tables necessary:\n\n# 1.  Funding Data\n# 1.1 Level 1\nfc_wrangled_dt |&gt; setDT()\n\n# 1.2 Funding Data Level 2 (funding_rounds)\npurrr::walk(fc_wrangled_dt$funding_rounds, setDT)\n\n# 1.3 Remove unnecessary columns + initialize dummy for before_join\npurrr::walk(fc_wrangled_dt$funding_rounds, ~ .x[, \n          `:=`(round_uuid_pb = NULL, round_uuid_cb        = NULL, round_new          = NULL, round          = NULL,\n               exit_cycle    = NULL, last                 = NULL, round_type         = NULL, round_type_new = NULL, \n               round_types   = NULL, post_money_valuation = NULL, investors_in_round = NULL, before_join    = NA)\n          ]\n        )\n\n# 2. Matching Table\ncb_cs_joined_slct_dt |&gt; setDT()\n\n# 3. Member experiences\nme_wrangled_dt &lt;- me_wrangled_prqt |&gt; collect()\n\n\n\nWorking data.table solution (efficiency increase through join by reference possible).\n\n# 1. Add company_id from funded_companies to member experiences\nme_joined_dt &lt;- cb_cs_joined_slct_dt[me_wrangled_dt, on = .(id_cs = company_id_cs), allow.cartesian = TRUE]\n#&gt; 12.978.226\n\n# 2. Add funding data from funded_companies\nme_joined_dt &lt;- fc_wrangled_dt[me_joined_dt, on = .(company_id)]\n#&gt; 12.270.572\n\n# 3. Remove duplicates (why are there any?)\nme_joined_dt &lt;- unique(me_joined_dt, by = setdiff(names(me_joined_dt), \"funding_rounds\"))\n#&gt; 12.270.572 .... No duplicates anymore. Removed from cb_cs_joined_slct_dt\n\nNot working dplyr solution\n\nme_joined_dt_dplyr &lt;- me_wrangled_dt |&gt;\n\n  # Add company_id from funded_companies\n  left_join(cb_cs_joined_slct_dt,\n            by = c(company_id_cs = \"id_cs\")) |&gt;\n\n  # Add data from funded_companies\n  left_join(funded_companies_wrangled_dt,\n            by = \"company_id\")  |&gt;\n  distinct()\n\nArrow because of nested funding data not possible."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#overview",
    "href": "revealjs/slides/2023/cs/cs_test.html#overview",
    "title": "CoreSignal Analysis",
    "section": "1. Overview",
    "text": "1. Overview\nFrom here on almost everything is in\n\n\n\nScript and File locations\n\n\nScripts 05_analyses/03_cbpbcs/01_scripts\n\n01_founding_vs_employment (Company Funding vs. Time of Employment (I. Time, II. Capital, III. Rounds))\n02_stage_affiliation (stages based on Age and funding. was discarded later on –&gt; see 04_funding_history)\n03_employment_history (Fortune500, Startup, Founding, Research Experiences)\n04_funding_history (1. prior raised amount (person), 2. further funding (company), 3. funding per round, 4. Dataset based on series B)\n05_education_history (merge with rankings, extract degrees)\n06_skills\n07_analyses/02_per_company/ (last plots)"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs_test.html#list-of-all-variables-not-up-to-date",
    "href": "revealjs/slides/2023/cs/cs_test.html#list-of-all-variables-not-up-to-date",
    "title": "CoreSignal Analysis",
    "section": "2. List of all variables (not up to date)",
    "text": "2. List of all variables (not up to date)\n\nAllGeneralEDAExp (dummy)Exp (quant)EduFund\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 67\n#&gt; $ company_id_cbpb                      &lt;int&gt; 90591, 152845, 90440, 138208, 116…\n#&gt; $ funding_after_mid                    &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"…\n#&gt; $ funding_after_early                  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\",…\n#&gt; $ member_id                            &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005…\n#&gt; $ id_tie                               &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175…\n#&gt; $ exp_id_cs                            &lt;dbl&gt; 2481733250, 1423977093, 2638, 263…\n#&gt; $ exp_corporate                        &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.000…\n#&gt; $ exp_funded_startup                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0,…\n#&gt; $ exp_founder                          &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0…\n#&gt; $ exp_f500                             &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.0000…\n#&gt; $ exp_research                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ company_id_cs                        &lt;int&gt; 140537, 10644128, 6068905, 606890…\n#&gt; $ company_name_cs                      &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"…\n#&gt; $ company_name_cbpb                    &lt;chr&gt; \"receptos\", \"HERE Technologies Ch…\n#&gt; $ founded_on_cbpb                      &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-…\n#&gt; $ closed_on_cbpb                       &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-…\n#&gt; $ title_cs                             &lt;chr&gt; \"Key Account Manager\", \"GIS Analy…\n#&gt; $ date_from_parsed_cs                  &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_to_parsed_cs                    &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-…\n#&gt; $ tjoin_tfound                         &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, …\n#&gt; $ raised_amount_before_join_company    &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333…\n#&gt; $ num_rounds_before_join               &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, …\n#&gt; $ is_f500                              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, F…\n#&gt; $ is_founder                           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research                          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research_ivy                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ date_1st_founder_exp                 &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_f500_exp                    &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010…\n#&gt; $ date_1st_funded_startup_exp          &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_1st_research_exp                &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_research_ivy_exp            &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_corporate_exp               &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, …\n#&gt; $ time_since_1st_corporate_exp         &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40,…\n#&gt; $ time_since_1st_founder_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_f500_exp              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_funded_startup_exp    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, N…\n#&gt; $ time_since_1st_research_exp          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_research_ivy_exp      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_experience            &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176…\n#&gt; $ raised_amount_before_founder_member  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ raised_amount_before_all_member      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA,…\n#&gt; $ was_corporate_before                 &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, …\n#&gt; $ was_founder_before                   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_f500_before                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_fc_before                        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_uni_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_ivy_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ stage_mid                            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ stage_late                           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, FALSE, NA…\n#&gt; $ date_from_stage                      &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\",…\n#&gt; $ company_start_mid                    &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-…\n#&gt; $ company_start_late                   &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n#&gt; $ num_rounds_cumulated_founder         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ num_rounds_cumulated_all             &lt;int&gt; NA, NA, NA, NA, NA, NA, 1, 1, NA,…\n#&gt; $ announced_on_sB                      &lt;date&gt; 2012-02-03, 2018-01-04, 2011-03-…\n#&gt; $ round_type_new_next                  &lt;fct&gt; Series C, Series C, Series C, Ser…\n#&gt; $ raised_amount_cumsum_sB              &lt;dbl&gt; 46043054, 0, 1905000, 11022796, 2…\n#&gt; $ raised_amount_cumsum_sB_next         &lt;dbl&gt; 76043054, 0, 8712306, 13854868, 4…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(id_tie, member_id, exp_id_cs, company_id_cbpb, company_name_cbpb, company_id_cs, company_name_cs, \n         founded_on_cbpb, closed_on_cbpb,\n         title_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ id_tie            &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175, 209, 243, 321, 37…\n#&gt; $ member_id         &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005, 4224, 4224, 4317,…\n#&gt; $ exp_id_cs         &lt;dbl&gt; 2481733250, 1423977093, 2638, 2638, 1736317868, 3084…\n#&gt; $ company_id_cbpb   &lt;int&gt; 90591, 152845, 90440, 138208, 116099, 97810, 40123, …\n#&gt; $ company_name_cbpb &lt;chr&gt; \"receptos\", \"HERE Technologies Chicago\", \"crowdtwist…\n#&gt; $ company_id_cs     &lt;int&gt; 140537, 10644128, 6068905, 6068905, 11825305, 194148…\n#&gt; $ company_name_cs   &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"Oracle\", \"Oracle\", …\n#&gt; $ founded_on_cbpb   &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-01, 2006-01-01, 201…\n#&gt; $ closed_on_cbpb    &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-09, NA, NA, NA, NA,…\n#&gt; $ title_cs          &lt;chr&gt; \"Key Account Manager\", \"GIS Analyst I\", \"QA\", \"QA\", …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_parsed_cs, date_to_parsed_cs, \n         tjoin_tfound, raised_amount_before_join_company, num_rounds_before_join) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 5\n#&gt; $ date_from_parsed_cs               &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_to_parsed_cs                 &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-01,…\n#&gt; $ tjoin_tfound                      &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, 44,…\n#&gt; $ raised_amount_before_join_company &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333333…\n#&gt; $ num_rounds_before_join            &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, 2, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(starts_with(\"is_\"),\n         starts_with(\"was_\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ is_f500              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FAL…\n#&gt; $ is_founder           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research_ivy      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_corporate_before &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRU…\n#&gt; $ was_founder_before   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_f500_before      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_fc_before        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n#&gt; $ was_uni_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_ivy_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(\n    starts_with(\"date_1st_\"),\n    starts_with(\"time_since_1st_\"),\n    starts_with(\"exp_\"), -exp_id_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 19\n#&gt; $ date_1st_founder_exp              &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_f500_exp                 &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010-01…\n#&gt; $ date_1st_funded_startup_exp       &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_1st_research_exp             &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_research_ivy_exp         &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_corporate_exp            &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, 200…\n#&gt; $ time_since_1st_corporate_exp      &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40, 17…\n#&gt; $ time_since_1st_founder_exp        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_f500_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_funded_startup_exp &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, NA, …\n#&gt; $ time_since_1st_research_exp       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_research_ivy_exp   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_experience         &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176, 4…\n#&gt; $ exp_corporate                     &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.00000,…\n#&gt; $ exp_funded_startup                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0,…\n#&gt; $ exp_founder                       &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.00…\n#&gt; $ exp_f500                          &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.00000, …\n#&gt; $ exp_research                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(score_global_2023_best,\n         starts_with(\"rank\"),\n         starts_with(\"degree\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 8\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_stage, company_start_mid, company_start_late,\n         raised_amount_before_founder_member, raised_amount_before_all_member,\n         funding_after_mid, funding_after_early) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 7\n#&gt; $ date_from_stage                     &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\", …\n#&gt; $ company_start_mid                   &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-0…\n#&gt; $ company_start_late                  &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-0…\n#&gt; $ raised_amount_before_founder_member &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n#&gt; $ raised_amount_before_all_member     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA, …\n#&gt; $ funding_after_mid                   &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"y…\n#&gt; $ funding_after_early                 &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", …"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#quarto",
    "href": "revealjs/slides/2022/first/first.html#quarto",
    "title": "First",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#bullets",
    "href": "revealjs/slides/2022/first/first.html#bullets",
    "title": "First",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#code",
    "href": "revealjs/slides/2022/first/first.html#code",
    "title": "First",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#why",
    "href": "revealjs/slides/2022/first/first.html#why",
    "title": "First",
    "section": "Why",
    "text": "Why\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#same",
    "href": "revealjs/slides/2022/first/first.html#same",
    "title": "First",
    "section": "Same",
    "text": "Same\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "revealjs/slides/2022/first/first.html#different",
    "href": "revealjs/slides/2022/first/first.html#different",
    "title": "First",
    "section": "Different",
    "text": "Different\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n#&gt; [1] 2"
  },
  {
    "objectID": "resumes/index.html",
    "href": "resumes/index.html",
    "title": "Resumes overview",
    "section": "",
    "text": "Classic\n\n\nClassical themed CV\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify\n\n\nSpotify themed CV\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resumes.html",
    "href": "resumes.html",
    "title": "Resumes",
    "section": "",
    "text": "Classic\n\n\nClassical themed CV\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify\n\n\nSpotify themed CV\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joschka Schwarz",
    "section": "",
    "text": "Hi There! Thanks for stopping by my little corner of the internet! My name is Joschka Schwarz and I am excited to meet you. Below you will find my current CV.\n\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        \n          \n          LinkedIn\n        \n        \n          \n          Github\n        \n        \n          \n          Email\n        \n        \n          \n          Download CV\n        \n      \n      \n        \n          5 years’ experience working with data. Expertise in quantitative modeling, growing people and making data-driven decisions. Currently working as a Research Associate (Ph.D. Candidate) at the intersection of computer science, statistics and the social sciences as member of both the Entrepreneurship & Data Science groups.\n        \n      \n    \n  \n\n\n  \n\n\n  \n    \n    \n      \n        \n          \n            \n              \n            \n            \n              Industry Experience\n            \n          \n        \n        \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Program Manager | Research Associate\n                      \n                        Hamburg University of Technology\n                      \n                      \n                        Jun 2018 - Present\n                        \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Conducting collaborative research, teaching students, managing industry projects while pursuing independent research and further academic and data science related qualifications.\n                    \n                      Lead of B2B/B2C industry innovation projects (focus: digital transformation, big data analytics)\n                      Designed and conducted multiple lecture units and exams in entrepreneurship and data science\n                      Developed and implemented a Digital Teaching and Learning Ecosystem to teach classes remotely\n                      Supervised student theses, projects and internships\n                      Presented at international conferences and about to publish in related journals\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Master’s Degree Candidate\n                      \n                        Lufthansa Technik AG\n                      \n                      \n                        Apr 2017 - Mar 2018\n                        \n                        Miama, USA & Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Created a custom AI-based solution for reducing cost & response time (in collaboration with IBM Watson).\n                    \n                      Communicated with stakeholders to define project parameters and keep them up to date on project status\n                      Optimized use of resources to complete projects ahead of deadline and under budget\n                      Designated resources, assigned tasks and monitored performance for each part of the project\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Management Consulting, intern\n                      \n                        Horváth\n                      \n                      Oct 2016 – Feb 2017 \n                        Frankfurt on the Main, Germany\n                      \n                    \n                  \n                  \n                  Worked as a PMO manager on a project for the development of new location concepts of a large European railroad company, replacing the entire existing infrastructure of this major corporate.\n                    \n                      Operational execution of PMO activities (reporting, budget tracking, preparation and follow-up of meetings)\n                      Assistance in establishing the project structure\n                      Preparation of profitability calculations and capacity / utilization analyses for the individual sites\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Student assistant\n                      \n                        Fraunhofer-Institut für Lasertechnik ILT\n                      \n                      Oct 2014 – Sep 2016 \n                        Aachen, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Process Optimization, intern\n                      \n                        Audi AG\n                      \n                      Sep 2013 – Dec 2013 \n                        Neckarsulm, Germany\n                      \n                    \n                  \n                  \n                    As coordinator of the Lean Manufacturing Team, I was responsible to help people to understand the Lean Management philosophy and the operational excellence approach of AUDI (Audi Production System, APS).\n                    \n                      Training and coaching of lean methods with focus on problem solving, logistics and process synchronisation\n                      Preparation, implementation and moderation of shopfloor management projects in direct and indirect areas\n                      Optimizing various processes via process mapping and value stream analysis\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Student assistant\n                      \n                        Fraunhofer-Institut für Produktionstechnologie IPT\n                      \n                      Nov 2012 – Jul 2013 \n                        Aachen, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n  \n  \n    \n    \n      \n        \n          \n            \n              \n            \n            \n              Teaching Experience\n            \n          \n        \n        \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Data Science\n                      \n                        Hamburg University of Technology\n                      \n                      Jan 2023 – Mar 2023 \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Designer & Instructor of multiple business oriented Data Science and Machine Learning courses:\n                    \n                      Business Data Science Basics\n                      Business Decisions with Machine Learning\n                      Building Business Data Products\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Data Science certificate program\n                      \n                        Northern Institute of Technology Management\n                      \n                      Jun 2020\n                        Hamburg, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Pytorch Geometric\n                      \n                        MLE-Days\n                      \n                      Jun 2021 \n                        Hamburg, Germany\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Entrepreneurship & Innovation\n                      \n                        Hamburg University of Technology\n                      \n                      Sep 2018 – Present \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Teaching students skills needed to build a technology startup from the ground up in varying classes:\n                    \n                      Startup Engineering\n                      Technology Entrepreunrship\n                      Sustainable Entrepreunrship\n                      Creation of Business Opportunities\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n  \n  \n    \n    \n      \n        \n           Education\n        \n        \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Visiting Research Scholar\n                      \n                        RMIT Melbourne\n                      \n                      Jan 2023 – Mar 2023 \n                        Melbourne, Australia\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Ph.D. Thesis in Innovation Management & Entrepreneurship Science\n                      \n                        Hamburg University of Technology\n                      \n                      Jun 2018 – Present \n                        Hamburg, Germany\n                      \n                    \n                  \n                  \n                    Quantitative dissertation (using SQL, R, Python) on the influence of social, structural and reputational effects on entrepreuneurial success.\n                    \n                      Creation of various databases by scraping different data providers and collaborative platforms\n                      Natural language processing on linkedin and startup databases to extract features for network analysis\n                      Machine Learning and statistical analysis on large (&gt; 5 billion records) github and stackoverflow datasets\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Visiting Research Scholar\n                      \n                        Maastricht University\n                      \n                      Feb 2016 – Jun 2016 \n                        Maastricht, Netherlands\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      M.Sc. in Business Administration & Engineering\n                      \n                        RWTH Aachen\n                      \n                      Apr 2015 – Mar 2018 \n                        Aachen, Germany\n                      \n                    \n                  \n                  \n                    \n                      Grade: 1.6 (Selected to the Dean’s List)\n                      Finished within the designated period of study\n                      Engineering specialization: Iron Metallurgy\n                      Economics specialization: Supply Chain Management & Controlling\n                      Final Thesis: The impact of an artificial intelligence-based e-mail parsing system on the aircraft spare parts quotation process - a Lufthansa Technik AG case\n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      Visiting Research Scholar\n                      \n                        Polytechnic University of Valencia\n                      \n                      Feb 2014 – Jun 2014 \n                        Valencia, Spain\n                      \n                    \n                  \n                \n              \n            \n          \n          \n            \n              \n                 \n                 \n              \n              \n                 \n              \n              \n                 \n                 \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        \n                      \n                    \n                    \n                      B.Sc. in Business Administration & Engineering\n                      \n                        RWTH Aachen\n                      \n                      Oct 2011 – Mar 2015 \n                        Aachen, Germany\n                      \n                    \n                  \n                  Final Thesis: Low distortion softening of flanges on b-pillars made of press-hardened steel through local heat treatment with laser radiation (in cooperation with Fraunhofer ILT)\n                \n              \n            \n          \n        \n      \n    \n  \n  \n    \n    \n      \n        \n          Accomplish­ments\n        \n        \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Statistician with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n              \n                Course List. Click to expand!\n                  \n                    \n                      1. Introduction to Statistics in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      2. Foundations of Probability in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      3. Introduction to Regression in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      4. Intermediate Regression in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      5. Generalized Linear Models in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      6. Modeling with Data in the Tidyverse\n                    \n                    See certificate\n                  \n                  \n                    \n                      7. Sampling in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      8. Hypothesis Testing in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      9. Experimental Design in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      10. Introduction to A/B Testing in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      11. Dealing With Missing Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      12. Handling Missing Data with Imputations in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      13. Analyzing Survey Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      14. Survey and Measurement Development in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      15. Hierarchical and Mixed Effects Models in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      16. Survival Analysis in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      17. Mixture Models in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      18. Fundamentals of Bayesian Data Analysis in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      19. Bayesian Regression Modeling with rstanarm\n                    \n                    See certificate\n                  \n                  \n                    \n                      20. Bayesian Modeling with RJAGS\n                    \n                    See certificate\n                  \n                  \n                    \n                      21. Factor Analysis in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      22. Structural Equation Modeling with lavaan in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      23. Foundations of Inference in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      24. Inference for Categorical Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      25. Inference for Numerical Data in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      26. Inference for Linear Regression in R\n                    \n                    See certificate\n                  \n                  \n                    \n                      27. Data Privacy and Anonymization in R\n                    \n                    See certificate\n                  \n              \n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Machine Learning Scientist with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n              \n                Course List. Click to expand!\n                  \n                    Introduction to Statistics in R\n                    See certificate\n                  \n                  \n                    table_row_1\n                    See certificate\n                  \n                  \n                    table_row_1\n                    See certificate\n                  \n              \n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Data Scientist with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  Data Analyst with R\n                  \n                    DataCamp\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              See certificate\n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  \n                    DS4B 101-R: Business Analysis With R\n                  \n                  \n                    business-science\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              A 7-week curriculum that methodically teaches the foundations of data science using R & tidyverse. \n                  \n                    Data Import: readr & odbc\n                  \n                  \n                    Data Cleaning & Wrangling: dplyr & tidyr\n                  \n                  \n                    Time Series, Text, & Categorical Data: lubridate, stringr, & forcats\n                  \n                  \n                    Visualization: ggplot2\n                  \n                  \n                    Functions & Iteration: purrr\n                  \n                  \n                    Modeling & Machine Learning: parnsip (xgboost, glmnet, kernlab, broom, & more)\n                  \n                  \n                    Business Reporting: rmarkdown\n                  \n                \n              \n              See certificate\n            \n          \n          \n            \n              \n                \n                  \n                    \n                  \n                \n                \n                  \n                    DS4B 201-R: Data Science For Business With R\n                  \n                  \n                    business-science\n                     Jul 2020 – Dec 2020\n                  \n                \n              \n              A 10-week curriculum that incorporates R & H2O AutoML to use machine learning within a business problem solving framework called the BSPF\n              See certificate"
  },
  {
    "objectID": "resumes/classic/index.html",
    "href": "resumes/classic/index.html",
    "title": "Classic",
    "section": "",
    "text": "Put spotify cv here"
  },
  {
    "objectID": "resumes/spotify/index.html",
    "href": "resumes/spotify/index.html",
    "title": "Spotify",
    "section": "",
    "text": "Put spotify cv here"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#table-of-contents",
    "href": "revealjs/slides/2023/cs/cs.html#table-of-contents",
    "title": "CoreSignal Analysis",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#load-init-companies-crunchbase-pitchbook-coresignal",
    "href": "revealjs/slides/2023/cs/cs.html#load-init-companies-crunchbase-pitchbook-coresignal",
    "title": "CoreSignal Analysis",
    "section": "Load & Init Companies (CrunchBase, PitchBook & CoreSignal)",
    "text": "Load & Init Companies (CrunchBase, PitchBook & CoreSignal)\n\nCBPB: SELECTCBPB: WRANGLECBPB: CLEANCS: SELECTCS: WRANGLECS: CLEAN\n\n\nCrunchbase data contains 150,838 startups with a valid funding trajectory.\n\np_load(arrow, dplyr, tidyr)\n\nfunded_companies_prqt &lt;- open_dataset(\"funded_companies_identifiers.parquet\") \nfunded_companies_prqt\n\n\n\n#&gt; # A tibble: 150,838 × 3\n#&gt;   company_id            domain linkedin_url                                     \n#&gt;        &lt;int&gt; &lt;list&lt;character&gt;&gt; &lt;chr&gt;                                            \n#&gt; 1          1               [1] &lt;NA&gt;                                             \n#&gt; 2          2               [1] https://www.linkedin.com/company/luna-pharmaceut…\n#&gt; 3          3               [1] http://www.linkedin.com/company/chainsync        \n#&gt; # ℹ 150,835 more rows\n\n\n\nMultiple domains (Unnesting via Arrow not possible. Options: Spark & sparklyr.nested):\n\nfc_unnested_tbl &lt;- funded_companies_prqt |&gt; collect() |&gt; \n                      # 1. Allow multiple domains per company. No multiple linkedin handles.\n                      unnest(domain) \nfc_unnested_tbl\n\n#&gt; # A tibble: 155,413 × 3\n#&gt;   company_id domain              linkedin_url                                   \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                                          \n#&gt; 1          1 zana.io             &lt;NA&gt;                                           \n#&gt; 2          2 premamawellness.com https://www.linkedin.com/company/luna-pharmace…\n#&gt; 3          3 chainsync.com       http://www.linkedin.com/company/chainsync      \n#&gt; # ℹ 155,410 more rows\n\n\n\n\n\n\nMust have identifier (domain, linkedin)\nClean identifiers\nRemove duplicates\n\n\nlibrary(stringr)\nfc_unnested_tbl |&gt; \n  \n  # 1. At least 1 identifier: 4.518 observations are filtered out\n  filter(if_any(c(domain, linkedin_url), ~!is.na(.))) |&gt;\n  \n  # 2. Extract linkedin handle & clean domains\n  mutate(linkedin_handle = linkedin_url |&gt; str_extract(\"(?&lt;=linkedin\\\\.com/company/).*?(?=(?:\\\\?|$|/))\")) |&gt;\n  mutate(domain          = domain |&gt; clean_domain()) |&gt;\n\n  # 3. Remove 532 duplicates\n  distinct()\n\n\n\n#&gt; # A tibble: 150,363 × 3\n#&gt;   company_id domain              linkedin_handle          \n#&gt;        &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;                    \n#&gt; 1          1 zana.io             &lt;NA&gt;                     \n#&gt; 2          2 premamawellness.com luna-pharmaceuticals-inc-\n#&gt; 3          3 chainsync.com       chainsync                \n#&gt; # ℹ 150,360 more rows\n\n\n–&gt; 145.991 distinct examineable companies.\n\n\nIssue: Some extracted domains are not unique and associated with multiple companies. Manual Cleaning: Domains with a count exceeding two were analyzed and set to NA if they do not correspond to the actual one.\n\n# ANALYZE\n# fc_wrangled_tbl |&gt; \n#   distinct(company_id, domain) |&gt; \n#   count(domain, sort = T) |&gt; \n#   filter(n&gt;2)`\n\nunwanted_domains_cb &lt;- c(\"webflow.io\", \"angel.co\", \"weebly.com\", \"wordpress.com\", \"wixsite.com\", \"squarespace.com\", \n                         \"webflow.io\", \"crypt2esports.com\", \"myshopify.com\", \"business.site\", \"mystrikingly.com\", \n                         \"launchrock.com\", \"square.site\", \"google.com\", \"sites.google.com\", \"t.co\", \"linktr.ee\",\n                         \"netlify.app\", \"itunes.apple.com\", \"apple.com\", \"crunchb.com\", \"tumblr.com\", \"linkedin.com\",\n                         \"godaddysites.com\", \"mit.edu\", \"paloaltonetworks.com\", \" wpengine.com\", \"facebook.com\",\n                         \"intuit.com\", \"medium.com\", \"salesforce.com\", \"strikingly.com\", \"wix.com\", \"cisco.com\",\n                         \"digi.me\", \"apps.apple.com\", \"bit.ly\", \"fleek.co\", \"harvard.edu\", \"ibm.com\", \"jimdo.com\",\n                         \"myftpupload.com\", \"odoo.com\", \"storenvy.com\", \"twitter.com\", \"umd.edu\", \"umich.edu\", \"vmware.com\", \"webs.com\")\n\n# Not all observations with unwanted domains are bad per se:\nwanted_ids_cb &lt;- c(angel = 128006, `catapult-centres-uk` = 115854, digime1 = 140904, digimi2 = 95430, fleek = 50738, \n                   jimdo = 108655, medium = 113415, storenvy = 85742, strikingly = 95831, substack = 34304, \n                   tumblr = 84838, twitter = 53139, weebly = 91365, wpengine = 91720)\n\n# Set misleading domains to NA\nfunded_companies_clnd &lt;- fc_wrangled_tbl |&gt; \n                              \n  mutate(domain = if_else(\n    domain %in% unwanted_domains_cb & !(company_id %in% wanted_ids_cb), \n    NA_character_, domain))\n\n\n\nIt appears that CoreSignal has been able to locate 45.026 companies within our gathered data.\n\n# Selection & Wrangle has been done already\ncs_companies_base_slct &lt;- readRDS(\"cs_companies_base_slct.rds\") \ncs_companies_base_slct\n\n\n\n#&gt; # A tibble: 45,362 × 4\n#&gt;      id name                                domain               linkedin_handle\n#&gt;   &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;                &lt;chr&gt;          \n#&gt; 1   305 Blueprint, a David's Bridal Company blueprintregistry.c… blueprint-regi…\n#&gt; 2   793 BookingLive                         bookinglive.com      bookinglive    \n#&gt; 3  2425 Brandvee                            momentum.ai          brandvee       \n#&gt; # ℹ 45,359 more rows\n\n\n\n\ncs_companies_base_slct$id |&gt; n_distinct()\n\n#&gt; [1] 45026\n\n\n\n\n\ncs_companies_base_slct |&gt; janitor::get_dupes(id)\n\n#&gt; # A tibble: 672 × 5\n#&gt;        id dupe_count name          domain           linkedin_handle\n#&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;          \n#&gt; 1  596494          2 Vi            vi.co            vitrainer      \n#&gt; 2  596494          2 Vi            vi.co            vi             \n#&gt; 3 1324413          2 Patch Lending patchlending.com patch-of-land  \n#&gt; # ℹ 669 more rows\n\n\n\n\n\nNothing to wrangle …\n\ncs_companies_base_wrangled &lt;- cs_companies_base_slct |&gt; select(-name) |&gt; \n  \n                                        # Add suffixes to col names\n                                        rename_with(~ paste(., \"cs\", sep = \"_\"))\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMore cleaning necessary (same as CBPB)! The task was undertaken with a limited degree of enthusiasm.\n\n\n\n\nunwanted_domains_cs    &lt;- c(\"bit.ly\", \"linktr.ee\", \"facebook.com\", \"linkedin.com\", \"twitter.com\", \"crunchbase.com\")\nwanted_ids_cs          &lt;- c(crunchbase = 1634413, linkedin = 8568581, twitter = 24745469)\n\ncs_companies_base_clnd &lt;- cs_companies_base_wrangled |&gt; \n  \n  mutate(domain_cs = if_else(\n    domain_cs %in% unwanted_domains_cs & !(id_cs %in% wanted_ids_cs), \n    NA_character_, \n    domain_cs)\n    )"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#join-companies-member-experiences-and-funding-information",
    "href": "revealjs/slides/2023/cs/cs.html#join-companies-member-experiences-and-funding-information",
    "title": "CoreSignal Analysis",
    "section": "Join companies, member experiences and funding information",
    "text": "Join companies, member experiences and funding information\n\nCompaniesJobs (all)Jobs (dist)Jobs (focal)FundingConversionJoin\n\n\nWe were able to match 37.287 CS & CB/PB companies.\n\ncb_cs_joined &lt;- funded_companies_clnd |&gt; \n\n    # Leftjoins\n    left_join(cs_companies_base_clnd |&gt; select(id_cs, domain_cs),          by = c(domain          = \"domain_cs\"),          na_matches = \"never\") |&gt; \n    left_join(cs_companies_base_clnd |&gt; select(id_cs, linkedin_handle_cs), by = c(linkedin_handle = \"linkedin_handle_cs\"), na_matches = \"never\") |&gt; \n\n    # Remove obs with no cs_id\n    filter(!is.na(id_cs)) |&gt;\n    \n    # Remove matches, that matched different domains, but same company (e.g. company_id: 83060, id_cs: 4507928) block.xyz & squareup.com\n    select(company_id, id_cs) |&gt; \n    distinct()\n    \ncb_cs_joined\n\n\n\n#&gt; # A tibble: 38,118 × 2\n#&gt;   company_id    id_cs\n#&gt;        &lt;int&gt;    &lt;int&gt;\n#&gt; 1          2  8345218\n#&gt; 2          5 28149599\n#&gt; 3          8  4469271\n#&gt; 4         11  5349023\n#&gt; 5         12  9364263\n#&gt; # ℹ 38,113 more rows\n\n\n\n\ncb_cs_joined |&gt; distinct(company_id) |&gt; nrow()\n\n#&gt; [1] 37287\n\n\n\n\n\nWe got over 460 million employment observations from CoreSignal.\n\n# Other data versions\n# 1. Complete: \nmember_experience_dt \n#&gt; {462.711.794}  \n\n# 2. Distinct1: \nmember_experience_dist_dt &lt;- unique(member_experience_dt) \n#&gt; {432.368.479}\n\n# 3. Distinct2: \nunique(member_experience_dist_dt[order(id)], by = setdiff(names(member_experience_dist_dt), \"id\")) \n#&gt; {431.899.547}\n\n\n\nBut only ~50 Mil distinct employments\n\n# Load distinct member experiences\nme_dist8_prqt &lt;- arrow::open_dataset(\"cs_me_dist8_unest_empl_hist.parquet\") \nme_dist8_prqt |&gt; glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 51,621,196 rows x 10 columns\n#&gt; $ id_tie                 &lt;int32&gt; 16615559, 16615560, 16615561, 16615562, 1661556…\n#&gt; $ id                    &lt;double&gt; 2244288231, 254049663, 948937291, 254049667, 25…\n#&gt; $ member_id              &lt;int32&gt; 179313066, 179313066, 179313066, 179313066, 179…\n#&gt; $ company_id             &lt;int32&gt; 865089, 9098713, 9098713, NA, 865089, 9020540, …\n#&gt; $ company_name          &lt;string&gt; \"heritage community bank\", \"aurora bank fsb\", \"…\n#&gt; $ title                 &lt;string&gt; \"AVP Chief Compliance/BSA Officer\", \"AVP Compli…\n#&gt; $ date_from_parsed &lt;date32[day]&gt; 2010-02-01, 2012-07-01, 2011-11-01, 1997-07-01,…\n#&gt; $ date_to_parsed   &lt;date32[day]&gt; 2011-11-01, 2013-06-01, 2012-07-01, 2006-05-01,…\n#&gt; $ date_from_parsed_year  &lt;int32&gt; 2010, 2012, 2011, 1997, 2006, 2019, 2017, 2021,…\n#&gt; $ date_to_parsed_year    &lt;int32&gt; 2011, 2013, 2012, 2006, 2010, 2021, 2018, NA, 1…\n#&gt; Call `print()` for full schema details\n\n\nExample\n\nme_orig &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_orig/\")\nme_dist &lt;- open_dataset(\"~/02_diss/01_coresignal/02_data/member_experience/me_dist/\")\n\nme_orig |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed) |&gt; print(n=19)\nme_dist |&gt; filter(member_id == 4257, company_id == 9007053) |&gt; collect() |&gt; as_tibble() |&gt; arrange(date_from_parsed)\n\n\n\nOver 10 million (valid: must have starting date) employments at our crunchbase / pitchbook data set companies. 385.100 with a title containing the string founder.\n\n# Distinct company ids\ncb_cs_joined_cs_ids &lt;- cb_cs_joined |&gt; distinct(id_cs) |&gt; pull(id_cs)\nme_wrangled_prqt    &lt;- me_dist8_prqt |&gt; \n  \n                          # Select features\n                          select(member_id, company_id, exp_id = \"id\", date_from_parsed) |&gt; \n                          \n                          # Select observations\n                          filter(company_id %in% cb_cs_joined_cs_ids) |&gt; \n                          # - 967.080 observations (date_to not considered yet)\n                          filter(!is.na(date_from_parsed)) |&gt; \n\n                          # Add suffix to col names\n                          rename_with(~ paste(., \"cs\", sep = \"_\")) |&gt; \n                          compute()\n\nme_wrangled_prqt |&gt; \n  glimpse()\n\n#&gt; Table\n#&gt; 11,050,164 rows x 4 columns\n#&gt; $ member_id_cs              &lt;int32&gt; 9436436, 9436453, 9436478, 9436478, 9436513,…\n#&gt; $ company_id_cs             &lt;int32&gt; 573738, 3073966, 4577566, 4577566, 4577566, …\n#&gt; $ exp_id_cs                &lt;double&gt; 1891262301, 923902432, 1399967039, 525842890…\n#&gt; $ date_from_parsed_cs &lt;date32[day]&gt; 2018-04-01, 2015-04-01, 2006-01-01, 2004-03-…\n#&gt; Call `print()` for full schema details\n\n\n\n\nMultiple Funding Dates –&gt; Take oldest\n\nfc_wrangled_tbl &lt;- funded_companies_tbl |&gt; \n  \n  # Consider multiple founding dates: Take oldest founding date\n  unnest(founded_on) |&gt; \n  arrange(company_id, founded_on) |&gt; \n  group_by(company_id) |&gt; slice(1) |&gt; ungroup()\n\n\n\n\nExample of funding round data:\n\nfc_wrangled_tbl$funding_rounds[[1]] |&gt;  \n  glimpse()\n\n\n\n#&gt; Rows: 15\n#&gt; Columns: 14\n#&gt; $ round_id             &lt;int&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n#&gt; $ round_uuid_pb        &lt;chr&gt; NA, \"47208-70T\", NA, \"58843-18T\", NA, NA, NA, \"78…\n#&gt; $ round_uuid_cb        &lt;chr&gt; \"a6d3bfd9-5afa-47ce-86de-30a3abad6c9b\", NA, \"ea3b…\n#&gt; $ announced_on         &lt;date&gt; 2013-01-01, 2014-04-01, 2015-06-01, 2015-10-07, …\n#&gt; $ round_new            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 12, 13\n#&gt; $ round                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n#&gt; $ exit_cycle           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n#&gt; $ last                 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1…\n#&gt; $ round_type_new       &lt;fct&gt; Seed, Series A, Series B, Series C, Series D, Ser…\n#&gt; $ round_type           &lt;list&gt; \"angel\", \"angel\", \"early_vc\", \"early_vc\", \"conver…\n#&gt; $ round_types          &lt;list&gt; &lt;\"angel\", \"angel_group\", \"investor\", \"company\", \"…\n#&gt; $ raised_amount        &lt;dbl&gt; NA, 520000, NA, 1399999, NA, NA, NA, 3250000, NA,…\n#&gt; $ post_money_valuation &lt;dbl&gt; NA, NA, NA, 3399998, NA, NA, NA, 10249998, NA, N…\n#&gt; $ investors_in_round   &lt;list&gt; [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df[1 x 11]&gt;], [&lt;tbl_df…\n\n\n\n\n\nJoining via dplyr due to memory constraint not possible.\nJoining via Arrow due to structure constraints not possible.\n–&gt; Joining via data.table most efficient.\n\nConversion to data.tables necessary:\n\n# 1.  Funding Data\n# 1.1 Level 1\nfc_wrangled_dt |&gt; setDT()\n\n# 1.2 Funding Data Level 2 (funding_rounds)\npurrr::walk(fc_wrangled_dt$funding_rounds, setDT)\n\n# 1.3 Remove unnecessary columns + initialize dummy for before_join\npurrr::walk(fc_wrangled_dt$funding_rounds, ~ .x[, \n          `:=`(round_uuid_pb = NULL, round_uuid_cb        = NULL, round_new          = NULL, round          = NULL,\n               exit_cycle    = NULL, last                 = NULL, round_type         = NULL, round_type_new = NULL, \n               round_types   = NULL, post_money_valuation = NULL, investors_in_round = NULL, before_join    = NA)\n          ]\n        )\n\n# 2. Matching Table\ncb_cs_joined_slct_dt |&gt; setDT()\n\n# 3. Member experiences\nme_wrangled_dt &lt;- me_wrangled_prqt |&gt; collect()\n\n\n\nWorking data.table solution (efficiency increase through join by reference possible).\n\n# 1. Add company_id from funded_companies to member experiences\nme_joined_dt &lt;- cb_cs_joined_slct_dt[me_wrangled_dt, on = .(id_cs = company_id_cs), allow.cartesian = TRUE]\n#&gt; 12.978.226\n\n# 2. Add funding data from funded_companies\nme_joined_dt &lt;- fc_wrangled_dt[me_joined_dt, on = .(company_id)]\n#&gt; 12.270.572\n\n# 3. Remove duplicates (why are there any?)\nme_joined_dt &lt;- unique(me_joined_dt, by = setdiff(names(me_joined_dt), \"funding_rounds\"))\n#&gt; 12.270.572 .... No duplicates anymore. Removed from cb_cs_joined_slct_dt\n\nNot working dplyr solution\n\nme_joined_dt_dplyr &lt;- me_wrangled_dt |&gt;\n\n  # Add company_id from funded_companies\n  left_join(cb_cs_joined_slct_dt,\n            by = c(company_id_cs = \"id_cs\")) |&gt;\n\n  # Add data from funded_companies\n  left_join(funded_companies_wrangled_dt,\n            by = \"company_id\")  |&gt;\n  distinct()\n\nArrow because of nested funding data not possible."
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#feature-engineering",
    "href": "revealjs/slides/2023/cs/cs.html#feature-engineering",
    "title": "CoreSignal Analysis",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nF1: Tjoin - TfoundPrep1F2, F3Prep2Titles\n\n\nHow many month have passed since the company was founded and before the person joined the company (in months)?\n\nlibrary(lubridate)\nme_joined_dt[, tjoin_tfound := (interval(founded_on, date_from_parsed_cs) %/% months(1))]\n\n\n\nUnnesting necessary due to memory constraints (takes multiple hours … to be measured).\n\n# Working: data.table\nme_joined_unnested_dt &lt;- me_joined_dt[,rbindlist(funding_rounds), by = setdiff(names(me_joined_dt), \"funding_rounds\")]\n# Not working: dplyr\nme_joined_unnested_tbl &lt;- me_joined_dt |&gt; unnest(funding_rounds)\n\nAdd feature whether or not member joined before Announcement of a funding round:\n\n# Add feature whether or not member joined before Announcement of a funding round\nme_joined_unnested_dt[,before_join := date_from_parsed_cs &gt;= announced_on]\n\n# Inspect\nopen_dataset(\"me_joined_unnested1.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 88,429,236 rows x 15 columns\n#&gt; $ company_id_cbpb           &lt;int32&gt; 85514, 85514, 85514, 85514, 85514, 85514, 85…\n#&gt; $ founded_on_cbpb     &lt;date32[day]&gt; 2007-01-01, 2007-01-01, 2007-01-01, 2007-01-…\n#&gt; $ company_id_cs             &lt;int32&gt; 10830353, 10830353, 10830353, 10830353, 1083…\n#&gt; $ id_tie                    &lt;int32&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 29…\n#&gt; $ exp_id_cs                &lt;double&gt; 606461989, 606461989, 606461989, 606461989, …\n#&gt; $ member_id_cs              &lt;int32&gt; 162, 162, 162, 162, 162, 162, 162, 162, 162,…\n#&gt; $ company_name_cs          &lt;string&gt; \"Kony, Inc.\", \"Kony, Inc.\", \"Kony, Inc.\", \"K…\n#&gt; $ title_cs           &lt;large_string&gt; \"Associate Engineer\", \"Associate Engineer\", …\n#&gt; $ date_from_parsed_cs &lt;date32[day]&gt; 2015-06-01, 2015-06-01, 2015-06-01, 2015-06-…\n#&gt; $ date_to_parsed_cs   &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ tjoin_tfound             &lt;double&gt; 101, 101, 101, 101, 101, 101, 101, 101, 101,…\n#&gt; $ round_id                  &lt;int32&gt; 259195, 259196, 259197, 259198, 259199, 2592…\n#&gt; $ announced_on        &lt;date32[day]&gt; 2011-02-24, 2011-03-01, 2011-06-07, 2012-01-…\n#&gt; $ raised_amount            &lt;double&gt; 13370002, 8280000, 2352002, 2000000, 1500000…\n#&gt; $ before_join                &lt;bool&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n#&gt; Call `print()` for full schema details\n\n\n\n\nF2. How much capital has been acquired by the time the person joins? F3. How many funding rounds have been acquired by the time the person joins?\n\n# Initialize empty columns (not sure yet if that increases performance)\nme_joined_unnested_dt[, `:=` (raised_amount_before_join = NA_real_, \n                              num_rounds_before_join    = NA_real_)]\n\n# Add features\nme_joined_unnested_dt[, `:=` (raised_amount_before_join = sum(raised_amount[before_join == T], na.rm = T),\n                              num_rounds_before_join    = sum(  before_join[before_join == T])), \n                      by = .(company_id, exp_id_cs)]\n\nopen_dataset(\"me_joined_unnested2.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 88,429,236 rows x 17 columns\n#&gt; $ company_id_cbpb            &lt;int32&gt; 85514, 85514, 85514, 85514, 85514, 85514, 8…\n#&gt; $ founded_on_cbpb      &lt;date32[day]&gt; 2007-01-01, 2007-01-01, 2007-01-01, 2007-01…\n#&gt; $ company_id_cs              &lt;int32&gt; 10830353, 10830353, 10830353, 10830353, 108…\n#&gt; $ id_tie                     &lt;int32&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n#&gt; $ exp_id_cs                 &lt;double&gt; 606461989, 606461989, 606461989, 606461989,…\n#&gt; $ member_id_cs               &lt;int32&gt; 162, 162, 162, 162, 162, 162, 162, 162, 162…\n#&gt; $ company_name_cs           &lt;string&gt; \"Kony, Inc.\", \"Kony, Inc.\", \"Kony, Inc.\", \"…\n#&gt; $ title_cs            &lt;large_string&gt; \"Associate Engineer\", \"Associate Engineer\",…\n#&gt; $ date_from_parsed_cs  &lt;date32[day]&gt; 2015-06-01, 2015-06-01, 2015-06-01, 2015-06…\n#&gt; $ date_to_parsed_cs    &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ tjoin_tfound              &lt;double&gt; 101, 101, 101, 101, 101, 101, 101, 101, 101…\n#&gt; $ round_id                   &lt;int32&gt; 259195, 259196, 259197, 259198, 259199, 259…\n#&gt; $ announced_on         &lt;date32[day]&gt; 2011-02-24, 2011-03-01, 2011-06-07, 2012-01…\n#&gt; $ raised_amount             &lt;double&gt; 13370002, 8280000, 2352002, 2000000, 150000…\n#&gt; $ before_join                 &lt;bool&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n#&gt; $ raised_amount_before_join &lt;double&gt; 120528113, 120528113, 120528113, 120528113,…\n#&gt; $ num_rounds_before_join    &lt;double&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5…\n#&gt; Call `print()` for full schema details\n\n\n\n\nNest again\n\n# data.table\nexcluded_cols       &lt;- setdiff(names(me_joined_unnested_dt), c(\"round_id\", \"announced_on\", \"raised_amount\", \"before_join\"))\nme_joined_nested_dt &lt;- me_joined_unnested_dt[, list(funding_rounds=list(.SD)), by=excluded_cols]\n\n# Dplyr (not working)\n# me_joined_nested_dt &lt;- me_joined_unnested_dt |&gt; \n#         nest(funding_rounds = c(\"round_id\", \"announced_on\", \"raised_amount\", \"before_join\"))\n\nopen_dataset(\"me_joined_nested.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 12,654,304 rows x 13 columns\n#&gt; $ company_id                 &lt;int32&gt; 71668, 5070, 117119, 5070, 117119, 7920, 52…\n#&gt; $ founded_on           &lt;date32[day]&gt; 2019-01-01, 2014-07-01, 2015-01-01, 2014-07…\n#&gt; $ id_cs                      &lt;int32&gt; 23865165, 10861408, 10861408, 10861408, 108…\n#&gt; $ exp_id_cs                 &lt;double&gt; 1927546132, 1267852578, 1267852578, 2670635…\n#&gt; $ member_id_cs               &lt;int32&gt; 1874511, 1874513, 1874513, 1874513, 1874513…\n#&gt; $ company_name_cs           &lt;string&gt; \"Three Good\", \"Point\", \"Point\", \"Point\", \"P…\n#&gt; $ title_cs                  &lt;string&gt; \"Founder & CEO\", \"Customer Operations at Po…\n#&gt; $ date_from_parsed_cs  &lt;date32[day]&gt; 2015-04-01, 2018-01-01, 2018-01-01, 2018-11…\n#&gt; $ date_to_parsed_cs    &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, 2019-04-01, 2019-04…\n#&gt; $ tjoin_tfound              &lt;double&gt; -45, 42, 36, 52, 46, 50, 105, -33, 49, 131,…\n#&gt; $ raised_amount_before_join &lt;double&gt; 0, 11399999, 12100000, 11399999, 12100000, …\n#&gt; $ num_rounds_before_join    &lt;double&gt; 0, 2, 3, 2, 3, 6, 9, 0, 6, 9, 12, 7, 4, 9, …\n#&gt; $ funding_rounds         &lt;list&lt;...&gt;&gt; [&lt;tbl_df[5 x 4]&gt;], [&lt;tbl_df[5 x 4]&gt;], [&lt;tbl…\n#&gt; Call `print()` for full schema details\n\n\n\n\nTo differentiate between founder and non-founder CS titles are needed\n\n# Prep data (shrink / remove unnecessary data)\nme_joined_nested_foc_dt[, funding_rounds := NULL]\n\n# Prep titles\nme_wrangled_wt_dt &lt;-  me_dist_prqt |&gt; \n                          filter(company_id %in% cb_cs_joined_cs_ids, !is.na(date_from_parsed)) |&gt;  \n                          select(exp_id_cs, title_cs) |&gt; \n                          collect() |&gt; \n                          setDT()\n\n# Join\nme_joined_nested_foc_dt[me_wrangled_wt_dt, on = .(exp_id_cs), title_cs := i.title_cs]\n\n# Inspect\nopen_dataset(\"me_joined_nested_foc.parquet\") |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 12,654,304 rows x 12 columns\n#&gt; $ company_id                 &lt;int32&gt; 71668, 5070, 117119, 5070, 117119, 7920, 52…\n#&gt; $ founded_on           &lt;date32[day]&gt; 2019-01-01, 2014-07-01, 2015-01-01, 2014-07…\n#&gt; $ id_cs                      &lt;int32&gt; 23865165, 10861408, 10861408, 10861408, 108…\n#&gt; $ exp_id_cs                 &lt;double&gt; 1927546132, 1267852578, 1267852578, 2670635…\n#&gt; $ member_id_cs               &lt;int32&gt; 1874511, 1874513, 1874513, 1874513, 1874513…\n#&gt; $ company_name_cs           &lt;string&gt; \"Three Good\", \"Point\", \"Point\", \"Point\", \"P…\n#&gt; $ title_cs                  &lt;string&gt; \"Founder & CEO\", \"Customer Operations at Po…\n#&gt; $ date_from_parsed_cs  &lt;date32[day]&gt; 2015-04-01, 2018-01-01, 2018-01-01, 2018-11…\n#&gt; $ date_to_parsed_cs    &lt;date32[day]&gt; NA, NA, NA, NA, NA, NA, 2019-04-01, 2019-04…\n#&gt; $ tjoin_tfound              &lt;double&gt; -45, 42, 36, 52, 46, 50, 105, -33, 49, 131,…\n#&gt; $ raised_amount_before_join &lt;double&gt; 0, 11399999, 12100000, 11399999, 12100000, …\n#&gt; $ num_rounds_before_join    &lt;double&gt; 0, 2, 3, 2, 3, 6, 9, 0, 6, 9, 12, 7, 4, 9, …\n#&gt; Call `print()` for full schema details"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#plots",
    "href": "revealjs/slides/2023/cs/cs.html#plots",
    "title": "CoreSignal Analysis",
    "section": "Plots",
    "text": "Plots\n\nDataF1F2F3\n\n\nSeparate between “Founder” & “Non-Founder” and calculate summary statistics necessary for plotting.\n\nlibrary(ggplot2)\n\nlookup_term &lt;- \"founder\"\ndata        &lt;- me_joined_nested_foc_prqt |&gt; \n                  filter(!is.na(title_cs)) |&gt; \n                  mutate(Role = title_cs |&gt; tolower() |&gt; str_detect(lookup_term)) |&gt; \n                  collect() |&gt; \n                  mutate(\n                    Role = Role |&gt; factor(levels = c(TRUE, FALSE), \n                                                labels = c('Founder', 'Non-Founder'))\n                    ) \n\n# Summary Statistics (Mean & Median)\ndf_vline_long &lt;- data |&gt; \n  group_by(Role) |&gt; \n  summarise(Mean = mean(tjoin_tfound), Median = median(tjoin_tfound), n = n()) |&gt; \n  pivot_longer(c(Mean, Median,n ), names_to = \"Statistic\", values_to = \"Value\") |&gt; \n  mutate(Value_chr    = format(round(Value, 1), big.mark=\".\", decimal.mark = \",\", drop0trailing = T),\n         gg_pos_y = rep(c(0.07,0.06, 0.05),2),\n         gg_color = rep(c(\"#8F8470\", \"#BF9240\", \"#FEA400\"), 2))\n\n\n\n\n\n\nHow many month have passed since the company was founded and before the person joined the company (binwidth: 3 months)?\n\n\ndata |&gt; \n  \n  # Plot\n  ggplot(aes(x = tjoin_tfound, fill = Role, color = Role)) +\n  geom_histogram(aes(y =..density..), size = .2, binwidth = 3, alpha = 0.5) +\n  facet_wrap(~Role, nrow=2) +\n  \n  # Statistics & Design\n  ggnewscale::new_scale_color() +\n  geom_vline(data = df_vline_long |&gt; filter(Statistic != \"n\"), aes(xintercept = Value, linetype = Statistic, color = Statistic), key_glyph = \"path\") +\n  scale_linetype_manual(values = c(2,3)) +\n  scale_color_manual(values = c(\"#8F8470\", \"#BF9240\", \"#FEA400\")) +\n  geom_label(data = df_vline_long, aes(x = 100, y = gg_pos_y, label = paste0(Statistic, ' = ', Value_chr)), \n             color = df_vline_long$gg_color, fill = \"transparent\", alpha = 0.5, size = 3, hjust = \"left\") +\n  xlim(-250, 250) +\n  labs(x = \"Δ T_join, T_foundation (in month)\", y = \"Density\") + \n  theme(legend.key=element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\nHow much capital has been acquired by the time the person joins?\n\n\ndata |&gt; \n  \n  # Plot\n  ggplot(aes(x = raised_amount_before_join, color = Role, fill = Role)) + \n  geom_histogram(aes(y =..density..), alpha=0.5) +\n  facet_wrap(~Role, nrow=2) +\n  \n  # Design\n  scale_x_continuous(labels = scales::label_number(prefix = \"$\", accuracy = 0.1, scale_cut = scales::cut_short_scale()), limits = c(NA,1e+09)) +\n  labs(x = \"Raised amount before join\", y = \"Density\", fill=\"\", color = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nHow many funding rounds have been acquired by the time the person joins?\n\n\ndata |&gt; \n  \n  ggplot(aes(x = num_rounds_before_join, color = Role, fill = Role)) + \n  geom_histogram(aes(y =..density..), binwidth = 1, alpha=0.5) +\n  facet_wrap(~Role, nrow=2) +\n  \n  # Design\n  xlim(NA, 20) +\n  labs(x = \"# Rounds before join\", y = \"Density\", fill=\"\", color = \"\")"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#fortune500",
    "href": "revealjs/slides/2023/cs/cs.html#fortune500",
    "title": "CoreSignal Analysis",
    "section": "Fortune500",
    "text": "Fortune500\n\nme_matched_members_dt &lt;- me_matched_members_prqt |&gt; collect()\n\n# 46 Batches (Chunks with 5 Million rows)\nslice_ids &lt;- tibble(from = seq(1, 230000000, 5000000),to = c(seq(5000000, 225000000, 5000000), 229065592))\nfor (i in 1:46) {\n  # Build Batch\n  x &lt;- slice_ids$from[i]; y &lt;- slice_ids$to[i]\n  me_matched_members_slice_dt &lt;- me_matched_members_dt[x:y,]\n  # Create Features\n  me_matched_members_slice_dt[, `:=` (f500 = (purrr::pmap_lgl(list(company_name, date_from_parsed, date_to_parsed), check_f500, .progress = TRUE)),\n                                      role = title |&gt; tolower() |&gt; stringr::str_detect(\"founder\"))]\n  # Save\n  me_matched_members_slice_dt |&gt; write_parquet(paste0(\"/media/tie/ssd2/joschka/me_f500/me_f500_\", cur_id, \".parquet\"))\n}\n\n\ncheck_f500 &lt;- function(name,year_from,year_to) {\n  \n  if (is.na(year_to))   {year_to &lt;- 2023}\n  if (is.na(year_from)) {return(NA)}\n  \n  data &lt;- fortune500 |&gt; \n    filter(year |&gt; between(year_from, year_to)) |&gt; \n    pull(company)\n  \n  name |&gt; tolower() %in% data\n}"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#employment-history-worked-for-fortune-500-company",
    "href": "revealjs/slides/2023/cs/cs.html#employment-history-worked-for-fortune-500-company",
    "title": "CoreSignal Analysis",
    "section": "Employment History (Worked for Fortune 500 company?)",
    "text": "Employment History (Worked for Fortune 500 company?)\nContent-related problems\n\nMatching problems:\n\namd &lt;&gt; advanced micro devices\nintel &lt;&gt; intel corporation\n\nGeographical issues:\n\nJust US companies (use Fortune Global data)\n\n\nTechnical issues\nTakes loooooooong time to calculate…"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#code-i-data-function",
    "href": "revealjs/slides/2023/cs/cs.html#code-i-data-function",
    "title": "CoreSignal Analysis",
    "section": "Code I (Data + Function)",
    "text": "Code I (Data + Function)\nFunction\n\n# 1. Function\ncheck_f500 &lt;- function(title, year_from, year_to) {\n  \n  # Handle NA inputs\n  if (is.na(year_to))   {year_to &lt;- 2023}\n  if (is.na(year_from)) {return(NA)}\n  \n  # Filter time frame\n  data &lt;- fortune500 |&gt; \n    \n    filter(year |&gt; between(year_from, year_to)) |&gt; \n    pull(company)\n  \n  # Check match and return bool\n  title |&gt; tolower() %in% data\n}\n\n\n\nCoreSignal data (n = 229.065.592)\n\n# 1. Data\nme_matched_members_prqt &lt;- open_dataset(\"me_matched_members.parquet\") \nme_matched_members_prqt |&gt; \n  \n  head() |&gt; collect()\n\n\n\n#&gt; # A tibble: 6 × 7\n#&gt;       id member_id company_id company_name title date_from_parsed date_to_parsed\n#&gt;    &lt;dbl&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;date&gt;           &lt;date&gt;        \n#&gt; 1 6.07e8   2605953         NA arkansas se… Corp… 2015-02-01       NA            \n#&gt; 2 6.07e8   2605953         NA freelance m… Free… 2016-05-01       NA            \n#&gt; 3 6.30e8   2605953   10415396 101 magazine Cont… 2012-08-01       2014-05-01    \n#&gt; # ℹ 3 more rows\n\n\n\nUS Fortune 500 data\n\nfortune500 &lt;- readRDS(\"fortune_500_1955-2022.rds\") |&gt; \n                \n                select(year, company) |&gt; \n                mutate(company = company |&gt; tolower())\nfortune500\n\n\n\n#&gt; # A tibble: 34,000 × 2\n#&gt;    year company       \n#&gt;   &lt;dbl&gt; &lt;chr&gt;         \n#&gt; 1  1955 general motors\n#&gt; 2  1955 exxon mobil   \n#&gt; 3  1955 u.s. steel    \n#&gt; # ℹ 33,997 more rows"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#code-ii-chunkwise-execution",
    "href": "revealjs/slides/2023/cs/cs.html#code-ii-chunkwise-execution",
    "title": "CoreSignal Analysis",
    "section": "Code II (Chunkwise Execution)",
    "text": "Code II (Chunkwise Execution)\n\n# 1. Collect data\nme_matched_members_dt &lt;- me_matched_members_prqt |&gt; collect()\n# 2.Build batches\nslice_ids &lt;- tibble(\n  from  = seq(1, 230000000, 5000000),\n  to    = c(seq(5000000, 225000000, 5000000), 229065592),\n)\n\nfor (i in 1:46) {\n\n  # Create current batch  \n  x &lt;- slice_ids$from[i]\n  y &lt;- slice_ids$to[i]\n  me_matched_members_slice_dt &lt;- me_matched_members_dt[x:y,]\n\n  # Add features\n  me_matched_members_slice_dt[, `:=` (f500 = (purrr::pmap_lgl(list(company_name, date_from_parsed, date_to_parsed), check_f500, .progress = TRUE)),\n                                      role = title |&gt; tolower() |&gt; stringr::str_detect(\"founder\"))]\n  \n  # Save\n  me_matched_members_slice_dt |&gt; write_parquet(paste0(\"me_f500/me_f500_\", cur_id, \".parquet\"))\n}"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#code-iii-build-feature",
    "href": "revealjs/slides/2023/cs/cs.html#code-iii-build-feature",
    "title": "CoreSignal Analysis",
    "section": "Code III (Build feature)",
    "text": "Code III (Build feature)\n\n# 1. Load data \nme_f500 &lt;- open_dataset(\"me_f500/\") |&gt; \n  collect()\n\n# 2. Add Ids\nme_dist_ids_prqt &lt;- arrow::open_dataset(\"me_dist4.parquet\") |&gt; select(id, member_id) |&gt; collect()\nme_f500_id       &lt;-  me_dist_ids_prqt[me_f500, on = .(id)] \n\n# 3. Add features\n# 3.1 Earliest founding date & earliest f500 date (if founded)\nme_f500_id[,`:=` (founding_min      = (ifelse(any(role == T),                  min(date_from_parsed[role==T]),   NA_real_)),\n                  f500_min_founding = (ifelse(any(role == T) & any(f500 == T), min(date_from_parsed[f500 == T]), NA_real_))),\n           by = .(member_id)]\n\n# 3.2 Compare\nme_f500_id[, f500_before_founding := f500_min_founding &lt;= founding_min]"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#plot",
    "href": "revealjs/slides/2023/cs/cs.html#plot",
    "title": "CoreSignal Analysis",
    "section": "Plot",
    "text": "Plot\nConstraints:\n\nAnalysis takes place on an annual level.\nFirst funding event is being considered\nExact matches only\n\n\n\nme_f500_id &lt;- arrow::open_dataset(\"~/02_diss/01_coresignal/02_data/me_f500_id.parquet\")\ndata &lt;- me_f500_id |&gt; \n  \n          # Filter \"Founder\"\n          filter(role == T) |&gt; collect() \n  \ndata |&gt; \n  ggplot(aes(x = f500_before_founding)) + \n    geom_bar() +\n    scale_x_discrete(labels=c(\"Employment at Fortune500\\nAFTER\\nfounding\", \"Employment at Fortune500\\nBEFORE\\nfounding\", \"Neither case\")) +\n    scale_y_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6, accuracy = 0.1)) + \n    labs(x = \"\", y = \"Count\")"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#cluster-skills",
    "href": "revealjs/slides/2023/cs/cs.html#cluster-skills",
    "title": "CoreSignal Analysis",
    "section": "Cluster skills",
    "text": "Cluster skills\ntbd\n\nmember_skills_prqt &lt;- arrow::open_dataset(\"member_skills.parquet\")\nmember_skills_prqt |&gt; \n  glimpse()\n\n\nskill_names_tbl &lt;- readRDS(\"skill_names_tbl.rds\")\nskill_names_tbl\n\n\n\n#&gt; # A tibble: 2,423,690 × 2\n#&gt;   skill_id skill_name \n#&gt;      &lt;int&gt; &lt;fct&gt;      \n#&gt; 1        1 mathematics\n#&gt; 2        2 swimming   \n#&gt; 3        3 analytics  \n#&gt; # ℹ 2,423,687 more rows"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#built-variables",
    "href": "revealjs/slides/2023/cs/cs.html#built-variables",
    "title": "CoreSignal Analysis",
    "section": "Built variables",
    "text": "Built variables\n\nAllGeneralEDAExp (dummy)Exp (quant)EduFund\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 67\n#&gt; $ company_id_cbpb                      &lt;int&gt; 90591, 152845, 90440, 138208, 116…\n#&gt; $ funding_after_mid                    &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"…\n#&gt; $ funding_after_early                  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\",…\n#&gt; $ member_id                            &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005…\n#&gt; $ id_tie                               &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175…\n#&gt; $ exp_id_cs                            &lt;dbl&gt; 2481733250, 1423977093, 2638, 263…\n#&gt; $ exp_corporate                        &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.000…\n#&gt; $ exp_funded_startup                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0,…\n#&gt; $ exp_founder                          &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0…\n#&gt; $ exp_f500                             &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.0000…\n#&gt; $ exp_research                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ company_id_cs                        &lt;int&gt; 140537, 10644128, 6068905, 606890…\n#&gt; $ company_name_cs                      &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"…\n#&gt; $ company_name_cbpb                    &lt;chr&gt; \"receptos\", \"HERE Technologies Ch…\n#&gt; $ founded_on_cbpb                      &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-…\n#&gt; $ closed_on_cbpb                       &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-…\n#&gt; $ title_cs                             &lt;chr&gt; \"Key Account Manager\", \"GIS Analy…\n#&gt; $ date_from_parsed_cs                  &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_to_parsed_cs                    &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-…\n#&gt; $ tjoin_tfound                         &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, …\n#&gt; $ raised_amount_before_join_company    &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333…\n#&gt; $ num_rounds_before_join               &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, …\n#&gt; $ is_f500                              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, F…\n#&gt; $ is_founder                           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research                          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ is_research_ivy                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ date_1st_founder_exp                 &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_f500_exp                    &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010…\n#&gt; $ date_1st_funded_startup_exp          &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-…\n#&gt; $ date_1st_research_exp                &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_research_ivy_exp            &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ date_1st_corporate_exp               &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, …\n#&gt; $ time_since_1st_corporate_exp         &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40,…\n#&gt; $ time_since_1st_founder_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_f500_exp              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_funded_startup_exp    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, N…\n#&gt; $ time_since_1st_research_exp          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_research_ivy_exp      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ time_since_1st_experience            &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176…\n#&gt; $ raised_amount_before_founder_member  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ raised_amount_before_all_member      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA,…\n#&gt; $ was_corporate_before                 &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, …\n#&gt; $ was_founder_before                   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_f500_before                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_fc_before                        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_uni_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ was_ivy_before                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE…\n#&gt; $ stage_mid                            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ stage_late                           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, FALSE, NA…\n#&gt; $ date_from_stage                      &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\",…\n#&gt; $ company_start_mid                    &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-…\n#&gt; $ company_start_late                   &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n#&gt; $ num_rounds_cumulated_founder         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ num_rounds_cumulated_all             &lt;int&gt; NA, NA, NA, NA, NA, NA, 1, 1, NA,…\n#&gt; $ announced_on_sB                      &lt;date&gt; 2012-02-03, 2018-01-04, 2011-03-…\n#&gt; $ round_type_new_next                  &lt;fct&gt; Series C, Series C, Series C, Ser…\n#&gt; $ raised_amount_cumsum_sB              &lt;dbl&gt; 46043054, 0, 1905000, 11022796, 2…\n#&gt; $ raised_amount_cumsum_sB_next         &lt;dbl&gt; 76043054, 0, 8712306, 13854868, 4…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(id_tie, member_id, exp_id_cs, company_id_cbpb, company_name_cbpb, company_id_cs, company_name_cs, \n         founded_on_cbpb, closed_on_cbpb,\n         title_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ id_tie            &lt;int&gt; 38, 67, 89, 89, 96, 104, 183, 175, 209, 243, 321, 37…\n#&gt; $ member_id         &lt;int&gt; 878, 2104, 3548, 3548, 3970, 4005, 4224, 4224, 4317,…\n#&gt; $ exp_id_cs         &lt;dbl&gt; 2481733250, 1423977093, 2638, 2638, 1736317868, 3084…\n#&gt; $ company_id_cbpb   &lt;int&gt; 90591, 152845, 90440, 138208, 116099, 97810, 40123, …\n#&gt; $ company_name_cbpb &lt;chr&gt; \"receptos\", \"HERE Technologies Chicago\", \"crowdtwist…\n#&gt; $ company_id_cs     &lt;int&gt; 140537, 10644128, 6068905, 6068905, 11825305, 194148…\n#&gt; $ company_name_cs   &lt;chr&gt; \"Bristol-Myers Squibb\", \"HERE\", \"Oracle\", \"Oracle\", …\n#&gt; $ founded_on_cbpb   &lt;date&gt; 2007-01-01, 2012-11-13, 2009-07-01, 2006-01-01, 201…\n#&gt; $ closed_on_cbpb    &lt;date&gt; NA, NA, NA, NA, NA, NA, 2021-04-09, NA, NA, NA, NA,…\n#&gt; $ title_cs          &lt;chr&gt; \"Key Account Manager\", \"GIS Analyst I\", \"QA\", \"QA\", …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_parsed_cs, date_to_parsed_cs, \n         tjoin_tfound, raised_amount_before_join_company, num_rounds_before_join) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 5\n#&gt; $ date_from_parsed_cs               &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_to_parsed_cs                 &lt;date&gt; 2008-08-01, NA, NA, NA, 2011-10-01,…\n#&gt; $ tjoin_tfound                      &lt;dbl&gt; -12, 37, 6, 48, -47, 48, 17, 11, 44,…\n#&gt; $ raised_amount_before_join_company &lt;dbl&gt; 0, 0, 0, 7722796, 0, 9961692, 333333…\n#&gt; $ num_rounds_before_join            &lt;dbl&gt; 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 1, 2, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(starts_with(\"is_\"),\n         starts_with(\"was_\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 10\n#&gt; $ is_f500              &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FAL…\n#&gt; $ is_founder           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ is_research_ivy      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_corporate_before &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRU…\n#&gt; $ was_founder_before   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_f500_before      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_fc_before        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n#&gt; $ was_uni_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n#&gt; $ was_ivy_before       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(\n    starts_with(\"date_1st_\"),\n    starts_with(\"time_since_1st_\"),\n    starts_with(\"exp_\"), -exp_id_cs) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 19\n#&gt; $ date_1st_founder_exp              &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_f500_exp                 &lt;date&gt; 2006-01-01, NA, 2010-01-01, 2010-01…\n#&gt; $ date_1st_funded_startup_exp       &lt;date&gt; 2006-01-01, 2016-01-01, 2010-01-01,…\n#&gt; $ date_1st_research_exp             &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_research_ivy_exp         &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ date_1st_corporate_exp            &lt;date&gt; 2009-02-01, 2015-01-01, NA, NA, 200…\n#&gt; $ time_since_1st_corporate_exp      &lt;dbl&gt; NA, 12, NA, NA, 116, NA, 136, 40, 17…\n#&gt; $ time_since_1st_founder_exp        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_f500_exp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_funded_startup_exp &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 96, NA, NA, …\n#&gt; $ time_since_1st_research_exp       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_research_ivy_exp   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ time_since_1st_experience         &lt;dbl&gt; 0, 12, 0, 0, 116, 0, 136, 40, 176, 4…\n#&gt; $ exp_corporate                     &lt;dbl&gt; 0.00000, 12.00000, 0.00000, 0.00000,…\n#&gt; $ exp_funded_startup                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0,…\n#&gt; $ exp_founder                       &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.00…\n#&gt; $ exp_f500                          &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.00000, …\n#&gt; $ exp_research                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ exp_research_ivy                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(score_global_2023_best,\n         starts_with(\"rank\"),\n         starts_with(\"degree\")) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 8\n#&gt; $ score_global_2023_best               &lt;dbl&gt; 40.6, 26.8, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_global_2023_best                &lt;int&gt; 917, 1549, NA, NA, NA, NA, NA, NA…\n#&gt; $ rank_national_2023_best              &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ rank_national_during_enrollment_best &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ degree_ba2                           &lt;lgl&gt; FALSE, TRUE, NA, NA, TRUE, NA, NA…\n#&gt; $ degree_ma2                           &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_phd2                          &lt;lgl&gt; FALSE, FALSE, NA, NA, FALSE, NA, …\n#&gt; $ degree_mba2                          &lt;lgl&gt; TRUE, FALSE, NA, NA, FALSE, NA, N…\n\n\n\n\n\ncs_me_dist8_unest_wedu_dt |&gt; \n  select(date_from_stage, company_start_mid, company_start_late,\n         raised_amount_before_founder_member, raised_amount_before_all_member,\n         funding_after_mid, funding_after_early) |&gt; \n  glimpse()\n\n#&gt; Rows: 2,659,657\n#&gt; Columns: 7\n#&gt; $ date_from_stage                     &lt;chr&gt; \"early1\", \"mid\", \"early2\", \"mid\", …\n#&gt; $ company_start_mid                   &lt;date&gt; 2009-01-01, 2014-11-13, 2011-07-0…\n#&gt; $ company_start_late                  &lt;date&gt; 2009-11-23, 2017-11-13, 2014-07-0…\n#&gt; $ raised_amount_before_founder_member &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA…\n#&gt; $ raised_amount_before_all_member     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 0, 0, NA, …\n#&gt; $ funding_after_mid                   &lt;chr&gt; \"yes\", NA, \"yes\", \"yes\", \"yes\", \"y…\n#&gt; $ funding_after_early                 &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", …"
  },
  {
    "objectID": "revealjs/slides/2023/cs/cs.html#further-analysis-is-necessary",
    "href": "revealjs/slides/2023/cs/cs.html#further-analysis-is-necessary",
    "title": "CoreSignal Analysis",
    "section": "Further analysis is necessary",
    "text": "Further analysis is necessary\n\nMatch employment history with Fortune500 (revenue based selection)\nCluster job titles\nCluster skills"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#table-of-contents",
    "href": "revealjs/slides/2023/diss/diss.html#table-of-contents",
    "title": "Diss",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#possible-overarching-topics",
    "href": "revealjs/slides/2023/diss/diss.html#possible-overarching-topics",
    "title": "Diss",
    "section": "Possible Overarching Topics",
    "text": "Possible Overarching Topics\n\nBricolage and effectuation\nTechnical Entrepreneur (prior knowledge)\nContingency\nNovelty and Technological uncertainty\nKnowledge spillover\n\nWe are interested in how actors are influenced by and interact with their social and cultural environments to bring about novelty, e.g. with regard to ideas, teams, products or business practices."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#research-scope",
    "href": "revealjs/slides/2023/diss/diss.html#research-scope",
    "title": "Diss",
    "section": "Research Scope",
    "text": "Research Scope\n\n \n\n  \n  \n  W-11 Research Focus & Fields\n  We are interested in how actors are influenced by and interact with their social and cultural environments to bring about novelty, e.g. with regard to ideas, teams, products or business practices."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#step1-identity-matching",
    "href": "revealjs/slides/2023/diss/diss.html#step1-identity-matching",
    "title": "Diss",
    "section": "Step1: Identity matching",
    "text": "Step1: Identity matching\nLinking Developer with startup data"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#github-1.-ghtorrent-and-2.-github-api",
    "href": "revealjs/slides/2023/diss/diss.html#github-1.-ghtorrent-and-2.-github-api",
    "title": "Diss",
    "section": " GitHub: 1. GHTorrent and 2. Github API",
    "text": "GitHub: 1. GHTorrent and 2. Github API\n\n\n1. Schema2. Conn3. User (GHT)4. User (Website)5. User (API)6. Orgs7. Merged\n\n\n\n\n\n\n\n\np_load(RMariaDB, dplyr)\ncon &lt;- dbConnect(\n  drv      = MariaDB(),\n  dbname   = \"ghtorrent_restore\",\n  username = \"ghtorrentuser\",\n1  password = Sys.getenv(\"GHTORRENTPASSWORD\"),\n2  host     = \"127.0.0.1\",\n  port     = 3307\n)\n\ncon |&gt; dbListTables()\n\n\n1\n\nPassword is located in a .Renviron file that is not stored in version control (GitHub)\n\n2\n\nSSH port forwarding/tunneling for MySQL connection is used\n\n\n\n\n\n\n\n\ntbl(con, \"users\") |&gt; \n  count()\n\n\n\n\n\n\ntbl(con,\"projects\") |&gt; \n  count()\n\n\n\n\n\n\ntbl(con,\"project_commits\") |&gt; \n  count()\n\n\n\n\n\n\n\n\n\nGHTorrent user data does not contain much valuable information:\n\n\n\nusers &lt;- tbl(con, \"users\") \nusers |&gt;   \n  glimpse()\n\n\n\n\nusers |&gt; \n  filter(login == \"christophihl\") |&gt; \n  glimpse()\n\n\n\n\n\n\n\n\n\n\n\n\n\n       \n\n\n       \n\n\n       \n\n\n       \n\n\n\n\n       \n\n\n       \n\n\n       \n\n\n       \n\n\n       \n\n\n\n\nhttps://github.com/jspolsky\n\n\n\n\nMore sensible data via API: Name, Email, Avatar, Blog/URL, Bio\n\n\nSingle API Request\n\nhttr::GET(\"https://api.github.com/users/christophihl\")\nhttr::GET(\"https://api.github.com/user/8004978\")\n\n\n\n\n\n\n#&gt; Rows: 1\n#&gt; Columns: 28\n#&gt; $ login               &lt;chr&gt; \"christophihl\"\n#&gt; $ id                  &lt;int&gt; 8004978\n#&gt; $ node_id             &lt;chr&gt; \"MDQ6VXNlcjgwMDQ5Nzg=\"\n#&gt; $ avatar_url          &lt;chr&gt; \"https://avatars.githu…\n#&gt; $ gravatar_id         &lt;chr&gt; \"\"\n#&gt; $ url                 &lt;chr&gt; \"https://api.github.co…\n#&gt; $ html_url            &lt;chr&gt; \"https://github.com/ch…\n#&gt; $ followers_url       &lt;chr&gt; \"https://api.github.co…\n#&gt; $ following_url       &lt;chr&gt; \"https://api.github.co…\n#&gt; $ gists_url           &lt;chr&gt; \"https://api.github.co…\n#&gt; $ starred_url         &lt;chr&gt; \"https://api.github.co…\n#&gt; $ subscriptions_url   &lt;chr&gt; \"https://api.github.co…\n#&gt; $ organizations_url   &lt;chr&gt; \"https://api.github.co…\n#&gt; $ repos_url           &lt;chr&gt; \"https://api.github.co…\n#&gt; $ events_url          &lt;chr&gt; \"https://api.github.co…\n#&gt; $ received_events_url &lt;chr&gt; \"https://api.github.co…\n#&gt; $ type                &lt;chr&gt; \"User\"\n#&gt; $ site_admin          &lt;lgl&gt; FALSE\n#&gt; $ name                &lt;chr&gt; \"Christoph Ihl\"\n#&gt; $ company             &lt;chr&gt; \"Hamburg University of…\n#&gt; $ blog                &lt;chr&gt; \"www.startupengineer.i…\n#&gt; $ location            &lt;chr&gt; \"Hamburg\"\n#&gt; $ public_repos        &lt;int&gt; 21\n#&gt; $ public_gists        &lt;int&gt; 1\n#&gt; $ followers           &lt;int&gt; 12\n#&gt; $ following           &lt;int&gt; 1\n#&gt; $ created_at          &lt;chr&gt; \"2014-06-27T11:22:22Z\"\n#&gt; $ updated_at          &lt;chr&gt; \"2024-04-23T13:22:01Z\"\n\n\n\n\nFinal API dataset\n\nopen_dataset(\"gh_api_users_wrangled.parquet\") |&gt; \n  glimpse() \n\n\n\n\n\n\nTwo different types: Organizations and User accounts\n\n\n\nhttr::GET(\"https://api.github.com/users/TUHHStartupEngineers\")\nhttr::GET(\"https://api.github.com/user/30825260\")\n\n\n\n#&gt; Rows: 1\n#&gt; Columns: 29\n#&gt; $ login               &lt;chr&gt; \"TUHHStartupEngineers\"\n#&gt; $ id                  &lt;int&gt; 30825260\n#&gt; $ node_id             &lt;chr&gt; \"MDEyOk9yZ2FuaXphdGlvbjMwODI1M…\n#&gt; $ avatar_url          &lt;chr&gt; \"https://avatars.githubusercon…\n#&gt; $ gravatar_id         &lt;chr&gt; \"\"\n#&gt; $ url                 &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ html_url            &lt;chr&gt; \"https://github.com/TUHHStartu…\n#&gt; $ followers_url       &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ following_url       &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ gists_url           &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ starred_url         &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ subscriptions_url   &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ organizations_url   &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ repos_url           &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ events_url          &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ received_events_url &lt;chr&gt; \"https://api.github.com/users/…\n#&gt; $ type                &lt;chr&gt; \"Organization\"\n#&gt; $ site_admin          &lt;lgl&gt; FALSE\n#&gt; $ name                &lt;chr&gt; \"TUHH Institute of Entrepreneu…\n#&gt; $ blog                &lt;chr&gt; \"www.startupengineer.io\"\n#&gt; $ location            &lt;chr&gt; \"Hamburg University of Technol…\n#&gt; $ email               &lt;chr&gt; \"startup.engineer@tuhh.de\"\n#&gt; $ bio                 &lt;chr&gt; \"Data Science, Research & Prac…\n#&gt; $ public_repos        &lt;int&gt; 8\n#&gt; $ public_gists        &lt;int&gt; 0\n#&gt; $ followers           &lt;int&gt; 9\n#&gt; $ following           &lt;int&gt; 0\n#&gt; $ created_at          &lt;chr&gt; \"2017-08-08T07:40:56Z\"\n#&gt; $ updated_at          &lt;chr&gt; \"2024-05-03T11:35:16Z\"\n\n\n\n\n\norg_members &lt;- tbl(con, \"organization_members\")\norg_members\n\n\norg_members |&gt; \n  left_join(users, by = c(org_id  = \"id\")) |&gt; \n  left_join(users, by = c(user_id = \"id\")) |&gt; \n  filter(login.x == \"TUHHStartupEngineers\") |&gt; \n  select(login.y)\n\n\n\n\n\n\n\n\n\n\n\n\nOrganization affiliation:\n\n\ngh_org_affil &lt;- org_members |&gt; \n  # Logins for users & orgs (via GHT)\n  left_join(gh_ght_users, by = c(org_id  = \"id\")) |&gt; \n  left_join(gh_ght_users, by = c(user_id = \"id\")) |&gt; \n  \n  # Domains for orgs (via API)\n  left_join(gh_api_users, by = c(org_login = \"login\"))\n\n\n\n\n\n\nFinal dataset:\n\n\ngh_api_users |&gt; \n  \n  # Add org memberships (+ domains)\n  left_join(gh_org_affil) |&gt; \n\n  # Add location data\n  left_join(gh_ght_users)"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#stackoverflow",
    "href": "revealjs/slides/2023/diss/diss.html#stackoverflow",
    "title": "Diss",
    "section": " Stackoverflow",
    "text": "Stackoverflow\n\nOverviewDumpsIdentity MatchingWebsiteScraped DataMerged\n\n\n\nTwo among the most widely adopted and studied platforms are GitHub and StackOverflow\nThese two platforms serve different purposes: code sharing and collaborative development vs. information and knowledge exchange.\nAt the same time, they both serve potentially the same community of developers for the same overall goal, i.e., software development.\n\n\n\n\n\nCurrent Dump from 2021\n\n\n\nopen_dataset(\"so_dump_2021_09_users.parquet\") |&gt; \n  \n  glimpse() \n\n\n\n\nopen_dataset(\"so_dump_2021_09_users.parquet\") |&gt; \n  filter(DisplayName == \"Christoph Ihl\")\n  glimpse() \n\n\n\n\n\n\nOld Dump from 2013\n\n\n\nopen_dataset(\"so_dump_2013_09_users.parquet\") |&gt; \n  \n  glimpse() \n\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\n\n\n\n\n\n\n\nName and Location\n\n\n\n\n\n\nProfile images\n\n\n\n\n\nProfile images\n\n\n\n\n\n\n\n\njoin &lt;- open_dataset(\"gh_api_users.parquet\") |&gt; \n  \n  inner_join( \n    open_dataset(\"so_users_joined.parquet\") , \n      by = c(name  = \"DisplayName\", location = \"Location\"), \n      na_matches = \"never\"\n    ) |&gt; \n  \n  compute()\n\njoin|&gt; \n  nrow()\n\n\n\n#&gt; [1] 724961\n\n\n\n\njoin |&gt; \n  filter(location == \"Hamburg\") |&gt; \n  count(name, sort = T)\n\n\n\n#&gt; # A tibble: 43 × 2\n#&gt;   name        n\n#&gt;   &lt;chr&gt;   &lt;int&gt;\n#&gt; 1 Jan        10\n#&gt; 2 Alex        8\n#&gt; 3 Chris       8\n#&gt; 4 Patrick     6\n#&gt; 5 Nils        5\n#&gt; 6 Fabian      4\n#&gt; 7 Dennis      3\n#&gt; # ℹ 36 more rows\n\n\n\n\nOptimization: OpenStreetMap API and only full names\n\n\n\n\n\n# Stackoverflow\nknitr::include_graphics(\n  \"https://graph.facebook.com/920949401423102/picture?type=large\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n# GitHub\nknitr::include_graphics(\n  \"https://avatars.githubusercontent.com/u/8004978?v=4\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       \n\n\n\n\nJob title and company\n\n\n\n\n       \n\n\n\n\nUnique Identifier\n\n\n\n\n       \n\n\n\n\nAble to run through a specific ID\n\n\n\n\n\n\n\nopen_dataset(\"so_users_joined.parquet\") |&gt; \n  \n  glimpse()\n\n\n\nopen_dataset(\"so_users_joined.parquet\") |&gt; \n  filter(DisplayName == \"Christoph Ihl\") |&gt; \n  glimpse()\n\n\n\n\n\n\ngh_api_users_orgs_locs_nested_tbl &lt;- gh_api_users_orgs_locs_tbl |&gt; \n                                        nest(organization = c(ght_org_id, org_login, org_domain, member_created_at))\n\ngh_so_joined_tbl &lt;- gh_api_users_orgs_locs_nested_tbl |&gt; \n  left_join(so_joined_tbl, by = c(login      = \"github_handle\"), na_matches = \"never\") |&gt; \n  left_join(so_joined_tbl, by = c(email_hash = \"EmailHash\"),     na_matches = \"never\")\n\ngh_so_joined_tbl |&gt; \n  glimpse()\n\n\n\n#&gt; FileSystemDataset with 1 Parquet file\n#&gt; 42,557,276 rows x 22 columns\n#&gt; $ api_usr_id                     &lt;int32&gt; 1, 2, 3, 4, 5, 6, 7, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30…\n#&gt; $ ght_usr_id                     &lt;int32&gt; 9236, 1570, 13256, 3892, 96349, 17407, 52402, 171316, 41811, 2159, 1300…\n#&gt; $ login                         &lt;string&gt; \"mojombo\", \"defunkt\", \"pjhyett\", \"wycats\", \"ezmobius\", \"ivey\", \"evanphx…\n#&gt; $ type                 &lt;dictionary&lt;...&gt;&gt; User, User, User, User, User, User, User, User, User, User, User, User,…\n#&gt; $ name                          &lt;string&gt; \"Tom Preston-Werner\", \"Chris Wanstrath\", \"PJ Hyett\", \"Yehuda Katz\", \"Ez…\n#&gt; $ company                       &lt;string&gt; NA, \"@github \", \"GitHub, Inc.\", \"Tilde, Inc.\", \"Stuffstr PBC\", \"@RiotGa…\n#&gt; $ blog                          &lt;string&gt; \"http://tom.preston-werner.com\", \"http://chriswanstrath.com/\", \"https:/…\n#&gt; $ email                         &lt;string&gt; \"tom@mojombo.com\", \"chris@github.com\", \"pj@hyett.com\", \"wycats@gmail.co…\n#&gt; $ email_hash                    &lt;string&gt; \"25c7c18223fb42a4c6ae1c8db6f50f9b\", \"74858be1905a8bbdb565109107384bd9\",…\n#&gt; $ bio                           &lt;string&gt; NA, \"🍔 \", NA, NA, NA, NA, NA, NA, NA, \"Co-founder and CEO, Code Climat…\n#&gt; $ location                      &lt;string&gt; \"San Francisco\", \"San Francisco\", \"San Francisco\", \"San Francisco\", \"In…\n#&gt; $ country_code         &lt;dictionary&lt;...&gt;&gt; US, NA, us, US, DE, US, US, US, US, US, NA, US, CA, US, US, US, US, AL,…\n#&gt; $ state                         &lt;string&gt; \"CA\", NA, \"San Francisco County\", \"CA\", \"Nordrhein-Westfalen\", \"AL\", \"C…\n#&gt; $ city                          &lt;string&gt; \"San Francisco\", NA, \"San Francisco\", \"San Francisco\", \"Hennef (Sieg)\",…\n#&gt; $ usr_created_at &lt;timestamp[us, tz=UTC]&gt; 2007-10-20 05:24:19, 2007-10-20 05:24:19, 2008-01-07 17:54:22, 2008-01-…\n#&gt; $ organization               &lt;list&lt;...&gt;&gt; [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[1 x 4]&gt;], [&lt;tbl_df[16 x …\n#&gt; $ so_id                         &lt;double&gt; NA, NA, NA, 122162, NA, 239960, 1335022, NA, 365701, 7392312, NA, 26342…\n#&gt; $ so_login                      &lt;string&gt; NA, NA, NA, \"Yehuda Katz\", NA, \"Michael D. Ivey\", \"Evan Phoenix\", NA, \"…\n#&gt; $ so_current_position           &lt;string&gt; NA, NA, NA, \"Founder at Tilde, Inc.\", NA, NA, NA, NA, NA, \"CEO at Code …\n#&gt; $ so_twitter_handle             &lt;string&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"brynary\", NA, NA, NA, NA, NA, \"top…\n#&gt; $ so_website_url                &lt;string&gt; NA, NA, NA, \"http://www.tilde.io\", NA, \"http://gweezlebur.com\", \"http:/…\n#&gt; $ so_location                   &lt;string&gt; NA, NA, NA, \"Portland, OR\", NA, \"Bay Minette, AL\", NA, NA, \"Buffalo, NY…\n#&gt; Call `print()` for full schema details"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#angellist",
    "href": "revealjs/slides/2023/diss/diss.html#angellist",
    "title": "Diss",
    "section": " Angellist",
    "text": "Angellist\n\n1. User2. OrgsData ISchemaMergedMerged CB\n\n\n\n\n\n\n\n\n\nHTML is missing in source folder\n\n\n\n\n\n\n\n\n\n\n\nIterated over 10,3 million user ids\n\n\nopen_dataset(\"al_profiles.parquet\") |&gt; \n  glimpse() \n\n\n\nIterated over 7,3 million company ids\n\n\nopen_dataset(\"al_orgs.parquet\") |&gt; \n  glimpse() \n\n\n\n\n\n\n\n\nIterated over 10,3 million user ids\n\n\nopen_dataset(\"al_profiles_main_wrangled.parquet\") |&gt; \n  glimpse() \n\n\n\nIterated over 7,3 million company ids\n\n\nopen_dataset(\"al_employees.parquet\") |&gt; \n  glimpse() \n\n\n\n\n\n\n\n\n\n\n\n\n\ngh_so_al_final &lt;- gh_so_joined_tbl |&gt; \n  \n  # 1. Via github_handle\n  left_join(al_profiles_main_wrangled, by = c(\"login\"    = \"github_handle\"),  na_matches = \"never\") |&gt; \n  # 2. Via SO_handle(s) \n  left_join(al_profiles_main_wrangled, by = c(\"so_login\" = \"so_handle\"),      na_matches = \"never\") |&gt; \n  # 3. Via twitter_handle\n  left_join(al_profiles_main_wrangled, by = c(\"so_twitter_handle\" = \"twitter_handle\"), na_matches = \"never\")\n\n\n\n\ngh_so_al_unnested_tbl &lt;- gh_so_al_unnested_tbl |&gt; \n  \n  left_join(cb_people_handles, by = c(facebook_usr_al = \"facebook_handle\"), na_matches = \"never\") |&gt; \n  left_join(cb_people_handles, by = c(twitter_usr     = \"twitter_handle\"),  na_matches = \"never\") |&gt; \n  left_join(cb_people_handles, by = c(linkedin_usr_al = \"linkedin_handle\"), na_matches = \"never\") |&gt;  \n\n  left_join(cb_orgs_handles,   by = c(facebook_org_al = \"facebook_handle\"), na_matches = \"never\") |&gt; \n  left_join(cb_orgs_handles,   by = c(twitter_org_al  = \"twitter_handle\"),  na_matches = \"never\") |&gt; \n  left_join(cb_orgs_handles,   by = c(linkedin_org_al = \"linkedin_handle\"), na_matches = \"never\") |&gt; \n  \n  left_join(cb_jobs,           by = c(uuid_usr_cb     = \"person_uuid\"))"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#coresignal",
    "href": "revealjs/slides/2023/diss/diss.html#coresignal",
    "title": "Diss",
    "section": " Coresignal",
    "text": "Coresignal\n\nOverview12\n\n\nasd\n\n\nasd\n\n\nasd"
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-1-menubarclass",
    "href": "revealjs/slides/2023/diss/diss.html#option-1-menubarclass",
    "title": "Diss",
    "section": "Option 1: menubarclass",
    "text": "Option 1: menubarclass\nThe menubarclass option sets the classname of menubars.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      menubarclass: \"menubar\"\n\nSimplemenu will show the menubar(s) on all pages. If you do not want to show the menubar on certain pages, use data-state=“hide-menubar” on that section. This behaviour also works when exporting to PDF using the Reveal.js ?print-pdf option."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-2-menuclass",
    "href": "revealjs/slides/2023/diss/diss.html#option-2-menuclass",
    "title": "Diss",
    "section": "Option 2: menuclass",
    "text": "Option 2: menuclass\nThe menuclass option sets the classname of the menu.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      menuclass: \"menu\"\nSimplemenu looks inside this menu for list items (LI’s)."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-3-activeclass",
    "href": "revealjs/slides/2023/diss/diss.html#option-3-activeclass",
    "title": "Diss",
    "section": "Option 3: activeclass",
    "text": "Option 3: activeclass\nThe activeclass option is the class an active menuitem gets.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      activeclass: \"active\""
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-4-activeelement",
    "href": "revealjs/slides/2023/diss/diss.html#option-4-activeelement",
    "title": "Diss",
    "section": "Option 4: activeelement",
    "text": "Option 4: activeelement\nThe activeelement option sets the item that gets the active class.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      activeelement: \"li\"\nYou may want to change it to the anchor inside the li, like this: activeelement: \"a\"."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-5-barhtml",
    "href": "revealjs/slides/2023/diss/diss.html#option-5-barhtml",
    "title": "Diss",
    "section": "Option 5: barhtml",
    "text": "Option 5: barhtml\nYou can add the HTML for the header (and/or footer) through this option. This way you no longer need to edit the template.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      barhtml:\n        header: \"&lt;div class='menubar'&gt;&lt;ul class='menu'&gt;&lt;/ul&gt;&lt;/div&gt;\"\n        footer: \"\""
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-5-barhtml-continued",
    "href": "revealjs/slides/2023/diss/diss.html#option-5-barhtml-continued",
    "title": "Diss",
    "section": "Option 5: barhtml (Continued)",
    "text": "Option 5: barhtml (Continued)\nYou can also move the slide number or the controls to your header or footer. If they are nested there manually, or through the barhtml option, they will then display inside that header or footer.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      barhtml:\n        header: \"\"\n        footer: \"&lt;div class='menubar'&gt;&lt;ul class='menu'&gt;&lt;/ul&gt;&lt;div class='slide-number'&gt;&lt;/div&gt;&lt;/div&gt;\""
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-6-flat",
    "href": "revealjs/slides/2023/diss/diss.html#option-6-flat",
    "title": "Diss",
    "section": "Option 6: flat",
    "text": "Option 6: flat\nSometimes you’ll want to limit your presentation to horizontal slides only. To still use ‘chapters’, you can use the flat option. By default, it is set to false, but you can set it to true. Then, when a data-name is set for a slide, any following slides will keep that menu name.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      flat: true\nTo stop inheriting the previous slide menu name, start a new named section, or add data-sm=\"false\" to your slide."
  },
  {
    "objectID": "revealjs/slides/2023/diss/diss.html#option-7-scale",
    "href": "revealjs/slides/2023/diss/diss.html#option-7-scale",
    "title": "Diss",
    "section": "Option 7: scale",
    "text": "Option 7: scale\nWhen you have a lot of subjects/chapters in your menubar, they might not all fit in a row. You can then tweak the scale in the options. Simplemenu copies the Reveal.js (slide) scaling and adds a scale option on top of that.\nformat:\n  revealjs:\n    ...\n    simplemenu:\n      scale: 0.67\nIt is set to be two-thirds of the main scaling."
  },
  {
    "objectID": "slides/random-numbers.html",
    "href": "slides/random-numbers.html",
    "title": "Generating random numbers",
    "section": "",
    "text": "In your final project, you will generate a synthetic dataset and use it to conduct an evaluation of some social program. Generating fake or simulated data is an incredibly powerful skill, but it takes some practice. Here are a bunch of helpful resources and code examples of how to use different R functions to generate random numbers that follow specific distributions (or probability shapes).\nThis example focuses primarily on distributions. Each of the columns you’ll generate will be completely independent from each other and there will be no correlation between them. The example for generating synthetic data provides code and a bunch of examples of how to build in correlations between columns.\nFirst, make sure you load the libraries we’ll use throughout the example:\nlibrary(tidyverse)\nlibrary(patchwork)",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#seeds",
    "href": "slides/random-numbers.html#seeds",
    "title": "Generating random numbers",
    "section": "1 Seeds",
    "text": "1 Seeds\nWhen R (or any computer program, really) generates random numbers, it uses an algorithm to simulate randomness. This algorithm always starts with an initial number, or seed. Typically it will use something like the current number of milliseconds since some date, so that every time you generate random numbers they’ll be different. Look at this, for instance:\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\n\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\n\nThey’re different both times.\nThat’s ordinarily totally fine, but if you care about reproducibility (like having a synthetic dataset with the same random values, or having jittered points in a plot be in the same position every time you knit), it’s a good idea to set your own seed. This ensures that the random numbers you generate are the same every time you generate them.\nDo this by feeding set.seed() some numbers. It doesn’t matter what number you use—it just has to be a whole number. People have all sorts of favorite seeds:\n\n1\n13\n42\n1234\n12345\n20201101 (i.e. the current date)\n8675309\n\nYou could even go to random.org and use atmospheric noise to generate a seed, and then use that in R.\nHere’s what happens when you generate random numbers after setting a seed:\n\n# Set a seed\nset.seed(1234)\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\n\n# Set a seed\nset.seed(1234)\n\n# Choose another 3 numbers between 1 and 10\nsample(1:10, 3)\n\nThey’re the same!\nOnce you set a seed, it influences any function that does anything random, but it doesn’t reset. For instance, if you set a seed once and then run sample() twice, you’ll get different numbers the second time, but you’ll get the same different numbers every time:\n\n# Set a seed\nset.seed(1234)\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\nsample(1:10, 3)  # This will be different!\n\n# Set a seed again\nset.seed(1234)\n\n# Choose 3 numbers between 1 and 10\nsample(1:10, 3)\nsample(1:10, 3)  # This will be different, but the same as before!\n\nTypically it’s easiest to just include set.seed(SOME_NUMBER) at the top of your script after you load all the libraries. Some functions have a seed argument, and it’s a good idea to use it: position_jitter(..., seed = 1234).",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#distributions",
    "href": "slides/random-numbers.html#distributions",
    "title": "Generating random numbers",
    "section": "2 Distributions",
    "text": "2 Distributions\nRemember in elementary school when you’d decide on playground turns by saying “Pick a number between 1 and 10” and whoever was the closest would win? When you generate random numbers in R, you’re essentially doing the same thing, only with some fancier bells and whistles.\nWhen you ask someone to choose a number between 1 and 10, any of those numbers should be equally likely. 1 isn’t really less common than 5 or anything. In some situations, though, there are numbers that are more likely to appear than others (i.e. when you roll two dice, it’s pretty rare to get a 2, but pretty common to get a 7). These different kinds of likelihood change the shape of the distribution of possible values. There are hundreds of different distributions, but for the sake of generating data, there are only a few that you need to know.\n\nUniform distribution\nIn a uniform distribution, every number is equally likely. This is the “pick a number between 1 and 10” scenario, or rolling a single die. There are a couple ways to work with a uniform distribution in R: (1) sample() and (2) runif().\n\nsample()\nThe sample() function chooses an element from a list.\nFor instance, let’s pretend we have six possible numbers (like a die, or like 6 categories on a survey), like this:\n\npossible_answers &lt;- c(1, 2, 3, 4, 5, 6)  # We could also write this as 1:6 instead\n\nIf we want to randomly choose from this list, you’d use sample(). The size argument defines how many numbers to choose.\n\n# Choose 1 random number\nsample(possible_answers, size = 1)\n\n# Choose 3 random numbers\nsample(possible_answers, size = 3)\n\nOne important argument you can use is replace, which essentially puts the number back into the pool of possible numbers. Imagine having a bowl full of ping pong balls with the numbers 1–6 on them. If you take the number “3” out, you can’t draw it again. If you put it back in, you can pull it out again. The replace argument puts the number back after it’s drawn:\n\n# Choose 10 random numbers, with replacement\nsample(possible_answers, size = 10, replace = TRUE)\n\nIf you don’t specify replace = TRUE, and you try to choose more numbers than are in the set, you’ll get an error:\n\n# Choose 8 numbers between 1 and 6, but don't replace them.\n# This won't work!\nsample(possible_answers, size = 8)\n\nIt’s hard to see patterns in the outcomes when generating just a handful of numbers, but easier when you do a lot. Let’s roll a die 1,000 times:\n\nset.seed(1234)\ndie &lt;- tibble(value = sample(possible_answers,\n                             size = 1000,\n                             replace = TRUE))\ndie %&gt;%\n  count(value)\n\nggplot(die, aes(x = value)) +\n  geom_bar() +\n  labs(title = \"1,000 rolls of a single die\")\n\nIn this case, 3 and 6 came up more often than the others, but that’s just because of randomness. If we rolled the die 100,000 times, the bars should basically be the same:\n\nset.seed(1234)\ndie &lt;- tibble(value = sample(possible_answers,\n                             size = 100000,\n                             replace = TRUE))\n\nggplot(die, aes(x = value)) +\n  geom_bar() +\n  labs(title = \"100,000 rolls of a single die\")\n\n\n\nrunif()\nAnother way to generate uniformly distributed numbers is to use the runif() function (which is short for “random uniform”, and which took me years to realize, and for years I wondered why people used a function named “run if” when there’s no if statement anywhere??)\nrunif() will choose numbers between a minimum and a maximum. These numbers will not be whole numbers. By default, the min and max are 0 and 1:\n\nrunif(5)\n\nHere are 5 numbers between 35 and 56:\n\nrunif(5, min = 35, max = 56)\n\nSince these aren’t whole numbers, you can round them to make them look more realistic (like, if you were generating a column for age, you probably don’t want people who are 21.5800283 years old):\n\n# Generate 5 people between the ages of 18 and 35\nround(runif(5, min = 18, max = 35), 0)\n\nYou can confirm that each number has equal probability if you make a histogram. Here are 5,000 random people between 18 and 35:\n\nset.seed(1234)\nlots_of_numbers &lt;- tibble(x = runif(5000, min = 18, max = 35))\n\nggplot(lots_of_numbers, aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 18)\n\n\n\n\nNormal distribution\nThe whole “choose a number between 1 and 10” idea of a uniform distribution is neat and conceptually makes sense, but most numbers that exist in the world tend to have higher probabilities around certain values—almost like gravity around a specific point. For instance, income in the United States is not uniformly distributed—a handful of people are really really rich, lots are very poor, and most are kind of clustered around an average.\nThe idea of having possible values clustered around an average is how the rest of these distributions work (uniform distributions don’t have any sort of central gravity point; all these others do). Each distribution is defined by different things called parameters, or values that determine the shape of the probabilities and locations of the clusters.\nA super common type of distribution is the normal distribution. This is the famous “bell curve” you learn about in earlier statistics classes. A normal distribution has two parameters:\n\nA mean (the center of the cluster)\nA standard deviation (how much spread there is around the mean).\n\nIn R, you can generate random numbers from a normal distribution with the rnorm() function. It takes three arguments: the number of numbers you want to generate, the mean, and the standard deviation. It defaults to a mean of 0 and a standard deviation of 1, which means most numbers will cluster around 0, with a lot between −1 and 1, and some going up to −2 and 2 (technically 67% of numbers will be between −1 and 1, while 95% of numbers will be between −2–2ish)\n\nrnorm(5)\n\n# Cluster around 10, with an SD of 4\nrnorm(5, mean = 10, sd = 4)\n\nWhen working with uniform distributions, it’s easy to know how high or low your random values might go, since you specify a minimum and maximum number. With a normal distribution, you don’t specify starting and ending points—you specify a middle and a spread, so it’s harder to guess the whole range. Plotting random values is thus essential. Here’s 1,000 random numbers clustered around 10 with a standard deviation of 4:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(x = rnorm(1000, mean = 10, sd = 4))\nhead(plot_data)\n\nggplot(plot_data, aes(x = x)) +\n  geom_histogram(binwidth = 1, boundary = 0, color = \"white\")\n\nNeat. Most numbers are around 10; lots are between 5 and 15; some go as high as 25 and as low as −5.\nWatch what happens if you change the standard deviation to 10 to make the spread wider:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(x = rnorm(1000, mean = 10, sd = 10))\nhead(plot_data)\n\nggplot(plot_data, aes(x = x)) +\n  geom_histogram(binwidth = 1, boundary = 0, color = \"white\")\n\nIt’s still centered around 10, but now you get values as high as 40 and as low as −20. The data is more spread out now.\nWhen simulating data, you’ll most often use a normal distribution just because it’s easy and lots of things follow that pattern in the real world. Incomes, ages, education, etc. all have a kind of gravity to them, and a normal distribution is a good way of showing that gravity. For instance, here are 1,000 simulated people with reasonable random incomes, ages, and years of education:\n\nset.seed(1234)\n\nfake_people &lt;- tibble(income = rnorm(1000, mean = 40000, sd = 15000),\n                      age = rnorm(1000, mean = 25, sd = 8),\n                      education = rnorm(1000, mean = 16, sd = 4))\nhead(fake_people)\n\nfake_income &lt;- ggplot(fake_people, aes(x = income)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0) +\n  labs(title = \"Simulated income\")\n\nfake_age &lt;- ggplot(fake_people, aes(x = age)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0) +\n  labs(title = \"Simulated age\")\n\nfake_education &lt;- ggplot(fake_people, aes(x = education)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0) +\n  labs(title = \"Simulated education\")\n\nfake_income + fake_age + fake_education\n\nThese three columns all have different centers and spreads. Income is centered around $45,000, going up to almost $100,000 and as low as −$10,000; age is centered around 25, going as low as 0 and as high as 50; education is centered around 16, going as low as 3 and as high as 28. Cool.\nAgain, when generating these numbers, it’s really hard to know how high or low these ranges will be, so it’s a good idea to plot them constantly. I settled on sd = 4 for education only because I tried things like 1 and 10 and got wild looking values (everyone basically at 16 with little variation, or everyone ranging from −20 to 50, which makes no sense when thinking about years of education). Really it’s just a process of trial and error until the data looks good and reasonable.\n\n\nTruncated normal distribution\nSometimes you’ll end up with negative numbers that make no sense. Look at income in the plot above, for instance. Some people are earning −$10,000 year. The rest of the distribution looks okay, but those negative values are annoying.\nTo fix this, you can use something called a truncated normal distribution, which lets you specify a mean and standard deviation, just like a regular normal distribution, but also lets you specify a minimum and/or maximum so you don’t get values that go too high or too low.\nR doesn’t have a truncated normal function built-in, but you can install the truncnorm package and use the rtruncnorm() function. A truncated normal distribution has four parameters:\n\nA mean (mean)\nA standard deviation (sd)\nA minimum (optional) (a)\nA maximum (optional) (b)\n\nFor instance, let’s pretend you have a youth program designed to target people who are between 12 and 21 years old, with most around 14. You can generate numbers with a mean of 14 and a standard deviation of 5, but you’ll create people who are too old, too young, or even negatively aged!\n\nset.seed(1234)\n\nplot_data &lt;- tibble(fake_age = rnorm(1000, mean = 14, sd = 5))\nhead(plot_data)\n\nggplot(plot_data, aes(x = fake_age)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0)\n\nTo fix this, truncate the range at 12 and 21:\n\nlibrary(truncnorm)  # For rtruncnorm()\n\nset.seed(1234)\n\nplot_data &lt;- tibble(fake_age = rtruncnorm(1000, mean = 14, sd = 5, a = 12, b = 21))\nhead(plot_data)\n\nggplot(plot_data, aes(x = fake_age)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 0)\n\nAnd voila! A bunch of people between 12 and 21, with most around 14, with no invalid values.\n\n\nBeta distribution\nNormal distributions are neat, but they’re symmetrical around the mean (unless you truncate them). What if your program involves a test with a maximum of 100 points where most people score around 85, but a sizable portion score below that. In other words, it’s not centered at 85, but is skewed left.\nTo simulate this kind of distribution, we can use a Beta distribution. Beta distributions are neat because they naturally only range between 0 and 1—they’re perfect for things like percentages or proportions or or 100-based exams.\nUnlike a normal distribution, where you use the mean and standard deviation as parameters, Beta distributions take two non-intuitive parameters:\n\nshape1\nshape2\n\nWhat the heck are these shapes though?! This answer at Cross Validated does an excellent job of explaining the intuition behind Beta distributions and it’d be worth it to read it.\nBasically, Beta distributions are good at modeling probabilities of things, and shape1 and shape2 represent specific parts of a probability formula.\nLet’s say that there’s an exam with 10 points where most people score a 6/10. Another way to think about this is that an exam is a collection of correct answers and incorrect answers, and that the percent correct follows this equation:\n\\[\n\\frac{\\text{Number correct}}{\\text{Number correct} + \\text{Number incorrect}}\n\\]\nIf you scored a 6, you could write that as:\n\\[\n\\frac{6}{6 + 4}\n\\]\nTo make it more general, we can use Greek variable names: \\(\\alpha\\) for the number correct and \\(\\beta\\) for the number incorrect, leaving us with this:\n\\[\n\\frac{\\alpha}{\\alpha + \\beta}\n\\]\nNeat.\nIn a Beta distribution, the \\(\\alpha\\) and \\(\\beta\\) in that equation correspond to shape1 and shape2. If we want to generate random scores for this test where most people get 6/10, we can use rbeta():\n\nset.seed(1234)\n\nplot_data &lt;- tibble(exam_score = rbeta(1000, shape1 = 6, shape2 = 4)) %&gt;%\n  # rbeta() generates numbers between 0 and 1, so multiply everything by 10 to\n  # scale up the exam scores\n  mutate(exam_score = exam_score * 10)\n\nggplot(plot_data, aes(x = exam_score)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 0:10)\n\nMost people score around 6, with a bunch at 5 and 7, and fewer in the tails. Importantly, it’s not centered at 6—the distribution is asymmetric.\nThe magic of—and most confusing part about—Beta distributions is that you can get all sorts of curves by just changing the shape parameters. To make this easier to see, we can make a bunch of different Beta distributions. Instead of plotting them with histograms, we’ll use density plots (and instead of generating random numbers, we’ll plot the actual full range of the distribution (that’s what dbeta and geom_function() do in all these examples)).\nHere’s what we saw before, with \\(\\alpha\\) (shape1) = 6 and \\(\\beta\\) (shape2) = 4:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 6, shape2 = 4))\n\nAgain, there’s a peak at 0.6 (or 6), which is what we expected.\nWe can make the distribution narrower if we scale the shapes up. Here pretty much everyone scores around 50% and 75%.\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 60, shape2 = 40))\n\nSo far all these curves look like normal distributions, just slightly skewed. But when if most people score 90–100%? Or most fail? A Beta distribution can handle that too:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 9, shape2 = 1), color = \"blue\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 1, shape2 = 9), color = \"red\")\n\nWith shape1 = 9 and shape2 = 1 (or \\(\\frac{9}{9 + 1}\\)) we get most around 90%, while shape1 = 1 and shape2 = 9 (or \\(\\frac{1}{1 + 9}\\)) gets us most around 10%.\nCheck out all these other shapes too:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 5, shape2 = 5), color = \"blue\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 2, shape2 = 5), color = \"red\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 80, shape2 = 23), color = \"orange\") +\n  geom_function(fun = ~dbeta(.x, shape1 = 13, shape2 = 17), color = \"brown\")\n\nIn real life, if I don’t want to figure out the math behind the \\(\\frac{\\alpha}{\\alpha + \\beta}\\) shape values, I end up just choosing different numbers until it looks like the shape I want, and then I use rbeta() with those parameter values. Like, how about we generate some numbers based on the red line above, with shape1 = 2 and shape2 = 5, which looks like it should be centered around 0.2ish (\\(\\frac{2}{2 + 5} = 0.2857\\)):\n\nset.seed(1234)\n\nplot_data &lt;- tibble(thing = rbeta(1000, shape1 = 2, shape2 = 5)) %&gt;%\n  mutate(thing = thing * 100)\nhead(plot_data)\n\nggplot(plot_data, aes(x = thing)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0)\n\nIt worked! Most values are around 20ish, but some go up to 60–80.\n\n\nBinomial distribution\nOften you’ll want to generate a column that only has two values: yes/no, treated/untreated, before/after, big/small, red/blue, etc. You’ll also likely want to control the proportions (25% treated, 62% blue, etc.). You can do this in two different ways: (1) sample() and (2) rbinom().\n\nsample()\nWe already saw sample() when we talked about uniform distributions. To generate a binary variable with sample(), just feed it a list of two possible values:\n\nset.seed(1234)\n\n# Choose 5 random T/F values\npossible_things &lt;- c(TRUE, FALSE)\nsample(possible_things, 5, replace = TRUE)\n\nR will choose these values with equal/uniform probability by default, but you can change that in sample() with the prob argument. For instance, pretend you want to simulate an election. According to the latest polls, one candidate has an 80% chance of winning. You want to randomly choose a winner based on that chance. Here’s how to do that with sample():\n\nset.seed(1234)\ncandidates &lt;- c(\"Person 1\", \"Person 2\")\nsample(candidates, size = 1, prob = c(0.8, 0.2))\n\nPerson 1 wins!\nIt’s hard to see the weighted probabilities when you just choose one, so let’s pretend there are 1,000 elections:\n\nset.seed(1234)\nfake_elections &lt;- tibble(winner = sample(candidates,\n                                         size = 1000,\n                                         prob = c(0.8, 0.2),\n                                         replace = TRUE))\nfake_elections %&gt;%\n  count(winner)\n\nggplot(fake_elections, aes(x = winner)) +\n  geom_bar()\n\nPerson 1 won 792 of the elections. Neat.\n(This is essentially what election forecasting websites like FiveThirtyEight do! They just do it with way more sophisticated simulations.)\n\n\nrbinom()\nInstead of using sample(), you can use a formal distribution called the binomial distribution. This distribution is often used for things that might have “trials” or binary outcomes that are like success/failure or yes/no or true/false\nThe binomial distribution takes two parameters:\n\nsize: The number of “trials”, or times that an event happens\nprob: The probability of success in each trial\n\nIt’s easiest to see some examples of this. Let’s say you have a program that has a 60% success rate and it is tried on groups of 20 people 5 times. The parameters are thus size = 20 (since there are twenty people per group) and prob = 0.6 (since there is a 60% chance of success):\n\nset.seed(1234)\n\nrbinom(5, size = 20, prob = 0.6)\n\nThe results here mean that in group 1, 15/20 (75%) people had success, in group 2, 11/20 (55%) people had success, and so on. Not every group will have exactly 60%, but they’re all kind of clustered around that.\nHOWEVER, I don’t like using rbinom() like this, since this is all group-based, and when you’re generating fake people you generally want to use individuals, or groups of 1. So instead, I assume that size = 1, which means that each “group” is only one person large. This forces the generated numbers to either be 0 or 1:\n\nset.seed(1234)\n\nrbinom(5, size = 1, prob = 0.6)\n\nHere, only 1 of the 5 people were 1/TRUE/yes, which is hardly close to a 60% chance overall, but that’s because we only generated 5 numbers. If we generate lots, we can see the probability of yes emerge:\n\nset.seed(12345)\n\nplot_data &lt;- tibble(thing = rbinom(2000, 1, prob = 0.6)) %&gt;%\n  # Make this a factor since it's basically a yes/no categorical variable\n  mutate(thing = factor(thing))\n\nplot_data %&gt;%\n  count(thing) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(plot_data, aes(x = thing)) +\n  geom_bar()\n\n58% of the 2,000 fake people here were 1/TRUE/yes, which is close to the goal of 60%. Perfect.\n\n\n\nPoisson distribution\nOne last common distribution that you might find helpful when simulating data is the Poisson distribution (in French, “poisson” = fish, but here it’s not actually named after the animal, but after French mathematician Siméon Denis Poisson).\nA Poisson distribution is special because it generates whole numbers (i.e. nothing like 1.432) that follow a skewed pattern (i.e. more smaller values than larger values). There’s all sorts of fancy math behind it that you don’t need to worry about so much—all you need to know is that it’s good at modeling things called Poisson processes.\nFor instance, let’s say you’re sitting at the front door of a coffee shop (in pre-COVID days) and you count how many people are in each arriving group. You’ll see something like this:\n\n1 person\n1 person\n2 people\n1 person\n3 people\n2 people\n1 person\n\nLots of groups of one, some groups of two, fewer groups of three, and so on. That’s a Poisson process: a bunch of independent random events that combine into grouped events.\nThat sounds weird and esoteric (and it is!), but it reflects lots of real world phenomena, and things you’ll potentially want to measure in a program. For instance, the number of kids a family has follows a type of Poisson process. Lots of families have 1, some have 2, fewer have 3, even fewer have 4, and so on. The number of cars in traffic, the number of phone calls received by an office, arrival times in a line, and even the outbreak of wars are all examples of Poisson processes.\nYou can generate numbers from a Poisson distribution with the rpois() function in R. This distribution only takes a single parameter:\n\nlambda (\\(\\lambda\\))\n\nThe \\(\\lambda\\) value controls the rate or speed that a Poisson process increases (i.e. jumps from 1 to 2, from 2 to 3, from 3 to 4, etc.). I have absolutely zero mathematical intuition for how it works. The two shape parameters for a Beta distribution at least fit in a fraction and you can wrap your head around that, but the lambda in a Poisson distribution is just a mystery to me. So whenever I use a Poisson distribution for something, I just play with the lambda until the data looks reasonable.\nLet’s assume that the number of kids a family has follows a Poisson process. Here’s how we can use rpois() to generate that data:\n\nset.seed(123)\n\n# 10 different families\nrpois(10, lambda = 1)\n\nCool. Most families have 0–1 kids; some have 2; one has 3.\nIt’s easier to see these patterns with a plot:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(num_kids = rpois(500, lambda = 1))\nhead(plot_data)\n\nplot_data %&gt;%\n  group_by(num_kids) %&gt;%\n  summarize(count = n()) %&gt;%\n  mutate(proportion = count / sum(count))\n\nggplot(plot_data, aes(x = num_kids)) +\n  geom_bar()\n\nHere 75ish% of families have 0–1 kids (36% + 37.4%), 17% have 2 kids, 6% have 3, 2% have 4, and only 0.6% have 5.\nWe can play with the \\(\\lambda\\) to increase the rate of kids per family:\n\nset.seed(1234)\n\nplot_data &lt;- tibble(num_kids = rpois(500, lambda = 2))\nhead(plot_data)\n\nplot_data %&gt;%\n  group_by(num_kids) %&gt;%\n  summarize(count = n()) %&gt;%\n  mutate(proportion = count / sum(count))\n\nggplot(plot_data, aes(x = num_kids)) +\n  geom_bar()\n\nNow most families have 1–2 kids. Cool.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#rescaling-numbers",
    "href": "slides/random-numbers.html#rescaling-numbers",
    "title": "Generating random numbers",
    "section": "3 Rescaling numbers",
    "text": "3 Rescaling numbers\nAll these different distributions are good at generating general shapes:\n\nUniform: a bunch of random numbers with no central gravity\nNormal: an average ± some variation\nBeta: different shapes and skews and gravities between 0 and 1\nBinomial: yes/no outcomes that follow some probability\n\nThe shapes are great, but you also care about the values of these numbers. This can be tricky. As we saw earlier with a normal distribution, sometimes you’ll get values that go below zero or above some value you care about. We fixed that with a truncated normal distribution, but not all distributions have truncated versions. Additionally, if you’re using a Beta distribution, you’re stuck in a 0–1 scale (or 0–10 or 0–100 if you multiply the value by 10 or 100 or whatever).\nWhat if you want a fun skewed Beta shape for a variable like income or some other value that doesn’t fit within a 0–1 range? You can rescale any set of numbers after-the-fact using the rescale() function from the scales library and rescale things to whatever range you want.\nFor instance, let’s say that income isn’t normally distributed, but is right-skewed with a handful of rich people. This might look like a Beta distribution with shape1 = 2 and shape2 = 5:\n\nggplot() +\n  geom_function(fun = ~dbeta(.x, shape1 = 2, shape2 = 5))\n\nIf we generate random numbers from this distribution, they’ll all be stuck between 0 and 1:\n\nset.seed(1234)\n\nfake_people &lt;- tibble(income = rbeta(1000, shape1 = 2, shape2 = 5))\n\nggplot(fake_people, aes(x = income)) +\n  geom_histogram(binwidth = 0.1, color = \"white\", boundary = 0)\n\nWe can take those underling 0–1 values and rescale them to some other range using the rescale() function. We can specify the minimum and maximum values in the to argument. Here we’ll scale it up so that 0 = $10,000 and 1 = $100,000. Our rescaled version follows the same skewed Beta distribution shape, but now we’re using better values!\n\nlibrary(scales)\n\nfake_people_scaled &lt;- fake_people %&gt;%\n  mutate(income_scaled = rescale(income, to = c(10000, 100000)))\nhead(fake_people_scaled)\n\nggplot(fake_people_scaled, aes(x = income_scaled)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0)\n\nThis works for anything, really. For instance, instead of specifying a mean and standard deviation for a normal distribution and hoping that the generated values don’t go too high or too low, you can generate a normal distribution with a mean of 0 and standard deviation of 1 and then rescale it to the range you want:\n\nset.seed(1234)\n\nfake_data &lt;- tibble(age_not_scaled = rnorm(1000, mean = 0, sd = 1)) %&gt;%\n  mutate(age = rescale(age_not_scaled, to = c(18, 65)))\nhead(fake_data)\n\nplot_unscaled &lt;- ggplot(fake_data, aes(x = age_not_scaled)) +\n  geom_histogram(binwidth = 0.5, color = \"white\", boundary = 0)\n\nplot_scaled &lt;- ggplot(fake_data, aes(x = age)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0)\n\nplot_unscaled + plot_scaled\n\nThis gives you less control over the center of the distribution (here it happens to be 40 because that’s in the middle of 18 and 65), but it gives you more control over the edges of the distribution.\nRescaling things is really helpful when building in effects and interacting columns with other columns, since multiplying variables by different coefficients can make the values go way out of the normal range. You’ll see a lot more of that in the synthetic data example.",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#summary",
    "href": "slides/random-numbers.html#summary",
    "title": "Generating random numbers",
    "section": "4 Summary",
    "text": "4 Summary\nPhew. We covered a lot here, and we barely scratched the surface of all the distributions that exist. Here’s a helpful summary of the main distributions you should care about:",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/random-numbers.html#example",
    "href": "slides/random-numbers.html#example",
    "title": "Generating random numbers",
    "section": "5 Example",
    "text": "5 Example\nAnd here’s an example dataset of 1,000 fake people and different characteristics. One shortcoming of this fake data is that each of these columns is completely independent—there’s no relationship between age and education and family size and income. You can see how to make these columns correlated (and make one cause another!) in the example for synthetic data.\n\nset.seed(1234)\n\n# Set the number of people here once so it's easier to change later\nn_people &lt;- 1000\n\nexample_fake_people &lt;- tibble(\n  id = 1:n_people,\n  opinion = sample(1:5, n_people, replace = TRUE),\n  age = runif(n_people, min = 18, max = 80),\n  income = rnorm(n_people, mean = 50000, sd = 10000),\n  education = rtruncnorm(n_people, mean = 16, sd = 6, a = 8, b = 24),\n  happiness = rbeta(n_people, shape1 = 2, shape2 = 1),\n  treatment = sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.3, 0.7)),\n  size = rbinom(n_people, size = 1, prob = 0.5),\n  family_size = rpois(n_people, lambda = 1) + 1  # Add one so there are no 0s\n) %&gt;%\n  # Adjust some of these columns\n  mutate(opinion = recode(opinion, \"1\" = \"Strongly disagree\",\n                          \"2\" = \"Disagree\", \"3\" = \"Neutral\",\n                          \"4\" = \"Agree\", \"5\" = \"Strongly agree\")) %&gt;%\n  mutate(size = recode(size, \"0\" = \"Small\", \"1\" = \"Large\")) %&gt;%\n  mutate(happiness = rescale(happiness, to = c(1, 8)))\n\nhead(example_fake_people)\n\n\nplot_opinion &lt;- ggplot(example_fake_people, aes(x = opinion)) +\n  geom_bar() +\n  guides(fill = \"none\") +\n  labs(title = \"Opinion (uniform with sample())\")\n\nplot_age &lt;- ggplot(example_fake_people, aes(x = age)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0) +\n  labs(title = \"Age (uniform with runif())\")\n\nplot_income &lt;- ggplot(example_fake_people, aes(x = income)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0) +\n  labs(title = \"Income (normal)\")\n\nplot_education &lt;- ggplot(example_fake_people, aes(x = education)) +\n  geom_histogram(binwidth = 2, color = \"white\", boundary = 0) +\n  labs(title = \"Education (truncated normal)\")\n\nplot_happiness &lt;- ggplot(example_fake_people, aes(x = happiness)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 1:8) +\n  labs(title = \"Happiness (Beta, rescaled to 1-8)\")\n\nplot_treatment &lt;- ggplot(example_fake_people, aes(x = treatment)) +\n  geom_bar() +\n  labs(title = \"Treatment (binary with sample())\")\n\nplot_size &lt;- ggplot(example_fake_people, aes(x = size)) +\n  geom_bar() +\n  labs(title = \"Size (binary with rbinom())\")\n\nplot_family &lt;- ggplot(example_fake_people, aes(x = family_size)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 1:7) +\n  labs(title = \"Family size (Poisson)\")\n\n(plot_opinion + plot_age) / (plot_income + plot_education)\n\n\n(plot_happiness + plot_treatment) / (plot_size + plot_family)",
    "crumbs": [
      "Presentations",
      "2021",
      "Pres 1"
    ]
  },
  {
    "objectID": "slides/slides_2022.html",
    "href": "slides/slides_2022.html",
    "title": "2022 - February",
    "section": "",
    "text": "First presentation",
    "crumbs": [
      "Presentations",
      "2022",
      "February"
    ]
  },
  {
    "objectID": "slides/slides_2023_LHT.html",
    "href": "slides/slides_2023_LHT.html",
    "title": "2023",
    "section": "",
    "text": "LHT - Data Scientist"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#how-failing-to-adapt-led-to-bankruptcy",
    "href": "revealjs/slides/2023/beyond/evomap.html#how-failing-to-adapt-led-to-bankruptcy",
    "title": "Beyond the beginning",
    "section": "How Failing to Adapt Led to Bankruptcy",
    "text": "How Failing to Adapt Led to Bankruptcy"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#creative-destruction-out-with-the-old-in-with-the-new",
    "href": "revealjs/slides/2023/beyond/evomap.html#creative-destruction-out-with-the-old-in-with-the-new",
    "title": "Beyond the beginning",
    "section": "Creative Destruction: Out With the Old, in With the New",
    "text": "Creative Destruction: Out With the Old, in With the New\n\n\n\n\n\n\nSchumpeter characterized creative destruction as innovations in the manufacturing process that increase productivity, describing it as the …\n\n\n… «process of industrial mutation that incessantly revolutionizes the economic structure from within, incessantly destroying the old one, incessantly creating a new one.»\n\n\n\n\n— Joseph Schumpeter (1942)"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#nflx-example",
    "href": "revealjs/slides/2023/beyond/evomap.html#nflx-example",
    "title": "Beyond the beginning",
    "section": "NFLX Example",
    "text": "NFLX Example"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#filter-13-embeddings",
    "href": "revealjs/slides/2023/beyond/evomap.html#filter-13-embeddings",
    "title": "Beyond the beginning",
    "section": "Filter 1/3: Embeddings",
    "text": "Filter 1/3: Embeddings"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#filter-23-clustering",
    "href": "revealjs/slides/2023/beyond/evomap.html#filter-23-clustering",
    "title": "Beyond the beginning",
    "section": "Filter 2/3: Clustering",
    "text": "Filter 2/3: Clustering\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster Crunchbase Description Embeddings (k=300)\nUse corresponding medoids/centroids as ultimate truth anchors\nFind appropriate similarity threshold as cutoff for website embeddings"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#filter-33-pooling",
    "href": "revealjs/slides/2023/beyond/evomap.html#filter-33-pooling",
    "title": "Beyond the beginning",
    "section": "Filter 3/3: Pooling",
    "text": "Filter 3/3: Pooling"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#nd-goal-time-indexed-sequence-of-pairwise-similarity-measures",
    "href": "revealjs/slides/2023/beyond/evomap.html#nd-goal-time-indexed-sequence-of-pairwise-similarity-measures",
    "title": "Beyond the beginning",
    "section": "2nd Goal: Time-indexed sequence of pairwise similarity measures",
    "text": "2nd Goal: Time-indexed sequence of pairwise similarity measures"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#result-1",
    "href": "revealjs/slides/2023/beyond/evomap.html#result-1",
    "title": "Beyond the beginning",
    "section": "Result",
    "text": "Result\n \n\n\n\n\n\n\nSequence of Maps\n\n\n\n\n\n\n\nDynamic Map"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#outlook-1",
    "href": "revealjs/slides/2023/beyond/evomap.html#outlook-1",
    "title": "Beyond the beginning",
    "section": "Outlook 1",
    "text": "Outlook 1"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#outlook-2",
    "href": "revealjs/slides/2023/beyond/evomap.html#outlook-2",
    "title": "Beyond the beginning",
    "section": "Outlook 2",
    "text": "Outlook 2"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#b1-the-evolutionary-path",
    "href": "revealjs/slides/2023/beyond/evomap.html#b1-the-evolutionary-path",
    "title": "Beyond the beginning",
    "section": "B1: The Evolutionary Path",
    "text": "B1: The Evolutionary Path"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#b2-websites-perceived-value-proposition",
    "href": "revealjs/slides/2023/beyond/evomap.html#b2-websites-perceived-value-proposition",
    "title": "Beyond the beginning",
    "section": "B2: Websites: Perceived value proposition",
    "text": "B2: Websites: Perceived value proposition\n \n\n\n\n\n\nValue propositions are derived from the descriptions that firms provide to the market through their websites\nThis contrasts with existing industry classification schemes, which often rely on external assessments\nBy this, we capture a more dynamic and internally generated perspective of their industry affiliations, potentially offering a richer understanding of their positioning and evolution over time"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#nounphrases-extracting-the-essence-text",
    "href": "revealjs/slides/2023/beyond/evomap.html#nounphrases-extracting-the-essence-text",
    "title": "Beyond the beginning",
    "section": "Nounphrases: Extracting the essence text",
    "text": "Nounphrases: Extracting the essence text\n \n\n\n\nUnlike simplistic keywords, nounph rases transcend single words, comprising compound expressions that encapsulate the essence of the text more comprehensively\nTools: SpaCy (nounchunk library) & Own Algo\nEmbeddings are generated by using openai text-embedding-3-large"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#start-up-roadmap-the-myth-of-the-linear-path",
    "href": "revealjs/slides/2023/beyond/evomap.html#start-up-roadmap-the-myth-of-the-linear-path",
    "title": "Beyond the beginning",
    "section": "Start-up Roadmap: The Myth of the Linear Path",
    "text": "Start-up Roadmap: The Myth of the Linear Path"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#st-goal-keyphrase-extraction",
    "href": "revealjs/slides/2023/beyond/evomap.html#st-goal-keyphrase-extraction",
    "title": "Beyond the beginning",
    "section": "1st Goal: Keyphrase Extraction",
    "text": "1st Goal: Keyphrase Extraction"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#nflx-shifts",
    "href": "revealjs/slides/2023/beyond/evomap.html#nflx-shifts",
    "title": "Beyond the beginning",
    "section": "NFLX shifts",
    "text": "NFLX shifts"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#business-model-lifecycle",
    "href": "revealjs/slides/2023/beyond/evomap.html#business-model-lifecycle",
    "title": "Beyond the beginning",
    "section": "Business Model Lifecycle",
    "text": "Business Model Lifecycle"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#constructive-destruction-what-has-to-be-changed",
    "href": "revealjs/slides/2023/beyond/evomap.html#constructive-destruction-what-has-to-be-changed",
    "title": "Beyond the beginning",
    "section": "Constructive destruction: What has to be changed?",
    "text": "Constructive destruction: What has to be changed?\n\n\n\n\n\n\n\n[…] it is not that kind of competition [price, ed] which counts but the competition from the new commodity, the new technology, the new source of supply, the new type of organization – competition which commands a decisive cost or quality advantage and which strikes not at the margins of the profits and the outputs of the existing firms but at their foundations and their very lives.»\n\n\n\n\n— Joseph Schumpeter (1942)\n\n\n\n\n\n\n\nBusiness Model Canvas\n\n\n\n\n\n\n\n\n\n\n\nValue Proposition Canvas"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#objective-evolution-scenario-maps",
    "href": "revealjs/slides/2023/beyond/evomap.html#objective-evolution-scenario-maps",
    "title": "Beyond the beginning",
    "section": "Objective: Evolution Scenario Maps",
    "text": "Objective: Evolution Scenario Maps\n\n\n\nMarket structure map: Spatial representation of firms’ competitive positions relative to one another based on some measure of their competitive relationships (DeSarbo et al. 1993).\n\n\n\nSuch maps typically capture static snapshots in time. Yet, competitive positions tend to change.\n\n\n\nFirms’ trajectories: Evolutionary paths of firms’ positions over time relative to all other firms in a market based on their value proposition"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#value-propositions-where-to-find-them",
    "href": "revealjs/slides/2023/beyond/evomap.html#value-propositions-where-to-find-them",
    "title": "Beyond the beginning",
    "section": "Value Propositions: Where to find them?",
    "text": "Value Propositions: Where to find them?\n \n \n\n\n\n\n\nApproach\n\n\n\nStatic: Venture Capital Databases (Crunchbase, Pitchbook, … )\n\n\n\n\n\n\n\nDynamic: Natural language processing and historical websites (WayBack Machine)"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#evolution-of-netflixs-business-models",
    "href": "revealjs/slides/2023/beyond/evomap.html#evolution-of-netflixs-business-models",
    "title": "Beyond the beginning",
    "section": "Evolution of Netflix’s Business Models",
    "text": "Evolution of Netflix’s Business Models"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#incessant-value-creation-through-the-evolution-of-business-models",
    "href": "revealjs/slides/2023/beyond/evomap.html#incessant-value-creation-through-the-evolution-of-business-models",
    "title": "Beyond the beginning",
    "section": "Incessant Value Creation through the Evolution of Business Models",
    "text": "Incessant Value Creation through the Evolution of Business Models"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#how-they-faced-a-digital-transformation-changes-in-netflixs-business-model-over-time",
    "href": "revealjs/slides/2023/beyond/evomap.html#how-they-faced-a-digital-transformation-changes-in-netflixs-business-model-over-time",
    "title": "Beyond the beginning",
    "section": "How they faced a Digital Transformation: Changes in Netflix’s business model over time",
    "text": "How they faced a Digital Transformation: Changes in Netflix’s business model over time"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-13-embeddings",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-13-embeddings",
    "title": "Beyond the beginning",
    "section": "Handling Noisy Data 1/3: Embeddings",
    "text": "Handling Noisy Data 1/3: Embeddings"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-23-clustering",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-23-clustering",
    "title": "Beyond the beginning",
    "section": "Handling Noisy Data 2/3: Clustering",
    "text": "Handling Noisy Data 2/3: Clustering\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster Crunchbase Description Embeddings (k=300)\n\n\n\n\nUse corresponding medoids/centroids as ultimate truth anchors\n\n\n\n\nFind appropriate similarity threshold as cutoff for website embeddings"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-33-pooling",
    "href": "revealjs/slides/2023/beyond/evomap.html#handling-noisy-data-33-pooling",
    "title": "Beyond the beginning",
    "section": "Handling Noisy Data 3/3: Pooling",
    "text": "Handling Noisy Data 3/3: Pooling"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#time-indexed-sequence-of-pairwise-similarity-measures",
    "href": "revealjs/slides/2023/beyond/evomap.html#time-indexed-sequence-of-pairwise-similarity-measures",
    "title": "Beyond the beginning",
    "section": "Time-indexed sequence of pairwise similarity measures",
    "text": "Time-indexed sequence of pairwise similarity measures"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#proof-of-concept",
    "href": "revealjs/slides/2023/beyond/evomap.html#proof-of-concept",
    "title": "Beyond the beginning",
    "section": "Proof of Concept",
    "text": "Proof of Concept\n \n\n\n\n\n\n\nSequence of Maps\n\n\n\n\n\n\n\nDynamic Map"
  },
  {
    "objectID": "revealjs/slides/2023/beyond/evomap.html#value-creation-through-the-evolution-of-business-models",
    "href": "revealjs/slides/2023/beyond/evomap.html#value-creation-through-the-evolution-of-business-models",
    "title": "Beyond the beginning",
    "section": "Value Creation through the Evolution of Business Models",
    "text": "Value Creation through the Evolution of Business Models"
  }
]